# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======






Params Turning Set
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 1)
('lr', 0.0003)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))






args.base_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/
args.graph_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/
args.min_dfscode_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/
args.min_dfscode_tensor_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/
*** 1 train_index:  [  0   2   3   6   9  10  11  12  13  14  15  16  17  19  20  21  22  23
  24  26  27  30  31  32  33  35  36  38  39  41  42  43  44  45  46  47
  48  49  50  52  53  54  56  57  59  60  61  62  63  64  65  66  67  69
  70  71  72  73  74  76  77  78  79  80  81  82  83  84  87  88  89  91
  92  94  95  96  97  98 100 101 102 103 104 106 107 108 109 110 112 113
 115 116 117 118 119 120 121 122 124 125 126 127 129 130 131 132 134 136
 138 139 140 141 142 144 145 146 147 148 149 151 152 153 154 155 156 157
 158 159 160 161 164 165 166 167 169 170 171 172 173 175 176 177 178 179
 180 182 184 185 186 187]
*** 2 test_index:  [  1   4   5   7   8  18  25  28  29  34  37  40  51  55  58  68  75  85
  86  90  93  99 105 111 114 123 128 133 135 137 143 150 162 163 168 174
 181 183]
*** 1 train_index:  [  0   1   2   4   5   6   7   8  10  11  12  13  14  15  16  17  18  19
  20  21  22  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39
  40  41  43  44  45  46  49  50  51  52  53  54  55  56  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  74  75  76  77  79  80  81  82
  83  84  85  86  88  90  91  92  93  94  96  97  98  99 101 102 103 104
 105 106 107 111 112 113 114 115 120 121 122 123 124 125 126 128 129 130
 131 133 134 135 136 137 138 139 140 143 144 145 147 148 150 153 154 155
 156 159 162 163 164 165 167 168 169 170 171 172 173 174 175 177 178 179
 180 181 182 183 186 187]
*** 2 test_index:  [  3   9  23  31  42  47  48  57  72  73  78  87  89  95 100 108 109 110
 116 117 118 119 127 132 141 142 146 149 151 152 157 158 160 161 166 176
 184 185]
*** 1 train_index:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19
  20  23  24  25  26  28  29  31  32  34  35  36  37  38  40  41  42  43
  44  47  48  51  53  54  55  56  57  58  59  60  64  66  68  70  71  72
  73  74  75  76  77  78  79  81  82  84  85  86  87  88  89  90  92  93
  95  96  97  98  99 100 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134
 135 137 139 140 141 142 143 144 146 148 149 150 151 152 153 156 157 158
 159 160 161 162 163 165 166 167 168 169 171 172 173 174 175 176 177 178
 179 181 183 184 185 186]
*** 2 test_index:  [  0  16  21  22  27  30  33  39  45  46  49  50  52  61  62  63  65  67
  69  80  83  91  94 101 102 103 122 136 138 145 147 154 155 164 170 180
 182 187]
*** 1 train_index:  [  0   1   3   4   5   6   7   8   9  13  14  16  18  19  21  22  23  25
  26  27  28  29  30  31  33  34  36  37  38  39  40  42  43  45  46  47
  48  49  50  51  52  53  55  56  57  58  60  61  62  63  65  66  67  68
  69  71  72  73  75  76  77  78  79  80  81  83  84  85  86  87  88  89
  90  91  92  93  94  95  97  98  99 100 101 102 103 105 106 108 109 110
 111 113 114 115 116 117 118 119 120 121 122 123 125 127 128 131 132 133
 134 135 136 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152
 154 155 156 157 158 160 161 162 163 164 165 166 168 170 171 174 175 176
 180 181 182 183 184 185 187]
*** 2 test_index:  [  2  10  11  12  15  17  20  24  32  35  41  44  54  59  64  70  74  82
  96 104 107 112 124 126 129 130 139 153 159 167 169 172 173 177 178 179
 186]
*** 1 train_index:  [  0   1   2   3   4   5   7   8   9  10  11  12  15  16  17  18  20  21
  22  23  24  25  27  28  29  30  31  32  33  34  35  37  39  40  41  42
  44  45  46  47  48  49  50  51  52  54  55  57  58  59  61  62  63  64
  65  67  68  69  70  72  73  74  75  78  80  82  83  85  86  87  89  90
  91  93  94  95  96  99 100 101 102 103 104 105 107 108 109 110 111 112
 114 116 117 118 119 122 123 124 126 127 128 129 130 132 133 135 136 137
 138 139 141 142 143 145 146 147 149 150 151 152 153 154 155 157 158 159
 160 161 162 163 164 166 167 168 169 170 172 173 174 176 177 178 179 180
 181 182 183 184 185 186 187]
*** 2 test_index:  [  6  13  14  19  26  36  38  43  53  56  60  66  71  76  77  79  81  84
  88  92  97  98 106 113 115 120 121 125 131 134 140 144 148 156 165 171
 175]
{'base_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/',
 'batch_size': 1,
 'clean_temp': False,
 'clean_tensorboard': False,
 'current_dataset_path': None,
 'current_min_dfscode_path': None,
 'current_model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-16-57-09/',
 'current_processed_dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'current_temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-16-57-09/',
 'dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/datasets/',
 'device': device(type='cuda', index=0),
 'dfscode_rnn_dropout': 0.2,
 'embedding_size_dfscode_rnn': 8,
 'epochs': 50,
 'epochs_end': 10000,
 'epochs_save': 20,
 'epochs_validate': 1,
 'fname': 'DFScodeRNN_cls_LSTM_MUTAG',
 'gradient_clipping': True,
 'graph_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/',
 'graph_type': 'MUTAG',
 'graphgen_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/',
 'hidden_size_dfscode_rnn': 4,
 'load_device': device(type='cuda', index=0),
 'load_model': False,
 'load_model_path': '',
 'log_tensorboard': False,
 'lr': 0.0003,
 'max_prev_node': None,
 'min_dfscode_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/',
 'min_dfscode_tensor_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/',
 'note': 'DFScodeRNN_cls_LSTM',
 'num_graphs': None,
 'num_layers': 1,
 'num_workers': 8,
 'number_of_mlp_layer': 1,
 'rnn_type': 'LSTM',
 'save_model': False,
 'temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/',
 'tensorboard_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tensorboard/',
 'time': '2021-01-17-16-57-09',
 'weights': False}


graphgen args.__dict__: None




dataset_features: {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}




config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 50, 'learning_rate': 0.0003, 'seed': 1800, 'k_fold': 5, 'model': 'DFScodeRNN_cls_LSTM', 'dataset': 'MUTAG'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}}


Training a new model: DFScodeRNN_cls_LSTM
Training model with dataset, testing using fold 0
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss 0.04611 acc 0.33333 roc_auc 0.56460 prc_auc 0.71898[0m
[93maverage test of epoch 0: loss -0.57300 acc 0.34211 roc_auc 0.56615 prc_auc 0.80979[0m
[92maverage training of epoch 1: loss -1.29434 acc 0.33333 roc_auc 0.58100 prc_auc 0.72623[0m
[93maverage test of epoch 1: loss -1.97355 acc 0.34211 roc_auc 0.51538 prc_auc 0.78606[0m
[92maverage training of epoch 2: loss -2.60812 acc 0.33333 roc_auc 0.58340 prc_auc 0.73448[0m
[93maverage test of epoch 2: loss -3.35559 acc 0.34211 roc_auc 0.52923 prc_auc 0.79976[0m
[92maverage training of epoch 3: loss -4.46388 acc 0.33333 roc_auc 0.58320 prc_auc 0.73276[0m
[93maverage test of epoch 3: loss -5.86514 acc 0.34211 roc_auc 0.69846 prc_auc 0.86127[0m
[92maverage training of epoch 4: loss -7.23217 acc 0.33333 roc_auc 0.58700 prc_auc 0.73576[0m
[93maverage test of epoch 4: loss -8.47449 acc 0.34211 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 5: loss -9.34193 acc 0.33333 roc_auc 0.57060 prc_auc 0.72924[0m
[93maverage test of epoch 5: loss -10.14929 acc 0.34211 roc_auc 0.84615 prc_auc 0.92798[0m
[92maverage training of epoch 6: loss -10.80974 acc 0.33333 roc_auc 0.46220 prc_auc 0.64305[0m
[93maverage test of epoch 6: loss -11.48053 acc 0.34211 roc_auc 0.82923 prc_auc 0.92431[0m
[92maverage training of epoch 7: loss -12.07600 acc 0.33333 roc_auc 0.43820 prc_auc 0.60773[0m
[93maverage test of epoch 7: loss -12.69975 acc 0.34211 roc_auc 0.79077 prc_auc 0.91221[0m
[92maverage training of epoch 8: loss -13.26850 acc 0.33333 roc_auc 0.41760 prc_auc 0.60060[0m
[93maverage test of epoch 8: loss -13.87338 acc 0.34211 roc_auc 0.36769 prc_auc 0.65794[0m
[92maverage training of epoch 9: loss -14.43164 acc 0.33333 roc_auc 0.40260 prc_auc 0.59591[0m
[93maverage test of epoch 9: loss -15.03055 acc 0.34211 roc_auc 0.16462 prc_auc 0.53016[0m
[92maverage training of epoch 10: loss -15.58697 acc 0.33333 roc_auc 0.38010 prc_auc 0.58769[0m
[93maverage test of epoch 10: loss -16.18709 acc 0.34211 roc_auc 0.08000 prc_auc 0.47622[0m
[92maverage training of epoch 11: loss -16.74705 acc 0.33333 roc_auc 0.38340 prc_auc 0.58955[0m
[93maverage test of epoch 11: loss -17.35278 acc 0.34211 roc_auc 0.10769 prc_auc 0.47678[0m
[92maverage training of epoch 12: loss -17.91969 acc 0.33333 roc_auc 0.37300 prc_auc 0.58484[0m
[93maverage test of epoch 12: loss -18.53377 acc 0.34211 roc_auc 0.16000 prc_auc 0.50656[0m
[92maverage training of epoch 13: loss -19.10995 acc 0.33333 roc_auc 0.37280 prc_auc 0.58403[0m
[93maverage test of epoch 13: loss -19.73415 acc 0.34211 roc_auc 0.12308 prc_auc 0.50388[0m
[92maverage training of epoch 14: loss -20.32121 acc 0.33333 roc_auc 0.37270 prc_auc 0.58177[0m
[93maverage test of epoch 14: loss -20.95675 acc 0.34211 roc_auc 0.12769 prc_auc 0.48835[0m
[92maverage training of epoch 15: loss -21.55587 acc 0.33333 roc_auc 0.36760 prc_auc 0.57900[0m
[93maverage test of epoch 15: loss -22.20356 acc 0.34211 roc_auc 0.15538 prc_auc 0.51758[0m
[92maverage training of epoch 16: loss -22.81562 acc 0.33333 roc_auc 0.36680 prc_auc 0.57763[0m
[93maverage test of epoch 16: loss -23.47602 acc 0.34211 roc_auc 0.12000 prc_auc 0.49338[0m
[92maverage training of epoch 17: loss -24.10072 acc 0.33333 roc_auc 0.36220 prc_auc 0.57445[0m
[93maverage test of epoch 17: loss -24.77218 acc 0.34211 roc_auc 0.23231 prc_auc 0.61312[0m
[92maverage training of epoch 18: loss -25.40738 acc 0.33333 roc_auc 0.35900 prc_auc 0.57141[0m
[93maverage test of epoch 18: loss -26.08899 acc 0.34211 roc_auc 0.07385 prc_auc 0.50885[0m
[92maverage training of epoch 19: loss -26.73564 acc 0.33333 roc_auc 0.35740 prc_auc 0.56981[0m
[93maverage test of epoch 19: loss -27.42804 acc 0.34211 roc_auc 0.14462 prc_auc 0.50726[0m
[92maverage training of epoch 20: loss -28.08693 acc 0.33333 roc_auc 0.35980 prc_auc 0.57001[0m
[93maverage test of epoch 20: loss -28.79061 acc 0.34211 roc_auc 0.26769 prc_auc 0.57212[0m
[92maverage training of epoch 21: loss -29.46240 acc 0.33333 roc_auc 0.36060 prc_auc 0.56942[0m
[93maverage test of epoch 21: loss -30.17769 acc 0.34211 roc_auc 0.19077 prc_auc 0.54528[0m
[92maverage training of epoch 22: loss -30.86292 acc 0.33333 roc_auc 0.36160 prc_auc 0.56903[0m
[93maverage test of epoch 22: loss -31.59010 acc 0.34211 roc_auc 0.20154 prc_auc 0.53461[0m
[92maverage training of epoch 23: loss -32.28924 acc 0.33333 roc_auc 0.36230 prc_auc 0.56877[0m
[93maverage test of epoch 23: loss -33.02851 acc 0.34211 roc_auc 0.13385 prc_auc 0.58939[0m
[92maverage training of epoch 24: loss -33.74194 acc 0.33333 roc_auc 0.36180 prc_auc 0.56823[0m
[93maverage test of epoch 24: loss -34.49346 acc 0.34211 roc_auc 0.27846 prc_auc 0.57540[0m
[92maverage training of epoch 25: loss -35.22152 acc 0.33333 roc_auc 0.36180 prc_auc 0.56793[0m
[93maverage test of epoch 25: loss -35.98541 acc 0.34211 roc_auc 0.23077 prc_auc 0.58334[0m
[92maverage training of epoch 26: loss -36.72841 acc 0.33333 roc_auc 0.36260 prc_auc 0.56842[0m
[93maverage test of epoch 26: loss -37.50472 acc 0.34211 roc_auc 0.42615 prc_auc 0.62695[0m
[92maverage training of epoch 27: loss -38.26292 acc 0.33333 roc_auc 0.36280 prc_auc 0.56796[0m
[93maverage test of epoch 27: loss -39.05173 acc 0.34211 roc_auc 0.40000 prc_auc 0.61643[0m
[92maverage training of epoch 28: loss -39.82536 acc 0.33333 roc_auc 0.36280 prc_auc 0.56775[0m
[93maverage test of epoch 28: loss -40.62672 acc 0.34211 roc_auc 0.39077 prc_auc 0.61804[0m
[92maverage training of epoch 29: loss -41.41600 acc 0.33333 roc_auc 0.36220 prc_auc 0.56671[0m
[93maverage test of epoch 29: loss -42.22990 acc 0.34211 roc_auc 0.38923 prc_auc 0.62000[0m
[92maverage training of epoch 30: loss -43.03502 acc 0.60000 roc_auc 0.36020 prc_auc 0.56469[0m
[93maverage test of epoch 30: loss -43.86148 acc 0.65789 roc_auc 0.42000 prc_auc 0.62298[0m
[92maverage training of epoch 31: loss -44.68266 acc 0.66667 roc_auc 0.35750 prc_auc 0.56226[0m
[93maverage test of epoch 31: loss -45.52163 acc 0.65789 roc_auc 0.33231 prc_auc 0.60435[0m
[92maverage training of epoch 32: loss -46.35896 acc 0.66667 roc_auc 0.35740 prc_auc 0.56240[0m
[93maverage test of epoch 32: loss -47.21045 acc 0.65789 roc_auc 0.60769 prc_auc 0.71048[0m
[92maverage training of epoch 33: loss -48.06413 acc 0.66667 roc_auc 0.35840 prc_auc 0.56545[0m
[93maverage test of epoch 33: loss -48.92812 acc 0.65789 roc_auc 0.34769 prc_auc 0.62812[0m
[92maverage training of epoch 34: loss -49.79834 acc 0.66667 roc_auc 0.35860 prc_auc 0.56543[0m
[93maverage test of epoch 34: loss -50.67481 acc 0.65789 roc_auc 0.44615 prc_auc 0.63743[0m
[92maverage training of epoch 35: loss -51.56168 acc 0.66667 roc_auc 0.35800 prc_auc 0.56507[0m
[93maverage test of epoch 35: loss -52.45054 acc 0.65789 roc_auc 0.84000 prc_auc 0.87063[0m
[92maverage training of epoch 36: loss -53.35413 acc 0.66667 roc_auc 0.35910 prc_auc 0.56654[0m
[93maverage test of epoch 36: loss -54.25533 acc 0.65789 roc_auc 0.66000 prc_auc 0.76737[0m
[92maverage training of epoch 37: loss -55.17582 acc 0.66667 roc_auc 0.35780 prc_auc 0.56626[0m
[93maverage test of epoch 37: loss -56.08936 acc 0.65789 roc_auc 0.59077 prc_auc 0.70261[0m
[92maverage training of epoch 38: loss -57.02690 acc 0.66667 roc_auc 0.35900 prc_auc 0.56721[0m
[93maverage test of epoch 38: loss -57.95271 acc 0.65789 roc_auc 0.48154 prc_auc 0.65158[0m
[92maverage training of epoch 39: loss -58.90741 acc 0.66667 roc_auc 0.35940 prc_auc 0.56746[0m
[93maverage test of epoch 39: loss -59.84543 acc 0.65789 roc_auc 0.12462 prc_auc 0.55137[0m
[92maverage training of epoch 40: loss -60.81734 acc 0.66667 roc_auc 0.35960 prc_auc 0.56778[0m
[93maverage test of epoch 40: loss -61.76750 acc 0.65789 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 41: loss -62.75671 acc 0.66667 roc_auc 0.35960 prc_auc 0.56790[0m
[93maverage test of epoch 41: loss -63.71892 acc 0.65789 roc_auc 0.65385 prc_auc 0.73529[0m
[92maverage training of epoch 42: loss -64.72565 acc 0.66667 roc_auc 0.35960 prc_auc 0.56807[0m
[93maverage test of epoch 42: loss -65.69988 acc 0.65789 roc_auc 0.32000 prc_auc 0.58995[0m
[92maverage training of epoch 43: loss -66.72416 acc 0.66667 roc_auc 0.35980 prc_auc 0.56854[0m
[93maverage test of epoch 43: loss -67.71037 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 44: loss -68.75235 acc 0.66667 roc_auc 0.36020 prc_auc 0.56919[0m
[93maverage test of epoch 44: loss -69.75044 acc 0.65789 roc_auc 0.34615 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -70.81017 acc 0.66667 roc_auc 0.36120 prc_auc 0.56984[0m
[93maverage test of epoch 45: loss -71.82000 acc 0.65789 roc_auc 0.52000 prc_auc 0.67158[0m
[92maverage training of epoch 46: loss -72.89760 acc 0.66667 roc_auc 0.36100 prc_auc 0.57010[0m
[93maverage test of epoch 46: loss -73.91915 acc 0.65789 roc_auc 0.34615 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -75.01468 acc 0.66667 roc_auc 0.36000 prc_auc 0.56911[0m
[93maverage test of epoch 47: loss -76.04786 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -77.16147 acc 0.66667 roc_auc 0.36060 prc_auc 0.57014[0m
[93maverage test of epoch 48: loss -78.20620 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -79.33784 acc 0.66667 roc_auc 0.36100 prc_auc 0.57000[0m
[93maverage test of epoch 49: loss -80.39395 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 1
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss 0.01850 acc 0.66667 roc_auc 0.35080 prc_auc 0.56512[0m
[93maverage test of epoch 0: loss -0.12556 acc 0.65789 roc_auc 0.17846 prc_auc 0.55282[0m
[92maverage training of epoch 1: loss -0.25335 acc 0.66667 roc_auc 0.35630 prc_auc 0.57847[0m
[93maverage test of epoch 1: loss -0.36440 acc 0.65789 roc_auc 0.36000 prc_auc 0.70936[0m
[92maverage training of epoch 2: loss -0.48958 acc 0.66667 roc_auc 0.45300 prc_auc 0.66609[0m
[93maverage test of epoch 2: loss -0.60903 acc 0.65789 roc_auc 0.64308 prc_auc 0.74132[0m
[92maverage training of epoch 3: loss -0.76806 acc 0.66667 roc_auc 0.51400 prc_auc 0.70246[0m
[93maverage test of epoch 3: loss -0.94704 acc 0.65789 roc_auc 0.82462 prc_auc 0.88929[0m
[92maverage training of epoch 4: loss -1.49821 acc 0.66667 roc_auc 0.51940 prc_auc 0.68946[0m
[93maverage test of epoch 4: loss -2.25354 acc 0.65789 roc_auc 0.82769 prc_auc 0.89316[0m
[92maverage training of epoch 5: loss -2.87461 acc 0.66667 roc_auc 0.50360 prc_auc 0.67749[0m
[93maverage test of epoch 5: loss -3.42927 acc 0.65789 roc_auc 0.84615 prc_auc 0.90903[0m
[92maverage training of epoch 6: loss -3.96116 acc 0.66667 roc_auc 0.47820 prc_auc 0.65946[0m
[93maverage test of epoch 6: loss -4.44019 acc 0.65789 roc_auc 0.85231 prc_auc 0.91488[0m
[92maverage training of epoch 7: loss -4.93457 acc 0.66667 roc_auc 0.47080 prc_auc 0.65647[0m
[93maverage test of epoch 7: loss -5.37286 acc 0.65789 roc_auc 0.85538 prc_auc 0.91637[0m
[92maverage training of epoch 8: loss -5.85168 acc 0.66667 roc_auc 0.46980 prc_auc 0.65597[0m
[93maverage test of epoch 8: loss -6.27164 acc 0.65789 roc_auc 0.85846 prc_auc 0.91138[0m
[92maverage training of epoch 9: loss -6.77952 acc 0.66667 roc_auc 0.47000 prc_auc 0.65602[0m
[93maverage test of epoch 9: loss -7.27802 acc 0.65789 roc_auc 0.85846 prc_auc 0.91554[0m
[92maverage training of epoch 10: loss -8.21079 acc 0.66667 roc_auc 0.47960 prc_auc 0.66940[0m
[93maverage test of epoch 10: loss -9.10526 acc 0.65789 roc_auc 0.87692 prc_auc 0.92962[0m
[92maverage training of epoch 11: loss -9.83339 acc 0.66667 roc_auc 0.47560 prc_auc 0.65836[0m
[93maverage test of epoch 11: loss -10.43105 acc 0.65789 roc_auc 0.87385 prc_auc 0.92251[0m
[92maverage training of epoch 12: loss -11.08447 acc 0.66667 roc_auc 0.47110 prc_auc 0.65652[0m
[93maverage test of epoch 12: loss -11.62087 acc 0.65789 roc_auc 0.86462 prc_auc 0.89079[0m
[92maverage training of epoch 13: loss -12.25207 acc 0.66667 roc_auc 0.46990 prc_auc 0.65597[0m
[93maverage test of epoch 13: loss -12.74451 acc 0.65789 roc_auc 0.82462 prc_auc 0.85158[0m
[92maverage training of epoch 14: loss -13.35366 acc 0.66667 roc_auc 0.46980 prc_auc 0.65597[0m
[93maverage test of epoch 14: loss -13.83038 acc 0.65789 roc_auc 0.81846 prc_auc 0.84819[0m
[92maverage training of epoch 15: loss -14.44936 acc 0.66667 roc_auc 0.46970 prc_auc 0.65558[0m
[93maverage test of epoch 15: loss -14.92559 acc 0.65789 roc_auc 0.81846 prc_auc 0.84819[0m
[92maverage training of epoch 16: loss -15.55808 acc 0.66667 roc_auc 0.47000 prc_auc 0.65569[0m
[93maverage test of epoch 16: loss -16.03736 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 17: loss -16.68545 acc 0.66667 roc_auc 0.46960 prc_auc 0.65621[0m
[93maverage test of epoch 17: loss -17.16985 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 18: loss -17.83485 acc 0.66667 roc_auc 0.46900 prc_auc 0.65645[0m
[93maverage test of epoch 18: loss -18.32570 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 19: loss -19.00851 acc 0.66667 roc_auc 0.47040 prc_auc 0.64913[0m
[93maverage test of epoch 19: loss -19.50669 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 20: loss -20.20798 acc 0.66667 roc_auc 0.46660 prc_auc 0.64553[0m
[93maverage test of epoch 20: loss -20.71412 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 21: loss -21.43439 acc 0.66667 roc_auc 0.45630 prc_auc 0.63993[0m
[93maverage test of epoch 21: loss -21.94892 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 22: loss -22.68851 acc 0.66667 roc_auc 0.46450 prc_auc 0.64742[0m
[93maverage test of epoch 22: loss -23.21144 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 23: loss -23.96961 acc 0.66667 roc_auc 0.47240 prc_auc 0.65447[0m
[93maverage test of epoch 23: loss -24.50005 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 24: loss -25.27687 acc 0.66667 roc_auc 0.48000 prc_auc 0.65797[0m
[93maverage test of epoch 24: loss -25.81507 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 25: loss -26.61089 acc 0.66667 roc_auc 0.48500 prc_auc 0.66009[0m
[93maverage test of epoch 25: loss -27.15713 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 26: loss -27.97207 acc 0.66667 roc_auc 0.48500 prc_auc 0.66083[0m
[93maverage test of epoch 26: loss -28.52584 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 27: loss -29.35885 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 27: loss -29.91930 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 28: loss -30.77063 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 28: loss -31.33805 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 29: loss -32.20806 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 29: loss -32.78272 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 30: loss -33.67172 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 30: loss -34.25381 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 31: loss -35.16210 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 31: loss -35.75179 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 32: loss -36.67963 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 32: loss -37.27704 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 33: loss -38.22464 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 33: loss -38.82986 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 34: loss -39.79743 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 34: loss -40.41054 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 35: loss -41.39829 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 35: loss -42.01933 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 36: loss -43.02743 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 36: loss -43.65644 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 37: loss -44.68504 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 37: loss -45.32205 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 38: loss -46.37130 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 38: loss -47.01628 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 39: loss -48.08635 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 39: loss -48.73930 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 40: loss -49.83033 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 40: loss -50.49123 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 41: loss -51.60335 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 41: loss -52.27218 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -53.40548 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 42: loss -54.08221 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -55.23687 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 43: loss -55.92145 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 44: loss -57.09754 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 44: loss -57.78995 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -58.98756 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 45: loss -59.68773 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -60.90703 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 46: loss -61.61489 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -62.85592 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 47: loss -63.57146 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -64.83445 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 48: loss -65.55760 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -66.84243 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 49: loss -67.57296 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 2
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss -0.39226 acc 0.34667 roc_auc 0.46380 prc_auc 0.67826[0m
[93maverage test of epoch 0: loss -0.43142 acc 0.36842 roc_auc 0.60000 prc_auc 0.83099[0m
[92maverage training of epoch 1: loss -0.46083 acc 0.59333 roc_auc 0.40800 prc_auc 0.61124[0m
[93maverage test of epoch 1: loss -0.47922 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 2: loss -0.50131 acc 0.66667 roc_auc 0.40310 prc_auc 0.61863[0m
[93maverage test of epoch 2: loss -0.51223 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 3: loss -0.53646 acc 0.66667 roc_auc 0.39860 prc_auc 0.60993[0m
[93maverage test of epoch 3: loss -0.54525 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 4: loss -0.57354 acc 0.66667 roc_auc 0.41980 prc_auc 0.63911[0m
[93maverage test of epoch 4: loss -0.58026 acc 0.65789 roc_auc 0.52000 prc_auc 0.67158[0m
[92maverage training of epoch 5: loss -0.71858 acc 0.66667 roc_auc 0.46360 prc_auc 0.66780[0m
[93maverage test of epoch 5: loss -0.97884 acc 0.65789 roc_auc 0.27692 prc_auc 0.65781[0m
[92maverage training of epoch 6: loss -1.39892 acc 0.66667 roc_auc 0.46020 prc_auc 0.66802[0m
[93maverage test of epoch 6: loss -1.86433 acc 0.65789 roc_auc 0.94769 prc_auc 0.97702[0m
[92maverage training of epoch 7: loss -2.35032 acc 0.66667 roc_auc 0.47940 prc_auc 0.68314[0m
[93maverage test of epoch 7: loss -2.76331 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 8: loss -3.21048 acc 0.66667 roc_auc 0.48720 prc_auc 0.68237[0m
[93maverage test of epoch 8: loss -3.60763 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 9: loss -3.98702 acc 0.66667 roc_auc 0.47520 prc_auc 0.67522[0m
[93maverage test of epoch 9: loss -4.29098 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 10: loss -4.62726 acc 0.66667 roc_auc 0.46140 prc_auc 0.66336[0m
[93maverage test of epoch 10: loss -4.89578 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 11: loss -5.22278 acc 0.66667 roc_auc 0.44060 prc_auc 0.64637[0m
[93maverage test of epoch 11: loss -5.47985 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 12: loss -5.80841 acc 0.66667 roc_auc 0.42900 prc_auc 0.63630[0m
[93maverage test of epoch 12: loss -6.06297 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 13: loss -6.39796 acc 0.66667 roc_auc 0.42000 prc_auc 0.62912[0m
[93maverage test of epoch 13: loss -6.65472 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 14: loss -6.99918 acc 0.66667 roc_auc 0.41590 prc_auc 0.62267[0m
[93maverage test of epoch 14: loss -7.26119 acc 0.65789 roc_auc 0.95077 prc_auc 0.97663[0m
[92maverage training of epoch 15: loss -7.61729 acc 0.66667 roc_auc 0.41120 prc_auc 0.61935[0m
[93maverage test of epoch 15: loss -7.88676 acc 0.65789 roc_auc 0.95231 prc_auc 0.97697[0m
[92maverage training of epoch 16: loss -8.25617 acc 0.66667 roc_auc 0.40800 prc_auc 0.61688[0m
[93maverage test of epoch 16: loss -8.53479 acc 0.65789 roc_auc 0.94923 prc_auc 0.97264[0m
[92maverage training of epoch 17: loss -8.91884 acc 0.66667 roc_auc 0.40610 prc_auc 0.61549[0m
[93maverage test of epoch 17: loss -9.20796 acc 0.65789 roc_auc 0.91846 prc_auc 0.94483[0m
[92maverage training of epoch 18: loss -9.60772 acc 0.66667 roc_auc 0.40520 prc_auc 0.61492[0m
[93maverage test of epoch 18: loss -9.90842 acc 0.65789 roc_auc 0.91538 prc_auc 0.94115[0m
[92maverage training of epoch 19: loss -10.32477 acc 0.66667 roc_auc 0.40290 prc_auc 0.61272[0m
[93maverage test of epoch 19: loss -10.63793 acc 0.65789 roc_auc 0.80000 prc_auc 0.86316[0m
[92maverage training of epoch 20: loss -11.07164 acc 0.66667 roc_auc 0.40150 prc_auc 0.61090[0m
[93maverage test of epoch 20: loss -11.39798 acc 0.65789 roc_auc 0.67231 prc_auc 0.74632[0m
[92maverage training of epoch 21: loss -11.84967 acc 0.66667 roc_auc 0.40130 prc_auc 0.61040[0m
[93maverage test of epoch 21: loss -12.18980 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 22: loss -12.65999 acc 0.66667 roc_auc 0.40040 prc_auc 0.60968[0m
[93maverage test of epoch 22: loss -13.01438 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 23: loss -13.50354 acc 0.66667 roc_auc 0.40020 prc_auc 0.60900[0m
[93maverage test of epoch 23: loss -13.87257 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 24: loss -14.38106 acc 0.66667 roc_auc 0.40100 prc_auc 0.61058[0m
[93maverage test of epoch 24: loss -14.76505 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 25: loss -15.29318 acc 0.66667 roc_auc 0.40060 prc_auc 0.60702[0m
[93maverage test of epoch 25: loss -15.69233 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 26: loss -16.24037 acc 0.66667 roc_auc 0.40090 prc_auc 0.60778[0m
[93maverage test of epoch 26: loss -16.65484 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 27: loss -17.22298 acc 0.66667 roc_auc 0.40070 prc_auc 0.60529[0m
[93maverage test of epoch 27: loss -17.65285 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 28: loss -18.24126 acc 0.66667 roc_auc 0.40200 prc_auc 0.60773[0m
[93maverage test of epoch 28: loss -18.68658 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 29: loss -19.29538 acc 0.66667 roc_auc 0.40340 prc_auc 0.60256[0m
[93maverage test of epoch 29: loss -19.75611 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 30: loss -20.38541 acc 0.66667 roc_auc 0.40450 prc_auc 0.60783[0m
[93maverage test of epoch 30: loss -20.86151 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 31: loss -21.51137 acc 0.66667 roc_auc 0.40660 prc_auc 0.60401[0m
[93maverage test of epoch 31: loss -22.00273 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 32: loss -22.67321 acc 0.66667 roc_auc 0.40230 prc_auc 0.60122[0m
[93maverage test of epoch 32: loss -23.17973 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 33: loss -23.87085 acc 0.66667 roc_auc 0.40870 prc_auc 0.60404[0m
[93maverage test of epoch 33: loss -24.39239 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 34: loss -25.10418 acc 0.66667 roc_auc 0.40960 prc_auc 0.61094[0m
[93maverage test of epoch 34: loss -25.64056 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 35: loss -26.37304 acc 0.66667 roc_auc 0.40780 prc_auc 0.60320[0m
[93maverage test of epoch 35: loss -26.92408 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 36: loss -27.67725 acc 0.66667 roc_auc 0.41390 prc_auc 0.60925[0m
[93maverage test of epoch 36: loss -28.24276 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 37: loss -29.01662 acc 0.66667 roc_auc 0.41040 prc_auc 0.60826[0m
[93maverage test of epoch 37: loss -29.59640 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 38: loss -30.39095 acc 0.66667 roc_auc 0.42160 prc_auc 0.61816[0m
[93maverage test of epoch 38: loss -30.98478 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 39: loss -31.79999 acc 0.66667 roc_auc 0.43660 prc_auc 0.63397[0m
[93maverage test of epoch 39: loss -32.40767 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 40: loss -33.24356 acc 0.66667 roc_auc 0.42610 prc_auc 0.62516[0m
[93maverage test of epoch 40: loss -33.86486 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 41: loss -34.72124 acc 0.66667 roc_auc 0.43590 prc_auc 0.63514[0m
[93maverage test of epoch 41: loss -35.35510 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -36.22889 acc 0.66667 roc_auc 0.46020 prc_auc 0.64968[0m
[93maverage test of epoch 42: loss -36.87241 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -37.76331 acc 0.66667 roc_auc 0.43500 prc_auc 0.64004[0m
[93maverage test of epoch 43: loss -38.41659 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 44: loss -39.32512 acc 0.66667 roc_auc 0.43000 prc_auc 0.63831[0m
[93maverage test of epoch 44: loss -39.98836 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -40.91500 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 45: loss -41.58834 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -42.53343 acc 0.66667 roc_auc 0.41000 prc_auc 0.63212[0m
[93maverage test of epoch 46: loss -43.21695 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -44.18083 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 47: loss -44.87457 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -45.85749 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 48: loss -46.56147 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -47.56366 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 49: loss -48.27785 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 3
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss -0.12119 acc 0.34437 roc_auc 0.45137 prc_auc 0.66822[0m
[93maverage test of epoch 0: loss -0.24031 acc 0.35135 roc_auc 0.84000 prc_auc 0.90374[0m
[92maverage training of epoch 1: loss -0.37631 acc 0.35099 roc_auc 0.48235 prc_auc 0.69663[0m
[93maverage test of epoch 1: loss -0.55752 acc 0.35135 roc_auc 0.60667 prc_auc 0.84056[0m
[92maverage training of epoch 2: loss -0.79348 acc 0.35099 roc_auc 0.46902 prc_auc 0.69417[0m
[93maverage test of epoch 2: loss -1.05799 acc 0.35135 roc_auc 0.87167 prc_auc 0.92839[0m
[92maverage training of epoch 3: loss -1.32515 acc 0.36424 roc_auc 0.46275 prc_auc 0.69399[0m
[93maverage test of epoch 3: loss -1.60962 acc 0.37838 roc_auc 0.76333 prc_auc 0.89276[0m
[92maverage training of epoch 4: loss -1.94458 acc 0.54305 roc_auc 0.47196 prc_auc 0.68544[0m
[93maverage test of epoch 4: loss -2.35731 acc 0.67568 roc_auc 0.37667 prc_auc 0.71991[0m
[92maverage training of epoch 5: loss -2.82276 acc 0.66225 roc_auc 0.47608 prc_auc 0.70261[0m
[93maverage test of epoch 5: loss -3.31641 acc 0.67568 roc_auc 0.49333 prc_auc 0.78204[0m
[92maverage training of epoch 6: loss -3.76501 acc 0.66225 roc_auc 0.52863 prc_auc 0.75642[0m
[93maverage test of epoch 6: loss -4.27299 acc 0.67568 roc_auc 0.86000 prc_auc 0.92243[0m
[92maverage training of epoch 7: loss -4.78282 acc 0.66225 roc_auc 0.51255 prc_auc 0.71972[0m
[93maverage test of epoch 7: loss -5.33355 acc 0.67568 roc_auc 0.86667 prc_auc 0.92618[0m
[92maverage training of epoch 8: loss -5.85694 acc 0.66225 roc_auc 0.46275 prc_auc 0.66036[0m
[93maverage test of epoch 8: loss -6.46445 acc 0.67568 roc_auc 0.84667 prc_auc 0.91752[0m
[92maverage training of epoch 9: loss -7.04928 acc 0.66225 roc_auc 0.44627 prc_auc 0.64121[0m
[93maverage test of epoch 9: loss -7.72328 acc 0.67568 roc_auc 0.81500 prc_auc 0.89932[0m
[92maverage training of epoch 10: loss -8.31627 acc 0.66225 roc_auc 0.43765 prc_auc 0.63381[0m
[93maverage test of epoch 10: loss -8.99076 acc 0.67568 roc_auc 0.88333 prc_auc 0.93097[0m
[92maverage training of epoch 11: loss -9.57010 acc 0.66225 roc_auc 0.42706 prc_auc 0.62356[0m
[93maverage test of epoch 11: loss -10.24110 acc 0.67568 roc_auc 0.85667 prc_auc 0.92322[0m
[92maverage training of epoch 12: loss -10.81688 acc 0.66225 roc_auc 0.41745 prc_auc 0.61324[0m
[93maverage test of epoch 12: loss -11.49492 acc 0.67568 roc_auc 0.86333 prc_auc 0.92498[0m
[92maverage training of epoch 13: loss -12.07561 acc 0.66225 roc_auc 0.40922 prc_auc 0.60126[0m
[93maverage test of epoch 13: loss -12.76768 acc 0.67568 roc_auc 0.86333 prc_auc 0.92477[0m
[92maverage training of epoch 14: loss -13.35828 acc 0.66225 roc_auc 0.40235 prc_auc 0.59628[0m
[93maverage test of epoch 14: loss -14.06845 acc 0.67568 roc_auc 0.86333 prc_auc 0.92477[0m
[92maverage training of epoch 15: loss -14.67194 acc 0.66225 roc_auc 0.39784 prc_auc 0.59244[0m
[93maverage test of epoch 15: loss -15.40292 acc 0.67568 roc_auc 0.86333 prc_auc 0.92496[0m
[92maverage training of epoch 16: loss -16.02120 acc 0.66225 roc_auc 0.39431 prc_auc 0.59002[0m
[93maverage test of epoch 16: loss -16.77499 acc 0.67568 roc_auc 0.86500 prc_auc 0.92578[0m
[92maverage training of epoch 17: loss -17.40912 acc 0.66225 roc_auc 0.39196 prc_auc 0.58086[0m
[93maverage test of epoch 17: loss -18.18651 acc 0.67568 roc_auc 0.86667 prc_auc 0.92730[0m
[92maverage training of epoch 18: loss -18.83482 acc 0.66225 roc_auc 0.39294 prc_auc 0.58214[0m
[93maverage test of epoch 18: loss -19.63468 acc 0.67568 roc_auc 0.86667 prc_auc 0.92407[0m
[92maverage training of epoch 19: loss -20.29715 acc 0.66225 roc_auc 0.39235 prc_auc 0.58206[0m
[93maverage test of epoch 19: loss -21.12065 acc 0.67568 roc_auc 0.86000 prc_auc 0.91553[0m
[92maverage training of epoch 20: loss -21.79809 acc 0.66225 roc_auc 0.39343 prc_auc 0.58259[0m
[93maverage test of epoch 20: loss -22.64652 acc 0.67568 roc_auc 0.88500 prc_auc 0.92422[0m
[92maverage training of epoch 21: loss -23.33882 acc 0.66225 roc_auc 0.39373 prc_auc 0.58282[0m
[93maverage test of epoch 21: loss -24.21155 acc 0.67568 roc_auc 0.87667 prc_auc 0.91098[0m
[92maverage training of epoch 22: loss -24.91635 acc 0.66225 roc_auc 0.39500 prc_auc 0.58430[0m
[93maverage test of epoch 22: loss -25.81278 acc 0.67568 roc_auc 0.79500 prc_auc 0.84477[0m
[92maverage training of epoch 23: loss -26.53063 acc 0.66225 roc_auc 0.39657 prc_auc 0.58543[0m
[93maverage test of epoch 23: loss -27.45202 acc 0.67568 roc_auc 0.70000 prc_auc 0.79751[0m
[92maverage training of epoch 24: loss -28.18345 acc 0.66225 roc_auc 0.39892 prc_auc 0.58710[0m
[93maverage test of epoch 24: loss -29.13085 acc 0.67568 roc_auc 0.58000 prc_auc 0.71644[0m
[92maverage training of epoch 25: loss -29.87628 acc 0.66225 roc_auc 0.39990 prc_auc 0.58794[0m
[93maverage test of epoch 25: loss -30.85062 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 26: loss -31.61033 acc 0.66225 roc_auc 0.40127 prc_auc 0.58959[0m
[93maverage test of epoch 26: loss -32.61239 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 27: loss -33.38656 acc 0.66225 roc_auc 0.40216 prc_auc 0.58931[0m
[93maverage test of epoch 27: loss -34.41707 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 28: loss -35.20584 acc 0.66225 roc_auc 0.40353 prc_auc 0.58972[0m
[93maverage test of epoch 28: loss -36.26542 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 29: loss -37.06883 acc 0.66225 roc_auc 0.40324 prc_auc 0.58936[0m
[93maverage test of epoch 29: loss -38.15806 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 30: loss -38.97614 acc 0.66225 roc_auc 0.40539 prc_auc 0.59097[0m
[93maverage test of epoch 30: loss -40.09554 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 31: loss -40.92827 acc 0.66225 roc_auc 0.40804 prc_auc 0.59329[0m
[93maverage test of epoch 31: loss -42.07831 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 32: loss -42.92565 acc 0.66225 roc_auc 0.40863 prc_auc 0.59136[0m
[93maverage test of epoch 32: loss -44.10678 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 33: loss -44.96865 acc 0.66225 roc_auc 0.40490 prc_auc 0.59182[0m
[93maverage test of epoch 33: loss -46.18125 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 34: loss -47.05754 acc 0.66225 roc_auc 0.40676 prc_auc 0.59667[0m
[93maverage test of epoch 34: loss -48.30203 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 35: loss -49.19261 acc 0.66225 roc_auc 0.41088 prc_auc 0.60125[0m
[93maverage test of epoch 35: loss -50.46932 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 36: loss -51.37406 acc 0.66225 roc_auc 0.41108 prc_auc 0.60705[0m
[93maverage test of epoch 36: loss -52.68333 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 37: loss -53.60207 acc 0.66225 roc_auc 0.40667 prc_auc 0.61522[0m
[93maverage test of epoch 37: loss -54.94421 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 38: loss -55.87677 acc 0.66225 roc_auc 0.42549 prc_auc 0.63087[0m
[93maverage test of epoch 38: loss -57.25208 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 39: loss -58.19829 acc 0.66225 roc_auc 0.45588 prc_auc 0.64363[0m
[93maverage test of epoch 39: loss -59.60703 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 40: loss -60.56669 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 40: loss -62.00913 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 41: loss -62.98206 acc 0.66225 roc_auc 0.48363 prc_auc 0.65502[0m
[93maverage test of epoch 41: loss -64.45848 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 42: loss -65.44442 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 42: loss -66.95505 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 43: loss -67.95382 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 43: loss -69.49886 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 44: loss -70.51021 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 44: loss -72.08988 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 45: loss -73.11363 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 45: loss -74.72821 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 46: loss -75.76405 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 46: loss -77.41361 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 47: loss -78.46135 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 47: loss -80.14620 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 48: loss -81.20569 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 48: loss -82.92600 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 49: loss -83.99685 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 49: loss -85.75272 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
Training model with dataset, testing using fold 4
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss 0.25884 acc 0.33775 roc_auc 0.26059 prc_auc 0.53835[0m
[93maverage test of epoch 0: loss 0.08674 acc 0.32432 roc_auc 0.06667 prc_auc 0.48823[0m
[92maverage training of epoch 1: loss -0.14281 acc 0.33775 roc_auc 0.24706 prc_auc 0.53773[0m
[93maverage test of epoch 1: loss -0.31797 acc 0.32432 roc_auc 0.06667 prc_auc 0.48823[0m
[92maverage training of epoch 2: loss -0.55073 acc 0.33775 roc_auc 0.22176 prc_auc 0.54013[0m
[93maverage test of epoch 2: loss -0.73448 acc 0.32432 roc_auc 0.07000 prc_auc 0.48890[0m
[92maverage training of epoch 3: loss -1.02536 acc 0.33775 roc_auc 0.29000 prc_auc 0.57808[0m
[93maverage test of epoch 3: loss -1.35903 acc 0.32432 roc_auc 0.93000 prc_auc 0.97264[0m
Using backend: pytorch
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[92maverage training of epoch 4: loss -2.04065 acc 0.33775 roc_auc 0.66941 prc_auc 0.82931[0m
[93maverage test of epoch 4: loss -2.50971 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 5: loss -2.80923 acc 0.33775 roc_auc 0.83706 prc_auc 0.91405[0m
[93maverage test of epoch 5: loss -3.06200 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 6: loss -3.31497 acc 0.33775 roc_auc 0.84902 prc_auc 0.91162[0m
[93maverage test of epoch 6: loss -3.54235 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 7: loss -3.79684 acc 0.33775 roc_auc 0.86353 prc_auc 0.91645[0m
[93maverage test of epoch 7: loss -4.02520 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 8: loss -4.29578 acc 0.33775 roc_auc 0.87020 prc_auc 0.91912[0m
[93maverage test of epoch 8: loss -4.53440 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 9: loss -4.82648 acc 0.33775 roc_auc 0.87431 prc_auc 0.92017[0m
[93maverage test of epoch 9: loss -5.07738 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 10: loss -5.38776 acc 0.33775 roc_auc 0.87588 prc_auc 0.91878[0m
[93maverage test of epoch 10: loss -5.66285 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 11: loss -5.97084 acc 0.33775 roc_auc 0.86627 prc_auc 0.89632[0m
[93maverage test of epoch 11: loss -6.24442 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 12: loss -6.54057 acc 0.33775 roc_auc 0.85863 prc_auc 0.88481[0m
[93maverage test of epoch 12: loss -6.80733 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 13: loss -7.08718 acc 0.33775 roc_auc 0.84569 prc_auc 0.86063[0m
[93maverage test of epoch 13: loss -7.34908 acc 0.32432 roc_auc 0.93000 prc_auc 0.97235[0m
[92maverage training of epoch 14: loss -7.62415 acc 0.33775 roc_auc 0.84333 prc_auc 0.85615[0m
[93maverage test of epoch 14: loss -7.88505 acc 0.32432 roc_auc 0.93000 prc_auc 0.97235[0m
[92maverage training of epoch 15: loss -8.15731 acc 0.33775 roc_auc 0.82980 prc_auc 0.83896[0m
[93maverage test of epoch 15: loss -8.41459 acc 0.32432 roc_auc 0.93000 prc_auc 0.97235[0m
[92maverage training of epoch 16: loss -8.69637 acc 0.33775 roc_auc 0.82961 prc_auc 0.83720[0m
[93maverage test of epoch 16: loss -8.95176 acc 0.32432 roc_auc 0.93000 prc_auc 0.97235[0m
[92maverage training of epoch 17: loss -9.24143 acc 0.33775 roc_auc 0.83000 prc_auc 0.83476[0m
[93maverage test of epoch 17: loss -9.49253 acc 0.32432 roc_auc 0.93000 prc_auc 0.97235[0m
[92maverage training of epoch 18: loss -9.79608 acc 0.33775 roc_auc 0.82824 prc_auc 0.83453[0m
[93maverage test of epoch 18: loss -10.03974 acc 0.32432 roc_auc 0.93000 prc_auc 0.97235[0m
[92maverage training of epoch 19: loss -10.35865 acc 0.33775 roc_auc 0.82765 prc_auc 0.83291[0m
[93maverage test of epoch 19: loss -10.60151 acc 0.32432 roc_auc 0.93000 prc_auc 0.97235[0m
[92maverage training of epoch 20: loss -10.93383 acc 0.33775 roc_auc 0.82725 prc_auc 0.82902[0m
[93maverage test of epoch 20: loss -11.18166 acc 0.32432 roc_auc 0.93333 prc_auc 0.97377[0m
[92maverage training of epoch 21: loss -11.52015 acc 0.33775 roc_auc 0.82706 prc_auc 0.82460[0m
[93maverage test of epoch 21: loss -11.76528 acc 0.32432 roc_auc 0.93333 prc_auc 0.97377[0m
[92maverage training of epoch 22: loss -12.11888 acc 0.33775 roc_auc 0.82725 prc_auc 0.82464[0m
[93maverage test of epoch 22: loss -12.36290 acc 0.32432 roc_auc 0.93333 prc_auc 0.97377[0m
[92maverage training of epoch 23: loss -12.73006 acc 0.33775 roc_auc 0.82647 prc_auc 0.82147[0m
[93maverage test of epoch 23: loss -12.97338 acc 0.32432 roc_auc 0.93333 prc_auc 0.97377[0m
[92maverage training of epoch 24: loss -13.35411 acc 0.33775 roc_auc 0.82686 prc_auc 0.82167[0m
[93maverage test of epoch 24: loss -13.59758 acc 0.32432 roc_auc 0.93333 prc_auc 0.97377[0m
[92maverage training of epoch 25: loss -13.99137 acc 0.33775 roc_auc 0.82667 prc_auc 0.82128[0m
[93maverage test of epoch 25: loss -14.23575 acc 0.32432 roc_auc 0.93333 prc_auc 0.97377[0m
[92maverage training of epoch 26: loss -14.64066 acc 0.33775 roc_auc 0.82667 prc_auc 0.82161[0m
[93maverage test of epoch 26: loss -14.88163 acc 0.32432 roc_auc 0.93333 prc_auc 0.97377[0m
[92maverage training of epoch 27: loss -15.30312 acc 0.33775 roc_auc 0.82725 prc_auc 0.82217[0m
[93maverage test of epoch 27: loss -15.54753 acc 0.32432 roc_auc 0.93667 prc_auc 0.97474[0m
[92maverage training of epoch 28: loss -15.97672 acc 0.33775 roc_auc 0.82824 prc_auc 0.82295[0m
[93maverage test of epoch 28: loss -16.22512 acc 0.32432 roc_auc 0.93667 prc_auc 0.97474[0m
[92maverage training of epoch 29: loss -16.66533 acc 0.33775 roc_auc 0.82941 prc_auc 0.83629[0m
[93maverage test of epoch 29: loss -16.91770 acc 0.32432 roc_auc 0.94000 prc_auc 0.97577[0m
[92maverage training of epoch 30: loss -17.36676 acc 0.41722 roc_auc 0.84549 prc_auc 0.86961[0m
[93maverage test of epoch 30: loss -17.61841 acc 0.64865 roc_auc 0.94000 prc_auc 0.97577[0m
[92maverage training of epoch 31: loss -18.07928 acc 0.68874 roc_auc 0.84510 prc_auc 0.86906[0m
[93maverage test of epoch 31: loss -18.33165 acc 0.83784 roc_auc 0.94333 prc_auc 0.97687[0m
[92maverage training of epoch 32: loss -18.80462 acc 0.80132 roc_auc 0.84353 prc_auc 0.86707[0m
[93maverage test of epoch 32: loss -19.05747 acc 0.86486 roc_auc 0.94333 prc_auc 0.97687[0m
[92maverage training of epoch 33: loss -19.54220 acc 0.82119 roc_auc 0.84255 prc_auc 0.86528[0m
[93maverage test of epoch 33: loss -19.79590 acc 0.89189 roc_auc 0.94333 prc_auc 0.97687[0m
[92maverage training of epoch 34: loss -20.29215 acc 0.83444 roc_auc 0.83980 prc_auc 0.86237[0m
[93maverage test of epoch 34: loss -20.54694 acc 0.83784 roc_auc 0.94333 prc_auc 0.97687[0m
[92maverage training of epoch 35: loss -21.05356 acc 0.86755 roc_auc 0.83980 prc_auc 0.86161[0m
[93maverage test of epoch 35: loss -21.31051 acc 0.86486 roc_auc 0.94333 prc_auc 0.97687[0m
[92maverage training of epoch 36: loss -21.81608 acc 0.88742 roc_auc 0.83373 prc_auc 0.85640[0m
[93maverage test of epoch 36: loss -22.09409 acc 0.81081 roc_auc 0.94333 prc_auc 0.97640[0m
[92maverage training of epoch 37: loss -22.60158 acc 0.86755 roc_auc 0.82059 prc_auc 0.84450[0m
[93maverage test of epoch 37: loss -22.90352 acc 0.81081 roc_auc 0.93667 prc_auc 0.97514[0m
[92maverage training of epoch 38: loss -23.36692 acc 0.84106 roc_auc 0.80490 prc_auc 0.82990[0m
[93maverage test of epoch 38: loss -23.72148 acc 0.83784 roc_auc 0.93667 prc_auc 0.97514[0m
[92maverage training of epoch 39: loss -24.17804 acc 0.84106 roc_auc 0.79098 prc_auc 0.82127[0m
[93maverage test of epoch 39: loss -24.54466 acc 0.83784 roc_auc 0.93000 prc_auc 0.97293[0m
[92maverage training of epoch 40: loss -25.00696 acc 0.86093 roc_auc 0.79353 prc_auc 0.81530[0m
[93maverage test of epoch 40: loss -25.27817 acc 0.81081 roc_auc 0.94667 prc_auc 0.97741[0m
[92maverage training of epoch 41: loss -25.77949 acc 0.82119 roc_auc 0.79039 prc_auc 0.81290[0m
[93maverage test of epoch 41: loss -26.04162 acc 0.78378 roc_auc 0.94667 prc_auc 0.97741[0m
[92maverage training of epoch 42: loss -26.63660 acc 0.82119 roc_auc 0.78804 prc_auc 0.81228[0m
[93maverage test of epoch 42: loss -26.88024 acc 0.75676 roc_auc 0.94667 prc_auc 0.97741[0m
[92maverage training of epoch 43: loss -27.49337 acc 0.80795 roc_auc 0.79078 prc_auc 0.81489[0m
[93maverage test of epoch 43: loss -27.78085 acc 0.75676 roc_auc 0.94667 prc_auc 0.97741[0m
[92maverage training of epoch 44: loss -28.41360 acc 0.81457 roc_auc 0.78863 prc_auc 0.81262[0m
[93maverage test of epoch 44: loss -28.70783 acc 0.75676 roc_auc 0.94667 prc_auc 0.97741[0m
[92maverage training of epoch 45: loss -29.33258 acc 0.81457 roc_auc 0.78275 prc_auc 0.81217[0m
[93maverage test of epoch 45: loss -29.69043 acc 0.78378 roc_auc 0.94667 prc_auc 0.97741[0m
[92maverage training of epoch 46: loss -30.25669 acc 0.80795 roc_auc 0.79725 prc_auc 0.82091[0m
[93maverage test of epoch 46: loss -30.73009 acc 0.81081 roc_auc 0.94667 prc_auc 0.97741[0m
[92maverage training of epoch 47: loss -31.25591 acc 0.82119 roc_auc 0.80824 prc_auc 0.82649[0m
[93maverage test of epoch 47: loss -31.78211 acc 0.83784 roc_auc 0.94000 prc_auc 0.97546[0m
[92maverage training of epoch 48: loss -32.31866 acc 0.84768 roc_auc 0.80039 prc_auc 0.81966[0m
[93maverage test of epoch 48: loss -32.78614 acc 0.83784 roc_auc 0.93667 prc_auc 0.97457[0m
[92maverage training of epoch 49: loss -33.35346 acc 0.85430 roc_auc 0.79569 prc_auc 0.81835[0m
[93maverage test of epoch 49: loss -33.81880 acc 0.83784 roc_auc 0.94000 prc_auc 0.97598[0m
Run statistics: 
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 1)
('lr', 0.0003)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))


Accuracy (avg): 0.69744 ROC_AUC (avg): 0.588 PRC_AUC (avg): 0.72507 

Average forward propagation time taken(ms): 2.791424162484556
Average backward propagation time taken(ms): 0.9399338279000052

