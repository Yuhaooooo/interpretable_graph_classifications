# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======






Params Turning Set
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 16)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 1)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))






args.base_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/
args.graph_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/
args.min_dfscode_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/
args.min_dfscode_tensor_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/
*** 1 train_index:  [  0   2   3   6   9  10  11  12  13  14  15  16  17  19  20  21  22  23
  24  26  27  30  31  32  33  35  36  38  39  41  42  43  44  45  46  47
  48  49  50  52  53  54  56  57  59  60  61  62  63  64  65  66  67  69
  70  71  72  73  74  76  77  78  79  80  81  82  83  84  87  88  89  91
  92  94  95  96  97  98 100 101 102 103 104 106 107 108 109 110 112 113
 115 116 117 118 119 120 121 122 124 125 126 127 129 130 131 132 134 136
 138 139 140 141 142 144 145 146 147 148 149 151 152 153 154 155 156 157
 158 159 160 161 164 165 166 167 169 170 171 172 173 175 176 177 178 179
 180 182 184 185 186 187]
*** 2 test_index:  [  1   4   5   7   8  18  25  28  29  34  37  40  51  55  58  68  75  85
  86  90  93  99 105 111 114 123 128 133 135 137 143 150 162 163 168 174
 181 183]
*** 1 train_index:  [  0   1   2   4   5   6   7   8  10  11  12  13  14  15  16  17  18  19
  20  21  22  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39
  40  41  43  44  45  46  49  50  51  52  53  54  55  56  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  74  75  76  77  79  80  81  82
  83  84  85  86  88  90  91  92  93  94  96  97  98  99 101 102 103 104
 105 106 107 111 112 113 114 115 120 121 122 123 124 125 126 128 129 130
 131 133 134 135 136 137 138 139 140 143 144 145 147 148 150 153 154 155
 156 159 162 163 164 165 167 168 169 170 171 172 173 174 175 177 178 179
 180 181 182 183 186 187]
*** 2 test_index:  [  3   9  23  31  42  47  48  57  72  73  78  87  89  95 100 108 109 110
 116 117 118 119 127 132 141 142 146 149 151 152 157 158 160 161 166 176
 184 185]
*** 1 train_index:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19
  20  23  24  25  26  28  29  31  32  34  35  36  37  38  40  41  42  43
  44  47  48  51  53  54  55  56  57  58  59  60  64  66  68  70  71  72
  73  74  75  76  77  78  79  81  82  84  85  86  87  88  89  90  92  93
  95  96  97  98  99 100 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134
 135 137 139 140 141 142 143 144 146 148 149 150 151 152 153 156 157 158
 159 160 161 162 163 165 166 167 168 169 171 172 173 174 175 176 177 178
 179 181 183 184 185 186]
*** 2 test_index:  [  0  16  21  22  27  30  33  39  45  46  49  50  52  61  62  63  65  67
  69  80  83  91  94 101 102 103 122 136 138 145 147 154 155 164 170 180
 182 187]
*** 1 train_index:  [  0   1   3   4   5   6   7   8   9  13  14  16  18  19  21  22  23  25
  26  27  28  29  30  31  33  34  36  37  38  39  40  42  43  45  46  47
  48  49  50  51  52  53  55  56  57  58  60  61  62  63  65  66  67  68
  69  71  72  73  75  76  77  78  79  80  81  83  84  85  86  87  88  89
  90  91  92  93  94  95  97  98  99 100 101 102 103 105 106 108 109 110
 111 113 114 115 116 117 118 119 120 121 122 123 125 127 128 131 132 133
 134 135 136 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152
 154 155 156 157 158 160 161 162 163 164 165 166 168 170 171 174 175 176
 180 181 182 183 184 185 187]
*** 2 test_index:  [  2  10  11  12  15  17  20  24  32  35  41  44  54  59  64  70  74  82
  96 104 107 112 124 126 129 130 139 153 159 167 169 172 173 177 178 179
 186]
*** 1 train_index:  [  0   1   2   3   4   5   7   8   9  10  11  12  15  16  17  18  20  21
  22  23  24  25  27  28  29  30  31  32  33  34  35  37  39  40  41  42
  44  45  46  47  48  49  50  51  52  54  55  57  58  59  61  62  63  64
  65  67  68  69  70  72  73  74  75  78  80  82  83  85  86  87  89  90
  91  93  94  95  96  99 100 101 102 103 104 105 107 108 109 110 111 112
 114 116 117 118 119 122 123 124 126 127 128 129 130 132 133 135 136 137
 138 139 141 142 143 145 146 147 149 150 151 152 153 154 155 157 158 159
 160 161 162 163 164 166 167 168 169 170 172 173 174 176 177 178 179 180
 181 182 183 184 185 186 187]
*** 2 test_index:  [  6  13  14  19  26  36  38  43  53  56  60  66  71  76  77  79  81  84
  88  92  97  98 106 113 115 120 121 125 131 134 140 144 148 156 165 171
 175]
{'base_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/',
 'batch_size': 1,
 'clean_temp': False,
 'clean_tensorboard': False,
 'current_dataset_path': None,
 'current_min_dfscode_path': None,
 'current_model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-18-23-19/',
 'current_processed_dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'current_temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-18-23-19/',
 'dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/datasets/',
 'device': device(type='cuda', index=0),
 'dfscode_rnn_dropout': 0.2,
 'embedding_size_dfscode_rnn': 16,
 'epochs': 50,
 'epochs_end': 10000,
 'epochs_save': 20,
 'epochs_validate': 1,
 'fname': 'DFScodeRNN_cls_LSTM_MUTAG',
 'gradient_clipping': True,
 'graph_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/',
 'graph_type': 'MUTAG',
 'graphgen_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/',
 'hidden_size_dfscode_rnn': 4,
 'load_device': device(type='cuda', index=0),
 'load_model': False,
 'load_model_path': '',
 'log_tensorboard': False,
 'lr': 0.0001,
 'max_prev_node': None,
 'min_dfscode_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/',
 'min_dfscode_tensor_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/',
 'note': 'DFScodeRNN_cls_LSTM',
 'num_graphs': None,
 'num_layers': 1,
 'num_workers': 8,
 'number_of_mlp_layer': 1,
 'rnn_type': 'LSTM',
 'save_model': False,
 'temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/',
 'tensorboard_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tensorboard/',
 'time': '2021-01-17-18-23-19',
 'weights': False}


graphgen args.__dict__: None




dataset_features: {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}




config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 50, 'learning_rate': 0.0001, 'seed': 1800, 'k_fold': 5, 'model': 'DFScodeRNN_cls_LSTM', 'dataset': 'MUTAG'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}}


Training a new model: DFScodeRNN_cls_LSTM
Training model with dataset, testing using fold 0
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.34835 acc 0.66667 roc_auc 0.43760 prc_auc 0.64968[0m
[93maverage test of epoch 0: loss -0.45235 acc 0.65789 roc_auc 0.74462 prc_auc 0.87061[0m
[92maverage training of epoch 1: loss -0.57489 acc 0.66667 roc_auc 0.44860 prc_auc 0.66055[0m
[93maverage test of epoch 1: loss -0.68678 acc 0.65789 roc_auc 0.87077 prc_auc 0.92518[0m
[92maverage training of epoch 2: loss -0.81973 acc 0.66667 roc_auc 0.45820 prc_auc 0.67200[0m
[93maverage test of epoch 2: loss -0.93527 acc 0.65789 roc_auc 0.77846 prc_auc 0.90128[0m
[92maverage training of epoch 3: loss -1.07256 acc 0.66667 roc_auc 0.46820 prc_auc 0.68022[0m
[93maverage test of epoch 3: loss -1.18682 acc 0.65789 roc_auc 0.84000 prc_auc 0.92129[0m
[92maverage training of epoch 4: loss -1.31359 acc 0.66667 roc_auc 0.51380 prc_auc 0.73119[0m
[93maverage test of epoch 4: loss -1.40477 acc 0.65789 roc_auc 0.89538 prc_auc 0.94416[0m
[92maverage training of epoch 5: loss -1.52795 acc 0.66667 roc_auc 0.53960 prc_auc 0.74871[0m
[93maverage test of epoch 5: loss -1.61928 acc 0.65789 roc_auc 0.88923 prc_auc 0.94137[0m
[92maverage training of epoch 6: loss -1.75210 acc 0.66667 roc_auc 0.55920 prc_auc 0.76458[0m
[93maverage test of epoch 6: loss -1.85050 acc 0.65789 roc_auc 0.87692 prc_auc 0.93740[0m
[92maverage training of epoch 7: loss -1.99348 acc 0.66667 roc_auc 0.57800 prc_auc 0.77992[0m
[93maverage test of epoch 7: loss -2.09840 acc 0.65789 roc_auc 0.86462 prc_auc 0.93326[0m
[92maverage training of epoch 8: loss -2.24809 acc 0.66667 roc_auc 0.59860 prc_auc 0.79534[0m
[93maverage test of epoch 8: loss -2.35338 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 9: loss -2.50297 acc 0.66667 roc_auc 0.61900 prc_auc 0.80328[0m
[93maverage test of epoch 9: loss -2.59890 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 10: loss -2.74319 acc 0.66667 roc_auc 0.60420 prc_auc 0.78516[0m
[93maverage test of epoch 10: loss -2.82586 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 11: loss -2.96364 acc 0.66667 roc_auc 0.56440 prc_auc 0.75301[0m
[93maverage test of epoch 11: loss -3.03563 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 12: loss -3.17020 acc 0.66667 roc_auc 0.52800 prc_auc 0.72486[0m
[93maverage test of epoch 12: loss -3.23562 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 13: loss -3.37033 acc 0.66667 roc_auc 0.50160 prc_auc 0.70706[0m
[93maverage test of epoch 13: loss -3.43166 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 14: loss -3.63987 acc 0.66667 roc_auc 0.48480 prc_auc 0.69222[0m
[93maverage test of epoch 14: loss -3.77864 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 15: loss -3.98259 acc 0.66667 roc_auc 0.47240 prc_auc 0.68276[0m
[93maverage test of epoch 15: loss -4.10445 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 16: loss -4.30173 acc 0.66667 roc_auc 0.46260 prc_auc 0.67794[0m
[93maverage test of epoch 16: loss -4.41590 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 17: loss -4.60894 acc 0.66667 roc_auc 0.45560 prc_auc 0.67189[0m
[93maverage test of epoch 17: loss -4.71705 acc 0.65789 roc_auc 0.85846 prc_auc 0.93153[0m
[92maverage training of epoch 18: loss -4.90666 acc 0.66667 roc_auc 0.45120 prc_auc 0.66959[0m
[93maverage test of epoch 18: loss -5.00952 acc 0.65789 roc_auc 0.85846 prc_auc 0.93153[0m
[92maverage training of epoch 19: loss -5.19657 acc 0.66667 roc_auc 0.44780 prc_auc 0.66772[0m
[93maverage test of epoch 19: loss -5.29512 acc 0.65789 roc_auc 0.86000 prc_auc 0.93153[0m
[92maverage training of epoch 20: loss -5.47978 acc 0.66667 roc_auc 0.44520 prc_auc 0.66603[0m
[93maverage test of epoch 20: loss -5.57401 acc 0.65789 roc_auc 0.85846 prc_auc 0.93111[0m
[92maverage training of epoch 21: loss -5.75678 acc 0.66667 roc_auc 0.44290 prc_auc 0.66376[0m
[93maverage test of epoch 21: loss -5.84753 acc 0.65789 roc_auc 0.86000 prc_auc 0.93148[0m
[92maverage training of epoch 22: loss -6.02914 acc 0.66667 roc_auc 0.43990 prc_auc 0.65959[0m
[93maverage test of epoch 22: loss -6.11713 acc 0.65789 roc_auc 0.86000 prc_auc 0.93165[0m
[92maverage training of epoch 23: loss -6.29816 acc 0.66667 roc_auc 0.43840 prc_auc 0.65885[0m
[93maverage test of epoch 23: loss -6.38396 acc 0.65789 roc_auc 0.85846 prc_auc 0.93184[0m
[92maverage training of epoch 24: loss -6.56489 acc 0.66667 roc_auc 0.43720 prc_auc 0.65276[0m
[93maverage test of epoch 24: loss -6.64894 acc 0.65789 roc_auc 0.86000 prc_auc 0.93187[0m
[92maverage training of epoch 25: loss -6.83015 acc 0.66667 roc_auc 0.43570 prc_auc 0.65069[0m
[93maverage test of epoch 25: loss -6.91282 acc 0.65789 roc_auc 0.86615 prc_auc 0.93320[0m
[92maverage training of epoch 26: loss -7.09465 acc 0.66667 roc_auc 0.43460 prc_auc 0.64931[0m
[93maverage test of epoch 26: loss -7.17623 acc 0.65789 roc_auc 0.86615 prc_auc 0.93303[0m
[92maverage training of epoch 27: loss -7.35896 acc 0.66667 roc_auc 0.43210 prc_auc 0.64729[0m
[93maverage test of epoch 27: loss -7.43971 acc 0.65789 roc_auc 0.86308 prc_auc 0.90724[0m
[92maverage training of epoch 28: loss -7.62352 acc 0.66667 roc_auc 0.43070 prc_auc 0.64662[0m
[93maverage test of epoch 28: loss -7.70370 acc 0.65789 roc_auc 0.84000 prc_auc 0.89564[0m
[92maverage training of epoch 29: loss -7.88877 acc 0.66667 roc_auc 0.43070 prc_auc 0.64662[0m
[93maverage test of epoch 29: loss -7.96857 acc 0.65789 roc_auc 0.88462 prc_auc 0.92019[0m
[92maverage training of epoch 30: loss -8.15510 acc 0.66667 roc_auc 0.43040 prc_auc 0.64640[0m
[93maverage test of epoch 30: loss -8.23467 acc 0.65789 roc_auc 0.87538 prc_auc 0.89850[0m
[92maverage training of epoch 31: loss -8.42279 acc 0.66667 roc_auc 0.43020 prc_auc 0.64623[0m
[93maverage test of epoch 31: loss -8.50225 acc 0.65789 roc_auc 0.85231 prc_auc 0.88546[0m
[92maverage training of epoch 32: loss -8.69211 acc 0.66667 roc_auc 0.42950 prc_auc 0.64600[0m
[93maverage test of epoch 32: loss -8.77156 acc 0.65789 roc_auc 0.89231 prc_auc 0.90896[0m
[92maverage training of epoch 33: loss -8.96328 acc 0.66667 roc_auc 0.42890 prc_auc 0.64570[0m
[93maverage test of epoch 33: loss -9.04280 acc 0.65789 roc_auc 0.82308 prc_auc 0.85662[0m
[92maverage training of epoch 34: loss -9.23648 acc 0.66667 roc_auc 0.42840 prc_auc 0.64214[0m
[93maverage test of epoch 34: loss -9.31615 acc 0.65789 roc_auc 0.65385 prc_auc 0.73529[0m
[92maverage training of epoch 35: loss -9.51188 acc 0.66667 roc_auc 0.42760 prc_auc 0.63951[0m
[93maverage test of epoch 35: loss -9.59176 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 36: loss -9.78963 acc 0.66667 roc_auc 0.42700 prc_auc 0.63818[0m
[93maverage test of epoch 36: loss -9.86977 acc 0.65789 roc_auc 0.84154 prc_auc 0.87989[0m
[92maverage training of epoch 37: loss -10.06984 acc 0.66667 roc_auc 0.42690 prc_auc 0.63812[0m
[93maverage test of epoch 37: loss -10.15029 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 38: loss -10.35264 acc 0.66667 roc_auc 0.42650 prc_auc 0.63782[0m
[93maverage test of epoch 38: loss -10.43343 acc 0.65789 roc_auc 0.76615 prc_auc 0.81086[0m
[92maverage training of epoch 39: loss -10.63811 acc 0.66667 roc_auc 0.42650 prc_auc 0.63806[0m
[93maverage test of epoch 39: loss -10.71927 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 40: loss -10.92634 acc 0.66667 roc_auc 0.42600 prc_auc 0.63723[0m
[93maverage test of epoch 40: loss -11.00790 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 41: loss -11.21742 acc 0.66667 roc_auc 0.42590 prc_auc 0.63687[0m
[93maverage test of epoch 41: loss -11.29940 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -11.51139 acc 0.66667 roc_auc 0.42590 prc_auc 0.63693[0m
[93maverage test of epoch 42: loss -11.59381 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -11.80828 acc 0.66667 roc_auc 0.42550 prc_auc 0.63607[0m
[93maverage test of epoch 43: loss -11.89100 acc 0.65789 roc_auc 0.80462 prc_auc 0.84026[0m
[92maverage training of epoch 44: loss -12.10774 acc 0.66667 roc_auc 0.42420 prc_auc 0.63516[0m
[93maverage test of epoch 44: loss -12.19065 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -12.40972 acc 0.66667 roc_auc 0.42420 prc_auc 0.63528[0m
[93maverage test of epoch 45: loss -12.49287 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -12.71435 acc 0.66667 roc_auc 0.42160 prc_auc 0.63279[0m
[93maverage test of epoch 46: loss -12.79778 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -13.02173 acc 0.66667 roc_auc 0.42210 prc_auc 0.63291[0m
[93maverage test of epoch 47: loss -13.10548 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -13.33195 acc 0.66667 roc_auc 0.42130 prc_auc 0.63252[0m
[93maverage test of epoch 48: loss -13.41606 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -13.64510 acc 0.66667 roc_auc 0.42180 prc_auc 0.63297[0m
[93maverage test of epoch 49: loss -13.72957 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 1
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.13990 acc 0.66667 roc_auc 0.42500 prc_auc 0.61837[0m
[93maverage test of epoch 0: loss -0.17275 acc 0.65789 roc_auc 0.16308 prc_auc 0.49746[0m
[92maverage training of epoch 1: loss -0.22550 acc 0.66667 roc_auc 0.42620 prc_auc 0.61869[0m
[93maverage test of epoch 1: loss -0.25862 acc 0.65789 roc_auc 0.14462 prc_auc 0.49565[0m
[92maverage training of epoch 2: loss -0.31182 acc 0.66667 roc_auc 0.43110 prc_auc 0.62036[0m
[93maverage test of epoch 2: loss -0.34708 acc 0.65789 roc_auc 0.14769 prc_auc 0.49538[0m
[92maverage training of epoch 3: loss -0.39988 acc 0.66667 roc_auc 0.43580 prc_auc 0.62152[0m
[93maverage test of epoch 3: loss -0.43741 acc 0.65789 roc_auc 0.15077 prc_auc 0.49557[0m
[92maverage training of epoch 4: loss -0.48947 acc 0.66667 roc_auc 0.44500 prc_auc 0.62690[0m
[93maverage test of epoch 4: loss -0.52976 acc 0.65789 roc_auc 0.16308 prc_auc 0.49842[0m
[92maverage training of epoch 5: loss -0.58092 acc 0.66667 roc_auc 0.45920 prc_auc 0.63779[0m
[93maverage test of epoch 5: loss -0.62425 acc 0.65789 roc_auc 0.63077 prc_auc 0.84105[0m
[92maverage training of epoch 6: loss -0.67581 acc 0.66667 roc_auc 0.46780 prc_auc 0.65721[0m
[93maverage test of epoch 6: loss -0.72340 acc 0.65789 roc_auc 0.87231 prc_auc 0.95188[0m
[92maverage training of epoch 7: loss -0.77883 acc 0.66667 roc_auc 0.47510 prc_auc 0.66195[0m
[93maverage test of epoch 7: loss -0.83475 acc 0.65789 roc_auc 0.91385 prc_auc 0.95365[0m
[92maverage training of epoch 8: loss -0.90248 acc 0.66667 roc_auc 0.47760 prc_auc 0.66518[0m
[93maverage test of epoch 8: loss -0.97898 acc 0.65789 roc_auc 0.89846 prc_auc 0.94428[0m
[92maverage training of epoch 9: loss -1.08054 acc 0.66667 roc_auc 0.48420 prc_auc 0.67127[0m
[93maverage test of epoch 9: loss -1.20409 acc 0.65789 roc_auc 0.89231 prc_auc 0.93725[0m
[92maverage training of epoch 10: loss -1.33669 acc 0.66667 roc_auc 0.50760 prc_auc 0.68860[0m
[93maverage test of epoch 10: loss -1.45937 acc 0.65789 roc_auc 0.88923 prc_auc 0.93317[0m
[92maverage training of epoch 11: loss -1.55282 acc 0.66667 roc_auc 0.50620 prc_auc 0.68633[0m
[93maverage test of epoch 11: loss -1.63478 acc 0.65789 roc_auc 0.89846 prc_auc 0.94003[0m
[92maverage training of epoch 12: loss -1.71304 acc 0.66667 roc_auc 0.49480 prc_auc 0.67936[0m
[93maverage test of epoch 12: loss -1.78212 acc 0.65789 roc_auc 0.90154 prc_auc 0.94336[0m
[92maverage training of epoch 13: loss -1.85562 acc 0.66667 roc_auc 0.48680 prc_auc 0.67288[0m
[93maverage test of epoch 13: loss -1.91977 acc 0.65789 roc_auc 0.90154 prc_auc 0.94367[0m
[92maverage training of epoch 14: loss -1.99118 acc 0.66667 roc_auc 0.48020 prc_auc 0.66569[0m
[93maverage test of epoch 14: loss -2.05284 acc 0.65789 roc_auc 0.90769 prc_auc 0.94930[0m
[92maverage training of epoch 15: loss -2.12324 acc 0.66667 roc_auc 0.47510 prc_auc 0.65875[0m
[93maverage test of epoch 15: loss -2.18347 acc 0.65789 roc_auc 0.90769 prc_auc 0.94930[0m
[92maverage training of epoch 16: loss -2.25343 acc 0.66667 roc_auc 0.47300 prc_auc 0.65813[0m
[93maverage test of epoch 16: loss -2.31279 acc 0.65789 roc_auc 0.90462 prc_auc 0.94797[0m
[92maverage training of epoch 17: loss -2.38265 acc 0.66667 roc_auc 0.47120 prc_auc 0.65694[0m
[93maverage test of epoch 17: loss -2.44146 acc 0.65789 roc_auc 0.90769 prc_auc 0.94943[0m
[92maverage training of epoch 18: loss -2.51143 acc 0.66667 roc_auc 0.46940 prc_auc 0.65529[0m
[93maverage test of epoch 18: loss -2.56988 acc 0.65789 roc_auc 0.89692 prc_auc 0.93753[0m
[92maverage training of epoch 19: loss -2.64012 acc 0.66667 roc_auc 0.46820 prc_auc 0.65445[0m
[93maverage test of epoch 19: loss -2.69834 acc 0.65789 roc_auc 0.89231 prc_auc 0.93772[0m
[92maverage training of epoch 20: loss -2.76894 acc 0.66667 roc_auc 0.46680 prc_auc 0.65320[0m
[93maverage test of epoch 20: loss -2.82701 acc 0.65789 roc_auc 0.89385 prc_auc 0.93800[0m
[92maverage training of epoch 21: loss -2.89807 acc 0.66667 roc_auc 0.46580 prc_auc 0.65230[0m
[93maverage test of epoch 21: loss -2.95606 acc 0.65789 roc_auc 0.89385 prc_auc 0.93800[0m
[92maverage training of epoch 22: loss -3.02764 acc 0.66667 roc_auc 0.46480 prc_auc 0.65135[0m
[93maverage test of epoch 22: loss -3.08561 acc 0.65789 roc_auc 0.89077 prc_auc 0.93595[0m
[92maverage training of epoch 23: loss -3.15777 acc 0.66667 roc_auc 0.46360 prc_auc 0.65046[0m
[93maverage test of epoch 23: loss -3.21576 acc 0.65789 roc_auc 0.89077 prc_auc 0.93664[0m
[92maverage training of epoch 24: loss -3.28857 acc 0.66667 roc_auc 0.46220 prc_auc 0.64954[0m
[93maverage test of epoch 24: loss -3.34663 acc 0.65789 roc_auc 0.89077 prc_auc 0.93382[0m
[92maverage training of epoch 25: loss -3.42013 acc 0.66667 roc_auc 0.46050 prc_auc 0.64776[0m
[93maverage test of epoch 25: loss -3.47830 acc 0.65789 roc_auc 0.89385 prc_auc 0.93715[0m
[92maverage training of epoch 26: loss -3.55254 acc 0.66667 roc_auc 0.45960 prc_auc 0.64723[0m
[93maverage test of epoch 26: loss -3.61085 acc 0.65789 roc_auc 0.88769 prc_auc 0.93177[0m
[92maverage training of epoch 27: loss -3.68589 acc 0.66667 roc_auc 0.45900 prc_auc 0.64686[0m
[93maverage test of epoch 27: loss -3.74438 acc 0.65789 roc_auc 0.89846 prc_auc 0.93675[0m
[92maverage training of epoch 28: loss -3.82024 acc 0.66667 roc_auc 0.45820 prc_auc 0.64618[0m
[93maverage test of epoch 28: loss -3.87893 acc 0.65789 roc_auc 0.88769 prc_auc 0.93103[0m
[92maverage training of epoch 29: loss -3.95560 acc 0.66667 roc_auc 0.45790 prc_auc 0.64578[0m
[93maverage test of epoch 29: loss -4.01441 acc 0.65789 roc_auc 0.90308 prc_auc 0.94406[0m
[92maverage training of epoch 30: loss -4.09175 acc 0.66667 roc_auc 0.45760 prc_auc 0.64563[0m
[93maverage test of epoch 30: loss -4.15056 acc 0.65789 roc_auc 0.89538 prc_auc 0.93192[0m
[92maverage training of epoch 31: loss -4.22855 acc 0.66667 roc_auc 0.45680 prc_auc 0.64512[0m
[93maverage test of epoch 31: loss -4.28738 acc 0.65789 roc_auc 0.89231 prc_auc 0.92875[0m
[92maverage training of epoch 32: loss -4.36609 acc 0.66667 roc_auc 0.45660 prc_auc 0.64507[0m
[93maverage test of epoch 32: loss -4.42501 acc 0.65789 roc_auc 0.89077 prc_auc 0.92635[0m
[92maverage training of epoch 33: loss -4.50452 acc 0.66667 roc_auc 0.45640 prc_auc 0.64518[0m
[93maverage test of epoch 33: loss -4.56358 acc 0.65789 roc_auc 0.88923 prc_auc 0.92499[0m
[92maverage training of epoch 34: loss -4.64393 acc 0.66667 roc_auc 0.45630 prc_auc 0.64483[0m
[93maverage test of epoch 34: loss -4.70318 acc 0.65789 roc_auc 0.88923 prc_auc 0.92349[0m
[92maverage training of epoch 35: loss -4.78442 acc 0.66667 roc_auc 0.45620 prc_auc 0.64483[0m
[93maverage test of epoch 35: loss -4.84390 acc 0.65789 roc_auc 0.89077 prc_auc 0.92425[0m
[92maverage training of epoch 36: loss -4.92608 acc 0.66667 roc_auc 0.45600 prc_auc 0.64477[0m
[93maverage test of epoch 36: loss -4.98583 acc 0.65789 roc_auc 0.87538 prc_auc 0.90048[0m
[92maverage training of epoch 37: loss -5.06897 acc 0.66667 roc_auc 0.45580 prc_auc 0.64474[0m
[93maverage test of epoch 37: loss -5.12900 acc 0.65789 roc_auc 0.88923 prc_auc 0.92349[0m
[92maverage training of epoch 38: loss -5.21315 acc 0.66667 roc_auc 0.45580 prc_auc 0.64474[0m
[93maverage test of epoch 38: loss -5.27350 acc 0.65789 roc_auc 0.87538 prc_auc 0.90094[0m
[92maverage training of epoch 39: loss -5.35867 acc 0.66667 roc_auc 0.45580 prc_auc 0.64474[0m
[93maverage test of epoch 39: loss -5.41936 acc 0.65789 roc_auc 0.86769 prc_auc 0.89651[0m
[92maverage training of epoch 40: loss -5.50558 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 40: loss -5.56662 acc 0.65789 roc_auc 0.88923 prc_auc 0.91671[0m
[92maverage training of epoch 41: loss -5.65391 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 41: loss -5.71532 acc 0.65789 roc_auc 0.88923 prc_auc 0.91150[0m
[92maverage training of epoch 42: loss -5.80370 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 42: loss -5.86549 acc 0.65789 roc_auc 0.87231 prc_auc 0.90062[0m
[92maverage training of epoch 43: loss -5.95498 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 43: loss -6.01716 acc 0.65789 roc_auc 0.86769 prc_auc 0.89556[0m
[92maverage training of epoch 44: loss -6.10777 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 44: loss -6.17034 acc 0.65789 roc_auc 0.87077 prc_auc 0.89689[0m
[92maverage training of epoch 45: loss -6.26209 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 45: loss -6.32506 acc 0.65789 roc_auc 0.88615 prc_auc 0.90595[0m
[92maverage training of epoch 46: loss -6.41795 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 46: loss -6.48133 acc 0.65789 roc_auc 0.89231 prc_auc 0.91449[0m
[92maverage training of epoch 47: loss -6.57538 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 47: loss -6.63918 acc 0.65789 roc_auc 0.86000 prc_auc 0.88857[0m
[92maverage training of epoch 48: loss -6.73440 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 48: loss -6.79860 acc 0.65789 roc_auc 0.85538 prc_auc 0.87407[0m
[92maverage training of epoch 49: loss -6.89500 acc 0.66667 roc_auc 0.45560 prc_auc 0.63977[0m
[93maverage test of epoch 49: loss -6.95962 acc 0.65789 roc_auc 0.89077 prc_auc 0.90676[0m
Training model with dataset, testing using fold 2
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.24475 acc 0.33333 roc_auc 0.41300 prc_auc 0.60643[0m
[93maverage test of epoch 0: loss -0.28349 acc 0.34211 roc_auc 0.55231 prc_auc 0.78687[0m
[92maverage training of epoch 1: loss -0.28284 acc 0.33333 roc_auc 0.37240 prc_auc 0.58699[0m
[93maverage test of epoch 1: loss -0.30083 acc 0.34211 roc_auc 0.54000 prc_auc 0.78012[0m
[92maverage training of epoch 2: loss -0.30069 acc 0.33333 roc_auc 0.36560 prc_auc 0.58017[0m
[93maverage test of epoch 2: loss -0.31740 acc 0.34211 roc_auc 0.53231 prc_auc 0.77764[0m
[92maverage training of epoch 3: loss -0.31802 acc 0.33333 roc_auc 0.36600 prc_auc 0.57981[0m
[93maverage test of epoch 3: loss -0.33368 acc 0.34211 roc_auc 0.52308 prc_auc 0.77712[0m
[92maverage training of epoch 4: loss -0.33508 acc 0.33333 roc_auc 0.36580 prc_auc 0.57799[0m
[93maverage test of epoch 4: loss -0.34963 acc 0.34211 roc_auc 0.53385 prc_auc 0.78882[0m
[92maverage training of epoch 5: loss -0.35184 acc 0.33333 roc_auc 0.36640 prc_auc 0.57852[0m
[93maverage test of epoch 5: loss -0.36526 acc 0.34211 roc_auc 0.53385 prc_auc 0.78882[0m
[92maverage training of epoch 6: loss -0.36833 acc 0.33333 roc_auc 0.36500 prc_auc 0.57781[0m
[93maverage test of epoch 6: loss -0.38059 acc 0.34211 roc_auc 0.53692 prc_auc 0.79789[0m
[92maverage training of epoch 7: loss -0.38431 acc 0.33333 roc_auc 0.36540 prc_auc 0.57794[0m
[93maverage test of epoch 7: loss -0.39579 acc 0.34211 roc_auc 0.56462 prc_auc 0.81008[0m
[92maverage training of epoch 8: loss -0.40002 acc 0.33333 roc_auc 0.36740 prc_auc 0.57725[0m
[93maverage test of epoch 8: loss -0.41074 acc 0.34211 roc_auc 0.59231 prc_auc 0.82029[0m
[92maverage training of epoch 9: loss -0.41547 acc 0.33333 roc_auc 0.37160 prc_auc 0.57937[0m
[93maverage test of epoch 9: loss -0.42557 acc 0.34211 roc_auc 0.65385 prc_auc 0.85453[0m
[92maverage training of epoch 10: loss -0.43082 acc 0.33333 roc_auc 0.37720 prc_auc 0.58212[0m
[93maverage test of epoch 10: loss -0.44033 acc 0.34211 roc_auc 0.69231 prc_auc 0.87184[0m
[92maverage training of epoch 11: loss -0.44552 acc 0.33333 roc_auc 0.37350 prc_auc 0.57381[0m
[93maverage test of epoch 11: loss -0.45239 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 12: loss -0.45695 acc 0.33333 roc_auc 0.37180 prc_auc 0.57359[0m
[93maverage test of epoch 12: loss -0.46341 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 13: loss -0.46806 acc 0.33333 roc_auc 0.37340 prc_auc 0.57373[0m
[93maverage test of epoch 13: loss -0.47442 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 14: loss -0.47915 acc 0.33333 roc_auc 0.37550 prc_auc 0.57429[0m
[93maverage test of epoch 14: loss -0.48544 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 15: loss -0.49023 acc 0.33333 roc_auc 0.37500 prc_auc 0.57406[0m
[93maverage test of epoch 15: loss -0.49645 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 16: loss -0.50128 acc 0.33333 roc_auc 0.37550 prc_auc 0.57451[0m
[93maverage test of epoch 16: loss -0.50747 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 17: loss -0.51236 acc 0.33333 roc_auc 0.37540 prc_auc 0.57426[0m
[93maverage test of epoch 17: loss -0.51848 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 18: loss -0.52344 acc 0.33333 roc_auc 0.37540 prc_auc 0.57426[0m
[93maverage test of epoch 18: loss -0.52950 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 19: loss -0.53451 acc 0.33333 roc_auc 0.37670 prc_auc 0.57458[0m
[93maverage test of epoch 19: loss -0.54052 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 20: loss -0.54556 acc 0.33333 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 20: loss -0.55153 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 21: loss -0.55661 acc 0.33333 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 21: loss -0.56255 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 22: loss -0.56766 acc 0.33333 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 22: loss -0.57357 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 23: loss -0.57871 acc 0.33333 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 23: loss -0.58458 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 24: loss -0.58975 acc 0.33333 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 24: loss -0.59560 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 25: loss -0.60080 acc 0.33333 roc_auc 0.37640 prc_auc 0.57439[0m
[93maverage test of epoch 25: loss -0.60661 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 26: loss -0.61185 acc 0.33333 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 26: loss -0.61763 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 27: loss -0.62290 acc 0.33333 roc_auc 0.37640 prc_auc 0.57448[0m
[93maverage test of epoch 27: loss -0.62865 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 28: loss -0.63394 acc 0.33333 roc_auc 0.37640 prc_auc 0.57448[0m
[93maverage test of epoch 28: loss -0.63966 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 29: loss -0.64499 acc 0.33333 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 29: loss -0.65068 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 30: loss -0.65604 acc 0.33333 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 30: loss -0.66170 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 31: loss -0.66709 acc 0.33333 roc_auc 0.37640 prc_auc 0.57448[0m
[93maverage test of epoch 31: loss -0.67271 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 32: loss -0.67814 acc 0.58000 roc_auc 0.37640 prc_auc 0.57445[0m
[93maverage test of epoch 32: loss -0.68373 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 33: loss -0.68918 acc 0.66667 roc_auc 0.37660 prc_auc 0.57445[0m
[93maverage test of epoch 33: loss -0.69474 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 34: loss -0.70023 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 34: loss -0.70576 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 35: loss -0.71128 acc 0.66667 roc_auc 0.37650 prc_auc 0.57445[0m
[93maverage test of epoch 35: loss -0.71678 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 36: loss -0.72233 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 36: loss -0.72779 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 37: loss -0.73337 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 37: loss -0.73881 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 38: loss -0.74442 acc 0.66667 roc_auc 0.37650 prc_auc 0.57448[0m
[93maverage test of epoch 38: loss -0.74983 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 39: loss -0.75547 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 39: loss -0.76084 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 40: loss -0.76652 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 40: loss -0.77186 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 41: loss -0.77756 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 41: loss -0.78287 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -0.78861 acc 0.66667 roc_auc 0.37640 prc_auc 0.57439[0m
[93maverage test of epoch 42: loss -0.79389 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -0.79966 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 43: loss -0.80491 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 44: loss -0.81071 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 44: loss -0.81592 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -0.82175 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 45: loss -0.82694 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -0.83280 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 46: loss -0.83795 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -0.84385 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 47: loss -0.84897 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -0.85490 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 48: loss -0.85999 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -0.86589 acc 0.66667 roc_auc 0.36890 prc_auc 0.57174[0m
[93maverage test of epoch 49: loss -0.87100 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 3
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.74547 acc 0.33775 roc_auc 0.46529 prc_auc 0.62572[0m
[93maverage test of epoch 0: loss 0.61892 acc 0.32432 roc_auc 0.32833 prc_auc 0.67325[0m
[92maverage training of epoch 1: loss 0.47883 acc 0.33775 roc_auc 0.49392 prc_auc 0.64874[0m
[93maverage test of epoch 1: loss 0.33289 acc 0.32432 roc_auc 0.15500 prc_auc 0.51313[0m
[92maverage training of epoch 2: loss 0.15326 acc 0.33775 roc_auc 0.49353 prc_auc 0.65700[0m
[93maverage test of epoch 2: loss -0.03397 acc 0.32432 roc_auc 0.15000 prc_auc 0.51156[0m
[92maverage training of epoch 3: loss -0.16248 acc 0.33775 roc_auc 0.37431 prc_auc 0.62332[0m
[93maverage test of epoch 3: loss -0.18835 acc 0.32432 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 4: loss -0.19674 acc 0.34437 roc_auc 0.41549 prc_auc 0.62725[0m
[93maverage test of epoch 4: loss -0.20035 acc 0.32432 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 5: loss -0.20857 acc 0.35099 roc_auc 0.42735 prc_auc 0.63176[0m
[93maverage test of epoch 5: loss -0.21244 acc 0.35135 roc_auc 0.54000 prc_auc 0.70162[0m
[92maverage training of epoch 6: loss -0.22033 acc 0.35099 roc_auc 0.46010 prc_auc 0.64872[0m
[93maverage test of epoch 6: loss -0.22476 acc 0.35135 roc_auc 0.54000 prc_auc 0.70162[0m
[92maverage training of epoch 7: loss -0.23236 acc 0.35099 roc_auc 0.42108 prc_auc 0.63651[0m
[93maverage test of epoch 7: loss -0.23733 acc 0.35135 roc_auc 0.54000 prc_auc 0.70162[0m
[92maverage training of epoch 8: loss -0.24478 acc 0.35762 roc_auc 0.44039 prc_auc 0.66462[0m
[93maverage test of epoch 8: loss -0.25055 acc 0.35135 roc_auc 0.54000 prc_auc 0.70162[0m
[92maverage training of epoch 9: loss -0.26262 acc 0.36424 roc_auc 0.67980 prc_auc 0.82174[0m
[93maverage test of epoch 9: loss -0.27943 acc 0.37838 roc_auc 0.87000 prc_auc 0.92071[0m
[92maverage training of epoch 10: loss -0.32664 acc 0.56954 roc_auc 0.64196 prc_auc 0.82665[0m
[93maverage test of epoch 10: loss -0.35771 acc 0.67568 roc_auc 0.85000 prc_auc 0.91538[0m
[92maverage training of epoch 11: loss -0.41457 acc 0.66225 roc_auc 0.72706 prc_auc 0.86907[0m
[93maverage test of epoch 11: loss -0.44771 acc 0.67568 roc_auc 0.85667 prc_auc 0.91693[0m
[92maverage training of epoch 12: loss -0.50379 acc 0.66225 roc_auc 0.78059 prc_auc 0.89112[0m
[93maverage test of epoch 12: loss -0.54325 acc 0.67568 roc_auc 0.85667 prc_auc 0.91693[0m
[92maverage training of epoch 13: loss -0.60036 acc 0.66225 roc_auc 0.81196 prc_auc 0.89870[0m
[93maverage test of epoch 13: loss -0.65247 acc 0.67568 roc_auc 0.85667 prc_auc 0.91693[0m
[92maverage training of epoch 14: loss -0.71933 acc 0.66225 roc_auc 0.82118 prc_auc 0.89691[0m
[93maverage test of epoch 14: loss -0.79005 acc 0.67568 roc_auc 0.86000 prc_auc 0.92137[0m
[92maverage training of epoch 15: loss -0.86862 acc 0.66225 roc_auc 0.83843 prc_auc 0.89859[0m
[93maverage test of epoch 15: loss -0.95239 acc 0.67568 roc_auc 0.86000 prc_auc 0.92137[0m
[92maverage training of epoch 16: loss -1.02185 acc 0.66225 roc_auc 0.86784 prc_auc 0.90774[0m
[93maverage test of epoch 16: loss -1.10010 acc 0.67568 roc_auc 0.85667 prc_auc 0.92034[0m
[92maverage training of epoch 17: loss -1.14881 acc 0.66225 roc_auc 0.87902 prc_auc 0.91453[0m
[93maverage test of epoch 17: loss -1.21921 acc 0.67568 roc_auc 0.85667 prc_auc 0.92034[0m
[92maverage training of epoch 18: loss -1.25336 acc 0.66225 roc_auc 0.87784 prc_auc 0.91198[0m
[93maverage test of epoch 18: loss -1.32101 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 19: loss -1.34660 acc 0.66225 roc_auc 0.87686 prc_auc 0.91024[0m
[93maverage test of epoch 19: loss -1.41424 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 20: loss -1.43448 acc 0.66225 roc_auc 0.87431 prc_auc 0.90657[0m
[93maverage test of epoch 20: loss -1.50323 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 21: loss -1.51988 acc 0.66225 roc_auc 0.87431 prc_auc 0.90560[0m
[93maverage test of epoch 21: loss -1.59003 acc 0.67568 roc_auc 0.85667 prc_auc 0.92337[0m
[92maverage training of epoch 22: loss -1.60413 acc 0.66225 roc_auc 0.87255 prc_auc 0.90364[0m
[93maverage test of epoch 22: loss -1.67594 acc 0.67568 roc_auc 0.85667 prc_auc 0.92337[0m
[92maverage training of epoch 23: loss -1.68809 acc 0.66225 roc_auc 0.87157 prc_auc 0.90319[0m
[93maverage test of epoch 23: loss -1.76161 acc 0.67568 roc_auc 0.85667 prc_auc 0.92337[0m
[92maverage training of epoch 24: loss -1.77221 acc 0.66225 roc_auc 0.87078 prc_auc 0.90269[0m
[93maverage test of epoch 24: loss -1.84751 acc 0.67568 roc_auc 0.86000 prc_auc 0.92585[0m
[92maverage training of epoch 25: loss -1.85688 acc 0.66225 roc_auc 0.87059 prc_auc 0.90303[0m
[93maverage test of epoch 25: loss -1.93389 acc 0.67568 roc_auc 0.86000 prc_auc 0.92585[0m
[92maverage training of epoch 26: loss -1.94227 acc 0.66225 roc_auc 0.86922 prc_auc 0.90248[0m
[93maverage test of epoch 26: loss -2.02098 acc 0.67568 roc_auc 0.86000 prc_auc 0.92585[0m
[92maverage training of epoch 27: loss -2.02857 acc 0.66225 roc_auc 0.86941 prc_auc 0.90378[0m
[93maverage test of epoch 27: loss -2.10892 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 28: loss -2.11589 acc 0.66225 roc_auc 0.87039 prc_auc 0.90542[0m
[93maverage test of epoch 28: loss -2.19791 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 29: loss -2.20433 acc 0.66225 roc_auc 0.87118 prc_auc 0.90745[0m
[93maverage test of epoch 29: loss -2.28792 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 30: loss -2.29390 acc 0.66225 roc_auc 0.87235 prc_auc 0.90839[0m
[93maverage test of epoch 30: loss -2.37901 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 31: loss -2.38467 acc 0.66225 roc_auc 0.87275 prc_auc 0.90884[0m
[93maverage test of epoch 31: loss -2.47120 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 32: loss -2.47681 acc 0.66225 roc_auc 0.87373 prc_auc 0.91032[0m
[93maverage test of epoch 32: loss -2.56444 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 33: loss -2.57034 acc 0.66225 roc_auc 0.87451 prc_auc 0.91185[0m
[93maverage test of epoch 33: loss -2.65882 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 34: loss -2.66555 acc 0.66225 roc_auc 0.87627 prc_auc 0.91318[0m
[93maverage test of epoch 34: loss -2.75443 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 35: loss -2.76256 acc 0.66225 roc_auc 0.87569 prc_auc 0.91405[0m
[93maverage test of epoch 35: loss -2.85148 acc 0.67568 roc_auc 0.86000 prc_auc 0.92282[0m
[92maverage training of epoch 36: loss -2.86154 acc 0.66225 roc_auc 0.87745 prc_auc 0.91784[0m
[93maverage test of epoch 36: loss -2.95027 acc 0.67568 roc_auc 0.86000 prc_auc 0.92282[0m
[92maverage training of epoch 37: loss -2.96253 acc 0.66225 roc_auc 0.87902 prc_auc 0.92013[0m
[93maverage test of epoch 37: loss -3.05100 acc 0.67568 roc_auc 0.86000 prc_auc 0.92282[0m
[92maverage training of epoch 38: loss -3.06541 acc 0.66225 roc_auc 0.88157 prc_auc 0.92156[0m
[93maverage test of epoch 38: loss -3.15380 acc 0.67568 roc_auc 0.86000 prc_auc 0.92282[0m
[92maverage training of epoch 39: loss -3.17008 acc 0.66225 roc_auc 0.88216 prc_auc 0.92100[0m
[93maverage test of epoch 39: loss -3.25867 acc 0.67568 roc_auc 0.85667 prc_auc 0.92034[0m
[92maverage training of epoch 40: loss -3.27645 acc 0.66225 roc_auc 0.88157 prc_auc 0.91862[0m
[93maverage test of epoch 40: loss -3.36546 acc 0.67568 roc_auc 0.85667 prc_auc 0.92034[0m
[92maverage training of epoch 41: loss -3.38440 acc 0.66225 roc_auc 0.88176 prc_auc 0.91647[0m
[93maverage test of epoch 41: loss -3.47404 acc 0.67568 roc_auc 0.85667 prc_auc 0.92034[0m
[92maverage training of epoch 42: loss -3.49389 acc 0.66225 roc_auc 0.88118 prc_auc 0.91422[0m
[93maverage test of epoch 42: loss -3.58437 acc 0.67568 roc_auc 0.85667 prc_auc 0.92034[0m
[92maverage training of epoch 43: loss -3.60496 acc 0.66225 roc_auc 0.88078 prc_auc 0.91318[0m
[93maverage test of epoch 43: loss -3.69644 acc 0.67568 roc_auc 0.85667 prc_auc 0.92034[0m
[92maverage training of epoch 44: loss -3.71773 acc 0.66225 roc_auc 0.88010 prc_auc 0.90998[0m
[93maverage test of epoch 44: loss -3.81028 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 45: loss -3.83228 acc 0.66225 roc_auc 0.88000 prc_auc 0.91004[0m
[93maverage test of epoch 45: loss -3.92598 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 46: loss -3.94874 acc 0.66225 roc_auc 0.87980 prc_auc 0.90790[0m
[93maverage test of epoch 46: loss -4.04366 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 47: loss -4.06721 acc 0.66225 roc_auc 0.87922 prc_auc 0.90713[0m
[93maverage test of epoch 47: loss -4.16349 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 48: loss -4.18787 acc 0.66225 roc_auc 0.87961 prc_auc 0.90764[0m
[93maverage test of epoch 48: loss -4.28564 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 49: loss -4.31099 acc 0.66225 roc_auc 0.87863 prc_auc 0.90696[0m
[93maverage test of epoch 49: loss -4.41039 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
Training model with dataset, testing using fold 4
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.62180 acc 0.33775 roc_auc 0.43569 prc_auc 0.64988[0m
[93maverage test of epoch 0: loss 0.54548 acc 0.32432 roc_auc 0.89333 prc_auc 0.95487[0m
[92maverage training of epoch 1: loss 0.44922 acc 0.33775 roc_auc 0.44196 prc_auc 0.65268[0m
[93maverage test of epoch 1: loss 0.39215 acc 0.32432 roc_auc 0.92333 prc_auc 0.96923[0m
[92maverage training of epoch 2: loss 0.29521 acc 0.33775 roc_auc 0.44549 prc_auc 0.65746[0m
[93maverage test of epoch 2: loss 0.23224 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 3: loss 0.13515 acc 0.33775 roc_auc 0.45020 prc_auc 0.66167[0m
[93maverage test of epoch 3: loss 0.06055 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 4: loss 0.01679 acc 0.33775 roc_auc 0.37706 prc_auc 0.57020[0m
[93maverage test of epoch 4: loss 0.01828 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0mUsing backend: pytorch
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

[92maverage training of epoch 5: loss 0.00265 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 5: loss 0.00715 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 6: loss -0.00844 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 6: loss -0.00399 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 7: loss -0.01954 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 7: loss -0.01514 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 8: loss -0.03063 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 8: loss -0.02628 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 9: loss -0.04173 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 9: loss -0.03742 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 10: loss -0.05282 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 10: loss -0.04856 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 11: loss -0.06392 acc 0.33775 roc_auc 0.36980 prc_auc 0.56840[0m
[93maverage test of epoch 11: loss -0.05971 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 12: loss -0.07502 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 12: loss -0.07085 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 13: loss -0.08611 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 13: loss -0.08199 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 14: loss -0.09721 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 14: loss -0.09314 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 15: loss -0.10831 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 15: loss -0.10428 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 16: loss -0.11940 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 16: loss -0.11543 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 17: loss -0.13050 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 17: loss -0.12657 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 18: loss -0.14160 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 18: loss -0.13772 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 19: loss -0.15270 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 19: loss -0.14886 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 20: loss -0.16379 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 20: loss -0.16001 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 21: loss -0.17489 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 21: loss -0.17115 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 22: loss -0.18599 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 22: loss -0.18230 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 23: loss -0.19709 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 23: loss -0.19344 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 24: loss -0.20819 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 24: loss -0.20459 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 25: loss -0.21928 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 25: loss -0.21573 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 26: loss -0.23038 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 26: loss -0.22688 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 27: loss -0.24148 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 27: loss -0.23802 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 28: loss -0.25258 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 28: loss -0.24917 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 29: loss -0.26368 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 29: loss -0.26031 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 30: loss -0.27477 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 30: loss -0.27146 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 31: loss -0.28587 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 31: loss -0.28261 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 32: loss -0.29697 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 32: loss -0.29375 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 33: loss -0.30807 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 33: loss -0.30490 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 34: loss -0.31917 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 34: loss -0.31604 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 35: loss -0.33027 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 35: loss -0.32719 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 36: loss -0.34136 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 36: loss -0.33833 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 37: loss -0.35246 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 37: loss -0.34948 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 38: loss -0.36356 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 38: loss -0.36062 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 39: loss -0.37466 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 39: loss -0.37177 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 40: loss -0.38576 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 40: loss -0.38291 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 41: loss -0.39685 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 41: loss -0.39406 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 42: loss -0.40795 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 42: loss -0.40520 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 43: loss -0.41905 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 43: loss -0.41635 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 44: loss -0.43015 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 44: loss -0.42750 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 45: loss -0.44125 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 45: loss -0.43864 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 46: loss -0.45234 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 46: loss -0.44979 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 47: loss -0.46344 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 47: loss -0.46093 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 48: loss -0.47454 acc 0.33775 roc_auc 0.36971 prc_auc 0.56808[0m
[93maverage test of epoch 48: loss -0.47208 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 49: loss -0.48564 acc 0.33775 roc_auc 0.36971 prc_auc 0.56808[0m
[93maverage test of epoch 49: loss -0.48322 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
Run statistics: 
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 16)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 1)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))


Accuracy (avg): 0.59474 ROC_AUC (avg): 0.64882 PRC_AUC (avg): 0.76352 

Average forward propagation time taken(ms): 2.806680480927785
Average backward propagation time taken(ms): 0.9397692482206336

