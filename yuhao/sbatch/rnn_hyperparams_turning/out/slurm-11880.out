# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======






Params Turning Set
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 2)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 1)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))






args.base_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/
args.graph_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/
args.min_dfscode_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/
args.min_dfscode_tensor_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/
*** 1 train_index:  [  0   2   3   6   9  10  11  12  13  14  15  16  17  19  20  21  22  23
  24  26  27  30  31  32  33  35  36  38  39  41  42  43  44  45  46  47
  48  49  50  52  53  54  56  57  59  60  61  62  63  64  65  66  67  69
  70  71  72  73  74  76  77  78  79  80  81  82  83  84  87  88  89  91
  92  94  95  96  97  98 100 101 102 103 104 106 107 108 109 110 112 113
 115 116 117 118 119 120 121 122 124 125 126 127 129 130 131 132 134 136
 138 139 140 141 142 144 145 146 147 148 149 151 152 153 154 155 156 157
 158 159 160 161 164 165 166 167 169 170 171 172 173 175 176 177 178 179
 180 182 184 185 186 187]
*** 2 test_index:  [  1   4   5   7   8  18  25  28  29  34  37  40  51  55  58  68  75  85
  86  90  93  99 105 111 114 123 128 133 135 137 143 150 162 163 168 174
 181 183]
*** 1 train_index:  [  0   1   2   4   5   6   7   8  10  11  12  13  14  15  16  17  18  19
  20  21  22  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39
  40  41  43  44  45  46  49  50  51  52  53  54  55  56  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  74  75  76  77  79  80  81  82
  83  84  85  86  88  90  91  92  93  94  96  97  98  99 101 102 103 104
 105 106 107 111 112 113 114 115 120 121 122 123 124 125 126 128 129 130
 131 133 134 135 136 137 138 139 140 143 144 145 147 148 150 153 154 155
 156 159 162 163 164 165 167 168 169 170 171 172 173 174 175 177 178 179
 180 181 182 183 186 187]
*** 2 test_index:  [  3   9  23  31  42  47  48  57  72  73  78  87  89  95 100 108 109 110
 116 117 118 119 127 132 141 142 146 149 151 152 157 158 160 161 166 176
 184 185]
*** 1 train_index:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19
  20  23  24  25  26  28  29  31  32  34  35  36  37  38  40  41  42  43
  44  47  48  51  53  54  55  56  57  58  59  60  64  66  68  70  71  72
  73  74  75  76  77  78  79  81  82  84  85  86  87  88  89  90  92  93
  95  96  97  98  99 100 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134
 135 137 139 140 141 142 143 144 146 148 149 150 151 152 153 156 157 158
 159 160 161 162 163 165 166 167 168 169 171 172 173 174 175 176 177 178
 179 181 183 184 185 186]
*** 2 test_index:  [  0  16  21  22  27  30  33  39  45  46  49  50  52  61  62  63  65  67
  69  80  83  91  94 101 102 103 122 136 138 145 147 154 155 164 170 180
 182 187]
*** 1 train_index:  [  0   1   3   4   5   6   7   8   9  13  14  16  18  19  21  22  23  25
  26  27  28  29  30  31  33  34  36  37  38  39  40  42  43  45  46  47
  48  49  50  51  52  53  55  56  57  58  60  61  62  63  65  66  67  68
  69  71  72  73  75  76  77  78  79  80  81  83  84  85  86  87  88  89
  90  91  92  93  94  95  97  98  99 100 101 102 103 105 106 108 109 110
 111 113 114 115 116 117 118 119 120 121 122 123 125 127 128 131 132 133
 134 135 136 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152
 154 155 156 157 158 160 161 162 163 164 165 166 168 170 171 174 175 176
 180 181 182 183 184 185 187]
*** 2 test_index:  [  2  10  11  12  15  17  20  24  32  35  41  44  54  59  64  70  74  82
  96 104 107 112 124 126 129 130 139 153 159 167 169 172 173 177 178 179
 186]
*** 1 train_index:  [  0   1   2   3   4   5   7   8   9  10  11  12  15  16  17  18  20  21
  22  23  24  25  27  28  29  30  31  32  33  34  35  37  39  40  41  42
  44  45  46  47  48  49  50  51  52  54  55  57  58  59  61  62  63  64
  65  67  68  69  70  72  73  74  75  78  80  82  83  85  86  87  89  90
  91  93  94  95  96  99 100 101 102 103 104 105 107 108 109 110 111 112
 114 116 117 118 119 122 123 124 126 127 128 129 130 132 133 135 136 137
 138 139 141 142 143 145 146 147 149 150 151 152 153 154 155 157 158 159
 160 161 162 163 164 166 167 168 169 170 172 173 174 176 177 178 179 180
 181 182 183 184 185 186 187]
*** 2 test_index:  [  6  13  14  19  26  36  38  43  53  56  60  66  71  76  77  79  81  84
  88  92  97  98 106 113 115 120 121 125 131 134 140 144 148 156 165 171
 175]
{'base_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/',
 'batch_size': 1,
 'clean_temp': False,
 'clean_tensorboard': False,
 'current_dataset_path': None,
 'current_min_dfscode_path': None,
 'current_model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-19-57-29/',
 'current_processed_dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'current_temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-19-57-29/',
 'dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/datasets/',
 'device': device(type='cuda', index=0),
 'dfscode_rnn_dropout': 0.2,
 'embedding_size_dfscode_rnn': 8,
 'epochs': 50,
 'epochs_end': 10000,
 'epochs_save': 20,
 'epochs_validate': 1,
 'fname': 'DFScodeRNN_cls_LSTM_MUTAG',
 'gradient_clipping': True,
 'graph_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/',
 'graph_type': 'MUTAG',
 'graphgen_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/',
 'hidden_size_dfscode_rnn': 4,
 'load_device': device(type='cuda', index=0),
 'load_model': False,
 'load_model_path': '',
 'log_tensorboard': False,
 'lr': 0.0001,
 'max_prev_node': None,
 'min_dfscode_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/',
 'min_dfscode_tensor_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/',
 'note': 'DFScodeRNN_cls_LSTM',
 'num_graphs': None,
 'num_layers': 2,
 'num_workers': 8,
 'number_of_mlp_layer': 1,
 'rnn_type': 'LSTM',
 'save_model': False,
 'temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/',
 'tensorboard_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tensorboard/',
 'time': '2021-01-17-19-57-29',
 'weights': False}


graphgen args.__dict__: None




dataset_features: {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}




config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 50, 'learning_rate': 0.0001, 'seed': 1800, 'k_fold': 5, 'model': 'DFScodeRNN_cls_LSTM', 'dataset': 'MUTAG'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}}


Training a new model: DFScodeRNN_cls_LSTM
Training model with dataset, testing using fold 0
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, num_layers=2, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.17154 acc 0.33333 roc_auc 0.56880 prc_auc 0.71091[0m
[93maverage test of epoch 0: loss 0.13088 acc 0.34211 roc_auc 0.54615 prc_auc 0.72536[0m
[92maverage training of epoch 1: loss 0.10255 acc 0.33333 roc_auc 0.45240 prc_auc 0.65501[0m
[93maverage test of epoch 1: loss 0.05688 acc 0.34211 roc_auc 0.57846 prc_auc 0.72273[0m
[92maverage training of epoch 2: loss 0.02592 acc 0.33333 roc_auc 0.55260 prc_auc 0.69450[0m
[93maverage test of epoch 2: loss -0.01147 acc 0.34211 roc_auc 0.36923 prc_auc 0.59820[0m
[92maverage training of epoch 3: loss -0.02174 acc 0.33333 roc_auc 0.48520 prc_auc 0.68956[0m
[93maverage test of epoch 3: loss -0.03536 acc 0.34211 roc_auc 0.27692 prc_auc 0.56751[0m
[92maverage training of epoch 4: loss -0.04109 acc 0.33333 roc_auc 0.44480 prc_auc 0.63969[0m
[93maverage test of epoch 4: loss -0.05653 acc 0.34211 roc_auc 0.46462 prc_auc 0.66074[0m
[92maverage training of epoch 5: loss -0.05735 acc 0.33333 roc_auc 0.41500 prc_auc 0.64579[0m
[93maverage test of epoch 5: loss -0.07577 acc 0.34211 roc_auc 0.63077 prc_auc 0.73238[0m
[92maverage training of epoch 6: loss -0.07457 acc 0.33333 roc_auc 0.43120 prc_auc 0.61541[0m
[93maverage test of epoch 6: loss -0.08569 acc 0.34211 roc_auc 0.39385 prc_auc 0.66159[0m
[92maverage training of epoch 7: loss -0.08984 acc 0.33333 roc_auc 0.37300 prc_auc 0.57878[0m
[93maverage test of epoch 7: loss -0.10495 acc 0.34211 roc_auc 0.51077 prc_auc 0.65591[0m
[92maverage training of epoch 8: loss -0.10378 acc 0.33333 roc_auc 0.40900 prc_auc 0.59137[0m
[93maverage test of epoch 8: loss -0.11458 acc 0.34211 roc_auc 0.52000 prc_auc 0.68233[0m
[92maverage training of epoch 9: loss -0.11587 acc 0.33333 roc_auc 0.38090 prc_auc 0.57085[0m
[93maverage test of epoch 9: loss -0.12750 acc 0.34211 roc_auc 0.58154 prc_auc 0.70607[0m
[92maverage training of epoch 10: loss -0.12728 acc 0.33333 roc_auc 0.36920 prc_auc 0.56846[0m
[93maverage test of epoch 10: loss -0.13796 acc 0.34211 roc_auc 0.44000 prc_auc 0.63096[0m
[92maverage training of epoch 11: loss -0.13913 acc 0.33333 roc_auc 0.38160 prc_auc 0.57223[0m
[93maverage test of epoch 11: loss -0.14907 acc 0.34211 roc_auc 0.49538 prc_auc 0.65683[0m
[92maverage training of epoch 12: loss -0.14954 acc 0.33333 roc_auc 0.31720 prc_auc 0.54985[0m
[93maverage test of epoch 12: loss -0.16057 acc 0.34211 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 13: loss -0.16154 acc 0.33333 roc_auc 0.38320 prc_auc 0.57010[0m
[93maverage test of epoch 13: loss -0.17169 acc 0.34211 roc_auc 0.44000 prc_auc 0.63096[0m
[92maverage training of epoch 14: loss -0.17283 acc 0.33333 roc_auc 0.43020 prc_auc 0.58998[0m
[93maverage test of epoch 14: loss -0.18258 acc 0.34211 roc_auc 0.46000 prc_auc 0.64004[0m
[92maverage training of epoch 15: loss -0.18329 acc 0.33333 roc_auc 0.37840 prc_auc 0.57273[0m
[93maverage test of epoch 15: loss -0.19387 acc 0.34211 roc_auc 0.46000 prc_auc 0.64004[0m
[92maverage training of epoch 16: loss -0.19482 acc 0.33333 roc_auc 0.40880 prc_auc 0.58425[0m
[93maverage test of epoch 16: loss -0.20379 acc 0.34211 roc_auc 0.45538 prc_auc 0.63825[0m
[92maverage training of epoch 17: loss -0.20578 acc 0.33333 roc_auc 0.38360 prc_auc 0.57333[0m
[93maverage test of epoch 17: loss -0.21601 acc 0.34211 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 18: loss -0.21660 acc 0.33333 roc_auc 0.37620 prc_auc 0.57014[0m
[93maverage test of epoch 18: loss -0.22709 acc 0.34211 roc_auc 0.52000 prc_auc 0.66703[0m
[92maverage training of epoch 19: loss -0.22784 acc 0.33333 roc_auc 0.40420 prc_auc 0.58165[0m
[93maverage test of epoch 19: loss -0.23775 acc 0.34211 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 20: loss -0.23884 acc 0.33333 roc_auc 0.38900 prc_auc 0.57456[0m
[93maverage test of epoch 20: loss -0.24909 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 21: loss -0.24965 acc 0.33333 roc_auc 0.35380 prc_auc 0.56106[0m
[93maverage test of epoch 21: loss -0.25986 acc 0.34211 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 22: loss -0.26085 acc 0.33333 roc_auc 0.36200 prc_auc 0.56391[0m
[93maverage test of epoch 22: loss -0.27112 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 23: loss -0.27199 acc 0.33333 roc_auc 0.36160 prc_auc 0.56161[0m
[93maverage test of epoch 23: loss -0.28214 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 24: loss -0.28298 acc 0.33333 roc_auc 0.36140 prc_auc 0.56486[0m
[93maverage test of epoch 24: loss -0.29318 acc 0.34211 roc_auc 0.53846 prc_auc 0.67568[0m
[92maverage training of epoch 25: loss -0.29421 acc 0.33333 roc_auc 0.36280 prc_auc 0.56325[0m
[93maverage test of epoch 25: loss -0.30417 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 26: loss -0.30514 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 26: loss -0.31519 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 27: loss -0.31613 acc 0.33333 roc_auc 0.38200 prc_auc 0.57291[0m
[93maverage test of epoch 27: loss -0.32596 acc 0.34211 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 28: loss -0.32715 acc 0.33333 roc_auc 0.34940 prc_auc 0.55947[0m
[93maverage test of epoch 28: loss -0.33721 acc 0.34211 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 29: loss -0.33832 acc 0.33333 roc_auc 0.36460 prc_auc 0.56407[0m
[93maverage test of epoch 29: loss -0.34802 acc 0.34211 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 30: loss -0.34932 acc 0.33333 roc_auc 0.35560 prc_auc 0.56116[0m
[93maverage test of epoch 30: loss -0.35926 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 31: loss -0.36035 acc 0.33333 roc_auc 0.35520 prc_auc 0.56143[0m
[93maverage test of epoch 31: loss -0.37027 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 32: loss -0.37141 acc 0.33333 roc_auc 0.34940 prc_auc 0.55854[0m
[93maverage test of epoch 32: loss -0.38129 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 33: loss -0.38248 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 33: loss -0.39231 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 34: loss -0.39353 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 34: loss -0.40332 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 35: loss -0.40457 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 35: loss -0.41434 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 36: loss -0.41562 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 36: loss -0.42521 acc 0.34211 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 37: loss -0.42662 acc 0.33333 roc_auc 0.35420 prc_auc 0.56112[0m
[93maverage test of epoch 37: loss -0.43637 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 38: loss -0.43772 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 38: loss -0.44739 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 39: loss -0.44874 acc 0.33333 roc_auc 0.35360 prc_auc 0.56092[0m
[93maverage test of epoch 39: loss -0.45848 acc 0.34211 roc_auc 0.53846 prc_auc 0.67568[0m
[92maverage training of epoch 40: loss -0.45982 acc 0.33333 roc_auc 0.35710 prc_auc 0.56188[0m
[93maverage test of epoch 40: loss -0.46942 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 41: loss -0.47086 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 41: loss -0.48044 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -0.48191 acc 0.33333 roc_auc 0.35780 prc_auc 0.56213[0m
[93maverage test of epoch 42: loss -0.49146 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -0.49296 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 43: loss -0.50239 acc 0.34211 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 44: loss -0.50401 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 44: loss -0.51349 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -0.51506 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 45: loss -0.52451 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -0.52608 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 46: loss -0.53552 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -0.53715 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 47: loss -0.54654 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -0.54820 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 48: loss -0.55756 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -0.55928 acc 0.33333 roc_auc 0.36320 prc_auc 0.56344[0m
[93maverage test of epoch 49: loss -0.56857 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 1
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, num_layers=2, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.45520 acc 0.33333 roc_auc 0.49860 prc_auc 0.65861[0m
[93maverage test of epoch 0: loss -0.55702 acc 0.34211 roc_auc 0.35692 prc_auc 0.59886[0m
[92maverage training of epoch 1: loss -0.66919 acc 0.33333 roc_auc 0.49680 prc_auc 0.67139[0m
[93maverage test of epoch 1: loss -0.81213 acc 0.34211 roc_auc 0.65846 prc_auc 0.81158[0m
[92maverage training of epoch 2: loss -0.90516 acc 0.33333 roc_auc 0.56020 prc_auc 0.72324[0m
[93maverage test of epoch 2: loss -1.02976 acc 0.34211 roc_auc 0.50154 prc_auc 0.73287[0m
[92maverage training of epoch 3: loss -1.14640 acc 0.33333 roc_auc 0.56340 prc_auc 0.72083[0m
[93maverage test of epoch 3: loss -1.27326 acc 0.34211 roc_auc 0.50154 prc_auc 0.70008[0m
[92maverage training of epoch 4: loss -1.38938 acc 0.33333 roc_auc 0.53600 prc_auc 0.69549[0m
[93maverage test of epoch 4: loss -1.51555 acc 0.34211 roc_auc 0.44308 prc_auc 0.68316[0m
[92maverage training of epoch 5: loss -1.63667 acc 0.33333 roc_auc 0.55240 prc_auc 0.68726[0m
[93maverage test of epoch 5: loss -1.76765 acc 0.34211 roc_auc 0.60308 prc_auc 0.77996[0m
[92maverage training of epoch 6: loss -1.87650 acc 0.33333 roc_auc 0.53680 prc_auc 0.68251[0m
[93maverage test of epoch 6: loss -1.99154 acc 0.34211 roc_auc 0.45538 prc_auc 0.72510[0m
[92maverage training of epoch 7: loss -2.08362 acc 0.33333 roc_auc 0.50860 prc_auc 0.67989[0m
[93maverage test of epoch 7: loss -2.21618 acc 0.34211 roc_auc 0.46154 prc_auc 0.66264[0m
[92maverage training of epoch 8: loss -2.31253 acc 0.33333 roc_auc 0.48860 prc_auc 0.66534[0m
[93maverage test of epoch 8: loss -2.46570 acc 0.34211 roc_auc 0.40615 prc_auc 0.67106[0m
[92maverage training of epoch 9: loss -2.54372 acc 0.33333 roc_auc 0.53680 prc_auc 0.71907[0m
[93maverage test of epoch 9: loss -2.68455 acc 0.34211 roc_auc 0.52615 prc_auc 0.69718[0m
[92maverage training of epoch 10: loss -2.77302 acc 0.33333 roc_auc 0.55040 prc_auc 0.72125[0m
[93maverage test of epoch 10: loss -2.93181 acc 0.34211 roc_auc 0.32615 prc_auc 0.61634[0m
[92maverage training of epoch 11: loss -3.00409 acc 0.33333 roc_auc 0.53540 prc_auc 0.69132[0m
[93maverage test of epoch 11: loss -3.14963 acc 0.34211 roc_auc 0.37846 prc_auc 0.63592[0m
[92maverage training of epoch 12: loss -3.25664 acc 0.33333 roc_auc 0.54400 prc_auc 0.69989[0m
[93maverage test of epoch 12: loss -3.37754 acc 0.34211 roc_auc 0.62154 prc_auc 0.78126[0m
[92maverage training of epoch 13: loss -3.49160 acc 0.33333 roc_auc 0.48280 prc_auc 0.66431[0m
[93maverage test of epoch 13: loss -3.62554 acc 0.34211 roc_auc 0.34154 prc_auc 0.63136[0m
[92maverage training of epoch 14: loss -3.72057 acc 0.33333 roc_auc 0.57260 prc_auc 0.73831[0m
[93maverage test of epoch 14: loss -3.84407 acc 0.34211 roc_auc 0.38462 prc_auc 0.64880[0m
[92maverage training of epoch 15: loss -3.95819 acc 0.33333 roc_auc 0.53600 prc_auc 0.69324[0m
[93maverage test of epoch 15: loss -4.11886 acc 0.34211 roc_auc 0.58154 prc_auc 0.75743[0m
[92maverage training of epoch 16: loss -4.21486 acc 0.33333 roc_auc 0.43920 prc_auc 0.62444[0m
[93maverage test of epoch 16: loss -4.35629 acc 0.34211 roc_auc 0.29846 prc_auc 0.61006[0m
[92maverage training of epoch 17: loss -4.44036 acc 0.33333 roc_auc 0.46500 prc_auc 0.66250[0m
[93maverage test of epoch 17: loss -4.59900 acc 0.34211 roc_auc 0.35077 prc_auc 0.58424[0m
[92maverage training of epoch 18: loss -4.70077 acc 0.33333 roc_auc 0.56360 prc_auc 0.68054[0m
[93maverage test of epoch 18: loss -4.81966 acc 0.34211 roc_auc 0.57538 prc_auc 0.75299[0m
[92maverage training of epoch 19: loss -4.93175 acc 0.33333 roc_auc 0.48220 prc_auc 0.65669[0m
[93maverage test of epoch 19: loss -5.08747 acc 0.34211 roc_auc 0.55077 prc_auc 0.79146[0m
[92maverage training of epoch 20: loss -5.16969 acc 0.33333 roc_auc 0.51000 prc_auc 0.68303[0m
[93maverage test of epoch 20: loss -5.31463 acc 0.34211 roc_auc 0.38769 prc_auc 0.65603[0m
[92maverage training of epoch 21: loss -5.42014 acc 0.33333 roc_auc 0.53080 prc_auc 0.72369[0m
[93maverage test of epoch 21: loss -5.56223 acc 0.34211 roc_auc 0.48000 prc_auc 0.70855[0m
[92maverage training of epoch 22: loss -5.67609 acc 0.33333 roc_auc 0.47660 prc_auc 0.63657[0m
[93maverage test of epoch 22: loss -5.81823 acc 0.34211 roc_auc 0.32000 prc_auc 0.62293[0m
[92maverage training of epoch 23: loss -5.91458 acc 0.33333 roc_auc 0.43040 prc_auc 0.62566[0m
[93maverage test of epoch 23: loss -6.03791 acc 0.34211 roc_auc 0.25846 prc_auc 0.53135[0m
[92maverage training of epoch 24: loss -6.16732 acc 0.33333 roc_auc 0.51280 prc_auc 0.67615[0m
[93maverage test of epoch 24: loss -6.30609 acc 0.34211 roc_auc 0.55077 prc_auc 0.68455[0m
[92maverage training of epoch 25: loss -6.40959 acc 0.33333 roc_auc 0.48420 prc_auc 0.63562[0m
[93maverage test of epoch 25: loss -6.57632 acc 0.34211 roc_auc 0.42769 prc_auc 0.63653[0m
[92maverage training of epoch 26: loss -6.68226 acc 0.33333 roc_auc 0.58120 prc_auc 0.75144[0m
[93maverage test of epoch 26: loss -6.84221 acc 0.34211 roc_auc 0.67692 prc_auc 0.84592[0m
[92maverage training of epoch 27: loss -6.93139 acc 0.33333 roc_auc 0.49600 prc_auc 0.69102[0m
[93maverage test of epoch 27: loss -7.06983 acc 0.34211 roc_auc 0.52923 prc_auc 0.71263[0m
[92maverage training of epoch 28: loss -7.18501 acc 0.33333 roc_auc 0.49580 prc_auc 0.67831[0m
[93maverage test of epoch 28: loss -7.36558 acc 0.34211 roc_auc 0.53538 prc_auc 0.74046[0m
[92maverage training of epoch 29: loss -7.44265 acc 0.33333 roc_auc 0.44740 prc_auc 0.65305[0m
[93maverage test of epoch 29: loss -7.62617 acc 0.34211 roc_auc 0.41538 prc_auc 0.63624[0m
[92maverage training of epoch 30: loss -7.70804 acc 0.33333 roc_auc 0.44800 prc_auc 0.65919[0m
[93maverage test of epoch 30: loss -7.88867 acc 0.34211 roc_auc 0.51385 prc_auc 0.69884[0m
[92maverage training of epoch 31: loss -7.97648 acc 0.33333 roc_auc 0.47720 prc_auc 0.65574[0m
[93maverage test of epoch 31: loss -8.14264 acc 0.34211 roc_auc 0.45538 prc_auc 0.68006[0m
[92maverage training of epoch 32: loss -8.25224 acc 0.33333 roc_auc 0.45860 prc_auc 0.68645[0m
[93maverage test of epoch 32: loss -8.43534 acc 0.34211 roc_auc 0.58154 prc_auc 0.72975[0m
[92maverage training of epoch 33: loss -8.51845 acc 0.33333 roc_auc 0.49260 prc_auc 0.66450[0m
[93maverage test of epoch 33: loss -8.67648 acc 0.34211 roc_auc 0.65231 prc_auc 0.75517[0m
[92maverage training of epoch 34: loss -8.79468 acc 0.33333 roc_auc 0.52840 prc_auc 0.69308[0m
[93maverage test of epoch 34: loss -8.97928 acc 0.34211 roc_auc 0.44615 prc_auc 0.61415[0m
[92maverage training of epoch 35: loss -9.07832 acc 0.33333 roc_auc 0.53660 prc_auc 0.70135[0m
[93maverage test of epoch 35: loss -9.24718 acc 0.34211 roc_auc 0.43077 prc_auc 0.60590[0m
[92maverage training of epoch 36: loss -9.35994 acc 0.33333 roc_auc 0.57260 prc_auc 0.74479[0m
[93maverage test of epoch 36: loss -9.53382 acc 0.34211 roc_auc 0.60308 prc_auc 0.75435[0m
[92maverage training of epoch 37: loss -9.64844 acc 0.33333 roc_auc 0.46760 prc_auc 0.65427[0m
[93maverage test of epoch 37: loss -9.81766 acc 0.34211 roc_auc 0.57846 prc_auc 0.75336[0m
[92maverage training of epoch 38: loss -9.92806 acc 0.33333 roc_auc 0.53960 prc_auc 0.69100[0m
[93maverage test of epoch 38: loss -10.09514 acc 0.34211 roc_auc 0.56615 prc_auc 0.75905[0m
[92maverage training of epoch 39: loss -10.22591 acc 0.33333 roc_auc 0.51400 prc_auc 0.67512[0m
[93maverage test of epoch 39: loss -10.41365 acc 0.34211 roc_auc 0.44308 prc_auc 0.72303[0m
[92maverage training of epoch 40: loss -10.50475 acc 0.33333 roc_auc 0.50680 prc_auc 0.66952[0m
[93maverage test of epoch 40: loss -10.70441 acc 0.34211 roc_auc 0.58615 prc_auc 0.71910[0m
[92maverage training of epoch 41: loss -10.80389 acc 0.33333 roc_auc 0.48440 prc_auc 0.65508[0m
[93maverage test of epoch 41: loss -11.00411 acc 0.34211 roc_auc 0.56615 prc_auc 0.73316[0m
[92maverage training of epoch 42: loss -11.10034 acc 0.33333 roc_auc 0.52640 prc_auc 0.69790[0m
[93maverage test of epoch 42: loss -11.29628 acc 0.34211 roc_auc 0.56308 prc_auc 0.71646[0m
[92maverage training of epoch 43: loss -11.39938 acc 0.33333 roc_auc 0.49720 prc_auc 0.67891[0m
[93maverage test of epoch 43: loss -11.60688 acc 0.34211 roc_auc 0.48615 prc_auc 0.68581[0m
[92maverage training of epoch 44: loss -11.70076 acc 0.33333 roc_auc 0.40700 prc_auc 0.59436[0m
[93maverage test of epoch 44: loss -11.89706 acc 0.34211 roc_auc 0.50769 prc_auc 0.67083[0m
[92maverage training of epoch 45: loss -12.01533 acc 0.33333 roc_auc 0.45060 prc_auc 0.63010[0m
[93maverage test of epoch 45: loss -12.21790 acc 0.34211 roc_auc 0.46462 prc_auc 0.64718[0m
[92maverage training of epoch 46: loss -12.32729 acc 0.33333 roc_auc 0.50020 prc_auc 0.66152[0m
[93maverage test of epoch 46: loss -12.52705 acc 0.34211 roc_auc 0.51385 prc_auc 0.70217[0m
[92maverage training of epoch 47: loss -12.63534 acc 0.33333 roc_auc 0.48720 prc_auc 0.67231[0m
[93maverage test of epoch 47: loss -12.84110 acc 0.34211 roc_auc 0.62615 prc_auc 0.79061[0m
[92maverage training of epoch 48: loss -12.95599 acc 0.33333 roc_auc 0.48260 prc_auc 0.64730[0m
[93maverage test of epoch 48: loss -13.15063 acc 0.34211 roc_auc 0.54308 prc_auc 0.69824[0m
[92maverage training of epoch 49: loss -13.27066 acc 0.33333 roc_auc 0.48380 prc_auc 0.64056[0m
[93maverage test of epoch 49: loss -13.47951 acc 0.34211 roc_auc 0.44308 prc_auc 0.68063[0m
Training model with dataset, testing using fold 2
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, num_layers=2, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.89711 acc 0.33333 roc_auc 0.53380 prc_auc 0.69143[0m
[93maverage test of epoch 0: loss -1.03057 acc 0.34211 roc_auc 0.50154 prc_auc 0.64952[0m
[92maverage training of epoch 1: loss -1.14687 acc 0.33333 roc_auc 0.49660 prc_auc 0.65136[0m
[93maverage test of epoch 1: loss -1.30501 acc 0.34211 roc_auc 0.35692 prc_auc 0.62189[0m
[92maverage training of epoch 2: loss -1.42969 acc 0.33333 roc_auc 0.49760 prc_auc 0.67986[0m
[93maverage test of epoch 2: loss -1.57075 acc 0.34211 roc_auc 0.55692 prc_auc 0.67692[0m
[92maverage training of epoch 3: loss -1.65999 acc 0.33333 roc_auc 0.50340 prc_auc 0.67485[0m
[93maverage test of epoch 3: loss -1.78174 acc 0.34211 roc_auc 0.55385 prc_auc 0.70170[0m
[92maverage training of epoch 4: loss -1.86283 acc 0.33333 roc_auc 0.54400 prc_auc 0.67676[0m
[93maverage test of epoch 4: loss -1.97816 acc 0.34211 roc_auc 0.55077 prc_auc 0.73133[0m
[92maverage training of epoch 5: loss -2.06526 acc 0.33333 roc_auc 0.44040 prc_auc 0.66767[0m
[93maverage test of epoch 5: loss -2.16580 acc 0.34211 roc_auc 0.36308 prc_auc 0.62184[0m
[92maverage training of epoch 6: loss -2.24998 acc 0.33333 roc_auc 0.42740 prc_auc 0.62423[0m
[93maverage test of epoch 6: loss -2.35794 acc 0.34211 roc_auc 0.42154 prc_auc 0.65419[0m
[92maverage training of epoch 7: loss -2.42757 acc 0.33333 roc_auc 0.44400 prc_auc 0.65818[0m
[93maverage test of epoch 7: loss -2.52024 acc 0.34211 roc_auc 0.45538 prc_auc 0.68666[0m
[92maverage training of epoch 8: loss -2.60409 acc 0.33333 roc_auc 0.50120 prc_auc 0.67656[0m
[93maverage test of epoch 8: loss -2.72165 acc 0.34211 roc_auc 0.68308 prc_auc 0.78273[0m
[92maverage training of epoch 9: loss -2.78188 acc 0.33333 roc_auc 0.54120 prc_auc 0.69535[0m
[93maverage test of epoch 9: loss -2.88867 acc 0.34211 roc_auc 0.60000 prc_auc 0.70025[0m
[92maverage training of epoch 10: loss -2.97955 acc 0.33333 roc_auc 0.53580 prc_auc 0.67993[0m
[93maverage test of epoch 10: loss -3.07883 acc 0.34211 roc_auc 0.45538 prc_auc 0.64485[0m
[92maverage training of epoch 11: loss -3.18214 acc 0.33333 roc_auc 0.61420 prc_auc 0.76481[0m
[93maverage test of epoch 11: loss -3.32380 acc 0.34211 roc_auc 0.69231 prc_auc 0.81909[0m
[92maverage training of epoch 12: loss -3.37369 acc 0.33333 roc_auc 0.56880 prc_auc 0.75497[0m
[93maverage test of epoch 12: loss -3.51186 acc 0.34211 roc_auc 0.62462 prc_auc 0.70734[0m
[92maverage training of epoch 13: loss -3.57259 acc 0.33333 roc_auc 0.58900 prc_auc 0.75362[0m
[93maverage test of epoch 13: loss -3.68835 acc 0.34211 roc_auc 0.41538 prc_auc 0.61744[0m
[92maverage training of epoch 14: loss -3.75852 acc 0.33333 roc_auc 0.41000 prc_auc 0.63698[0m
[93maverage test of epoch 14: loss -3.89276 acc 0.34211 roc_auc 0.57231 prc_auc 0.73922[0m
[92maverage training of epoch 15: loss -3.95918 acc 0.33333 roc_auc 0.50940 prc_auc 0.67214[0m
[93maverage test of epoch 15: loss -4.10025 acc 0.34211 roc_auc 0.61231 prc_auc 0.74520[0m
[92maverage training of epoch 16: loss -4.16031 acc 0.33333 roc_auc 0.50800 prc_auc 0.69833[0m
[93maverage test of epoch 16: loss -4.27212 acc 0.34211 roc_auc 0.62462 prc_auc 0.82007[0m
[92maverage training of epoch 17: loss -4.34504 acc 0.33333 roc_auc 0.52760 prc_auc 0.69516[0m
[93maverage test of epoch 17: loss -4.47175 acc 0.34211 roc_auc 0.43692 prc_auc 0.63737[0m
[92maverage training of epoch 18: loss -4.55258 acc 0.33333 roc_auc 0.49020 prc_auc 0.66415[0m
[93maverage test of epoch 18: loss -4.66586 acc 0.34211 roc_auc 0.37231 prc_auc 0.59121[0m
[92maverage training of epoch 19: loss -4.77216 acc 0.33333 roc_auc 0.57680 prc_auc 0.70601[0m
[93maverage test of epoch 19: loss -4.90526 acc 0.34211 roc_auc 0.47077 prc_auc 0.68909[0m
[92maverage training of epoch 20: loss -5.00476 acc 0.33333 roc_auc 0.50260 prc_auc 0.66372[0m
[93maverage test of epoch 20: loss -5.17441 acc 0.34211 roc_auc 0.56000 prc_auc 0.74511[0m
[92maverage training of epoch 21: loss -5.26324 acc 0.33333 roc_auc 0.51860 prc_auc 0.67664[0m
[93maverage test of epoch 21: loss -5.41506 acc 0.34211 roc_auc 0.47692 prc_auc 0.62509[0m
[92maverage training of epoch 22: loss -5.52427 acc 0.33333 roc_auc 0.45060 prc_auc 0.63058[0m
[93maverage test of epoch 22: loss -5.67477 acc 0.34211 roc_auc 0.57538 prc_auc 0.71277[0m
[92maverage training of epoch 23: loss -5.77814 acc 0.33333 roc_auc 0.54160 prc_auc 0.71490[0m
[93maverage test of epoch 23: loss -5.95008 acc 0.34211 roc_auc 0.55385 prc_auc 0.74181[0m
[92maverage training of epoch 24: loss -6.04012 acc 0.33333 roc_auc 0.53940 prc_auc 0.69513[0m
[93maverage test of epoch 24: loss -6.21351 acc 0.34211 roc_auc 0.57538 prc_auc 0.72943[0m
[92maverage training of epoch 25: loss -6.30559 acc 0.33333 roc_auc 0.54000 prc_auc 0.71718[0m
[93maverage test of epoch 25: loss -6.48397 acc 0.34211 roc_auc 0.39692 prc_auc 0.62075[0m
[92maverage training of epoch 26: loss -6.58388 acc 0.33333 roc_auc 0.55920 prc_auc 0.73086[0m
[93maverage test of epoch 26: loss -6.76630 acc 0.34211 roc_auc 0.60308 prc_auc 0.79266[0m
[92maverage training of epoch 27: loss -6.85666 acc 0.33333 roc_auc 0.51660 prc_auc 0.68821[0m
[93maverage test of epoch 27: loss -7.01877 acc 0.34211 roc_auc 0.53231 prc_auc 0.67829[0m
[92maverage training of epoch 28: loss -7.14405 acc 0.33333 roc_auc 0.55000 prc_auc 0.72608[0m
[93maverage test of epoch 28: loss -7.31960 acc 0.34211 roc_auc 0.57846 prc_auc 0.73313[0m
[92maverage training of epoch 29: loss -7.40137 acc 0.33333 roc_auc 0.46520 prc_auc 0.66543[0m
[93maverage test of epoch 29: loss -7.61557 acc 0.34211 roc_auc 0.48923 prc_auc 0.66590[0m
[92maverage training of epoch 30: loss -7.69761 acc 0.33333 roc_auc 0.56220 prc_auc 0.72595[0m
[93maverage test of epoch 30: loss -7.89021 acc 0.34211 roc_auc 0.45231 prc_auc 0.65098[0m
[92maverage training of epoch 31: loss -7.99139 acc 0.33333 roc_auc 0.57380 prc_auc 0.73730[0m
[93maverage test of epoch 31: loss -8.18644 acc 0.34211 roc_auc 0.56308 prc_auc 0.72672[0m
[92maverage training of epoch 32: loss -8.28593 acc 0.33333 roc_auc 0.55180 prc_auc 0.70279[0m
[93maverage test of epoch 32: loss -8.47847 acc 0.34211 roc_auc 0.54154 prc_auc 0.72387[0m
[92maverage training of epoch 33: loss -8.57954 acc 0.33333 roc_auc 0.54540 prc_auc 0.67961[0m
[93maverage test of epoch 33: loss -8.77148 acc 0.34211 roc_auc 0.49538 prc_auc 0.70506[0m
[92maverage training of epoch 34: loss -8.88760 acc 0.33333 roc_auc 0.46860 prc_auc 0.64686[0m
[93maverage test of epoch 34: loss -9.06976 acc 0.34211 roc_auc 0.51692 prc_auc 0.71654[0m
[92maverage training of epoch 35: loss -9.16819 acc 0.33333 roc_auc 0.45980 prc_auc 0.64189[0m
[93maverage test of epoch 35: loss -9.36462 acc 0.34211 roc_auc 0.46462 prc_auc 0.68164[0m
[92maverage training of epoch 36: loss -9.47703 acc 0.33333 roc_auc 0.54020 prc_auc 0.70521[0m
[93maverage test of epoch 36: loss -9.69012 acc 0.34211 roc_auc 0.55692 prc_auc 0.67226[0m
[92maverage training of epoch 37: loss -9.78196 acc 0.33333 roc_auc 0.52480 prc_auc 0.68608[0m
[93maverage test of epoch 37: loss -9.97101 acc 0.34211 roc_auc 0.63385 prc_auc 0.75396[0m
[92maverage training of epoch 38: loss -10.09664 acc 0.33333 roc_auc 0.50320 prc_auc 0.65948[0m
[93maverage test of epoch 38: loss -10.30429 acc 0.34211 roc_auc 0.56308 prc_auc 0.68738[0m
[92maverage training of epoch 39: loss -10.41366 acc 0.33333 roc_auc 0.48440 prc_auc 0.64229[0m
[93maverage test of epoch 39: loss -10.62420 acc 0.34211 roc_auc 0.45077 prc_auc 0.66612[0m
[92maverage training of epoch 40: loss -10.73825 acc 0.33333 roc_auc 0.54220 prc_auc 0.70060[0m
[93maverage test of epoch 40: loss -10.93412 acc 0.34211 roc_auc 0.49692 prc_auc 0.64092[0m
[92maverage training of epoch 41: loss -11.05256 acc 0.33333 roc_auc 0.51280 prc_auc 0.67847[0m
[93maverage test of epoch 41: loss -11.27180 acc 0.34211 roc_auc 0.53846 prc_auc 0.69175[0m
[92maverage training of epoch 42: loss -11.38676 acc 0.33333 roc_auc 0.47980 prc_auc 0.66968[0m
[93maverage test of epoch 42: loss -11.59793 acc 0.34211 roc_auc 0.44000 prc_auc 0.67681[0m
[92maverage training of epoch 43: loss -11.70919 acc 0.33333 roc_auc 0.57440 prc_auc 0.72537[0m
[93maverage test of epoch 43: loss -11.92469 acc 0.34211 roc_auc 0.46769 prc_auc 0.67170[0m
[92maverage training of epoch 44: loss -12.05429 acc 0.33333 roc_auc 0.48000 prc_auc 0.67482[0m
[93maverage test of epoch 44: loss -12.28371 acc 0.34211 roc_auc 0.39385 prc_auc 0.60090[0m
[92maverage training of epoch 45: loss -12.40452 acc 0.33333 roc_auc 0.55400 prc_auc 0.71335[0m
[93maverage test of epoch 45: loss -12.61680 acc 0.34211 roc_auc 0.34769 prc_auc 0.62955[0m
[92maverage training of epoch 46: loss -12.73692 acc 0.33333 roc_auc 0.53160 prc_auc 0.66963[0m
[93maverage test of epoch 46: loss -12.96845 acc 0.34211 roc_auc 0.72462 prc_auc 0.83771[0m
[92maverage training of epoch 47: loss -13.09423 acc 0.33333 roc_auc 0.47640 prc_auc 0.63224[0m
[93maverage test of epoch 47: loss -13.31798 acc 0.34211 roc_auc 0.49692 prc_auc 0.72827[0m
[92maverage training of epoch 48: loss -13.43953 acc 0.33333 roc_auc 0.47340 prc_auc 0.63950[0m
[93maverage test of epoch 48: loss -13.68389 acc 0.34211 roc_auc 0.41846 prc_auc 0.60177[0m
[92maverage training of epoch 49: loss -13.80944 acc 0.33333 roc_auc 0.53680 prc_auc 0.71902[0m
[93maverage test of epoch 49: loss -14.04847 acc 0.34211 roc_auc 0.45077 prc_auc 0.72912[0m
Training model with dataset, testing using fold 3
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, num_layers=2, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.18897 acc 0.66225 roc_auc 0.49059 prc_auc 0.64652[0m
[93maverage test of epoch 0: loss -0.22997 acc 0.67568 roc_auc 0.51333 prc_auc 0.70553[0m
[92maverage training of epoch 1: loss -0.25149 acc 0.66225 roc_auc 0.49980 prc_auc 0.66851[0m
[93maverage test of epoch 1: loss -0.29457 acc 0.67568 roc_auc 0.54667 prc_auc 0.72666[0m
[92maverage training of epoch 2: loss -0.32482 acc 0.66225 roc_auc 0.52020 prc_auc 0.65715[0m
[93maverage test of epoch 2: loss -0.36901 acc 0.67568 roc_auc 0.51333 prc_auc 0.76091[0m
[92maverage training of epoch 3: loss -0.40872 acc 0.66225 roc_auc 0.48176 prc_auc 0.64002[0m
[93maverage test of epoch 3: loss -0.46139 acc 0.67568 roc_auc 0.54333 prc_auc 0.71220[0m
[92maverage training of epoch 4: loss -0.52411 acc 0.58940 roc_auc 0.56725 prc_auc 0.69185[0m
[93maverage test of epoch 4: loss -0.60290 acc 0.35135 roc_auc 0.55000 prc_auc 0.74073[0m
[92maverage training of epoch 5: loss -0.73028 acc 0.33775 roc_auc 0.51686 prc_auc 0.68271[0m
[93maverage test of epoch 5: loss -0.85074 acc 0.32432 roc_auc 0.54333 prc_auc 0.68620[0m
[92maverage training of epoch 6: loss -0.95042 acc 0.33775 roc_auc 0.46569 prc_auc 0.66707[0m
[93maverage test of epoch 6: loss -1.02234 acc 0.32432 roc_auc 0.41000 prc_auc 0.62587[0m
[92maverage training of epoch 7: loss -1.16569 acc 0.33775 roc_auc 0.44059 prc_auc 0.66215[0m
[93maverage test of epoch 7: loss -1.27861 acc 0.32432 roc_auc 0.45000 prc_auc 0.71508[0m
[92maverage training of epoch 8: loss -1.40243 acc 0.33775 roc_auc 0.55412 prc_auc 0.73002[0m
[93maverage test of epoch 8: loss -1.48572 acc 0.32432 roc_auc 0.54667 prc_auc 0.75404[0m
[92maverage training of epoch 9: loss -1.62098 acc 0.33775 roc_auc 0.54157 prc_auc 0.71948[0m
[93maverage test of epoch 9: loss -1.69827 acc 0.32432 roc_auc 0.44000 prc_auc 0.65864[0m
[92maverage training of epoch 10: loss -1.87210 acc 0.33775 roc_auc 0.51431 prc_auc 0.67968[0m
[93maverage test of epoch 10: loss -1.96751 acc 0.32432 roc_auc 0.49667 prc_auc 0.68091[0m
[92maverage training of epoch 11: loss -2.12503 acc 0.33775 roc_auc 0.52176 prc_auc 0.67006[0m
[93maverage test of epoch 11: loss -2.20638 acc 0.32432 roc_auc 0.33667 prc_auc 0.61164[0m
[92maverage training of epoch 12: loss -2.37102 acc 0.33775 roc_auc 0.53784 prc_auc 0.68133[0m
[93maverage test of epoch 12: loss -2.41548 acc 0.32432 roc_auc 0.25333 prc_auc 0.54664[0m
[92maverage training of epoch 13: loss -2.59346 acc 0.33775 roc_auc 0.46725 prc_auc 0.63758[0m
[93maverage test of epoch 13: loss -2.66212 acc 0.32432 roc_auc 0.43333 prc_auc 0.66759[0m
[92maverage training of epoch 14: loss -2.79877 acc 0.33775 roc_auc 0.41569 prc_auc 0.62734[0m
[93maverage test of epoch 14: loss -2.84918 acc 0.32432 roc_auc 0.29000 prc_auc 0.57816[0m
[92maverage training of epoch 15: loss -2.99074 acc 0.33775 roc_auc 0.41882 prc_auc 0.63255[0m
[93maverage test of epoch 15: loss -3.04626 acc 0.32432 roc_auc 0.46667 prc_auc 0.72249[0m
[92maverage training of epoch 16: loss -3.19321 acc 0.33775 roc_auc 0.46373 prc_auc 0.65900[0m
[93maverage test of epoch 16: loss -3.27237 acc 0.32432 roc_auc 0.46667 prc_auc 0.75365[0m
[92maverage training of epoch 17: loss -3.40764 acc 0.33775 roc_auc 0.47588 prc_auc 0.66866[0m
[93maverage test of epoch 17: loss -3.48614 acc 0.32432 roc_auc 0.61333 prc_auc 0.79328[0m
[92maverage training of epoch 18: loss -3.61132 acc 0.33775 roc_auc 0.45078 prc_auc 0.61897[0m
[93maverage test of epoch 18: loss -3.67673 acc 0.32432 roc_auc 0.36000 prc_auc 0.62524[0m
[92maverage training of epoch 19: loss -3.82887 acc 0.33775 roc_auc 0.54451 prc_auc 0.70200[0m
[93maverage test of epoch 19: loss -3.89242 acc 0.32432 roc_auc 0.50000 prc_auc 0.72533[0m
[92maverage training of epoch 20: loss -4.03473 acc 0.33775 roc_auc 0.57706 prc_auc 0.72631[0m
[93maverage test of epoch 20: loss -4.06463 acc 0.32432 roc_auc 0.29333 prc_auc 0.56866[0m
[92maverage training of epoch 21: loss -4.23606 acc 0.33775 roc_auc 0.46157 prc_auc 0.63955[0m
[93maverage test of epoch 21: loss -4.31327 acc 0.32432 roc_auc 0.52333 prc_auc 0.78891[0m
[92maverage training of epoch 22: loss -4.45244 acc 0.33775 roc_auc 0.48020 prc_auc 0.70185[0m
[93maverage test of epoch 22: loss -4.49416 acc 0.32432 roc_auc 0.43667 prc_auc 0.71895[0m
[92maverage training of epoch 23: loss -4.64474 acc 0.33775 roc_auc 0.51784 prc_auc 0.67351[0m
[93maverage test of epoch 23: loss -4.68058 acc 0.32432 roc_auc 0.46667 prc_auc 0.62455[0m
[92maverage training of epoch 24: loss -4.84786 acc 0.33775 roc_auc 0.50333 prc_auc 0.64688[0m
[93maverage test of epoch 24: loss -4.90101 acc 0.32432 roc_auc 0.33667 prc_auc 0.58534[0m
[92maverage training of epoch 25: loss -5.06547 acc 0.33775 roc_auc 0.46627 prc_auc 0.63977[0m
[93maverage test of epoch 25: loss -5.11699 acc 0.32432 roc_auc 0.52000 prc_auc 0.72441[0m
[92maverage training of epoch 26: loss -5.26111 acc 0.33775 roc_auc 0.46471 prc_auc 0.64018[0m
[93maverage test of epoch 26: loss -5.31392 acc 0.32432 roc_auc 0.57000 prc_auc 0.76099[0m
[92maverage training of epoch 27: loss -5.48191 acc 0.33775 roc_auc 0.48451 prc_auc 0.65860[0m
[93maverage test of epoch 27: loss -5.51729 acc 0.32432 roc_auc 0.41667 prc_auc 0.63629[0m
[92maverage training of epoch 28: loss -5.70152 acc 0.33775 roc_auc 0.50980 prc_auc 0.68209[0m
[93maverage test of epoch 28: loss -5.74996 acc 0.32432 roc_auc 0.49667 prc_auc 0.68868[0m
[92maverage training of epoch 29: loss -5.91493 acc 0.33775 roc_auc 0.54863 prc_auc 0.66875[0m
[93maverage test of epoch 29: loss -5.96082 acc 0.32432 roc_auc 0.52667 prc_auc 0.71440[0m
[92maverage training of epoch 30: loss -6.14176 acc 0.33775 roc_auc 0.49902 prc_auc 0.66391[0m
[93maverage test of epoch 30: loss -6.19399 acc 0.32432 roc_auc 0.56667 prc_auc 0.74072[0m
[92maverage training of epoch 31: loss -6.35703 acc 0.33775 roc_auc 0.49216 prc_auc 0.65582[0m
[93maverage test of epoch 31: loss -6.41375 acc 0.32432 roc_auc 0.42667 prc_auc 0.70546[0m
[92maverage training of epoch 32: loss -6.58950 acc 0.33775 roc_auc 0.49020 prc_auc 0.65835[0m
[93maverage test of epoch 32: loss -6.63450 acc 0.32432 roc_auc 0.48000 prc_auc 0.75010[0m
[92maverage training of epoch 33: loss -6.81902 acc 0.33775 roc_auc 0.45647 prc_auc 0.61682[0m
[93maverage test of epoch 33: loss -6.85833 acc 0.32432 roc_auc 0.39000 prc_auc 0.64667[0m
[92maverage training of epoch 34: loss -7.05439 acc 0.33775 roc_auc 0.51686 prc_auc 0.68621[0m
[93maverage test of epoch 34: loss -7.09015 acc 0.32432 roc_auc 0.31667 prc_auc 0.60316[0m
[92maverage training of epoch 35: loss -7.29486 acc 0.33775 roc_auc 0.55529 prc_auc 0.73621[0m
[93maverage test of epoch 35: loss -7.32549 acc 0.32432 roc_auc 0.55667 prc_auc 0.76390[0m
[92maverage training of epoch 36: loss -7.52196 acc 0.33775 roc_auc 0.50608 prc_auc 0.67072[0m
[93maverage test of epoch 36: loss -7.58336 acc 0.32432 roc_auc 0.47000 prc_auc 0.71271[0m
[92maverage training of epoch 37: loss -7.77018 acc 0.33775 roc_auc 0.54608 prc_auc 0.71516[0m
[93maverage test of epoch 37: loss -7.81198 acc 0.32432 roc_auc 0.47000 prc_auc 0.65476[0m
[92maverage training of epoch 38: loss -8.01758 acc 0.33775 roc_auc 0.46078 prc_auc 0.64160[0m
[93maverage test of epoch 38: loss -8.05629 acc 0.32432 roc_auc 0.39667 prc_auc 0.62168[0m
[92maverage training of epoch 39: loss -8.26958 acc 0.33775 roc_auc 0.53588 prc_auc 0.72134[0m
[93maverage test of epoch 39: loss -8.32235 acc 0.32432 roc_auc 0.54000 prc_auc 0.76289[0m
[92maverage training of epoch 40: loss -8.51217 acc 0.33775 roc_auc 0.43961 prc_auc 0.64906[0m
[93maverage test of epoch 40: loss -8.56397 acc 0.32432 roc_auc 0.36667 prc_auc 0.66386[0m
[92maverage training of epoch 41: loss -8.77126 acc 0.33775 roc_auc 0.55373 prc_auc 0.68947[0m
[93maverage test of epoch 41: loss -8.82837 acc 0.32432 roc_auc 0.59333 prc_auc 0.79055[0m
[92maverage training of epoch 42: loss -9.02534 acc 0.33775 roc_auc 0.50902 prc_auc 0.66474[0m
[93maverage test of epoch 42: loss -9.08918 acc 0.32432 roc_auc 0.49000 prc_auc 0.69945[0m
[92maverage training of epoch 43: loss -9.29749 acc 0.33775 roc_auc 0.47647 prc_auc 0.64799[0m
[93maverage test of epoch 43: loss -9.35370 acc 0.32432 roc_auc 0.68667 prc_auc 0.82652[0m
[92maverage training of epoch 44: loss -9.56565 acc 0.33775 roc_auc 0.49392 prc_auc 0.69187[0m
[93maverage test of epoch 44: loss -9.61755 acc 0.32432 roc_auc 0.46667 prc_auc 0.68551[0m
[92maverage training of epoch 45: loss -9.83656 acc 0.33775 roc_auc 0.54000 prc_auc 0.69958[0m
[93maverage test of epoch 45: loss -9.89191 acc 0.32432 roc_auc 0.60333 prc_auc 0.74187[0m
[92maverage training of epoch 46: loss -10.10517 acc 0.33775 roc_auc 0.49529 prc_auc 0.68773[0m
[93maverage test of epoch 46: loss -10.15597 acc 0.32432 roc_auc 0.55000 prc_auc 0.76620[0m
[92maverage training of epoch 47: loss -10.37855 acc 0.33775 roc_auc 0.56353 prc_auc 0.71857[0m
[93maverage test of epoch 47: loss -10.44609 acc 0.32432 roc_auc 0.72667 prc_auc 0.84776[0m
[92maverage training of epoch 48: loss -10.66136 acc 0.33775 roc_auc 0.56961 prc_auc 0.72582[0m
[93maverage test of epoch 48: loss -10.71427 acc 0.32432 roc_auc 0.45333 prc_auc 0.67189[0m
[92maverage training of epoch 49: loss -10.93950 acc 0.33775 roc_auc 0.52098 prc_auc 0.69390[0m
[93maverage test of epoch 49: loss -10.99994 acc 0.32432 roc_auc 0.51333 prc_auc 0.77837[0m
Training model with dataset, testing using fold 4
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, num_layers=2, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.14333 acc 0.33775 roc_auc 0.45608 prc_auc 0.66944[0m
[93maverage test of epoch 0: loss 0.13294 acc 0.32432 roc_auc 0.50667 prc_auc 0.70682[0m
[92maverage training of epoch 1: loss 0.01371 acc 0.33775 roc_auc 0.42745 prc_auc 0.63301[0m
[93maverage test of epoch 1: loss -0.30426 acc 0.35135 roc_auc 0.48333 prc_auc 0.71413[0m
[92maverage training of epoch 2: loss -0.68429 acc 0.50331 roc_auc 0.50333 prc_auc 0.68722[0m
[93maverage test of epoch 2: loss -1.07632 acc 0.67568 roc_auc 0.61000 prc_auc 0.80118[0m
[92maverage training of epoch 3: loss -1.52803 acc 0.66225 roc_auc 0.41745 prc_auc 0.63432[0m
[93maverage test of epoch 3: loss -2.03888 acc 0.67568 roc_auc 0.38333 prc_auc 0.66024[0m
[92maverage training of epoch 4: loss -2.37118 acc 0.66225 roc_auc 0.42843 prc_auc 0.65028[0m
[93maverage test of epoch 4: loss -2.69476 acc 0.67568 roc_auc 0.55000 prc_auc 0.71915[0mUsing backend: pytorch

[92maverage training of epoch 5: loss -2.87376 acc 0.66225 roc_auc 0.46902 prc_auc 0.68553[0m
[93maverage test of epoch 5: loss -3.10803 acc 0.67568 roc_auc 0.53000 prc_auc 0.65692[0m
[92maverage training of epoch 6: loss -3.23722 acc 0.66225 roc_auc 0.42020 prc_auc 0.61476[0m
[93maverage test of epoch 6: loss -3.45273 acc 0.67568 roc_auc 0.52667 prc_auc 0.73080[0m
[92maverage training of epoch 7: loss -3.55446 acc 0.66225 roc_auc 0.44922 prc_auc 0.65468[0m
[93maverage test of epoch 7: loss -3.76949 acc 0.67568 roc_auc 0.48333 prc_auc 0.65208[0m
[92maverage training of epoch 8: loss -3.85986 acc 0.66225 roc_auc 0.41569 prc_auc 0.60899[0m
[93maverage test of epoch 8: loss -4.06232 acc 0.67568 roc_auc 0.57333 prc_auc 0.71159[0m
[92maverage training of epoch 9: loss -4.15561 acc 0.66225 roc_auc 0.51333 prc_auc 0.65234[0m
[93maverage test of epoch 9: loss -4.33751 acc 0.67568 roc_auc 0.34000 prc_auc 0.59896[0m
[92maverage training of epoch 10: loss -4.44603 acc 0.66225 roc_auc 0.41412 prc_auc 0.61201[0m
[93maverage test of epoch 10: loss -4.61568 acc 0.67568 roc_auc 0.33667 prc_auc 0.69834[0m
[92maverage training of epoch 11: loss -4.73106 acc 0.66225 roc_auc 0.42843 prc_auc 0.65955[0m
[93maverage test of epoch 11: loss -4.93362 acc 0.67568 roc_auc 0.50000 prc_auc 0.67899[0m
[92maverage training of epoch 12: loss -5.02951 acc 0.66225 roc_auc 0.45529 prc_auc 0.62775[0m
[93maverage test of epoch 12: loss -5.21736 acc 0.67568 roc_auc 0.48333 prc_auc 0.74665[0m
[92maverage training of epoch 13: loss -5.32002 acc 0.66225 roc_auc 0.44235 prc_auc 0.64270[0m
[93maverage test of epoch 13: loss -5.50373 acc 0.67568 roc_auc 0.50667 prc_auc 0.68658[0m
[92maverage training of epoch 14: loss -5.62634 acc 0.66225 roc_auc 0.43765 prc_auc 0.61482[0m
[93maverage test of epoch 14: loss -5.82302 acc 0.67568 roc_auc 0.57000 prc_auc 0.73209[0m
[92maverage training of epoch 15: loss -5.91113 acc 0.66225 roc_auc 0.44157 prc_auc 0.64575[0m
[93maverage test of epoch 15: loss -6.11987 acc 0.67568 roc_auc 0.38000 prc_auc 0.62949[0m
[92maverage training of epoch 16: loss -6.21868 acc 0.66225 roc_auc 0.43137 prc_auc 0.65384[0m
[93maverage test of epoch 16: loss -6.43050 acc 0.67568 roc_auc 0.38000 prc_auc 0.59540[0m
[92maverage training of epoch 17: loss -6.52716 acc 0.66225 roc_auc 0.40510 prc_auc 0.59707[0m
[93maverage test of epoch 17: loss -6.74600 acc 0.67568 roc_auc 0.71667 prc_auc 0.86408[0m
[92maverage training of epoch 18: loss -6.83661 acc 0.66225 roc_auc 0.43020 prc_auc 0.61007[0m
[93maverage test of epoch 18: loss -7.07370 acc 0.67568 roc_auc 0.58333 prc_auc 0.80161[0m
[92maverage training of epoch 19: loss -7.14916 acc 0.66225 roc_auc 0.42392 prc_auc 0.61281[0m
[93maverage test of epoch 19: loss -7.37780 acc 0.67568 roc_auc 0.60333 prc_auc 0.74345[0m
[92maverage training of epoch 20: loss -7.46656 acc 0.66225 roc_auc 0.44549 prc_auc 0.63785[0m
[93maverage test of epoch 20: loss -7.69342 acc 0.67568 roc_auc 0.50000 prc_auc 0.70306[0m
[92maverage training of epoch 21: loss -7.78433 acc 0.66225 roc_auc 0.42706 prc_auc 0.62703[0m
[93maverage test of epoch 21: loss -8.01688 acc 0.67568 roc_auc 0.56000 prc_auc 0.75141[0m
[92maverage training of epoch 22: loss -8.11194 acc 0.66225 roc_auc 0.42216 prc_auc 0.63366[0m
[93maverage test of epoch 22: loss -8.33755 acc 0.67568 roc_auc 0.58667 prc_auc 0.76620[0m
[92maverage training of epoch 23: loss -8.44469 acc 0.66225 roc_auc 0.44069 prc_auc 0.63424[0m
[93maverage test of epoch 23: loss -8.68262 acc 0.67568 roc_auc 0.47500 prc_auc 0.70755[0m
[92maverage training of epoch 24: loss -8.78060 acc 0.66225 roc_auc 0.51294 prc_auc 0.71830[0m
[93maverage test of epoch 24: loss -9.01740 acc 0.67568 roc_auc 0.59500 prc_auc 0.76581[0m
[92maverage training of epoch 25: loss -9.12539 acc 0.66225 roc_auc 0.43294 prc_auc 0.62676[0m
[93maverage test of epoch 25: loss -9.37198 acc 0.67568 roc_auc 0.62167 prc_auc 0.78544[0m
[92maverage training of epoch 26: loss -9.46083 acc 0.66225 roc_auc 0.41520 prc_auc 0.59972[0m
[93maverage test of epoch 26: loss -9.69565 acc 0.67568 roc_auc 0.50833 prc_auc 0.72148[0m
[92maverage training of epoch 27: loss -9.79879 acc 0.66225 roc_auc 0.38333 prc_auc 0.59647[0m
[93maverage test of epoch 27: loss -10.05727 acc 0.67568 roc_auc 0.67000 prc_auc 0.84097[0m
[92maverage training of epoch 28: loss -10.15555 acc 0.66225 roc_auc 0.40167 prc_auc 0.59717[0m
[93maverage test of epoch 28: loss -10.42139 acc 0.67568 roc_auc 0.56833 prc_auc 0.75969[0m
[92maverage training of epoch 29: loss -10.52068 acc 0.66225 roc_auc 0.44490 prc_auc 0.66456[0m
[93maverage test of epoch 29: loss -10.77734 acc 0.67568 roc_auc 0.44333 prc_auc 0.69202[0m
[92maverage training of epoch 30: loss -10.87845 acc 0.66225 roc_auc 0.43471 prc_auc 0.63518[0m
[93maverage test of epoch 30: loss -11.13137 acc 0.67568 roc_auc 0.54833 prc_auc 0.70939[0m
[92maverage training of epoch 31: loss -11.24522 acc 0.66225 roc_auc 0.41725 prc_auc 0.60860[0m
[93maverage test of epoch 31: loss -11.52472 acc 0.67568 roc_auc 0.53833 prc_auc 0.76490[0m
[92maverage training of epoch 32: loss -11.61504 acc 0.66225 roc_auc 0.42108 prc_auc 0.61906[0m
[93maverage test of epoch 32: loss -11.89530 acc 0.67568 roc_auc 0.49000 prc_auc 0.71895[0m
[92maverage training of epoch 33: loss -11.99480 acc 0.66225 roc_auc 0.44235 prc_auc 0.63950[0m
[93maverage test of epoch 33: loss -12.27434 acc 0.67568 roc_auc 0.52667 prc_auc 0.73964[0m
[92maverage training of epoch 34: loss -12.37425 acc 0.66225 roc_auc 0.42922 prc_auc 0.63361[0m
[93maverage test of epoch 34: loss -12.66429 acc 0.67568 roc_auc 0.53167 prc_auc 0.69748[0m
[92maverage training of epoch 35: loss -12.75273 acc 0.66225 roc_auc 0.40275 prc_auc 0.59654[0m
[93maverage test of epoch 35: loss -13.05533 acc 0.67568 roc_auc 0.63333 prc_auc 0.78479[0m
[92maverage training of epoch 36: loss -13.14083 acc 0.66225 roc_auc 0.42941 prc_auc 0.62640[0m
[93maverage test of epoch 36: loss -13.44728 acc 0.67568 roc_auc 0.70667 prc_auc 0.84365[0m
[92maverage training of epoch 37: loss -13.53941 acc 0.66225 roc_auc 0.41618 prc_auc 0.60160[0m
[93maverage test of epoch 37: loss -13.84628 acc 0.67568 roc_auc 0.56667 prc_auc 0.76257[0m
[92maverage training of epoch 38: loss -13.94484 acc 0.66225 roc_auc 0.39529 prc_auc 0.59514[0m
[93maverage test of epoch 38: loss -14.25744 acc 0.67568 roc_auc 0.51500 prc_auc 0.69733[0m
[92maverage training of epoch 39: loss -14.34721 acc 0.66225 roc_auc 0.42804 prc_auc 0.63254[0m
[93maverage test of epoch 39: loss -14.65670 acc 0.67568 roc_auc 0.56167 prc_auc 0.73783[0m
[92maverage training of epoch 40: loss -14.76511 acc 0.66225 roc_auc 0.40490 prc_auc 0.61609[0m
[93maverage test of epoch 40: loss -15.08579 acc 0.67568 roc_auc 0.59167 prc_auc 0.73190[0m
[92maverage training of epoch 41: loss -15.18150 acc 0.66225 roc_auc 0.41412 prc_auc 0.60223[0m
[93maverage test of epoch 41: loss -15.51213 acc 0.67568 roc_auc 0.58500 prc_auc 0.74094[0m
[92maverage training of epoch 42: loss -15.59410 acc 0.66225 roc_auc 0.42304 prc_auc 0.62915[0m
[93maverage test of epoch 42: loss -15.93006 acc 0.67568 roc_auc 0.57833 prc_auc 0.73651[0m
[92maverage training of epoch 43: loss -16.02551 acc 0.66225 roc_auc 0.44039 prc_auc 0.63109[0m
[93maverage test of epoch 43: loss -16.35435 acc 0.67568 roc_auc 0.47167 prc_auc 0.69306[0m
[92maverage training of epoch 44: loss -16.44824 acc 0.66225 roc_auc 0.37657 prc_auc 0.59914[0m
[93maverage test of epoch 44: loss -16.78875 acc 0.67568 roc_auc 0.45500 prc_auc 0.65373[0m
[92maverage training of epoch 45: loss -16.89171 acc 0.66225 roc_auc 0.42696 prc_auc 0.61863[0m
[93maverage test of epoch 45: loss -17.23069 acc 0.67568 roc_auc 0.66667 prc_auc 0.81816[0m
[92maverage training of epoch 46: loss -17.32597 acc 0.66225 roc_auc 0.41363 prc_auc 0.61104[0m
[93maverage test of epoch 46: loss -17.68136 acc 0.67568 roc_auc 0.25000 prc_auc 0.56403[0m
[92maverage training of epoch 47: loss -17.77732 acc 0.66225 roc_auc 0.40098 prc_auc 0.60594[0m
[93maverage test of epoch 47: loss -18.12919 acc 0.67568 roc_auc 0.49167 prc_auc 0.65716[0m
[92maverage training of epoch 48: loss -18.22430 acc 0.66225 roc_auc 0.40490 prc_auc 0.59922[0m
[93maverage test of epoch 48: loss -18.58906 acc 0.67568 roc_auc 0.55333 prc_auc 0.71228[0m
[92maverage training of epoch 49: loss -18.68249 acc 0.66225 roc_auc 0.39618 prc_auc 0.59604[0m
[93maverage test of epoch 49: loss -19.04680 acc 0.67568 roc_auc 0.31333 prc_auc 0.58590[0m
Run statistics: 
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 2)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 1)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))


Accuracy (avg): 0.40526 ROC_AUC (avg): 0.4441 PRC_AUC (avg): 0.68639 

Average forward propagation time taken(ms): 4.315197293072953
Average backward propagation time taken(ms): 1.588421627984651

