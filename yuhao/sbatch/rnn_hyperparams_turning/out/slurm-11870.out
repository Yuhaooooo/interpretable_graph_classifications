# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======






Params Turning Set
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 16)
('hidden_size_dfscode_rnn', 8)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 2)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))






args.base_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/
args.graph_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/
args.min_dfscode_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/
args.min_dfscode_tensor_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/
*** 1 train_index:  [  0   2   3   6   9  10  11  12  13  14  15  16  17  19  20  21  22  23
  24  26  27  30  31  32  33  35  36  38  39  41  42  43  44  45  46  47
  48  49  50  52  53  54  56  57  59  60  61  62  63  64  65  66  67  69
  70  71  72  73  74  76  77  78  79  80  81  82  83  84  87  88  89  91
  92  94  95  96  97  98 100 101 102 103 104 106 107 108 109 110 112 113
 115 116 117 118 119 120 121 122 124 125 126 127 129 130 131 132 134 136
 138 139 140 141 142 144 145 146 147 148 149 151 152 153 154 155 156 157
 158 159 160 161 164 165 166 167 169 170 171 172 173 175 176 177 178 179
 180 182 184 185 186 187]
*** 2 test_index:  [  1   4   5   7   8  18  25  28  29  34  37  40  51  55  58  68  75  85
  86  90  93  99 105 111 114 123 128 133 135 137 143 150 162 163 168 174
 181 183]
*** 1 train_index:  [  0   1   2   4   5   6   7   8  10  11  12  13  14  15  16  17  18  19
  20  21  22  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39
  40  41  43  44  45  46  49  50  51  52  53  54  55  56  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  74  75  76  77  79  80  81  82
  83  84  85  86  88  90  91  92  93  94  96  97  98  99 101 102 103 104
 105 106 107 111 112 113 114 115 120 121 122 123 124 125 126 128 129 130
 131 133 134 135 136 137 138 139 140 143 144 145 147 148 150 153 154 155
 156 159 162 163 164 165 167 168 169 170 171 172 173 174 175 177 178 179
 180 181 182 183 186 187]
*** 2 test_index:  [  3   9  23  31  42  47  48  57  72  73  78  87  89  95 100 108 109 110
 116 117 118 119 127 132 141 142 146 149 151 152 157 158 160 161 166 176
 184 185]
*** 1 train_index:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19
  20  23  24  25  26  28  29  31  32  34  35  36  37  38  40  41  42  43
  44  47  48  51  53  54  55  56  57  58  59  60  64  66  68  70  71  72
  73  74  75  76  77  78  79  81  82  84  85  86  87  88  89  90  92  93
  95  96  97  98  99 100 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134
 135 137 139 140 141 142 143 144 146 148 149 150 151 152 153 156 157 158
 159 160 161 162 163 165 166 167 168 169 171 172 173 174 175 176 177 178
 179 181 183 184 185 186]
*** 2 test_index:  [  0  16  21  22  27  30  33  39  45  46  49  50  52  61  62  63  65  67
  69  80  83  91  94 101 102 103 122 136 138 145 147 154 155 164 170 180
 182 187]
*** 1 train_index:  [  0   1   3   4   5   6   7   8   9  13  14  16  18  19  21  22  23  25
  26  27  28  29  30  31  33  34  36  37  38  39  40  42  43  45  46  47
  48  49  50  51  52  53  55  56  57  58  60  61  62  63  65  66  67  68
  69  71  72  73  75  76  77  78  79  80  81  83  84  85  86  87  88  89
  90  91  92  93  94  95  97  98  99 100 101 102 103 105 106 108 109 110
 111 113 114 115 116 117 118 119 120 121 122 123 125 127 128 131 132 133
 134 135 136 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152
 154 155 156 157 158 160 161 162 163 164 165 166 168 170 171 174 175 176
 180 181 182 183 184 185 187]
*** 2 test_index:  [  2  10  11  12  15  17  20  24  32  35  41  44  54  59  64  70  74  82
  96 104 107 112 124 126 129 130 139 153 159 167 169 172 173 177 178 179
 186]
*** 1 train_index:  [  0   1   2   3   4   5   7   8   9  10  11  12  15  16  17  18  20  21
  22  23  24  25  27  28  29  30  31  32  33  34  35  37  39  40  41  42
  44  45  46  47  48  49  50  51  52  54  55  57  58  59  61  62  63  64
  65  67  68  69  70  72  73  74  75  78  80  82  83  85  86  87  89  90
  91  93  94  95  96  99 100 101 102 103 104 105 107 108 109 110 111 112
 114 116 117 118 119 122 123 124 126 127 128 129 130 132 133 135 136 137
 138 139 141 142 143 145 146 147 149 150 151 152 153 154 155 157 158 159
 160 161 162 163 164 166 167 168 169 170 172 173 174 176 177 178 179 180
 181 182 183 184 185 186 187]
*** 2 test_index:  [  6  13  14  19  26  36  38  43  53  56  60  66  71  76  77  79  81  84
  88  92  97  98 106 113 115 120 121 125 131 134 140 144 148 156 165 171
 175]
{'base_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/',
 'batch_size': 1,
 'clean_temp': False,
 'clean_tensorboard': False,
 'current_dataset_path': None,
 'current_min_dfscode_path': None,
 'current_model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-19-22-42/',
 'current_processed_dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'current_temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-19-22-42/',
 'dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/datasets/',
 'device': device(type='cuda', index=0),
 'dfscode_rnn_dropout': 0.2,
 'embedding_size_dfscode_rnn': 16,
 'epochs': 50,
 'epochs_end': 10000,
 'epochs_save': 20,
 'epochs_validate': 1,
 'fname': 'DFScodeRNN_cls_LSTM_MUTAG',
 'gradient_clipping': True,
 'graph_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/',
 'graph_type': 'MUTAG',
 'graphgen_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/',
 'hidden_size_dfscode_rnn': 8,
 'load_device': device(type='cuda', index=0),
 'load_model': False,
 'load_model_path': '',
 'log_tensorboard': False,
 'lr': 0.0001,
 'max_prev_node': None,
 'min_dfscode_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/',
 'min_dfscode_tensor_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/',
 'note': 'DFScodeRNN_cls_LSTM',
 'num_graphs': None,
 'num_layers': 1,
 'num_workers': 8,
 'number_of_mlp_layer': 2,
 'rnn_type': 'LSTM',
 'save_model': False,
 'temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/',
 'tensorboard_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tensorboard/',
 'time': '2021-01-17-19-22-42',
 'weights': False}


graphgen args.__dict__: None




dataset_features: {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}




config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 50, 'learning_rate': 0.0001, 'seed': 1800, 'k_fold': 5, 'model': 'DFScodeRNN_cls_LSTM', 'dataset': 'MUTAG'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}}


Training a new model: DFScodeRNN_cls_LSTM
Training model with dataset, testing using fold 0
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 8, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=8, bias=True)
    (ReLU1): ReLU()
    (Linear2): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.39610 acc 0.33333 roc_auc 0.65840 prc_auc 0.83053[0m
[93maverage test of epoch 0: loss 0.21118 acc 0.34211 roc_auc 0.84615 prc_auc 0.91257[0m
[92maverage training of epoch 1: loss 0.09252 acc 0.33333 roc_auc 0.73360 prc_auc 0.86851[0m
[93maverage test of epoch 1: loss -0.08041 acc 0.34211 roc_auc 0.86154 prc_auc 0.93076[0m
[92maverage training of epoch 2: loss -0.19220 acc 0.33333 roc_auc 0.78360 prc_auc 0.89558[0m
[93maverage test of epoch 2: loss -0.36135 acc 0.34211 roc_auc 0.86462 prc_auc 0.93409[0m
[92maverage training of epoch 3: loss -0.47253 acc 0.33333 roc_auc 0.81800 prc_auc 0.91356[0m
[93maverage test of epoch 3: loss -0.64122 acc 0.34211 roc_auc 0.86462 prc_auc 0.93409[0m
[92maverage training of epoch 4: loss -0.74615 acc 0.33333 roc_auc 0.84100 prc_auc 0.92498[0m
[93maverage test of epoch 4: loss -0.90950 acc 0.34211 roc_auc 0.86462 prc_auc 0.93409[0m
[92maverage training of epoch 5: loss -1.00469 acc 0.33333 roc_auc 0.86320 prc_auc 0.93331[0m
[93maverage test of epoch 5: loss -1.16050 acc 0.34211 roc_auc 0.86462 prc_auc 0.93409[0m
[92maverage training of epoch 6: loss -1.24491 acc 0.33333 roc_auc 0.87200 prc_auc 0.93673[0m
[93maverage test of epoch 6: loss -1.39013 acc 0.34211 roc_auc 0.86462 prc_auc 0.93409[0m
[92maverage training of epoch 7: loss -1.44029 acc 0.33333 roc_auc 0.87380 prc_auc 0.93749[0m
[93maverage test of epoch 7: loss -1.57602 acc 0.34211 roc_auc 0.86462 prc_auc 0.93409[0m
[92maverage training of epoch 8: loss -1.62952 acc 0.33333 roc_auc 0.87540 prc_auc 0.93773[0m
[93maverage test of epoch 8: loss -1.78015 acc 0.34211 roc_auc 0.86462 prc_auc 0.93409[0m
[92maverage training of epoch 9: loss -1.83885 acc 0.33333 roc_auc 0.87660 prc_auc 0.93810[0m
[93maverage test of epoch 9: loss -2.00640 acc 0.34211 roc_auc 0.87077 prc_auc 0.93634[0m
[92maverage training of epoch 10: loss -2.07370 acc 0.33333 roc_auc 0.87640 prc_auc 0.93828[0m
[93maverage test of epoch 10: loss -2.26017 acc 0.34211 roc_auc 0.87077 prc_auc 0.93634[0m
[92maverage training of epoch 11: loss -2.34185 acc 0.33333 roc_auc 0.87460 prc_auc 0.93816[0m
[93maverage test of epoch 11: loss -2.55270 acc 0.34211 roc_auc 0.87385 prc_auc 0.93759[0m
[92maverage training of epoch 12: loss -2.65211 acc 0.33333 roc_auc 0.87280 prc_auc 0.93815[0m
[93maverage test of epoch 12: loss -2.89174 acc 0.34211 roc_auc 0.87385 prc_auc 0.93759[0m
[92maverage training of epoch 13: loss -3.01360 acc 0.33333 roc_auc 0.86240 prc_auc 0.93465[0m
[93maverage test of epoch 13: loss -3.28566 acc 0.34211 roc_auc 0.86769 prc_auc 0.93559[0m
[92maverage training of epoch 14: loss -3.42235 acc 0.33333 roc_auc 0.84800 prc_auc 0.93161[0m
[93maverage test of epoch 14: loss -3.72325 acc 0.34211 roc_auc 0.86462 prc_auc 0.93834[0m
[92maverage training of epoch 15: loss -3.87560 acc 0.33333 roc_auc 0.83340 prc_auc 0.92742[0m
[93maverage test of epoch 15: loss -4.20838 acc 0.34211 roc_auc 0.87077 prc_auc 0.93977[0m
[92maverage training of epoch 16: loss -4.39223 acc 0.33333 roc_auc 0.75640 prc_auc 0.88835[0m
[93maverage test of epoch 16: loss -4.77497 acc 0.34211 roc_auc 0.86154 prc_auc 0.93632[0m
[92maverage training of epoch 17: loss -4.99125 acc 0.33333 roc_auc 0.70100 prc_auc 0.85636[0m
[93maverage test of epoch 17: loss -5.41373 acc 0.34211 roc_auc 0.86462 prc_auc 0.93606[0m
[92maverage training of epoch 18: loss -5.64891 acc 0.33333 roc_auc 0.67860 prc_auc 0.83916[0m
[93maverage test of epoch 18: loss -6.09812 acc 0.34211 roc_auc 0.87077 prc_auc 0.94037[0m
[92maverage training of epoch 19: loss -6.35103 acc 0.33333 roc_auc 0.65320 prc_auc 0.81514[0m
[93maverage test of epoch 19: loss -6.82799 acc 0.34211 roc_auc 0.87692 prc_auc 0.94369[0m
[92maverage training of epoch 20: loss -7.09902 acc 0.33333 roc_auc 0.62500 prc_auc 0.78917[0m
[93maverage test of epoch 20: loss -7.60409 acc 0.34211 roc_auc 0.88615 prc_auc 0.95009[0m
[92maverage training of epoch 21: loss -7.89338 acc 0.33333 roc_auc 0.60160 prc_auc 0.76155[0m
[93maverage test of epoch 21: loss -8.42618 acc 0.34211 roc_auc 0.88615 prc_auc 0.95059[0m
[92maverage training of epoch 22: loss -8.73344 acc 0.33333 roc_auc 0.58720 prc_auc 0.74159[0m
[93maverage test of epoch 22: loss -9.29392 acc 0.34211 roc_auc 0.88615 prc_auc 0.94995[0m
[92maverage training of epoch 23: loss -9.61939 acc 0.33333 roc_auc 0.57720 prc_auc 0.72617[0m
[93maverage test of epoch 23: loss -10.20821 acc 0.34211 roc_auc 0.88308 prc_auc 0.94772[0m
[92maverage training of epoch 24: loss -10.55125 acc 0.33333 roc_auc 0.56680 prc_auc 0.71498[0m
[93maverage test of epoch 24: loss -11.16762 acc 0.34211 roc_auc 0.89846 prc_auc 0.95397[0m
[92maverage training of epoch 25: loss -11.53872 acc 0.33333 roc_auc 0.56500 prc_auc 0.71687[0m
[93maverage test of epoch 25: loss -12.19866 acc 0.34211 roc_auc 0.88615 prc_auc 0.94995[0m
[92maverage training of epoch 26: loss -12.58840 acc 0.33333 roc_auc 0.56240 prc_auc 0.71132[0m
[93maverage test of epoch 26: loss -13.27941 acc 0.34211 roc_auc 0.89846 prc_auc 0.95762[0m
[92maverage training of epoch 27: loss -13.69312 acc 0.33333 roc_auc 0.56480 prc_auc 0.71484[0m
[93maverage test of epoch 27: loss -14.41939 acc 0.34211 roc_auc 0.91385 prc_auc 0.96126[0m
[92maverage training of epoch 28: loss -14.85902 acc 0.33333 roc_auc 0.56580 prc_auc 0.71572[0m
[93maverage test of epoch 28: loss -15.62345 acc 0.34211 roc_auc 0.91231 prc_auc 0.95955[0m
[92maverage training of epoch 29: loss -16.09155 acc 0.33333 roc_auc 0.56640 prc_auc 0.71660[0m
[93maverage test of epoch 29: loss -16.89721 acc 0.34211 roc_auc 0.91077 prc_auc 0.95838[0m
[92maverage training of epoch 30: loss -17.39643 acc 0.33333 roc_auc 0.56720 prc_auc 0.71693[0m
[93maverage test of epoch 30: loss -18.24632 acc 0.34211 roc_auc 0.90462 prc_auc 0.95665[0m
[92maverage training of epoch 31: loss -18.77743 acc 0.33333 roc_auc 0.56780 prc_auc 0.71753[0m
[93maverage test of epoch 31: loss -19.67181 acc 0.34211 roc_auc 0.89231 prc_auc 0.95080[0m
[92maverage training of epoch 32: loss -20.23485 acc 0.33333 roc_auc 0.57000 prc_auc 0.71879[0m
[93maverage test of epoch 32: loss -21.17595 acc 0.34211 roc_auc 0.89231 prc_auc 0.95053[0m
[92maverage training of epoch 33: loss -21.77411 acc 0.33333 roc_auc 0.57060 prc_auc 0.71927[0m
[93maverage test of epoch 33: loss -22.76549 acc 0.34211 roc_auc 0.89385 prc_auc 0.95064[0m
[92maverage training of epoch 34: loss -23.40176 acc 0.33333 roc_auc 0.57180 prc_auc 0.72027[0m
[93maverage test of epoch 34: loss -24.44680 acc 0.34211 roc_auc 0.88615 prc_auc 0.94805[0m
[92maverage training of epoch 35: loss -25.12375 acc 0.33333 roc_auc 0.57240 prc_auc 0.72055[0m
[93maverage test of epoch 35: loss -26.22521 acc 0.34211 roc_auc 0.88769 prc_auc 0.94946[0m
[92maverage training of epoch 36: loss -26.94442 acc 0.33333 roc_auc 0.57320 prc_auc 0.72168[0m
[93maverage test of epoch 36: loss -28.10433 acc 0.34211 roc_auc 0.87385 prc_auc 0.94312[0m
[92maverage training of epoch 37: loss -28.86356 acc 0.33333 roc_auc 0.57380 prc_auc 0.72199[0m
[93maverage test of epoch 37: loss -30.07900 acc 0.34211 roc_auc 0.86462 prc_auc 0.93927[0m
[92maverage training of epoch 38: loss -30.87726 acc 0.33333 roc_auc 0.57460 prc_auc 0.72234[0m
[93maverage test of epoch 38: loss -32.14801 acc 0.34211 roc_auc 0.87846 prc_auc 0.94351[0m
[92maverage training of epoch 39: loss -32.98580 acc 0.33333 roc_auc 0.57500 prc_auc 0.72255[0m
[93maverage test of epoch 39: loss -34.31487 acc 0.34211 roc_auc 0.87692 prc_auc 0.94256[0m
[92maverage training of epoch 40: loss -35.19630 acc 0.33333 roc_auc 0.57520 prc_auc 0.72270[0m
[93maverage test of epoch 40: loss -36.58479 acc 0.34211 roc_auc 0.83077 prc_auc 0.92017[0m
[92maverage training of epoch 41: loss -37.50481 acc 0.33333 roc_auc 0.57560 prc_auc 0.72302[0m
[93maverage test of epoch 41: loss -38.94868 acc 0.34211 roc_auc 0.86923 prc_auc 0.93504[0m
[92maverage training of epoch 42: loss -39.90624 acc 0.33333 roc_auc 0.57560 prc_auc 0.72275[0m
[93maverage test of epoch 42: loss -41.40993 acc 0.34211 roc_auc 0.87231 prc_auc 0.94194[0m
[92maverage training of epoch 43: loss -42.40747 acc 0.33333 roc_auc 0.57580 prc_auc 0.72294[0m
[93maverage test of epoch 43: loss -43.97055 acc 0.34211 roc_auc 0.83077 prc_auc 0.91529[0m
[92maverage training of epoch 44: loss -45.00828 acc 0.33333 roc_auc 0.57540 prc_auc 0.72221[0m
[93maverage test of epoch 44: loss -46.63250 acc 0.34211 roc_auc 0.81538 prc_auc 0.89884[0m
[92maverage training of epoch 45: loss -47.71215 acc 0.33333 roc_auc 0.57540 prc_auc 0.72221[0m
[93maverage test of epoch 45: loss -49.39590 acc 0.34211 roc_auc 0.73231 prc_auc 0.87685[0m
[92maverage training of epoch 46: loss -50.51849 acc 0.33333 roc_auc 0.57540 prc_auc 0.72220[0m
[93maverage test of epoch 46: loss -52.26152 acc 0.34211 roc_auc 0.79077 prc_auc 0.87587[0m
[92maverage training of epoch 47: loss -53.42330 acc 0.33333 roc_auc 0.57520 prc_auc 0.72182[0m
[93maverage test of epoch 47: loss -55.22737 acc 0.34211 roc_auc 0.78462 prc_auc 0.88512[0m
[92maverage training of epoch 48: loss -56.43450 acc 0.33333 roc_auc 0.57520 prc_auc 0.72181[0m
[93maverage test of epoch 48: loss -58.30048 acc 0.34211 roc_auc 0.72462 prc_auc 0.84081[0m
[92maverage training of epoch 49: loss -59.54629 acc 0.33333 roc_auc 0.57500 prc_auc 0.72141[0m
[93maverage test of epoch 49: loss -61.47333 acc 0.34211 roc_auc 0.83077 prc_auc 0.90078[0m
Training model with dataset, testing using fold 1
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 8, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=8, bias=True)
    (ReLU1): ReLU()
    (Linear2): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -1.21531 acc 0.33333 roc_auc 0.46340 prc_auc 0.65561[0m
[93maverage test of epoch 0: loss -1.42790 acc 0.34211 roc_auc 0.66462 prc_auc 0.81743[0m
[92maverage training of epoch 1: loss -1.66745 acc 0.60000 roc_auc 0.47880 prc_auc 0.66462[0m
[93maverage test of epoch 1: loss -1.92652 acc 0.65789 roc_auc 0.77538 prc_auc 0.87365[0m
[92maverage training of epoch 2: loss -2.21669 acc 0.66667 roc_auc 0.47740 prc_auc 0.66372[0m
[93maverage test of epoch 2: loss -2.52843 acc 0.65789 roc_auc 0.73538 prc_auc 0.85282[0m
[92maverage training of epoch 3: loss -2.87605 acc 0.66667 roc_auc 0.45400 prc_auc 0.63589[0m
[93maverage test of epoch 3: loss -3.27431 acc 0.65789 roc_auc 0.62462 prc_auc 0.76750[0m
[92maverage training of epoch 4: loss -3.65904 acc 0.66667 roc_auc 0.42040 prc_auc 0.64040[0m
[93maverage test of epoch 4: loss -4.03893 acc 0.65789 roc_auc 0.66154 prc_auc 0.78076[0m
[92maverage training of epoch 5: loss -4.41633 acc 0.66667 roc_auc 0.42740 prc_auc 0.63443[0m
[93maverage test of epoch 5: loss -4.81243 acc 0.65789 roc_auc 0.54769 prc_auc 0.72445[0m
[92maverage training of epoch 6: loss -5.23182 acc 0.66667 roc_auc 0.36500 prc_auc 0.58691[0m
[93maverage test of epoch 6: loss -5.68085 acc 0.65789 roc_auc 0.21846 prc_auc 0.56989[0m
[92maverage training of epoch 7: loss -6.11648 acc 0.66667 roc_auc 0.36640 prc_auc 0.57434[0m
[93maverage test of epoch 7: loss -6.56492 acc 0.65789 roc_auc 0.15385 prc_auc 0.54669[0m
[92maverage training of epoch 8: loss -6.99165 acc 0.66667 roc_auc 0.38260 prc_auc 0.57693[0m
[93maverage test of epoch 8: loss -7.43394 acc 0.65789 roc_auc 0.14615 prc_auc 0.52015[0m
[92maverage training of epoch 9: loss -7.86679 acc 0.66667 roc_auc 0.39540 prc_auc 0.58912[0m
[93maverage test of epoch 9: loss -8.31573 acc 0.65789 roc_auc 0.14154 prc_auc 0.49959[0m
[92maverage training of epoch 10: loss -8.76410 acc 0.66667 roc_auc 0.40840 prc_auc 0.59745[0m
[93maverage test of epoch 10: loss -9.22699 acc 0.65789 roc_auc 0.13692 prc_auc 0.48935[0m
[92maverage training of epoch 11: loss -9.69593 acc 0.66667 roc_auc 0.41640 prc_auc 0.60474[0m
[93maverage test of epoch 11: loss -10.17633 acc 0.65789 roc_auc 0.14308 prc_auc 0.49357[0m
[92maverage training of epoch 12: loss -10.66886 acc 0.66667 roc_auc 0.42680 prc_auc 0.61159[0m
[93maverage test of epoch 12: loss -11.16885 acc 0.65789 roc_auc 0.14308 prc_auc 0.49959[0m
[92maverage training of epoch 13: loss -11.68767 acc 0.66667 roc_auc 0.43260 prc_auc 0.61761[0m
[93maverage test of epoch 13: loss -12.20993 acc 0.65789 roc_auc 0.15385 prc_auc 0.52443[0m
[92maverage training of epoch 14: loss -12.75728 acc 0.66667 roc_auc 0.43660 prc_auc 0.62080[0m
[93maverage test of epoch 14: loss -13.30329 acc 0.65789 roc_auc 0.17846 prc_auc 0.55223[0m
[92maverage training of epoch 15: loss -13.88166 acc 0.66667 roc_auc 0.44060 prc_auc 0.62406[0m
[93maverage test of epoch 15: loss -14.45366 acc 0.65789 roc_auc 0.23846 prc_auc 0.55796[0m
[92maverage training of epoch 16: loss -15.06545 acc 0.66667 roc_auc 0.44380 prc_auc 0.62613[0m
[93maverage test of epoch 16: loss -15.66523 acc 0.65789 roc_auc 0.36154 prc_auc 0.62419[0m
[92maverage training of epoch 17: loss -16.31235 acc 0.66667 roc_auc 0.45080 prc_auc 0.63248[0m
[93maverage test of epoch 17: loss -16.94095 acc 0.65789 roc_auc 0.41385 prc_auc 0.65913[0m
[92maverage training of epoch 18: loss -17.62416 acc 0.66667 roc_auc 0.45440 prc_auc 0.63600[0m
[93maverage test of epoch 18: loss -18.28226 acc 0.65789 roc_auc 0.40000 prc_auc 0.65386[0m
[92maverage training of epoch 19: loss -19.00369 acc 0.66667 roc_auc 0.45900 prc_auc 0.63972[0m
[93maverage test of epoch 19: loss -19.69304 acc 0.65789 roc_auc 0.40154 prc_auc 0.66622[0m
[92maverage training of epoch 20: loss -20.45523 acc 0.66667 roc_auc 0.46280 prc_auc 0.64262[0m
[93maverage test of epoch 20: loss -21.17784 acc 0.65789 roc_auc 0.38769 prc_auc 0.64502[0m
[92maverage training of epoch 21: loss -21.98344 acc 0.66667 roc_auc 0.46500 prc_auc 0.64422[0m
[93maverage test of epoch 21: loss -22.74149 acc 0.65789 roc_auc 0.41231 prc_auc 0.67143[0m
[92maverage training of epoch 22: loss -23.59375 acc 0.66667 roc_auc 0.46620 prc_auc 0.64553[0m
[93maverage test of epoch 22: loss -24.38995 acc 0.65789 roc_auc 0.33846 prc_auc 0.59835[0m
[92maverage training of epoch 23: loss -25.29256 acc 0.66667 roc_auc 0.46720 prc_auc 0.64625[0m
[93maverage test of epoch 23: loss -26.12971 acc 0.65789 roc_auc 0.37538 prc_auc 0.64143[0m
[92maverage training of epoch 24: loss -27.08709 acc 0.66667 roc_auc 0.46800 prc_auc 0.64755[0m
[93maverage test of epoch 24: loss -27.96998 acc 0.65789 roc_auc 0.26462 prc_auc 0.55143[0m
[92maverage training of epoch 25: loss -28.99357 acc 0.66667 roc_auc 0.46810 prc_auc 0.64818[0m
[93maverage test of epoch 25: loss -29.93635 acc 0.65789 roc_auc 0.30462 prc_auc 0.58090[0m
[92maverage training of epoch 26: loss -31.04682 acc 0.66667 roc_auc 0.46840 prc_auc 0.65531[0m
[93maverage test of epoch 26: loss -32.07007 acc 0.65789 roc_auc 0.21692 prc_auc 0.55656[0m
[92maverage training of epoch 27: loss -33.27575 acc 0.66667 roc_auc 0.46880 prc_auc 0.65571[0m
[93maverage test of epoch 27: loss -34.37334 acc 0.65789 roc_auc 0.42000 prc_auc 0.64717[0m
[92maverage training of epoch 28: loss -35.63412 acc 0.66667 roc_auc 0.47040 prc_auc 0.65632[0m
[93maverage test of epoch 28: loss -36.76357 acc 0.65789 roc_auc 0.74000 prc_auc 0.82211[0m
[92maverage training of epoch 29: loss -38.06166 acc 0.66667 roc_auc 0.47100 prc_auc 0.65694[0m
[93maverage test of epoch 29: loss -39.21773 acc 0.65789 roc_auc 0.75846 prc_auc 0.81158[0m
[92maverage training of epoch 30: loss -40.56693 acc 0.66667 roc_auc 0.47240 prc_auc 0.65229[0m
[93maverage test of epoch 30: loss -41.76023 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 31: loss -43.16803 acc 0.66667 roc_auc 0.47100 prc_auc 0.65180[0m
[93maverage test of epoch 31: loss -44.40457 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 32: loss -45.87532 acc 0.66667 roc_auc 0.45980 prc_auc 0.64205[0m
[93maverage test of epoch 32: loss -47.15839 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 33: loss -48.69489 acc 0.66667 roc_auc 0.48380 prc_auc 0.65861[0m
[93maverage test of epoch 33: loss -50.02620 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 34: loss -51.62949 acc 0.66667 roc_auc 0.45500 prc_auc 0.64854[0m
[93maverage test of epoch 34: loss -53.00801 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 35: loss -54.67690 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 35: loss -56.10158 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 36: loss -57.83794 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 36: loss -59.31004 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 37: loss -61.11657 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 37: loss -62.63765 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 38: loss -64.51682 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 38: loss -66.08822 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 39: loss -68.04169 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 39: loss -69.66336 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 40: loss -71.69231 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 40: loss -73.36473 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 41: loss -75.46994 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 41: loss -77.19181 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -79.37384 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 42: loss -81.14556 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -83.40519 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 43: loss -85.22587 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 44: loss -87.56424 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 44: loss -89.43444 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -91.85244 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 45: loss -93.77207 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -96.27147 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 46: loss -98.24163 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -100.82434 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 47: loss -102.84586 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -105.51325 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 48: loss -107.58654 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -110.34023 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 49: loss -112.46571 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 2
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 8, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=8, bias=True)
    (ReLU1): ReLU()
    (Linear2): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.09393 acc 0.33333 roc_auc 0.51720 prc_auc 0.71841[0m
[93maverage test of epoch 0: loss -0.18339 acc 0.34211 roc_auc 0.94154 prc_auc 0.97436[0m
[92maverage training of epoch 1: loss -0.24967 acc 0.33333 roc_auc 0.56240 prc_auc 0.74999[0m
[93maverage test of epoch 1: loss -0.34708 acc 0.34211 roc_auc 0.94462 prc_auc 0.97520[0m
[92maverage training of epoch 2: loss -0.41090 acc 0.33333 roc_auc 0.61220 prc_auc 0.79232[0m
[93maverage test of epoch 2: loss -0.50468 acc 0.34211 roc_auc 0.94462 prc_auc 0.97520[0m
[92maverage training of epoch 3: loss -0.57245 acc 0.33333 roc_auc 0.67500 prc_auc 0.82226[0m
[93maverage test of epoch 3: loss -0.66734 acc 0.34211 roc_auc 0.94769 prc_auc 0.97662[0m
[92maverage training of epoch 4: loss -0.74547 acc 0.33333 roc_auc 0.71160 prc_auc 0.84513[0m
[93maverage test of epoch 4: loss -0.85224 acc 0.34211 roc_auc 0.94769 prc_auc 0.97662[0m
[92maverage training of epoch 5: loss -0.94874 acc 0.33333 roc_auc 0.68960 prc_auc 0.83029[0m
[93maverage test of epoch 5: loss -1.07383 acc 0.34211 roc_auc 0.94462 prc_auc 0.97578[0m
[92maverage training of epoch 6: loss -1.18990 acc 0.33333 roc_auc 0.61960 prc_auc 0.77295[0m
[93maverage test of epoch 6: loss -1.33400 acc 0.34211 roc_auc 0.93538 prc_auc 0.97257[0m
[92maverage training of epoch 7: loss -1.47115 acc 0.33333 roc_auc 0.52160 prc_auc 0.70370[0m
[93maverage test of epoch 7: loss -1.63353 acc 0.34211 roc_auc 0.92308 prc_auc 0.96529[0m
[92maverage training of epoch 8: loss -1.79636 acc 0.33333 roc_auc 0.48600 prc_auc 0.68796[0m
[93maverage test of epoch 8: loss -1.98202 acc 0.34211 roc_auc 0.90462 prc_auc 0.95535[0m
[92maverage training of epoch 9: loss -2.17370 acc 0.33333 roc_auc 0.45020 prc_auc 0.67526[0m
[93maverage test of epoch 9: loss -2.39166 acc 0.34211 roc_auc 0.91077 prc_auc 0.96462[0m
[92maverage training of epoch 10: loss -2.61313 acc 0.33333 roc_auc 0.43520 prc_auc 0.64898[0m
[93maverage test of epoch 10: loss -2.86278 acc 0.34211 roc_auc 0.94769 prc_auc 0.97538[0m
[92maverage training of epoch 11: loss -3.11581 acc 0.33333 roc_auc 0.41240 prc_auc 0.62908[0m
[93maverage test of epoch 11: loss -3.39602 acc 0.34211 roc_auc 0.59077 prc_auc 0.82666[0m
[92maverage training of epoch 12: loss -3.68354 acc 0.33333 roc_auc 0.40640 prc_auc 0.61092[0m
[93maverage test of epoch 12: loss -3.99780 acc 0.34211 roc_auc 0.53231 prc_auc 0.80265[0m
[92maverage training of epoch 13: loss -4.32690 acc 0.33333 roc_auc 0.40800 prc_auc 0.61586[0m
[93maverage test of epoch 13: loss -4.68026 acc 0.34211 roc_auc 0.40923 prc_auc 0.72834[0m
[92maverage training of epoch 14: loss -5.04841 acc 0.33333 roc_auc 0.41460 prc_auc 0.62223[0m
[93maverage test of epoch 14: loss -5.43180 acc 0.34211 roc_auc 0.57385 prc_auc 0.81364[0m
[92maverage training of epoch 15: loss -5.81801 acc 0.38000 roc_auc 0.41560 prc_auc 0.62219[0m
[93maverage test of epoch 15: loss -6.20937 acc 0.65789 roc_auc 0.75692 prc_auc 0.90035[0m
[92maverage training of epoch 16: loss -6.60195 acc 0.66667 roc_auc 0.41620 prc_auc 0.62165[0m
[93maverage test of epoch 16: loss -6.99547 acc 0.65789 roc_auc 0.90462 prc_auc 0.96368[0m
[92maverage training of epoch 17: loss -7.39590 acc 0.66667 roc_auc 0.41840 prc_auc 0.62312[0m
[93maverage test of epoch 17: loss -7.79454 acc 0.65789 roc_auc 0.92769 prc_auc 0.97221[0m
[92maverage training of epoch 18: loss -8.20666 acc 0.66667 roc_auc 0.41980 prc_auc 0.62368[0m
[93maverage test of epoch 18: loss -8.61379 acc 0.65789 roc_auc 0.95077 prc_auc 0.97919[0m
[92maverage training of epoch 19: loss -9.04028 acc 0.66667 roc_auc 0.42040 prc_auc 0.62379[0m
[93maverage test of epoch 19: loss -9.45813 acc 0.65789 roc_auc 0.95692 prc_auc 0.98092[0m
[92maverage training of epoch 20: loss -9.90062 acc 0.66667 roc_auc 0.42240 prc_auc 0.62540[0m
[93maverage test of epoch 20: loss -10.33042 acc 0.65789 roc_auc 0.95692 prc_auc 0.98092[0m
[92maverage training of epoch 21: loss -10.78933 acc 0.66667 roc_auc 0.42320 prc_auc 0.62571[0m
[93maverage test of epoch 21: loss -11.23074 acc 0.65789 roc_auc 0.95692 prc_auc 0.98092[0m
[92maverage training of epoch 22: loss -11.70589 acc 0.66667 roc_auc 0.42360 prc_auc 0.62622[0m
[93maverage test of epoch 22: loss -12.15856 acc 0.65789 roc_auc 0.95385 prc_auc 0.97950[0m
[92maverage training of epoch 23: loss -12.64886 acc 0.66667 roc_auc 0.42520 prc_auc 0.62840[0m
[93maverage test of epoch 23: loss -13.11195 acc 0.65789 roc_auc 0.95538 prc_auc 0.97950[0m
[92maverage training of epoch 24: loss -13.61853 acc 0.66667 roc_auc 0.42640 prc_auc 0.62936[0m
[93maverage test of epoch 24: loss -14.09337 acc 0.65789 roc_auc 0.95538 prc_auc 0.97950[0m
[92maverage training of epoch 25: loss -14.61777 acc 0.66667 roc_auc 0.42700 prc_auc 0.63050[0m
[93maverage test of epoch 25: loss -15.10553 acc 0.65789 roc_auc 0.95385 prc_auc 0.97950[0m
[92maverage training of epoch 26: loss -15.64928 acc 0.66667 roc_auc 0.42860 prc_auc 0.63191[0m
[93maverage test of epoch 26: loss -16.15107 acc 0.65789 roc_auc 0.95846 prc_auc 0.98092[0m
[92maverage training of epoch 27: loss -16.71546 acc 0.66667 roc_auc 0.42840 prc_auc 0.63254[0m
[93maverage test of epoch 27: loss -17.23221 acc 0.65789 roc_auc 0.95692 prc_auc 0.97950[0m
[92maverage training of epoch 28: loss -17.81851 acc 0.66667 roc_auc 0.43020 prc_auc 0.63460[0m
[93maverage test of epoch 28: loss -18.35110 acc 0.65789 roc_auc 0.96154 prc_auc 0.97968[0m
[92maverage training of epoch 29: loss -18.96055 acc 0.66667 roc_auc 0.43120 prc_auc 0.63585[0m
[93maverage test of epoch 29: loss -19.50988 acc 0.65789 roc_auc 0.96154 prc_auc 0.97536[0m
[92maverage training of epoch 30: loss -20.14353 acc 0.66667 roc_auc 0.43240 prc_auc 0.63740[0m
[93maverage test of epoch 30: loss -20.71014 acc 0.65789 roc_auc 0.95077 prc_auc 0.95919[0m
[92maverage training of epoch 31: loss -21.36922 acc 0.66667 roc_auc 0.43240 prc_auc 0.63728[0m
[93maverage test of epoch 31: loss -21.95403 acc 0.65789 roc_auc 0.89077 prc_auc 0.91263[0m
[92maverage training of epoch 32: loss -22.63954 acc 0.66667 roc_auc 0.43450 prc_auc 0.63892[0m
[93maverage test of epoch 32: loss -23.24298 acc 0.65789 roc_auc 0.71077 prc_auc 0.76954[0m
[92maverage training of epoch 33: loss -23.95604 acc 0.66667 roc_auc 0.43670 prc_auc 0.64098[0m
[93maverage test of epoch 33: loss -24.57886 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 34: loss -25.32051 acc 0.66667 roc_auc 0.43870 prc_auc 0.64255[0m
[93maverage test of epoch 34: loss -25.96328 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 35: loss -26.73453 acc 0.66667 roc_auc 0.44020 prc_auc 0.64047[0m
[93maverage test of epoch 35: loss -27.39777 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 36: loss -28.19928 acc 0.66667 roc_auc 0.43460 prc_auc 0.63582[0m
[93maverage test of epoch 36: loss -28.88447 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 37: loss -29.71784 acc 0.66667 roc_auc 0.44220 prc_auc 0.64085[0m
[93maverage test of epoch 37: loss -30.42343 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 38: loss -31.28921 acc 0.66667 roc_auc 0.44340 prc_auc 0.64135[0m
[93maverage test of epoch 38: loss -32.01662 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 39: loss -32.91570 acc 0.66667 roc_auc 0.44390 prc_auc 0.64162[0m
[93maverage test of epoch 39: loss -33.66526 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 40: loss -34.59513 acc 0.66667 roc_auc 0.43500 prc_auc 0.63295[0m
[93maverage test of epoch 40: loss -35.37089 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 41: loss -36.33868 acc 0.66667 roc_auc 0.43960 prc_auc 0.63299[0m
[93maverage test of epoch 41: loss -37.13246 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -38.13430 acc 0.66667 roc_auc 0.44500 prc_auc 0.64370[0m
[93maverage test of epoch 42: loss -38.94898 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -39.98543 acc 0.66667 roc_auc 0.43790 prc_auc 0.62933[0m
[93maverage test of epoch 43: loss -40.82119 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 44: loss -41.89289 acc 0.66667 roc_auc 0.43600 prc_auc 0.63480[0m
[93maverage test of epoch 44: loss -42.74971 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -43.85582 acc 0.66667 roc_auc 0.49000 prc_auc 0.66226[0m
[93maverage test of epoch 45: loss -44.73098 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -45.86993 acc 0.66667 roc_auc 0.46500 prc_auc 0.65161[0m
[93maverage test of epoch 46: loss -46.76234 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -47.93488 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 47: loss -48.84509 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -50.05213 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 48: loss -50.98066 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -52.22297 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 49: loss -53.17019 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 3
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 8, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=8, bias=True)
    (ReLU1): ReLU()
    (Linear2): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.72979 acc 0.34437 roc_auc 0.46922 prc_auc 0.65854[0m
[93maverage test of epoch 0: loss 0.54271 acc 0.67568 roc_auc 0.85000 prc_auc 0.90577[0m
[92maverage training of epoch 1: loss 0.34047 acc 0.66225 roc_auc 0.46843 prc_auc 0.65852[0m
[93maverage test of epoch 1: loss 0.11014 acc 0.67568 roc_auc 0.86333 prc_auc 0.91835[0m
[92maverage training of epoch 2: loss 0.00732 acc 0.66225 roc_auc 0.47333 prc_auc 0.68049[0m
[93maverage test of epoch 2: loss -0.05785 acc 0.67568 roc_auc 0.78000 prc_auc 0.88029[0m
[92maverage training of epoch 3: loss -0.09699 acc 0.66225 roc_auc 0.40118 prc_auc 0.63337[0m
[93maverage test of epoch 3: loss -0.15110 acc 0.67568 roc_auc 0.69333 prc_auc 0.81348[0m
[92maverage training of epoch 4: loss -0.19392 acc 0.66225 roc_auc 0.23784 prc_auc 0.51821[0m
[93maverage test of epoch 4: loss -0.25748 acc 0.67568 roc_auc 0.18000 prc_auc 0.54809[0m
[92maverage training of epoch 5: loss -0.31961 acc 0.66225 roc_auc 0.31373 prc_auc 0.55695[0m
[93maverage test of epoch 5: loss -0.40751 acc 0.67568 roc_auc 0.18000 prc_auc 0.54809[0m
[92maverage training of epoch 6: loss -0.50415 acc 0.66225 roc_auc 0.33824 prc_auc 0.56019[0m
[93maverage test of epoch 6: loss -0.62706 acc 0.67568 roc_auc 0.18000 prc_auc 0.54809[0m
[92maverage training of epoch 7: loss -0.75800 acc 0.66225 roc_auc 0.38078 prc_auc 0.58335[0m
[93maverage test of epoch 7: loss -0.91058 acc 0.67568 roc_auc 0.18000 prc_auc 0.54809[0m
[92maverage training of epoch 8: loss -1.05639 acc 0.66225 roc_auc 0.33549 prc_auc 0.56183[0m
[93maverage test of epoch 8: loss -1.21618 acc 0.67568 roc_auc 0.18333 prc_auc 0.55209[0m
[92maverage training of epoch 9: loss -1.38013 acc 0.66225 roc_auc 0.38098 prc_auc 0.58178[0m
[93maverage test of epoch 9: loss -1.55964 acc 0.67568 roc_auc 0.18000 prc_auc 0.55121[0m
[92maverage training of epoch 10: loss -1.75421 acc 0.66225 roc_auc 0.42373 prc_auc 0.61506[0m
[93maverage test of epoch 10: loss -1.96728 acc 0.67568 roc_auc 0.18333 prc_auc 0.55255[0m
[92maverage training of epoch 11: loss -2.20186 acc 0.56954 roc_auc 0.44216 prc_auc 0.61598[0m
[93maverage test of epoch 11: loss -2.44928 acc 0.32432 roc_auc 0.14667 prc_auc 0.51146[0m
[92maverage training of epoch 12: loss -2.70225 acc 0.33775 roc_auc 0.38059 prc_auc 0.58404[0m
[93maverage test of epoch 12: loss -2.95479 acc 0.32432 roc_auc 0.14667 prc_auc 0.51146[0m
[92maverage training of epoch 13: loss -3.20472 acc 0.33775 roc_auc 0.35294 prc_auc 0.57176[0m
[93maverage test of epoch 13: loss -3.45347 acc 0.32432 roc_auc 0.18167 prc_auc 0.55161[0m
[92maverage training of epoch 14: loss -3.70497 acc 0.33775 roc_auc 0.34725 prc_auc 0.58428[0m
[93maverage test of epoch 14: loss -3.95785 acc 0.32432 roc_auc 0.18667 prc_auc 0.55421[0m
[92maverage training of epoch 15: loss -4.21691 acc 0.33775 roc_auc 0.35961 prc_auc 0.59150[0m
[93maverage test of epoch 15: loss -4.47926 acc 0.32432 roc_auc 0.21667 prc_auc 0.58875[0m
[92maverage training of epoch 16: loss -4.74820 acc 0.33775 roc_auc 0.37686 prc_auc 0.59031[0m
[93maverage test of epoch 16: loss -5.02176 acc 0.32432 roc_auc 0.23000 prc_auc 0.59220[0m
[92maverage training of epoch 17: loss -5.30177 acc 0.33775 roc_auc 0.37235 prc_auc 0.58556[0m
[93maverage test of epoch 17: loss -5.58840 acc 0.32432 roc_auc 0.25333 prc_auc 0.62058[0m
[92maverage training of epoch 18: loss -5.88143 acc 0.33775 roc_auc 0.36725 prc_auc 0.58210[0m
[93maverage test of epoch 18: loss -6.17982 acc 0.32432 roc_auc 0.58667 prc_auc 0.82020[0m
[92maverage training of epoch 19: loss -6.47688 acc 0.33775 roc_auc 0.36333 prc_auc 0.58027[0m
[93maverage test of epoch 19: loss -6.78244 acc 0.32432 roc_auc 0.78000 prc_auc 0.88938[0m
[92maverage training of epoch 20: loss -7.08887 acc 0.33775 roc_auc 0.36412 prc_auc 0.57778[0m
[93maverage test of epoch 20: loss -7.40583 acc 0.64865 roc_auc 0.78833 prc_auc 0.89007[0m
[92maverage training of epoch 21: loss -7.72249 acc 0.60265 roc_auc 0.36588 prc_auc 0.57688[0m
[93maverage test of epoch 21: loss -8.05253 acc 0.67568 roc_auc 0.76833 prc_auc 0.89421[0m
[92maverage training of epoch 22: loss -8.38064 acc 0.66225 roc_auc 0.36627 prc_auc 0.57360[0m
[93maverage test of epoch 22: loss -8.72502 acc 0.67568 roc_auc 0.78833 prc_auc 0.90024[0m
[92maverage training of epoch 23: loss -9.06570 acc 0.66225 roc_auc 0.37451 prc_auc 0.57512[0m
[93maverage test of epoch 23: loss -9.42584 acc 0.67568 roc_auc 0.79167 prc_auc 0.90489[0m
[92maverage training of epoch 24: loss -9.77949 acc 0.66225 roc_auc 0.37314 prc_auc 0.56753[0m
[93maverage test of epoch 24: loss -10.15650 acc 0.67568 roc_auc 0.80833 prc_auc 0.90598[0m
[92maverage training of epoch 25: loss -10.52478 acc 0.66225 roc_auc 0.37529 prc_auc 0.57050[0m
[93maverage test of epoch 25: loss -10.92071 acc 0.67568 roc_auc 0.81500 prc_auc 0.91089[0m
[92maverage training of epoch 26: loss -11.30479 acc 0.66225 roc_auc 0.37725 prc_auc 0.57128[0m
[93maverage test of epoch 26: loss -11.72059 acc 0.67568 roc_auc 0.80833 prc_auc 0.90340[0m
[92maverage training of epoch 27: loss -12.12180 acc 0.66225 roc_auc 0.37725 prc_auc 0.57103[0m
[93maverage test of epoch 27: loss -12.55910 acc 0.67568 roc_auc 0.81333 prc_auc 0.90608[0m
[92maverage training of epoch 28: loss -12.97902 acc 0.66225 roc_auc 0.37667 prc_auc 0.57045[0m
[93maverage test of epoch 28: loss -13.43942 acc 0.67568 roc_auc 0.81667 prc_auc 0.90398[0m
[92maverage training of epoch 29: loss -13.87959 acc 0.66225 roc_auc 0.37647 prc_auc 0.57093[0m
[93maverage test of epoch 29: loss -14.36459 acc 0.67568 roc_auc 0.81500 prc_auc 0.90484[0m
[92maverage training of epoch 30: loss -14.82653 acc 0.66225 roc_auc 0.37569 prc_auc 0.56965[0m
[93maverage test of epoch 30: loss -15.33752 acc 0.67568 roc_auc 0.81667 prc_auc 0.90693[0m
[92maverage training of epoch 31: loss -15.82230 acc 0.66225 roc_auc 0.37304 prc_auc 0.56660[0m
[93maverage test of epoch 31: loss -16.36012 acc 0.67568 roc_auc 0.82167 prc_auc 0.90721[0m
[92maverage training of epoch 32: loss -16.86888 acc 0.66225 roc_auc 0.37461 prc_auc 0.56907[0m
[93maverage test of epoch 32: loss -17.43472 acc 0.67568 roc_auc 0.83000 prc_auc 0.90240[0m
[92maverage training of epoch 33: loss -17.96851 acc 0.66225 roc_auc 0.37608 prc_auc 0.57297[0m
[93maverage test of epoch 33: loss -18.56289 acc 0.67568 roc_auc 0.86167 prc_auc 0.91769[0m
[92maverage training of epoch 34: loss -19.12187 acc 0.66225 roc_auc 0.37510 prc_auc 0.57352[0m
[93maverage test of epoch 34: loss -19.74522 acc 0.67568 roc_auc 0.84833 prc_auc 0.90230[0m
[92maverage training of epoch 35: loss -20.33020 acc 0.66225 roc_auc 0.37569 prc_auc 0.57490[0m
[93maverage test of epoch 35: loss -20.98321 acc 0.67568 roc_auc 0.83500 prc_auc 0.90001[0m
[92maverage training of epoch 36: loss -21.59496 acc 0.66225 roc_auc 0.37510 prc_auc 0.57643[0m
[93maverage test of epoch 36: loss -22.27848 acc 0.67568 roc_auc 0.83667 prc_auc 0.89145[0m
[92maverage training of epoch 37: loss -22.91767 acc 0.66225 roc_auc 0.37431 prc_auc 0.57446[0m
[93maverage test of epoch 37: loss -23.63211 acc 0.67568 roc_auc 0.83167 prc_auc 0.89241[0m
[92maverage training of epoch 38: loss -24.29739 acc 0.66225 roc_auc 0.37471 prc_auc 0.57264[0m
[93maverage test of epoch 38: loss -25.04143 acc 0.67568 roc_auc 0.82500 prc_auc 0.89375[0m
[92maverage training of epoch 39: loss -25.73213 acc 0.66225 roc_auc 0.37510 prc_auc 0.57106[0m
[93maverage test of epoch 39: loss -26.50633 acc 0.67568 roc_auc 0.76833 prc_auc 0.84625[0m
[92maverage training of epoch 40: loss -27.22343 acc 0.66225 roc_auc 0.37275 prc_auc 0.56719[0m
[93maverage test of epoch 40: loss -28.02868 acc 0.67568 roc_auc 0.80500 prc_auc 0.85872[0m
[92maverage training of epoch 41: loss -28.77292 acc 0.66225 roc_auc 0.37392 prc_auc 0.56754[0m
[93maverage test of epoch 41: loss -29.61024 acc 0.67568 roc_auc 0.80667 prc_auc 0.90332[0m
[92maverage training of epoch 42: loss -30.38166 acc 0.66225 roc_auc 0.37353 prc_auc 0.56723[0m
[93maverage test of epoch 42: loss -31.25036 acc 0.67568 roc_auc 0.73667 prc_auc 0.81760[0m
[92maverage training of epoch 43: loss -32.04748 acc 0.66225 roc_auc 0.37392 prc_auc 0.56749[0m
[93maverage test of epoch 43: loss -32.94713 acc 0.67568 roc_auc 0.78000 prc_auc 0.85165[0m
[92maverage training of epoch 44: loss -33.77071 acc 0.66225 roc_auc 0.37529 prc_auc 0.56959[0m
[93maverage test of epoch 44: loss -34.70224 acc 0.67568 roc_auc 0.73667 prc_auc 0.81817[0m
[92maverage training of epoch 45: loss -35.55173 acc 0.66225 roc_auc 0.37559 prc_auc 0.57009[0m
[93maverage test of epoch 45: loss -36.51512 acc 0.67568 roc_auc 0.64000 prc_auc 0.76649[0m
[92maverage training of epoch 46: loss -37.39118 acc 0.66225 roc_auc 0.37667 prc_auc 0.57103[0m
[93maverage test of epoch 46: loss -38.38780 acc 0.67568 roc_auc 0.66000 prc_auc 0.75975[0m
[92maverage training of epoch 47: loss -39.29103 acc 0.66225 roc_auc 0.37735 prc_auc 0.57014[0m
[93maverage test of epoch 47: loss -40.32157 acc 0.67568 roc_auc 0.66833 prc_auc 0.77789[0m
[92maverage training of epoch 48: loss -41.25278 acc 0.66225 roc_auc 0.37843 prc_auc 0.57118[0m
[93maverage test of epoch 48: loss -42.31847 acc 0.67568 roc_auc 0.58000 prc_auc 0.72346[0m
[92maverage training of epoch 49: loss -43.27852 acc 0.66225 roc_auc 0.37990 prc_auc 0.57197[0m
[93maverage test of epoch 49: loss -44.38063 acc 0.67568 roc_auc 0.86000 prc_auc 0.89108[0m
Training model with dataset, testing using fold 4
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 8, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=8, bias=True)
    (ReLU1): ReLU()
    (Linear2): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.06700 acc 0.33775 roc_auc 0.29510 prc_auc 0.56403[0m
[93maverage test of epoch 0: loss -0.10198 acc 0.32432 roc_auc 0.34333 prc_auc 0.70804[0m
[92maverage training of epoch 1: loss -0.12755 acc 0.33775 roc_auc 0.36275 prc_auc 0.58340[0m
[93maverage test of epoch 1: loss -0.16446 acc 0.67568 roc_auc 0.34333 prc_auc 0.70804[0m
[92maverage training of epoch 2: loss -0.19485 acc 0.62252 roc_auc 0.43804 prc_auc 0.66816[0mUsing backend: pytorch
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

[93maverage test of epoch 2: loss -0.24193 acc 0.67568 roc_auc 0.94000 prc_auc 0.97546[0m
[92maverage training of epoch 3: loss -0.27785 acc 0.65563 roc_auc 0.48922 prc_auc 0.70749[0m
[93maverage test of epoch 3: loss -0.33246 acc 0.67568 roc_auc 0.94000 prc_auc 0.97546[0m
[92maverage training of epoch 4: loss -0.37208 acc 0.65563 roc_auc 0.53667 prc_auc 0.73474[0m
[93maverage test of epoch 4: loss -0.42926 acc 0.67568 roc_auc 0.94000 prc_auc 0.97546[0m
[92maverage training of epoch 5: loss -0.46307 acc 0.66225 roc_auc 0.64078 prc_auc 0.81361[0m
[93maverage test of epoch 5: loss -0.51419 acc 0.67568 roc_auc 0.93667 prc_auc 0.97435[0m
[92maverage training of epoch 6: loss -0.54768 acc 0.66225 roc_auc 0.70765 prc_auc 0.85010[0m
[93maverage test of epoch 6: loss -0.60131 acc 0.67568 roc_auc 0.93333 prc_auc 0.97332[0m
[92maverage training of epoch 7: loss -0.64306 acc 0.66225 roc_auc 0.65078 prc_auc 0.82022[0m
[93maverage test of epoch 7: loss -0.70695 acc 0.67568 roc_auc 0.93333 prc_auc 0.97332[0m
[92maverage training of epoch 8: loss -0.76504 acc 0.66225 roc_auc 0.64000 prc_auc 0.80921[0m
[93maverage test of epoch 8: loss -0.85776 acc 0.67568 roc_auc 0.93333 prc_auc 0.97377[0m
[92maverage training of epoch 9: loss -0.94904 acc 0.66225 roc_auc 0.60235 prc_auc 0.77972[0m
[93maverage test of epoch 9: loss -1.07349 acc 0.67568 roc_auc 0.93333 prc_auc 0.97377[0m
[92maverage training of epoch 10: loss -1.18385 acc 0.66225 roc_auc 0.58118 prc_auc 0.76131[0m
[93maverage test of epoch 10: loss -1.33054 acc 0.67568 roc_auc 0.93667 prc_auc 0.97536[0m
[92maverage training of epoch 11: loss -1.45120 acc 0.66225 roc_auc 0.56078 prc_auc 0.74123[0m
[93maverage test of epoch 11: loss -1.61134 acc 0.67568 roc_auc 0.93667 prc_auc 0.97536[0m
[92maverage training of epoch 12: loss -1.73666 acc 0.66225 roc_auc 0.53647 prc_auc 0.71851[0m
[93maverage test of epoch 12: loss -1.90706 acc 0.67568 roc_auc 0.93667 prc_auc 0.97536[0m
[92maverage training of epoch 13: loss -2.03909 acc 0.66225 roc_auc 0.52392 prc_auc 0.70564[0m
[93maverage test of epoch 13: loss -2.22404 acc 0.67568 roc_auc 0.93667 prc_auc 0.97536[0m
[92maverage training of epoch 14: loss -2.36794 acc 0.66225 roc_auc 0.50902 prc_auc 0.69662[0m
[93maverage test of epoch 14: loss -2.57397 acc 0.67568 roc_auc 0.93667 prc_auc 0.97536[0m
[92maverage training of epoch 15: loss -2.73492 acc 0.66225 roc_auc 0.49784 prc_auc 0.68909[0m
[93maverage test of epoch 15: loss -2.96576 acc 0.67568 roc_auc 0.93667 prc_auc 0.97536[0m
[92maverage training of epoch 16: loss -3.14044 acc 0.66225 roc_auc 0.48706 prc_auc 0.68088[0m
[93maverage test of epoch 16: loss -3.38799 acc 0.67568 roc_auc 0.93667 prc_auc 0.97536[0m
[92maverage training of epoch 17: loss -3.57305 acc 0.66225 roc_auc 0.47941 prc_auc 0.67748[0m
[93maverage test of epoch 17: loss -3.83564 acc 0.67568 roc_auc 0.93333 prc_auc 0.97377[0m
[92maverage training of epoch 18: loss -4.03033 acc 0.66225 roc_auc 0.48843 prc_auc 0.69976[0m
[93maverage test of epoch 18: loss -4.30887 acc 0.67568 roc_auc 0.93333 prc_auc 0.97377[0m
[92maverage training of epoch 19: loss -4.51509 acc 0.66225 roc_auc 0.50451 prc_auc 0.71789[0m
[93maverage test of epoch 19: loss -4.81269 acc 0.67568 roc_auc 0.93000 prc_auc 0.97235[0m
[92maverage training of epoch 20: loss -5.03372 acc 0.66225 roc_auc 0.55098 prc_auc 0.75725[0m
[93maverage test of epoch 20: loss -5.35210 acc 0.67568 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 21: loss -5.59152 acc 0.66225 roc_auc 0.58059 prc_auc 0.78222[0m
[93maverage test of epoch 21: loss -5.93496 acc 0.67568 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 22: loss -6.20036 acc 0.66225 roc_auc 0.63373 prc_auc 0.81135[0m
[93maverage test of epoch 22: loss -6.57829 acc 0.67568 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 23: loss -6.88523 acc 0.66225 roc_auc 0.73373 prc_auc 0.86168[0m
[93maverage test of epoch 23: loss -7.31404 acc 0.67568 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 24: loss -7.66023 acc 0.66225 roc_auc 0.71275 prc_auc 0.85633[0m
[93maverage test of epoch 24: loss -8.13031 acc 0.67568 roc_auc 0.93000 prc_auc 0.97264[0m
[92maverage training of epoch 25: loss -8.50732 acc 0.66225 roc_auc 0.49275 prc_auc 0.68142[0m
[93maverage test of epoch 25: loss -9.00706 acc 0.67568 roc_auc 0.82500 prc_auc 0.92396[0m
[92maverage training of epoch 26: loss -9.39741 acc 0.66225 roc_auc 0.41275 prc_auc 0.62024[0m
[93maverage test of epoch 26: loss -9.91805 acc 0.67568 roc_auc 0.62167 prc_auc 0.82610[0m
[92maverage training of epoch 27: loss -10.33487 acc 0.66225 roc_auc 0.41735 prc_auc 0.61861[0m
[93maverage test of epoch 27: loss -10.89913 acc 0.67568 roc_auc 0.50500 prc_auc 0.74642[0m
[92maverage training of epoch 28: loss -11.34049 acc 0.66225 roc_auc 0.42255 prc_auc 0.62246[0m
[93maverage test of epoch 28: loss -11.94130 acc 0.67568 roc_auc 0.77500 prc_auc 0.87796[0m
[92maverage training of epoch 29: loss -12.40180 acc 0.66225 roc_auc 0.42412 prc_auc 0.62377[0m
[93maverage test of epoch 29: loss -13.03704 acc 0.67568 roc_auc 0.92167 prc_auc 0.96087[0m
[92maverage training of epoch 30: loss -13.51406 acc 0.66225 roc_auc 0.42431 prc_auc 0.62393[0m
[93maverage test of epoch 30: loss -14.18349 acc 0.67568 roc_auc 0.93167 prc_auc 0.95918[0m
[92maverage training of epoch 31: loss -14.67625 acc 0.66225 roc_auc 0.42451 prc_auc 0.62399[0m
[93maverage test of epoch 31: loss -15.37998 acc 0.67568 roc_auc 0.94667 prc_auc 0.96121[0m
[92maverage training of epoch 32: loss -15.88692 acc 0.66225 roc_auc 0.42451 prc_auc 0.62399[0m
[93maverage test of epoch 32: loss -16.62534 acc 0.67568 roc_auc 0.78000 prc_auc 0.85333[0m
[92maverage training of epoch 33: loss -17.14680 acc 0.66225 roc_auc 0.42451 prc_auc 0.62399[0m
[93maverage test of epoch 33: loss -17.92117 acc 0.67568 roc_auc 0.80000 prc_auc 0.87027[0m
[92maverage training of epoch 34: loss -18.45736 acc 0.66225 roc_auc 0.42441 prc_auc 0.62406[0m
[93maverage test of epoch 34: loss -19.26938 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 35: loss -19.82098 acc 0.66225 roc_auc 0.42412 prc_auc 0.62333[0m
[93maverage test of epoch 35: loss -20.67221 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 36: loss -21.23920 acc 0.66225 roc_auc 0.42412 prc_auc 0.62164[0m
[93maverage test of epoch 36: loss -22.13095 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 37: loss -22.71300 acc 0.66225 roc_auc 0.42235 prc_auc 0.62011[0m
[93maverage test of epoch 37: loss -23.64534 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 38: loss -24.24045 acc 0.66225 roc_auc 0.42010 prc_auc 0.61845[0m
[93maverage test of epoch 38: loss -25.21210 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 39: loss -25.81864 acc 0.66225 roc_auc 0.42451 prc_auc 0.62698[0m
[93maverage test of epoch 39: loss -26.82993 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 40: loss -27.44772 acc 0.66225 roc_auc 0.42010 prc_auc 0.62014[0m
[93maverage test of epoch 40: loss -28.49905 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 41: loss -29.12782 acc 0.66225 roc_auc 0.42892 prc_auc 0.63166[0m
[93maverage test of epoch 41: loss -30.22023 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 42: loss -30.86048 acc 0.66225 roc_auc 0.41588 prc_auc 0.62911[0m
[93maverage test of epoch 42: loss -31.99542 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 43: loss -32.64734 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 43: loss -33.82597 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 44: loss -34.48757 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 44: loss -35.71043 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 45: loss -36.38734 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 45: loss -37.65789 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 46: loss -38.34710 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 46: loss -39.66579 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 47: loss -40.36917 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 47: loss -41.73795 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 48: loss -42.45569 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 48: loss -43.86951 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 49: loss -44.60642 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 49: loss -46.08080 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
Run statistics: 
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 16)
('hidden_size_dfscode_rnn', 8)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 2)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))


Accuracy (avg): 0.60185 ROC_AUC (avg): 0.63815 PRC_AUC (avg): 0.75667 

Average forward propagation time taken(ms): 3.1304925954121514
Average backward propagation time taken(ms): 1.005066484553497

