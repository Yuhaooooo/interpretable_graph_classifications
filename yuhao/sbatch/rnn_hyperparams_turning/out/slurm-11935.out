# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======






Params Turning Set
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 100)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 0)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))






args.base_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/
args.graph_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/
args.min_dfscode_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/
args.min_dfscode_tensor_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/
*** 1 train_index:  [  0   2   3   6   9  10  11  12  13  14  15  16  17  19  20  21  22  23
  24  26  27  30  31  32  33  35  36  38  39  41  42  43  44  45  46  47
  48  49  50  52  53  54  56  57  59  60  61  62  63  64  65  66  67  69
  70  71  72  73  74  76  77  78  79  80  81  82  83  84  87  88  89  91
  92  94  95  96  97  98 100 101 102 103 104 106 107 108 109 110 112 113
 115 116 117 118 119 120 121 122 124 125 126 127 129 130 131 132 134 136
 138 139 140 141 142 144 145 146 147 148 149 151 152 153 154 155 156 157
 158 159 160 161 164 165 166 167 169 170 171 172 173 175 176 177 178 179
 180 182 184 185 186 187]
*** 2 test_index:  [  1   4   5   7   8  18  25  28  29  34  37  40  51  55  58  68  75  85
  86  90  93  99 105 111 114 123 128 133 135 137 143 150 162 163 168 174
 181 183]
*** 1 train_index:  [  0   1   2   4   5   6   7   8  10  11  12  13  14  15  16  17  18  19
  20  21  22  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39
  40  41  43  44  45  46  49  50  51  52  53  54  55  56  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  74  75  76  77  79  80  81  82
  83  84  85  86  88  90  91  92  93  94  96  97  98  99 101 102 103 104
 105 106 107 111 112 113 114 115 120 121 122 123 124 125 126 128 129 130
 131 133 134 135 136 137 138 139 140 143 144 145 147 148 150 153 154 155
 156 159 162 163 164 165 167 168 169 170 171 172 173 174 175 177 178 179
 180 181 182 183 186 187]
*** 2 test_index:  [  3   9  23  31  42  47  48  57  72  73  78  87  89  95 100 108 109 110
 116 117 118 119 127 132 141 142 146 149 151 152 157 158 160 161 166 176
 184 185]
*** 1 train_index:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19
  20  23  24  25  26  28  29  31  32  34  35  36  37  38  40  41  42  43
  44  47  48  51  53  54  55  56  57  58  59  60  64  66  68  70  71  72
  73  74  75  76  77  78  79  81  82  84  85  86  87  88  89  90  92  93
  95  96  97  98  99 100 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134
 135 137 139 140 141 142 143 144 146 148 149 150 151 152 153 156 157 158
 159 160 161 162 163 165 166 167 168 169 171 172 173 174 175 176 177 178
 179 181 183 184 185 186]
*** 2 test_index:  [  0  16  21  22  27  30  33  39  45  46  49  50  52  61  62  63  65  67
  69  80  83  91  94 101 102 103 122 136 138 145 147 154 155 164 170 180
 182 187]
*** 1 train_index:  [  0   1   3   4   5   6   7   8   9  13  14  16  18  19  21  22  23  25
  26  27  28  29  30  31  33  34  36  37  38  39  40  42  43  45  46  47
  48  49  50  51  52  53  55  56  57  58  60  61  62  63  65  66  67  68
  69  71  72  73  75  76  77  78  79  80  81  83  84  85  86  87  88  89
  90  91  92  93  94  95  97  98  99 100 101 102 103 105 106 108 109 110
 111 113 114 115 116 117 118 119 120 121 122 123 125 127 128 131 132 133
 134 135 136 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152
 154 155 156 157 158 160 161 162 163 164 165 166 168 170 171 174 175 176
 180 181 182 183 184 185 187]
*** 2 test_index:  [  2  10  11  12  15  17  20  24  32  35  41  44  54  59  64  70  74  82
  96 104 107 112 124 126 129 130 139 153 159 167 169 172 173 177 178 179
 186]
*** 1 train_index:  [  0   1   2   3   4   5   7   8   9  10  11  12  15  16  17  18  20  21
  22  23  24  25  27  28  29  30  31  32  33  34  35  37  39  40  41  42
  44  45  46  47  48  49  50  51  52  54  55  57  58  59  61  62  63  64
  65  67  68  69  70  72  73  74  75  78  80  82  83  85  86  87  89  90
  91  93  94  95  96  99 100 101 102 103 104 105 107 108 109 110 111 112
 114 116 117 118 119 122 123 124 126 127 128 129 130 132 133 135 136 137
 138 139 141 142 143 145 146 147 149 150 151 152 153 154 155 157 158 159
 160 161 162 163 164 166 167 168 169 170 172 173 174 176 177 178 179 180
 181 182 183 184 185 186 187]
*** 2 test_index:  [  6  13  14  19  26  36  38  43  53  56  60  66  71  76  77  79  81  84
  88  92  97  98 106 113 115 120 121 125 131 134 140 144 148 156 165 171
 175]
{'base_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/',
 'batch_size': 1,
 'clean_temp': False,
 'clean_tensorboard': False,
 'current_dataset_path': None,
 'current_min_dfscode_path': None,
 'current_model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-23-38-25/',
 'current_processed_dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'current_temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-23-38-25/',
 'dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/datasets/',
 'device': device(type='cuda', index=0),
 'dfscode_rnn_dropout': 0.2,
 'embedding_size_dfscode_rnn': 8,
 'epochs': 100,
 'epochs_end': 10000,
 'epochs_save': 20,
 'epochs_validate': 1,
 'fname': 'DFScodeRNN_cls_LSTM_MUTAG',
 'gradient_clipping': True,
 'graph_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/',
 'graph_type': 'MUTAG',
 'graphgen_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/',
 'hidden_size_dfscode_rnn': 4,
 'load_device': device(type='cuda', index=0),
 'load_model': False,
 'load_model_path': '',
 'log_tensorboard': False,
 'lr': 0.0001,
 'max_prev_node': None,
 'min_dfscode_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/',
 'min_dfscode_tensor_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/',
 'note': 'DFScodeRNN_cls_LSTM',
 'num_graphs': None,
 'num_layers': 1,
 'num_workers': 8,
 'number_of_mlp_layer': 0,
 'rnn_type': 'LSTM',
 'save_model': False,
 'temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/',
 'tensorboard_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tensorboard/',
 'time': '2021-01-17-23-38-25',
 'weights': False}


graphgen args.__dict__: None




dataset_features: {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}




config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 100, 'learning_rate': 0.0001, 'seed': 1800, 'k_fold': 5, 'model': 'DFScodeRNN_cls_LSTM', 'dataset': 'MUTAG'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}}


Training a new model: DFScodeRNN_cls_LSTM
Training model with dataset, testing using fold 0
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.02279 acc 0.33333 roc_auc 0.44400 prc_auc 0.69195[0m
[93maverage test of epoch 0: loss -0.03386 acc 0.34211 roc_auc 0.51692 prc_auc 0.78475[0m
[92maverage training of epoch 1: loss -0.08011 acc 0.33333 roc_auc 0.47240 prc_auc 0.71267[0m
[93maverage test of epoch 1: loss -0.13933 acc 0.34211 roc_auc 0.77538 prc_auc 0.89364[0m
[92maverage training of epoch 2: loss -0.19190 acc 0.33333 roc_auc 0.51380 prc_auc 0.74628[0m
[93maverage test of epoch 2: loss -0.25516 acc 0.34211 roc_auc 0.85538 prc_auc 0.92845[0m
[92maverage training of epoch 3: loss -0.31542 acc 0.33333 roc_auc 0.53360 prc_auc 0.75890[0m
[93maverage test of epoch 3: loss -0.38424 acc 0.34211 roc_auc 0.83692 prc_auc 0.92213[0m
[92maverage training of epoch 4: loss -0.44854 acc 0.33333 roc_auc 0.54520 prc_auc 0.76659[0m
[93maverage test of epoch 4: loss -0.51972 acc 0.34211 roc_auc 0.81846 prc_auc 0.91518[0m
[92maverage training of epoch 5: loss -0.58248 acc 0.35333 roc_auc 0.55900 prc_auc 0.77565[0m
[93maverage test of epoch 5: loss -0.65140 acc 0.34211 roc_auc 0.81846 prc_auc 0.91528[0m
[92maverage training of epoch 6: loss -0.71027 acc 0.35333 roc_auc 0.57420 prc_auc 0.78759[0m
[93maverage test of epoch 6: loss -0.77416 acc 0.34211 roc_auc 0.81846 prc_auc 0.91549[0m
[92maverage training of epoch 7: loss -0.82817 acc 0.35333 roc_auc 0.58760 prc_auc 0.79558[0m
[93maverage test of epoch 7: loss -0.88683 acc 0.39474 roc_auc 0.83077 prc_auc 0.92502[0m
[92maverage training of epoch 8: loss -0.93661 acc 0.37333 roc_auc 0.59740 prc_auc 0.80142[0m
[93maverage test of epoch 8: loss -0.99172 acc 0.39474 roc_auc 0.83692 prc_auc 0.92698[0m
[92maverage training of epoch 9: loss -1.03844 acc 0.37333 roc_auc 0.61220 prc_auc 0.81019[0m
[93maverage test of epoch 9: loss -1.09195 acc 0.42105 roc_auc 0.85538 prc_auc 0.93130[0m
[92maverage training of epoch 10: loss -1.13664 acc 0.44667 roc_auc 0.63140 prc_auc 0.82206[0m
[93maverage test of epoch 10: loss -1.18998 acc 0.50000 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 11: loss -1.23353 acc 0.56000 roc_auc 0.64680 prc_auc 0.83125[0m
[93maverage test of epoch 11: loss -1.28722 acc 0.81579 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 12: loss -1.33032 acc 0.68667 roc_auc 0.67800 prc_auc 0.84674[0m
[93maverage test of epoch 12: loss -1.38366 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 13: loss -1.42633 acc 0.66667 roc_auc 0.70920 prc_auc 0.86379[0m
[93maverage test of epoch 13: loss -1.47767 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 14: loss -1.51943 acc 0.66667 roc_auc 0.74700 prc_auc 0.88057[0m
[93maverage test of epoch 14: loss -1.56755 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 15: loss -1.60864 acc 0.66667 roc_auc 0.77740 prc_auc 0.89422[0m
[93maverage test of epoch 15: loss -1.65379 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 16: loss -1.69554 acc 0.66667 roc_auc 0.80320 prc_auc 0.90516[0m
[93maverage test of epoch 16: loss -1.73892 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 17: loss -1.78302 acc 0.66667 roc_auc 0.82080 prc_auc 0.91185[0m
[93maverage test of epoch 17: loss -1.82579 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 18: loss -1.87322 acc 0.66667 roc_auc 0.83680 prc_auc 0.91766[0m
[93maverage test of epoch 18: loss -1.91537 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 19: loss -1.96549 acc 0.66667 roc_auc 0.84430 prc_auc 0.91942[0m
[93maverage test of epoch 19: loss -2.00576 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 20: loss -2.05724 acc 0.66667 roc_auc 0.85140 prc_auc 0.92145[0m
[93maverage test of epoch 20: loss -2.09488 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 21: loss -2.14726 acc 0.66667 roc_auc 0.86080 prc_auc 0.92414[0m
[93maverage test of epoch 21: loss -2.18275 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 22: loss -2.23625 acc 0.66667 roc_auc 0.86580 prc_auc 0.92472[0m
[93maverage test of epoch 22: loss -2.27044 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 23: loss -2.32538 acc 0.66667 roc_auc 0.86660 prc_auc 0.92475[0m
[93maverage test of epoch 23: loss -2.35877 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 24: loss -2.41525 acc 0.66667 roc_auc 0.86520 prc_auc 0.92370[0m
[93maverage test of epoch 24: loss -2.44797 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 25: loss -2.50577 acc 0.66667 roc_auc 0.86220 prc_auc 0.92265[0m
[93maverage test of epoch 25: loss -2.53762 acc 0.65789 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 26: loss -2.59636 acc 0.66667 roc_auc 0.85760 prc_auc 0.92084[0m
[93maverage test of epoch 26: loss -2.62701 acc 0.65789 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 27: loss -2.68634 acc 0.66667 roc_auc 0.85500 prc_auc 0.92006[0m
[93maverage test of epoch 27: loss -2.71548 acc 0.65789 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 28: loss -2.77510 acc 0.66667 roc_auc 0.85360 prc_auc 0.91860[0m
[93maverage test of epoch 28: loss -2.80252 acc 0.65789 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 29: loss -2.86217 acc 0.66667 roc_auc 0.85240 prc_auc 0.91584[0m
[93maverage test of epoch 29: loss -2.88782 acc 0.65789 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 30: loss -2.94729 acc 0.66667 roc_auc 0.85260 prc_auc 0.91485[0m
[93maverage test of epoch 30: loss -2.97120 acc 0.65789 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 31: loss -3.03038 acc 0.66667 roc_auc 0.85120 prc_auc 0.91161[0m
[93maverage test of epoch 31: loss -3.05263 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 32: loss -3.11145 acc 0.66667 roc_auc 0.85000 prc_auc 0.90817[0m
[93maverage test of epoch 32: loss -3.13214 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 33: loss -3.19058 acc 0.66667 roc_auc 0.84820 prc_auc 0.90556[0m
[93maverage test of epoch 33: loss -3.20983 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 34: loss -3.26789 acc 0.66667 roc_auc 0.84500 prc_auc 0.90163[0m
[93maverage test of epoch 34: loss -3.28582 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 35: loss -3.34351 acc 0.66667 roc_auc 0.84120 prc_auc 0.89860[0m
[93maverage test of epoch 35: loss -3.36025 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 36: loss -3.41758 acc 0.66667 roc_auc 0.83590 prc_auc 0.89515[0m
[93maverage test of epoch 36: loss -3.43324 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 37: loss -3.49023 acc 0.66667 roc_auc 0.83100 prc_auc 0.89095[0m
[93maverage test of epoch 37: loss -3.50490 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 38: loss -3.56158 acc 0.66667 roc_auc 0.82240 prc_auc 0.88295[0m
[93maverage test of epoch 38: loss -3.57534 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 39: loss -3.63174 acc 0.66667 roc_auc 0.81540 prc_auc 0.87747[0m
[93maverage test of epoch 39: loss -3.64467 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 40: loss -3.70081 acc 0.66667 roc_auc 0.80860 prc_auc 0.87363[0m
[93maverage test of epoch 40: loss -3.71298 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 41: loss -3.76887 acc 0.66667 roc_auc 0.80140 prc_auc 0.86379[0m
[93maverage test of epoch 41: loss -3.78035 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 42: loss -3.83602 acc 0.66667 roc_auc 0.79280 prc_auc 0.85774[0m
[93maverage test of epoch 42: loss -3.84685 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 43: loss -3.90233 acc 0.66667 roc_auc 0.78540 prc_auc 0.85233[0m
[93maverage test of epoch 43: loss -3.91256 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 44: loss -3.96785 acc 0.66667 roc_auc 0.77380 prc_auc 0.84656[0m
[93maverage test of epoch 44: loss -3.97753 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 45: loss -4.03267 acc 0.66667 roc_auc 0.75760 prc_auc 0.83862[0m
[93maverage test of epoch 45: loss -4.04183 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 46: loss -4.09682 acc 0.66667 roc_auc 0.74080 prc_auc 0.82870[0m
[93maverage test of epoch 46: loss -4.10549 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 47: loss -4.16036 acc 0.66667 roc_auc 0.72550 prc_auc 0.82022[0m
[93maverage test of epoch 47: loss -4.16857 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 48: loss -4.22333 acc 0.66667 roc_auc 0.70860 prc_auc 0.81251[0m
[93maverage test of epoch 48: loss -4.23112 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 49: loss -4.28578 acc 0.66667 roc_auc 0.68930 prc_auc 0.80165[0m
[93maverage test of epoch 49: loss -4.29316 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 50: loss -4.34774 acc 0.66667 roc_auc 0.66900 prc_auc 0.78910[0m
[93maverage test of epoch 50: loss -4.35473 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 51: loss -4.40925 acc 0.66667 roc_auc 0.65130 prc_auc 0.77844[0m
[93maverage test of epoch 51: loss -4.41588 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 52: loss -4.47034 acc 0.66667 roc_auc 0.63620 prc_auc 0.77309[0m
[93maverage test of epoch 52: loss -4.47662 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 53: loss -4.53104 acc 0.66667 roc_auc 0.62220 prc_auc 0.76594[0m
[93maverage test of epoch 53: loss -4.53699 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 54: loss -4.59138 acc 0.66667 roc_auc 0.60830 prc_auc 0.75838[0m
[93maverage test of epoch 54: loss -4.59700 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 55: loss -4.65137 acc 0.66667 roc_auc 0.59340 prc_auc 0.75076[0m
[93maverage test of epoch 55: loss -4.65669 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 56: loss -4.71105 acc 0.66667 roc_auc 0.57720 prc_auc 0.74232[0m
[93maverage test of epoch 56: loss -4.71607 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 57: loss -4.77044 acc 0.66667 roc_auc 0.56360 prc_auc 0.73502[0m
[93maverage test of epoch 57: loss -4.77517 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 58: loss -4.82955 acc 0.66667 roc_auc 0.55140 prc_auc 0.72988[0m
[93maverage test of epoch 58: loss -4.83401 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 59: loss -4.88840 acc 0.66667 roc_auc 0.53920 prc_auc 0.72192[0m
[93maverage test of epoch 59: loss -4.89259 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 60: loss -4.94701 acc 0.66667 roc_auc 0.52440 prc_auc 0.71064[0m
[93maverage test of epoch 60: loss -4.95095 acc 0.65789 roc_auc 0.86462 prc_auc 0.93423[0m
[92maverage training of epoch 61: loss -5.00539 acc 0.66667 roc_auc 0.51240 prc_auc 0.70211[0m
[93maverage test of epoch 61: loss -5.00908 acc 0.65789 roc_auc 0.86462 prc_auc 0.93423[0m
[92maverage training of epoch 62: loss -5.06357 acc 0.66667 roc_auc 0.50180 prc_auc 0.69447[0m
[93maverage test of epoch 62: loss -5.06702 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 63: loss -5.12156 acc 0.66667 roc_auc 0.49110 prc_auc 0.68638[0m
[93maverage test of epoch 63: loss -5.12477 acc 0.65789 roc_auc 0.86462 prc_auc 0.93423[0m
[92maverage training of epoch 64: loss -5.17936 acc 0.66667 roc_auc 0.47870 prc_auc 0.67804[0m
[93maverage test of epoch 64: loss -5.18234 acc 0.65789 roc_auc 0.86462 prc_auc 0.93423[0m
[92maverage training of epoch 65: loss -5.23699 acc 0.66667 roc_auc 0.46690 prc_auc 0.66990[0m
[93maverage test of epoch 65: loss -5.23974 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 66: loss -5.29446 acc 0.66667 roc_auc 0.45720 prc_auc 0.66351[0m
[93maverage test of epoch 66: loss -5.29699 acc 0.65789 roc_auc 0.86462 prc_auc 0.93423[0m
[92maverage training of epoch 67: loss -5.35178 acc 0.66667 roc_auc 0.44780 prc_auc 0.65614[0m
[93maverage test of epoch 67: loss -5.35409 acc 0.65789 roc_auc 0.86462 prc_auc 0.93423[0m
[92maverage training of epoch 68: loss -5.40896 acc 0.66667 roc_auc 0.44040 prc_auc 0.65191[0m
[93maverage test of epoch 68: loss -5.41106 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 69: loss -5.46601 acc 0.66667 roc_auc 0.43380 prc_auc 0.64858[0m
[93maverage test of epoch 69: loss -5.46791 acc 0.65789 roc_auc 0.86462 prc_auc 0.93423[0m
[92maverage training of epoch 70: loss -5.52294 acc 0.66667 roc_auc 0.42960 prc_auc 0.64587[0m
[93maverage test of epoch 70: loss -5.52463 acc 0.65789 roc_auc 0.86308 prc_auc 0.93090[0m
[92maverage training of epoch 71: loss -5.57975 acc 0.66667 roc_auc 0.42520 prc_auc 0.64077[0m
[93maverage test of epoch 71: loss -5.58125 acc 0.65789 roc_auc 0.86462 prc_auc 0.93455[0m
[92maverage training of epoch 72: loss -5.63646 acc 0.66667 roc_auc 0.42060 prc_auc 0.63427[0m
[93maverage test of epoch 72: loss -5.63776 acc 0.65789 roc_auc 0.86462 prc_auc 0.93435[0m
[92maverage training of epoch 73: loss -5.69307 acc 0.66667 roc_auc 0.41490 prc_auc 0.62924[0m
[93maverage test of epoch 73: loss -5.69418 acc 0.65789 roc_auc 0.86769 prc_auc 0.93440[0m
[92maverage training of epoch 74: loss -5.74959 acc 0.66667 roc_auc 0.41160 prc_auc 0.62718[0m
[93maverage test of epoch 74: loss -5.75050 acc 0.65789 roc_auc 0.86308 prc_auc 0.93118[0m
[92maverage training of epoch 75: loss -5.80602 acc 0.66667 roc_auc 0.40760 prc_auc 0.62317[0m
[93maverage test of epoch 75: loss -5.80674 acc 0.65789 roc_auc 0.85231 prc_auc 0.90692[0m
[92maverage training of epoch 76: loss -5.86237 acc 0.66667 roc_auc 0.40320 prc_auc 0.62074[0m
[93maverage test of epoch 76: loss -5.86290 acc 0.65789 roc_auc 0.84769 prc_auc 0.89785[0m
[92maverage training of epoch 77: loss -5.91864 acc 0.66667 roc_auc 0.40040 prc_auc 0.61986[0m
[93maverage test of epoch 77: loss -5.91899 acc 0.65789 roc_auc 0.84615 prc_auc 0.89691[0m
[92maverage training of epoch 78: loss -5.97484 acc 0.66667 roc_auc 0.39730 prc_auc 0.61743[0m
[93maverage test of epoch 78: loss -5.97501 acc 0.65789 roc_auc 0.84615 prc_auc 0.89692[0m
[92maverage training of epoch 79: loss -6.03098 acc 0.66667 roc_auc 0.39450 prc_auc 0.61242[0m
[93maverage test of epoch 79: loss -6.03096 acc 0.65789 roc_auc 0.84615 prc_auc 0.89745[0m
[92maverage training of epoch 80: loss -6.08705 acc 0.66667 roc_auc 0.39100 prc_auc 0.60957[0m
[93maverage test of epoch 80: loss -6.08686 acc 0.65789 roc_auc 0.84923 prc_auc 0.90237[0m
[92maverage training of epoch 81: loss -6.14307 acc 0.66667 roc_auc 0.38910 prc_auc 0.60768[0m
[93maverage test of epoch 81: loss -6.14270 acc 0.65789 roc_auc 0.84923 prc_auc 0.90269[0m
[92maverage training of epoch 82: loss -6.19903 acc 0.66667 roc_auc 0.38720 prc_auc 0.60600[0m
[93maverage test of epoch 82: loss -6.19848 acc 0.65789 roc_auc 0.85077 prc_auc 0.90468[0m
[92maverage training of epoch 83: loss -6.25495 acc 0.66667 roc_auc 0.38540 prc_auc 0.60377[0m
[93maverage test of epoch 83: loss -6.25422 acc 0.65789 roc_auc 0.86923 prc_auc 0.93483[0m
[92maverage training of epoch 84: loss -6.31081 acc 0.66667 roc_auc 0.38220 prc_auc 0.60126[0m
[93maverage test of epoch 84: loss -6.30991 acc 0.65789 roc_auc 0.85385 prc_auc 0.91259[0m
[92maverage training of epoch 85: loss -6.36664 acc 0.66667 roc_auc 0.37970 prc_auc 0.59860[0m
[93maverage test of epoch 85: loss -6.36556 acc 0.65789 roc_auc 0.84462 prc_auc 0.89709[0m
[92maverage training of epoch 86: loss -6.42242 acc 0.66667 roc_auc 0.37770 prc_auc 0.59654[0m
[93maverage test of epoch 86: loss -6.42117 acc 0.65789 roc_auc 0.85231 prc_auc 0.90712[0m
[92maverage training of epoch 87: loss -6.47816 acc 0.66667 roc_auc 0.37630 prc_auc 0.59504[0m
[93maverage test of epoch 87: loss -6.47675 acc 0.65789 roc_auc 0.85077 prc_auc 0.90759[0m
[92maverage training of epoch 88: loss -6.53387 acc 0.66667 roc_auc 0.37500 prc_auc 0.59383[0m
[93maverage test of epoch 88: loss -6.53228 acc 0.65789 roc_auc 0.86923 prc_auc 0.93452[0m
[92maverage training of epoch 89: loss -6.58954 acc 0.66667 roc_auc 0.37430 prc_auc 0.59376[0m
[93maverage test of epoch 89: loss -6.58779 acc 0.65789 roc_auc 0.84769 prc_auc 0.90240[0m
[92maverage training of epoch 90: loss -6.64519 acc 0.66667 roc_auc 0.37350 prc_auc 0.59354[0m
[93maverage test of epoch 90: loss -6.64326 acc 0.65789 roc_auc 0.85231 prc_auc 0.90767[0m
[92maverage training of epoch 91: loss -6.70080 acc 0.66667 roc_auc 0.37270 prc_auc 0.59256[0m
[93maverage test of epoch 91: loss -6.69871 acc 0.65789 roc_auc 0.85077 prc_auc 0.90729[0m
[92maverage training of epoch 92: loss -6.75639 acc 0.66667 roc_auc 0.37180 prc_auc 0.59077[0m
[93maverage test of epoch 92: loss -6.75413 acc 0.65789 roc_auc 0.83846 prc_auc 0.88912[0m
[92maverage training of epoch 93: loss -6.81195 acc 0.66667 roc_auc 0.37120 prc_auc 0.58916[0m
[93maverage test of epoch 93: loss -6.80953 acc 0.65789 roc_auc 0.85692 prc_auc 0.91020[0m
[92maverage training of epoch 94: loss -6.86749 acc 0.66667 roc_auc 0.37070 prc_auc 0.58880[0m
[93maverage test of epoch 94: loss -6.86491 acc 0.65789 roc_auc 0.85077 prc_auc 0.90729[0m
[92maverage training of epoch 95: loss -6.92301 acc 0.66667 roc_auc 0.36950 prc_auc 0.58804[0m
[93maverage test of epoch 95: loss -6.92026 acc 0.65789 roc_auc 0.85077 prc_auc 0.90729[0m
[92maverage training of epoch 96: loss -6.97851 acc 0.66667 roc_auc 0.36930 prc_auc 0.58752[0m
[93maverage test of epoch 96: loss -6.97560 acc 0.65789 roc_auc 0.84615 prc_auc 0.90180[0m
[92maverage training of epoch 97: loss -7.03399 acc 0.66667 roc_auc 0.36850 prc_auc 0.58718[0m
[93maverage test of epoch 97: loss -7.03091 acc 0.65789 roc_auc 0.85077 prc_auc 0.90758[0m
[92maverage training of epoch 98: loss -7.08946 acc 0.66667 roc_auc 0.36800 prc_auc 0.58636[0m
[93maverage test of epoch 98: loss -7.08621 acc 0.65789 roc_auc 0.85385 prc_auc 0.90890[0m
[92maverage training of epoch 99: loss -7.14490 acc 0.66667 roc_auc 0.36770 prc_auc 0.58614[0m
[93maverage test of epoch 99: loss -7.14150 acc 0.65789 roc_auc 0.85385 prc_auc 0.90890[0m
Training model with dataset, testing using fold 1
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.33976 acc 0.33333 roc_auc 0.63320 prc_auc 0.80265[0m
[93maverage test of epoch 0: loss 0.28140 acc 0.34211 roc_auc 0.72308 prc_auc 0.87482[0m
[92maverage training of epoch 1: loss 0.25025 acc 0.33333 roc_auc 0.63100 prc_auc 0.80229[0m
[93maverage test of epoch 1: loss 0.19151 acc 0.34211 roc_auc 0.71692 prc_auc 0.87436[0m
[92maverage training of epoch 2: loss 0.15831 acc 0.33333 roc_auc 0.62320 prc_auc 0.79501[0m
[93maverage test of epoch 2: loss 0.09744 acc 0.34211 roc_auc 0.71385 prc_auc 0.87359[0m
[92maverage training of epoch 3: loss 0.06188 acc 0.33333 roc_auc 0.60600 prc_auc 0.77510[0m
[93maverage test of epoch 3: loss -0.00163 acc 0.34211 roc_auc 0.67692 prc_auc 0.84640[0m
[92maverage training of epoch 4: loss -0.03931 acc 0.33333 roc_auc 0.56620 prc_auc 0.72664[0m
[93maverage test of epoch 4: loss -0.10574 acc 0.34211 roc_auc 0.46462 prc_auc 0.73891[0m
[92maverage training of epoch 5: loss -0.14573 acc 0.33333 roc_auc 0.51360 prc_auc 0.68378[0m
[93maverage test of epoch 5: loss -0.21662 acc 0.34211 roc_auc 0.21231 prc_auc 0.55739[0m
[92maverage training of epoch 6: loss -0.26015 acc 0.33333 roc_auc 0.44780 prc_auc 0.62156[0m
[93maverage test of epoch 6: loss -0.33842 acc 0.34211 roc_auc 0.10154 prc_auc 0.48146[0m
[92maverage training of epoch 7: loss -0.38825 acc 0.33333 roc_auc 0.39220 prc_auc 0.59406[0m
[93maverage test of epoch 7: loss -0.47887 acc 0.34211 roc_auc 0.11692 prc_auc 0.48528[0m
[92maverage training of epoch 8: loss -0.53992 acc 0.33333 roc_auc 0.36440 prc_auc 0.58126[0m
[93maverage test of epoch 8: loss -0.65015 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 9: loss -0.72766 acc 0.33333 roc_auc 0.34320 prc_auc 0.57201[0m
[93maverage test of epoch 9: loss -0.86149 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 10: loss -0.95018 acc 0.33333 roc_auc 0.32360 prc_auc 0.56222[0m
[93maverage test of epoch 10: loss -1.09514 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 11: loss -1.17561 acc 0.33333 roc_auc 0.28680 prc_auc 0.54424[0m
[93maverage test of epoch 11: loss -1.30770 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 12: loss -1.36624 acc 0.33333 roc_auc 0.24380 prc_auc 0.52549[0m
[93maverage test of epoch 12: loss -1.47505 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 13: loss -1.51606 acc 0.33333 roc_auc 0.19360 prc_auc 0.50375[0m
[93maverage test of epoch 13: loss -1.60808 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 14: loss -1.63913 acc 0.33333 roc_auc 0.14660 prc_auc 0.48629[0m
[93maverage test of epoch 14: loss -1.72158 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 15: loss -1.74687 acc 0.33333 roc_auc 0.11720 prc_auc 0.47769[0m
[93maverage test of epoch 15: loss -1.82382 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 16: loss -1.84538 acc 0.33333 roc_auc 0.09620 prc_auc 0.47226[0m
[93maverage test of epoch 16: loss -1.91896 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 17: loss -1.93782 acc 0.33333 roc_auc 0.08420 prc_auc 0.46948[0m
[93maverage test of epoch 17: loss -2.00922 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 18: loss -2.02597 acc 0.33333 roc_auc 0.07760 prc_auc 0.46809[0m
[93maverage test of epoch 18: loss -2.09588 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 19: loss -2.11091 acc 0.33333 roc_auc 0.07720 prc_auc 0.46806[0m
[93maverage test of epoch 19: loss -2.17976 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 20: loss -2.19331 acc 0.33333 roc_auc 0.08560 prc_auc 0.46997[0m
[93maverage test of epoch 20: loss -2.26137 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 21: loss -2.27363 acc 0.33333 roc_auc 0.10260 prc_auc 0.47426[0m
[93maverage test of epoch 21: loss -2.34106 acc 0.34211 roc_auc 0.11692 prc_auc 0.48512[0m
[92maverage training of epoch 22: loss -2.35217 acc 0.33333 roc_auc 0.12120 prc_auc 0.47977[0m
[93maverage test of epoch 22: loss -2.41906 acc 0.34211 roc_auc 0.11692 prc_auc 0.48512[0m
[92maverage training of epoch 23: loss -2.42912 acc 0.33333 roc_auc 0.14280 prc_auc 0.48658[0m
[93maverage test of epoch 23: loss -2.49552 acc 0.34211 roc_auc 0.11692 prc_auc 0.48512[0m
[92maverage training of epoch 24: loss -2.50461 acc 0.33333 roc_auc 0.16600 prc_auc 0.49385[0m
[93maverage test of epoch 24: loss -2.57053 acc 0.34211 roc_auc 0.11692 prc_auc 0.48512[0m
[92maverage training of epoch 25: loss -2.57873 acc 0.33333 roc_auc 0.20040 prc_auc 0.50481[0m
[93maverage test of epoch 25: loss -2.64415 acc 0.34211 roc_auc 0.11692 prc_auc 0.48512[0m
[92maverage training of epoch 26: loss -2.65152 acc 0.33333 roc_auc 0.24320 prc_auc 0.51929[0m
[93maverage test of epoch 26: loss -2.71641 acc 0.34211 roc_auc 0.11692 prc_auc 0.48512[0m
[92maverage training of epoch 27: loss -2.72299 acc 0.33333 roc_auc 0.27600 prc_auc 0.53070[0m
[93maverage test of epoch 27: loss -2.78731 acc 0.34211 roc_auc 0.11692 prc_auc 0.48512[0m
[92maverage training of epoch 28: loss -2.79317 acc 0.33333 roc_auc 0.29880 prc_auc 0.53988[0m
[93maverage test of epoch 28: loss -2.85688 acc 0.34211 roc_auc 0.11692 prc_auc 0.48497[0m
[92maverage training of epoch 29: loss -2.86206 acc 0.33333 roc_auc 0.31640 prc_auc 0.54694[0m
[93maverage test of epoch 29: loss -2.92512 acc 0.34211 roc_auc 0.12000 prc_auc 0.48584[0m
[92maverage training of epoch 30: loss -2.92967 acc 0.33333 roc_auc 0.33200 prc_auc 0.55317[0m
[93maverage test of epoch 30: loss -2.99204 acc 0.34211 roc_auc 0.12923 prc_auc 0.48859[0m
[92maverage training of epoch 31: loss -2.99601 acc 0.33333 roc_auc 0.34520 prc_auc 0.55853[0m
[93maverage test of epoch 31: loss -3.05766 acc 0.34211 roc_auc 0.12923 prc_auc 0.48859[0m
[92maverage training of epoch 32: loss -3.06111 acc 0.33333 roc_auc 0.35960 prc_auc 0.56466[0m
[93maverage test of epoch 32: loss -3.12201 acc 0.34211 roc_auc 0.12923 prc_auc 0.48859[0m
[92maverage training of epoch 33: loss -3.12499 acc 0.33333 roc_auc 0.36770 prc_auc 0.56837[0m
[93maverage test of epoch 33: loss -3.18513 acc 0.34211 roc_auc 0.13846 prc_auc 0.49031[0m
[92maverage training of epoch 34: loss -3.18768 acc 0.33333 roc_auc 0.37120 prc_auc 0.57026[0m
[93maverage test of epoch 34: loss -3.24704 acc 0.34211 roc_auc 0.14462 prc_auc 0.49213[0m
[92maverage training of epoch 35: loss -3.24923 acc 0.33333 roc_auc 0.37940 prc_auc 0.57423[0m
[93maverage test of epoch 35: loss -3.30781 acc 0.34211 roc_auc 0.15077 prc_auc 0.49340[0m
[92maverage training of epoch 36: loss -3.30969 acc 0.33333 roc_auc 0.38380 prc_auc 0.57665[0m
[93maverage test of epoch 36: loss -3.36748 acc 0.34211 roc_auc 0.16615 prc_auc 0.50769[0m
[92maverage training of epoch 37: loss -3.36910 acc 0.33333 roc_auc 0.38660 prc_auc 0.57769[0m
[93maverage test of epoch 37: loss -3.42612 acc 0.34211 roc_auc 0.16615 prc_auc 0.52674[0m
[92maverage training of epoch 38: loss -3.42752 acc 0.33333 roc_auc 0.39100 prc_auc 0.57954[0m
[93maverage test of epoch 38: loss -3.48380 acc 0.34211 roc_auc 0.16923 prc_auc 0.52769[0m
[92maverage training of epoch 39: loss -3.48504 acc 0.33333 roc_auc 0.39300 prc_auc 0.58052[0m
[93maverage test of epoch 39: loss -3.54059 acc 0.34211 roc_auc 0.17077 prc_auc 0.52760[0m
[92maverage training of epoch 40: loss -3.54171 acc 0.33333 roc_auc 0.39540 prc_auc 0.58140[0m
[93maverage test of epoch 40: loss -3.59657 acc 0.34211 roc_auc 0.17231 prc_auc 0.52963[0m
[92maverage training of epoch 41: loss -3.59762 acc 0.33333 roc_auc 0.40000 prc_auc 0.58230[0m
[93maverage test of epoch 41: loss -3.65185 acc 0.34211 roc_auc 0.18000 prc_auc 0.53014[0m
[92maverage training of epoch 42: loss -3.65288 acc 0.33333 roc_auc 0.40340 prc_auc 0.58431[0m
[93maverage test of epoch 42: loss -3.70655 acc 0.34211 roc_auc 0.18923 prc_auc 0.53280[0m
[92maverage training of epoch 43: loss -3.70761 acc 0.33333 roc_auc 0.41040 prc_auc 0.59869[0m
[93maverage test of epoch 43: loss -3.76079 acc 0.34211 roc_auc 0.23385 prc_auc 0.57058[0m
[92maverage training of epoch 44: loss -3.76195 acc 0.33333 roc_auc 0.41620 prc_auc 0.60795[0m
[93maverage test of epoch 44: loss -3.81477 acc 0.34211 roc_auc 0.28308 prc_auc 0.61126[0m
[92maverage training of epoch 45: loss -3.81610 acc 0.33333 roc_auc 0.42140 prc_auc 0.61296[0m
[93maverage test of epoch 45: loss -3.86871 acc 0.34211 roc_auc 0.29692 prc_auc 0.62983[0m
[92maverage training of epoch 46: loss -3.87030 acc 0.33333 roc_auc 0.42680 prc_auc 0.61792[0m
[93maverage test of epoch 46: loss -3.92292 acc 0.34211 roc_auc 0.31846 prc_auc 0.63785[0m
[92maverage training of epoch 47: loss -3.92490 acc 0.33333 roc_auc 0.43120 prc_auc 0.62074[0m
[93maverage test of epoch 47: loss -3.97778 acc 0.34211 roc_auc 0.33077 prc_auc 0.64092[0m
[92maverage training of epoch 48: loss -3.98032 acc 0.33333 roc_auc 0.43760 prc_auc 0.62676[0m
[93maverage test of epoch 48: loss -4.03382 acc 0.34211 roc_auc 0.34769 prc_auc 0.64622[0m
[92maverage training of epoch 49: loss -4.03710 acc 0.33333 roc_auc 0.44220 prc_auc 0.63120[0m
[93maverage test of epoch 49: loss -4.09166 acc 0.34211 roc_auc 0.36923 prc_auc 0.66998[0m
[92maverage training of epoch 50: loss -4.09592 acc 0.33333 roc_auc 0.44900 prc_auc 0.63681[0m
[93maverage test of epoch 50: loss -4.15210 acc 0.34211 roc_auc 0.37385 prc_auc 0.67172[0m
[92maverage training of epoch 51: loss -4.15762 acc 0.33333 roc_auc 0.45800 prc_auc 0.64479[0m
[93maverage test of epoch 51: loss -4.21613 acc 0.34211 roc_auc 0.47846 prc_auc 0.75599[0m
[92maverage training of epoch 52: loss -4.22319 acc 0.33333 roc_auc 0.46240 prc_auc 0.64771[0m
[93maverage test of epoch 52: loss -4.28489 acc 0.34211 roc_auc 0.48308 prc_auc 0.75621[0m
[92maverage training of epoch 53: loss -4.29381 acc 0.33333 roc_auc 0.47180 prc_auc 0.65376[0m
[93maverage test of epoch 53: loss -4.35970 acc 0.34211 roc_auc 0.56615 prc_auc 0.80597[0m
[92maverage training of epoch 54: loss -4.37090 acc 0.33333 roc_auc 0.48360 prc_auc 0.66177[0m
[93maverage test of epoch 54: loss -4.44195 acc 0.34211 roc_auc 0.58154 prc_auc 0.82281[0m
[92maverage training of epoch 55: loss -4.45617 acc 0.33333 roc_auc 0.51080 prc_auc 0.69919[0m
[93maverage test of epoch 55: loss -4.53291 acc 0.34211 roc_auc 0.75385 prc_auc 0.89566[0m
[92maverage training of epoch 56: loss -4.55155 acc 0.33333 roc_auc 0.54320 prc_auc 0.74184[0m
[93maverage test of epoch 56: loss -4.63355 acc 0.34211 roc_auc 0.87077 prc_auc 0.93161[0m
[92maverage training of epoch 57: loss -4.65675 acc 0.33333 roc_auc 0.59240 prc_auc 0.78517[0m
[93maverage test of epoch 57: loss -4.73977 acc 0.34211 roc_auc 0.88308 prc_auc 0.93577[0m
[92maverage training of epoch 58: loss -4.76315 acc 0.33333 roc_auc 0.65240 prc_auc 0.82439[0m
[93maverage test of epoch 58: loss -4.83811 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 59: loss -4.85856 acc 0.33333 roc_auc 0.71860 prc_auc 0.86144[0m
[93maverage test of epoch 59: loss -4.92278 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 60: loss -4.94131 acc 0.33333 roc_auc 0.76240 prc_auc 0.88455[0m
[93maverage test of epoch 60: loss -4.99790 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 61: loss -5.01570 acc 0.33333 roc_auc 0.78400 prc_auc 0.89300[0m
[93maverage test of epoch 61: loss -5.06753 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 62: loss -5.08514 acc 0.33333 roc_auc 0.79300 prc_auc 0.89368[0m
[93maverage test of epoch 62: loss -5.13390 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 63: loss -5.15154 acc 0.33333 roc_auc 0.80060 prc_auc 0.89463[0m
[93maverage test of epoch 63: loss -5.19816 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 64: loss -5.21591 acc 0.33333 roc_auc 0.79920 prc_auc 0.88923[0m
[93maverage test of epoch 64: loss -5.26094 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 65: loss -5.27882 acc 0.33333 roc_auc 0.79080 prc_auc 0.88197[0m
[93maverage test of epoch 65: loss -5.32262 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 66: loss -5.34066 acc 0.33333 roc_auc 0.77720 prc_auc 0.87037[0m
[93maverage test of epoch 66: loss -5.38344 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 67: loss -5.40165 acc 0.33333 roc_auc 0.75580 prc_auc 0.85556[0m
[93maverage test of epoch 67: loss -5.44358 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 68: loss -5.46196 acc 0.33333 roc_auc 0.72660 prc_auc 0.83847[0m
[93maverage test of epoch 68: loss -5.50315 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 69: loss -5.52171 acc 0.33333 roc_auc 0.70260 prc_auc 0.82586[0m
[93maverage test of epoch 69: loss -5.56225 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 70: loss -5.58098 acc 0.33333 roc_auc 0.67780 prc_auc 0.80868[0m
[93maverage test of epoch 70: loss -5.62095 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 71: loss -5.63986 acc 0.33333 roc_auc 0.65990 prc_auc 0.79619[0m
[93maverage test of epoch 71: loss -5.67929 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 72: loss -5.69839 acc 0.33333 roc_auc 0.64140 prc_auc 0.78585[0m
[93maverage test of epoch 72: loss -5.73734 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 73: loss -5.75661 acc 0.33333 roc_auc 0.62380 prc_auc 0.77283[0m
[93maverage test of epoch 73: loss -5.79512 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 74: loss -5.81457 acc 0.33333 roc_auc 0.60720 prc_auc 0.76373[0m
[93maverage test of epoch 74: loss -5.85266 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 75: loss -5.87229 acc 0.33333 roc_auc 0.58910 prc_auc 0.75301[0m
[93maverage test of epoch 75: loss -5.90999 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 76: loss -5.92981 acc 0.33333 roc_auc 0.57100 prc_auc 0.74350[0m
[93maverage test of epoch 76: loss -5.96714 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 77: loss -5.98713 acc 0.33333 roc_auc 0.55520 prc_auc 0.73536[0m
[93maverage test of epoch 77: loss -6.02412 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 78: loss -6.04429 acc 0.33333 roc_auc 0.54740 prc_auc 0.72754[0m
[93maverage test of epoch 78: loss -6.08094 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 79: loss -6.10130 acc 0.33333 roc_auc 0.53860 prc_auc 0.71679[0m
[93maverage test of epoch 79: loss -6.13763 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 80: loss -6.15816 acc 0.33333 roc_auc 0.53080 prc_auc 0.71014[0m
[93maverage test of epoch 80: loss -6.19420 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 81: loss -6.21490 acc 0.33333 roc_auc 0.52260 prc_auc 0.70332[0m
[93maverage test of epoch 81: loss -6.25065 acc 0.34211 roc_auc 0.88615 prc_auc 0.93236[0m
[92maverage training of epoch 82: loss -6.27153 acc 0.33333 roc_auc 0.51340 prc_auc 0.69573[0m
[93maverage test of epoch 82: loss -6.30699 acc 0.34211 roc_auc 0.88615 prc_auc 0.93236[0m
[92maverage training of epoch 83: loss -6.32805 acc 0.33333 roc_auc 0.50700 prc_auc 0.68953[0m
[93maverage test of epoch 83: loss -6.36324 acc 0.34211 roc_auc 0.88615 prc_auc 0.93236[0m
[92maverage training of epoch 84: loss -6.38448 acc 0.33333 roc_auc 0.49920 prc_auc 0.68288[0m
[93maverage test of epoch 84: loss -6.41941 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 85: loss -6.44082 acc 0.33333 roc_auc 0.49300 prc_auc 0.67747[0m
[93maverage test of epoch 85: loss -6.47549 acc 0.34211 roc_auc 0.88615 prc_auc 0.93236[0m
[92maverage training of epoch 86: loss -6.49708 acc 0.33333 roc_auc 0.48740 prc_auc 0.67291[0m
[93maverage test of epoch 86: loss -6.53150 acc 0.34211 roc_auc 0.88923 prc_auc 0.93833[0m
[92maverage training of epoch 87: loss -6.55326 acc 0.33333 roc_auc 0.48160 prc_auc 0.66569[0m
[93maverage test of epoch 87: loss -6.58745 acc 0.34211 roc_auc 0.88615 prc_auc 0.93262[0m
[92maverage training of epoch 88: loss -6.60938 acc 0.33333 roc_auc 0.47760 prc_auc 0.66301[0m
[93maverage test of epoch 88: loss -6.64333 acc 0.34211 roc_auc 0.88462 prc_auc 0.93091[0m
[92maverage training of epoch 89: loss -6.66543 acc 0.33333 roc_auc 0.47180 prc_auc 0.65666[0m
[93maverage test of epoch 89: loss -6.69916 acc 0.34211 roc_auc 0.88462 prc_auc 0.93091[0m
[92maverage training of epoch 90: loss -6.72143 acc 0.33333 roc_auc 0.46540 prc_auc 0.65146[0m
[93maverage test of epoch 90: loss -6.75493 acc 0.34211 roc_auc 0.88154 prc_auc 0.92572[0m
[92maverage training of epoch 91: loss -6.77737 acc 0.33333 roc_auc 0.46160 prc_auc 0.64837[0m
[93maverage test of epoch 91: loss -6.81065 acc 0.34211 roc_auc 0.88462 prc_auc 0.93143[0m
[92maverage training of epoch 92: loss -6.83326 acc 0.33333 roc_auc 0.45570 prc_auc 0.63973[0m
[93maverage test of epoch 92: loss -6.86633 acc 0.34211 roc_auc 0.88462 prc_auc 0.93143[0m
[92maverage training of epoch 93: loss -6.88911 acc 0.33333 roc_auc 0.45260 prc_auc 0.63707[0m
[93maverage test of epoch 93: loss -6.92196 acc 0.34211 roc_auc 0.87846 prc_auc 0.92329[0m
[92maverage training of epoch 94: loss -6.94491 acc 0.33333 roc_auc 0.44860 prc_auc 0.63446[0m
[93maverage test of epoch 94: loss -6.97756 acc 0.34211 roc_auc 0.87846 prc_auc 0.92329[0m
[92maverage training of epoch 95: loss -7.00068 acc 0.33333 roc_auc 0.44620 prc_auc 0.63284[0m
[93maverage test of epoch 95: loss -7.03312 acc 0.34211 roc_auc 0.88615 prc_auc 0.93288[0m
[92maverage training of epoch 96: loss -7.05640 acc 0.33333 roc_auc 0.44440 prc_auc 0.63131[0m
[93maverage test of epoch 96: loss -7.08864 acc 0.34211 roc_auc 0.86769 prc_auc 0.90329[0m
[92maverage training of epoch 97: loss -7.11209 acc 0.33333 roc_auc 0.44160 prc_auc 0.62803[0m
[93maverage test of epoch 97: loss -7.14414 acc 0.34211 roc_auc 0.86769 prc_auc 0.90329[0m
[92maverage training of epoch 98: loss -7.16775 acc 0.33333 roc_auc 0.44000 prc_auc 0.62642[0m
[93maverage test of epoch 98: loss -7.19960 acc 0.34211 roc_auc 0.87077 prc_auc 0.90422[0m
[92maverage training of epoch 99: loss -7.22338 acc 0.33333 roc_auc 0.43780 prc_auc 0.62485[0m
[93maverage test of epoch 99: loss -7.25504 acc 0.34211 roc_auc 0.86769 prc_auc 0.90329[0m
Training model with dataset, testing using fold 2
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.65427 acc 0.33333 roc_auc 0.40180 prc_auc 0.60779[0m
[93maverage test of epoch 0: loss -0.73609 acc 0.34211 roc_auc 0.06769 prc_auc 0.46855[0m
[92maverage training of epoch 1: loss -0.79677 acc 0.33333 roc_auc 0.40860 prc_auc 0.61227[0m
[93maverage test of epoch 1: loss -0.87554 acc 0.34211 roc_auc 0.07077 prc_auc 0.46973[0m
[92maverage training of epoch 2: loss -0.93667 acc 0.33333 roc_auc 0.41780 prc_auc 0.62102[0m
[93maverage test of epoch 2: loss -1.01197 acc 0.34211 roc_auc 0.07385 prc_auc 0.47052[0m
[92maverage training of epoch 3: loss -1.07348 acc 0.33333 roc_auc 0.42620 prc_auc 0.63390[0m
[93maverage test of epoch 3: loss -1.14531 acc 0.34211 roc_auc 0.08615 prc_auc 0.47428[0m
[92maverage training of epoch 4: loss -1.20730 acc 0.33333 roc_auc 0.43740 prc_auc 0.64300[0m
[93maverage test of epoch 4: loss -1.27583 acc 0.34211 roc_auc 0.14769 prc_auc 0.52391[0m
[92maverage training of epoch 5: loss -1.33851 acc 0.48667 roc_auc 0.44740 prc_auc 0.65567[0m
[93maverage test of epoch 5: loss -1.40414 acc 0.65789 roc_auc 0.21538 prc_auc 0.55160[0m
[92maverage training of epoch 6: loss -1.46808 acc 0.66667 roc_auc 0.45320 prc_auc 0.66085[0m
[93maverage test of epoch 6: loss -1.53176 acc 0.65789 roc_auc 0.69846 prc_auc 0.87310[0m
[92maverage training of epoch 7: loss -1.59838 acc 0.66667 roc_auc 0.45760 prc_auc 0.66396[0m
[93maverage test of epoch 7: loss -1.66215 acc 0.65789 roc_auc 0.80923 prc_auc 0.92594[0m
[92maverage training of epoch 8: loss -1.73469 acc 0.66667 roc_auc 0.46200 prc_auc 0.66587[0m
[93maverage test of epoch 8: loss -1.80259 acc 0.65789 roc_auc 0.86462 prc_auc 0.94617[0m
[92maverage training of epoch 9: loss -1.88743 acc 0.66667 roc_auc 0.46680 prc_auc 0.67100[0m
[93maverage test of epoch 9: loss -1.96634 acc 0.65789 roc_auc 0.90769 prc_auc 0.96508[0m
[92maverage training of epoch 10: loss -2.07306 acc 0.66667 roc_auc 0.47480 prc_auc 0.68154[0m
[93maverage test of epoch 10: loss -2.17058 acc 0.65789 roc_auc 0.95692 prc_auc 0.98286[0m
[92maverage training of epoch 11: loss -2.30308 acc 0.66667 roc_auc 0.49520 prc_auc 0.69801[0m
[93maverage test of epoch 11: loss -2.41377 acc 0.65789 roc_auc 0.96000 prc_auc 0.98200[0m
[92maverage training of epoch 12: loss -2.54219 acc 0.66667 roc_auc 0.56460 prc_auc 0.75918[0m
[93maverage test of epoch 12: loss -2.63017 acc 0.65789 roc_auc 0.95692 prc_auc 0.98047[0m
[92maverage training of epoch 13: loss -2.72763 acc 0.66667 roc_auc 0.60360 prc_auc 0.78710[0m
[93maverage test of epoch 13: loss -2.78514 acc 0.65789 roc_auc 0.96000 prc_auc 0.98229[0m
[92maverage training of epoch 14: loss -2.86566 acc 0.66667 roc_auc 0.59800 prc_auc 0.77687[0m
[93maverage test of epoch 14: loss -2.90849 acc 0.65789 roc_auc 0.95692 prc_auc 0.98069[0m
[92maverage training of epoch 15: loss -2.98160 acc 0.66667 roc_auc 0.58280 prc_auc 0.76041[0m
[93maverage test of epoch 15: loss -3.01707 acc 0.65789 roc_auc 0.95692 prc_auc 0.98069[0m
[92maverage training of epoch 16: loss -3.08611 acc 0.66667 roc_auc 0.56100 prc_auc 0.74183[0m
[93maverage test of epoch 16: loss -3.11701 acc 0.65789 roc_auc 0.95692 prc_auc 0.98069[0m
[92maverage training of epoch 17: loss -3.18328 acc 0.66667 roc_auc 0.53420 prc_auc 0.71896[0m
[93maverage test of epoch 17: loss -3.21090 acc 0.65789 roc_auc 0.95692 prc_auc 0.98069[0m
[92maverage training of epoch 18: loss -3.27505 acc 0.66667 roc_auc 0.51780 prc_auc 0.70697[0m
[93maverage test of epoch 18: loss -3.30009 acc 0.65789 roc_auc 0.95692 prc_auc 0.98069[0m
[92maverage training of epoch 19: loss -3.36249 acc 0.66667 roc_auc 0.49780 prc_auc 0.69490[0m
[93maverage test of epoch 19: loss -3.38541 acc 0.65789 roc_auc 0.95692 prc_auc 0.98069[0m
[92maverage training of epoch 20: loss -3.44631 acc 0.66667 roc_auc 0.47520 prc_auc 0.68127[0m
[93maverage test of epoch 20: loss -3.46743 acc 0.65789 roc_auc 0.95692 prc_auc 0.98069[0m
[92maverage training of epoch 21: loss -3.52702 acc 0.66667 roc_auc 0.46360 prc_auc 0.67610[0m
[93maverage test of epoch 21: loss -3.54660 acc 0.65789 roc_auc 0.94769 prc_auc 0.97585[0m
[92maverage training of epoch 22: loss -3.60502 acc 0.66667 roc_auc 0.45220 prc_auc 0.67145[0m
[93maverage test of epoch 22: loss -3.62326 acc 0.65789 roc_auc 0.94462 prc_auc 0.97443[0m
[92maverage training of epoch 23: loss -3.68065 acc 0.66667 roc_auc 0.44580 prc_auc 0.66943[0m
[93maverage test of epoch 23: loss -3.69772 acc 0.65789 roc_auc 0.93231 prc_auc 0.96984[0m
[92maverage training of epoch 24: loss -3.75418 acc 0.66667 roc_auc 0.44420 prc_auc 0.66755[0m
[93maverage test of epoch 24: loss -3.77023 acc 0.65789 roc_auc 0.90769 prc_auc 0.96125[0m
[92maverage training of epoch 25: loss -3.82586 acc 0.66667 roc_auc 0.43880 prc_auc 0.65992[0m
[93maverage test of epoch 25: loss -3.84102 acc 0.65789 roc_auc 0.90154 prc_auc 0.95779[0m
[92maverage training of epoch 26: loss -3.89591 acc 0.66667 roc_auc 0.42960 prc_auc 0.65316[0m
[93maverage test of epoch 26: loss -3.91030 acc 0.65789 roc_auc 0.89846 prc_auc 0.95626[0m
[92maverage training of epoch 27: loss -3.96452 acc 0.66667 roc_auc 0.42410 prc_auc 0.64900[0m
[93maverage test of epoch 27: loss -3.97825 acc 0.65789 roc_auc 0.88154 prc_auc 0.94702[0m
[92maverage training of epoch 28: loss -4.03188 acc 0.66667 roc_auc 0.42180 prc_auc 0.64667[0m
[93maverage test of epoch 28: loss -4.04503 acc 0.65789 roc_auc 0.88000 prc_auc 0.94816[0m
[92maverage training of epoch 29: loss -4.09813 acc 0.66667 roc_auc 0.41580 prc_auc 0.64350[0m
[93maverage test of epoch 29: loss -4.11079 acc 0.65789 roc_auc 0.86462 prc_auc 0.94410[0m
[92maverage training of epoch 30: loss -4.16342 acc 0.66667 roc_auc 0.41260 prc_auc 0.63954[0m
[93maverage test of epoch 30: loss -4.17566 acc 0.65789 roc_auc 0.85231 prc_auc 0.93760[0m
[92maverage training of epoch 31: loss -4.22788 acc 0.66667 roc_auc 0.41280 prc_auc 0.63960[0m
[93maverage test of epoch 31: loss -4.23978 acc 0.65789 roc_auc 0.84308 prc_auc 0.93389[0m
[92maverage training of epoch 32: loss -4.29162 acc 0.66667 roc_auc 0.41180 prc_auc 0.63771[0m
[93maverage test of epoch 32: loss -4.30323 acc 0.65789 roc_auc 0.82923 prc_auc 0.92908[0m
[92maverage training of epoch 33: loss -4.35475 acc 0.66667 roc_auc 0.40820 prc_auc 0.63230[0m
[93maverage test of epoch 33: loss -4.36613 acc 0.65789 roc_auc 0.80308 prc_auc 0.92092[0m
[92maverage training of epoch 34: loss -4.41736 acc 0.66667 roc_auc 0.40900 prc_auc 0.62985[0m
[93maverage test of epoch 34: loss -4.42855 acc 0.65789 roc_auc 0.79846 prc_auc 0.91989[0m
[92maverage training of epoch 35: loss -4.47953 acc 0.66667 roc_auc 0.41020 prc_auc 0.63118[0m
[93maverage test of epoch 35: loss -4.49057 acc 0.65789 roc_auc 0.78923 prc_auc 0.91596[0m
[92maverage training of epoch 36: loss -4.54133 acc 0.66667 roc_auc 0.40940 prc_auc 0.62903[0m
[93maverage test of epoch 36: loss -4.55226 acc 0.65789 roc_auc 0.78154 prc_auc 0.91262[0m
[92maverage training of epoch 37: loss -4.60282 acc 0.66667 roc_auc 0.40840 prc_auc 0.62819[0m
[93maverage test of epoch 37: loss -4.61366 acc 0.65789 roc_auc 0.77385 prc_auc 0.90867[0m
[92maverage training of epoch 38: loss -4.66405 acc 0.66667 roc_auc 0.40870 prc_auc 0.62816[0m
[93maverage test of epoch 38: loss -4.67483 acc 0.65789 roc_auc 0.75846 prc_auc 0.90333[0m
[92maverage training of epoch 39: loss -4.72506 acc 0.66667 roc_auc 0.40920 prc_auc 0.62857[0m
[93maverage test of epoch 39: loss -4.73580 acc 0.65789 roc_auc 0.74308 prc_auc 0.89631[0m
[92maverage training of epoch 40: loss -4.78589 acc 0.66667 roc_auc 0.41030 prc_auc 0.62916[0m
[93maverage test of epoch 40: loss -4.79660 acc 0.65789 roc_auc 0.72615 prc_auc 0.89085[0m
[92maverage training of epoch 41: loss -4.84656 acc 0.66667 roc_auc 0.40940 prc_auc 0.62868[0m
[93maverage test of epoch 41: loss -4.85725 acc 0.65789 roc_auc 0.71846 prc_auc 0.88733[0m
[92maverage training of epoch 42: loss -4.90710 acc 0.66667 roc_auc 0.40880 prc_auc 0.62843[0m
[93maverage test of epoch 42: loss -4.91777 acc 0.65789 roc_auc 0.70462 prc_auc 0.88365[0m
[92maverage training of epoch 43: loss -4.96751 acc 0.66667 roc_auc 0.40730 prc_auc 0.62719[0m
[93maverage test of epoch 43: loss -4.97817 acc 0.65789 roc_auc 0.69077 prc_auc 0.87749[0m
[92maverage training of epoch 44: loss -5.02781 acc 0.66667 roc_auc 0.40650 prc_auc 0.62682[0m
[93maverage test of epoch 44: loss -5.03845 acc 0.65789 roc_auc 0.69231 prc_auc 0.87591[0m
[92maverage training of epoch 45: loss -5.08801 acc 0.66667 roc_auc 0.40520 prc_auc 0.62636[0m
[93maverage test of epoch 45: loss -5.09862 acc 0.65789 roc_auc 0.69231 prc_auc 0.87563[0m
[92maverage training of epoch 46: loss -5.14809 acc 0.66667 roc_auc 0.40320 prc_auc 0.62542[0m
[93maverage test of epoch 46: loss -5.15867 acc 0.65789 roc_auc 0.66000 prc_auc 0.85675[0m
[92maverage training of epoch 47: loss -5.20806 acc 0.66667 roc_auc 0.40150 prc_auc 0.62380[0m
[93maverage test of epoch 47: loss -5.21859 acc 0.65789 roc_auc 0.63077 prc_auc 0.84545[0m
[92maverage training of epoch 48: loss -5.26792 acc 0.66667 roc_auc 0.39980 prc_auc 0.62269[0m
[93maverage test of epoch 48: loss -5.27840 acc 0.65789 roc_auc 0.60769 prc_auc 0.82718[0m
[92maverage training of epoch 49: loss -5.32766 acc 0.66667 roc_auc 0.39820 prc_auc 0.62187[0m
[93maverage test of epoch 49: loss -5.33807 acc 0.65789 roc_auc 0.58769 prc_auc 0.81295[0m
[92maverage training of epoch 50: loss -5.38727 acc 0.66667 roc_auc 0.39750 prc_auc 0.61966[0m
[93maverage test of epoch 50: loss -5.39760 acc 0.65789 roc_auc 0.54462 prc_auc 0.77809[0m
[92maverage training of epoch 51: loss -5.44676 acc 0.66667 roc_auc 0.39740 prc_auc 0.61941[0m
[93maverage test of epoch 51: loss -5.45700 acc 0.65789 roc_auc 0.49692 prc_auc 0.73439[0m
[92maverage training of epoch 52: loss -5.50611 acc 0.66667 roc_auc 0.39510 prc_auc 0.61753[0m
[93maverage test of epoch 52: loss -5.51624 acc 0.65789 roc_auc 0.48154 prc_auc 0.72848[0m
[92maverage training of epoch 53: loss -5.56532 acc 0.66667 roc_auc 0.39430 prc_auc 0.61754[0m
[93maverage test of epoch 53: loss -5.57533 acc 0.65789 roc_auc 0.38154 prc_auc 0.66106[0m
[92maverage training of epoch 54: loss -5.62439 acc 0.66667 roc_auc 0.39410 prc_auc 0.61739[0m
[93maverage test of epoch 54: loss -5.63427 acc 0.65789 roc_auc 0.36462 prc_auc 0.65320[0m
[92maverage training of epoch 55: loss -5.68331 acc 0.66667 roc_auc 0.39340 prc_auc 0.61679[0m
[93maverage test of epoch 55: loss -5.69305 acc 0.65789 roc_auc 0.30308 prc_auc 0.58119[0m
[92maverage training of epoch 56: loss -5.74208 acc 0.66667 roc_auc 0.39410 prc_auc 0.61559[0m
[93maverage test of epoch 56: loss -5.75168 acc 0.65789 roc_auc 0.31231 prc_auc 0.59152[0m
[92maverage training of epoch 57: loss -5.80071 acc 0.66667 roc_auc 0.39510 prc_auc 0.61462[0m
[93maverage test of epoch 57: loss -5.81014 acc 0.65789 roc_auc 0.30000 prc_auc 0.59755[0m
[92maverage training of epoch 58: loss -5.85918 acc 0.66667 roc_auc 0.39440 prc_auc 0.61415[0m
[93maverage test of epoch 58: loss -5.86844 acc 0.65789 roc_auc 0.25538 prc_auc 0.54589[0m
[92maverage training of epoch 59: loss -5.91750 acc 0.66667 roc_auc 0.39470 prc_auc 0.61356[0m
[93maverage test of epoch 59: loss -5.92659 acc 0.65789 roc_auc 0.24154 prc_auc 0.59109[0m
[92maverage training of epoch 60: loss -5.97567 acc 0.66667 roc_auc 0.39510 prc_auc 0.61298[0m
[93maverage test of epoch 60: loss -5.98458 acc 0.65789 roc_auc 0.20769 prc_auc 0.53266[0m
[92maverage training of epoch 61: loss -6.03369 acc 0.66667 roc_auc 0.39560 prc_auc 0.61240[0m
[93maverage test of epoch 61: loss -6.04242 acc 0.65789 roc_auc 0.18462 prc_auc 0.53225[0m
[92maverage training of epoch 62: loss -6.09157 acc 0.66667 roc_auc 0.39490 prc_auc 0.61080[0m
[93maverage test of epoch 62: loss -6.10011 acc 0.65789 roc_auc 0.18000 prc_auc 0.53680[0m
[92maverage training of epoch 63: loss -6.14931 acc 0.66667 roc_auc 0.39300 prc_auc 0.60876[0m
[93maverage test of epoch 63: loss -6.15765 acc 0.65789 roc_auc 0.15385 prc_auc 0.53155[0m
[92maverage training of epoch 64: loss -6.20691 acc 0.66667 roc_auc 0.39240 prc_auc 0.60772[0m
[93maverage test of epoch 64: loss -6.21506 acc 0.65789 roc_auc 0.11077 prc_auc 0.52741[0m
[92maverage training of epoch 65: loss -6.26438 acc 0.66667 roc_auc 0.39220 prc_auc 0.60737[0m
[93maverage test of epoch 65: loss -6.27234 acc 0.65789 roc_auc 0.15077 prc_auc 0.52794[0m
[92maverage training of epoch 66: loss -6.32172 acc 0.66667 roc_auc 0.39130 prc_auc 0.60553[0m
[93maverage test of epoch 66: loss -6.32948 acc 0.65789 roc_auc 0.19538 prc_auc 0.54576[0m
[92maverage training of epoch 67: loss -6.37893 acc 0.66667 roc_auc 0.39230 prc_auc 0.60612[0m
[93maverage test of epoch 67: loss -6.38650 acc 0.65789 roc_auc 0.18923 prc_auc 0.54083[0m
[92maverage training of epoch 68: loss -6.43603 acc 0.66667 roc_auc 0.38750 prc_auc 0.59576[0m
[93maverage test of epoch 68: loss -6.44341 acc 0.65789 roc_auc 0.24154 prc_auc 0.58089[0m
[92maverage training of epoch 69: loss -6.49301 acc 0.66667 roc_auc 0.38460 prc_auc 0.59458[0m
[93maverage test of epoch 69: loss -6.50020 acc 0.65789 roc_auc 0.20000 prc_auc 0.54942[0m
[92maverage training of epoch 70: loss -6.54989 acc 0.66667 roc_auc 0.38380 prc_auc 0.59356[0m
[93maverage test of epoch 70: loss -6.55688 acc 0.65789 roc_auc 0.22308 prc_auc 0.54949[0m
[92maverage training of epoch 71: loss -6.60666 acc 0.66667 roc_auc 0.38330 prc_auc 0.59253[0m
[93maverage test of epoch 71: loss -6.61347 acc 0.65789 roc_auc 0.13077 prc_auc 0.54658[0m
[92maverage training of epoch 72: loss -6.66334 acc 0.66667 roc_auc 0.38320 prc_auc 0.59270[0m
[93maverage test of epoch 72: loss -6.66995 acc 0.65789 roc_auc 0.12462 prc_auc 0.57568[0m
[92maverage training of epoch 73: loss -6.71992 acc 0.66667 roc_auc 0.38390 prc_auc 0.59377[0m
[93maverage test of epoch 73: loss -6.72634 acc 0.65789 roc_auc 0.24000 prc_auc 0.55454[0m
[92maverage training of epoch 74: loss -6.77642 acc 0.66667 roc_auc 0.38390 prc_auc 0.59324[0m
[93maverage test of epoch 74: loss -6.78265 acc 0.65789 roc_auc 0.29692 prc_auc 0.57983[0m
[92maverage training of epoch 75: loss -6.83283 acc 0.66667 roc_auc 0.38370 prc_auc 0.59291[0m
[93maverage test of epoch 75: loss -6.83888 acc 0.65789 roc_auc 0.34000 prc_auc 0.59586[0m
[92maverage training of epoch 76: loss -6.88917 acc 0.66667 roc_auc 0.38330 prc_auc 0.59267[0m
[93maverage test of epoch 76: loss -6.89503 acc 0.65789 roc_auc 0.27077 prc_auc 0.63209[0m
[92maverage training of epoch 77: loss -6.94543 acc 0.66667 roc_auc 0.38300 prc_auc 0.58892[0m
[93maverage test of epoch 77: loss -6.95111 acc 0.65789 roc_auc 0.22000 prc_auc 0.56096[0m
[92maverage training of epoch 78: loss -7.00162 acc 0.66667 roc_auc 0.37680 prc_auc 0.57258[0m
[93maverage test of epoch 78: loss -7.00713 acc 0.65789 roc_auc 0.14000 prc_auc 0.56540[0m
[92maverage training of epoch 79: loss -7.05775 acc 0.66667 roc_auc 0.37640 prc_auc 0.57127[0m
[93maverage test of epoch 79: loss -7.06308 acc 0.65789 roc_auc 0.16000 prc_auc 0.56927[0m
[92maverage training of epoch 80: loss -7.11382 acc 0.66667 roc_auc 0.37600 prc_auc 0.57117[0m
[93maverage test of epoch 80: loss -7.11897 acc 0.65789 roc_auc 0.14000 prc_auc 0.57168[0m
[92maverage training of epoch 81: loss -7.16983 acc 0.66667 roc_auc 0.37520 prc_auc 0.57076[0m
[93maverage test of epoch 81: loss -7.17480 acc 0.65789 roc_auc 0.19538 prc_auc 0.54786[0m
[92maverage training of epoch 82: loss -7.22579 acc 0.66667 roc_auc 0.37400 prc_auc 0.57041[0m
[93maverage test of epoch 82: loss -7.23058 acc 0.65789 roc_auc 0.46000 prc_auc 0.64041[0m
[92maverage training of epoch 83: loss -7.28170 acc 0.66667 roc_auc 0.37370 prc_auc 0.56989[0m
[93maverage test of epoch 83: loss -7.28632 acc 0.65789 roc_auc 0.46000 prc_auc 0.64041[0m
[92maverage training of epoch 84: loss -7.33756 acc 0.66667 roc_auc 0.37360 prc_auc 0.57012[0m
[93maverage test of epoch 84: loss -7.34200 acc 0.65789 roc_auc 0.49846 prc_auc 0.65720[0m
[92maverage training of epoch 85: loss -7.39338 acc 0.66667 roc_auc 0.37270 prc_auc 0.56963[0m
[93maverage test of epoch 85: loss -7.39765 acc 0.65789 roc_auc 0.42000 prc_auc 0.62409[0m
[92maverage training of epoch 86: loss -7.44915 acc 0.66667 roc_auc 0.37290 prc_auc 0.56973[0m
[93maverage test of epoch 86: loss -7.45325 acc 0.65789 roc_auc 0.52154 prc_auc 0.66895[0m
[92maverage training of epoch 87: loss -7.50489 acc 0.66667 roc_auc 0.37300 prc_auc 0.56992[0m
[93maverage test of epoch 87: loss -7.50882 acc 0.65789 roc_auc 0.37692 prc_auc 0.60931[0m
[92maverage training of epoch 88: loss -7.56059 acc 0.66667 roc_auc 0.37350 prc_auc 0.57089[0m
[93maverage test of epoch 88: loss -7.56435 acc 0.65789 roc_auc 0.40000 prc_auc 0.61643[0m
[92maverage training of epoch 89: loss -7.61626 acc 0.66667 roc_auc 0.37350 prc_auc 0.57102[0m
[93maverage test of epoch 89: loss -7.61986 acc 0.65789 roc_auc 0.64000 prc_auc 0.75368[0m
[92maverage training of epoch 90: loss -7.67190 acc 0.66667 roc_auc 0.37400 prc_auc 0.57109[0m
[93maverage test of epoch 90: loss -7.67533 acc 0.65789 roc_auc 0.15692 prc_auc 0.59530[0m
[92maverage training of epoch 91: loss -7.72751 acc 0.66667 roc_auc 0.37400 prc_auc 0.57159[0m
[93maverage test of epoch 91: loss -7.73077 acc 0.65789 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 92: loss -7.78309 acc 0.66667 roc_auc 0.37350 prc_auc 0.57078[0m
[93maverage test of epoch 92: loss -7.78618 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 93: loss -7.83865 acc 0.66667 roc_auc 0.37330 prc_auc 0.57065[0m
[93maverage test of epoch 93: loss -7.84157 acc 0.65789 roc_auc 0.52000 prc_auc 0.67158[0m
[92maverage training of epoch 94: loss -7.89419 acc 0.66667 roc_auc 0.37380 prc_auc 0.57071[0m
[93maverage test of epoch 94: loss -7.89694 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 95: loss -7.94970 acc 0.66667 roc_auc 0.37390 prc_auc 0.57148[0m
[93maverage test of epoch 95: loss -7.95229 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 96: loss -8.00519 acc 0.66667 roc_auc 0.37430 prc_auc 0.57180[0m
[93maverage test of epoch 96: loss -8.00762 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 97: loss -8.06066 acc 0.66667 roc_auc 0.37450 prc_auc 0.57205[0m
[93maverage test of epoch 97: loss -8.06293 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 98: loss -8.11612 acc 0.66667 roc_auc 0.37490 prc_auc 0.57202[0m
[93maverage test of epoch 98: loss -8.11822 acc 0.65789 roc_auc 0.64462 prc_auc 0.73829[0m
[92maverage training of epoch 99: loss -8.17156 acc 0.66667 roc_auc 0.37510 prc_auc 0.57264[0m
[93maverage test of epoch 99: loss -8.17349 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 3
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.08132 acc 0.33775 roc_auc 0.32529 prc_auc 0.55511[0m
[93maverage test of epoch 0: loss -0.09821 acc 0.32432 roc_auc 0.10333 prc_auc 0.49908[0m
[92maverage training of epoch 1: loss -0.15069 acc 0.33775 roc_auc 0.33882 prc_auc 0.56729[0m
[93maverage test of epoch 1: loss -0.16808 acc 0.32432 roc_auc 0.10000 prc_auc 0.49817[0m
[92maverage training of epoch 2: loss -0.22221 acc 0.33775 roc_auc 0.35824 prc_auc 0.57513[0m
[93maverage test of epoch 2: loss -0.23999 acc 0.32432 roc_auc 0.09667 prc_auc 0.49593[0m
[92maverage training of epoch 3: loss -0.29557 acc 0.33775 roc_auc 0.38059 prc_auc 0.58743[0m
[93maverage test of epoch 3: loss -0.31356 acc 0.32432 roc_auc 0.16000 prc_auc 0.54921[0m
[92maverage training of epoch 4: loss -0.37037 acc 0.33775 roc_auc 0.41000 prc_auc 0.60655[0m
[93maverage test of epoch 4: loss -0.38828 acc 0.32432 roc_auc 0.20667 prc_auc 0.57861[0m
[92maverage training of epoch 5: loss -0.44610 acc 0.33775 roc_auc 0.44510 prc_auc 0.63368[0m
[93maverage test of epoch 5: loss -0.46366 acc 0.32432 roc_auc 0.22333 prc_auc 0.58620[0m
[92maverage training of epoch 6: loss -0.52231 acc 0.33775 roc_auc 0.48431 prc_auc 0.66329[0m
[93maverage test of epoch 6: loss -0.53924 acc 0.32432 roc_auc 0.27000 prc_auc 0.64020[0m
[92maverage training of epoch 7: loss -0.59865 acc 0.33775 roc_auc 0.52176 prc_auc 0.70254[0m
[93maverage test of epoch 7: loss -0.61473 acc 0.32432 roc_auc 0.35000 prc_auc 0.68268[0m
[92maverage training of epoch 8: loss -0.67492 acc 0.33775 roc_auc 0.55706 prc_auc 0.73701[0m
[93maverage test of epoch 8: loss -0.69002 acc 0.32432 roc_auc 0.41000 prc_auc 0.71370[0m
[92maverage training of epoch 9: loss -0.75105 acc 0.33775 roc_auc 0.58941 prc_auc 0.76601[0m
[93maverage test of epoch 9: loss -0.76514 acc 0.32432 roc_auc 0.58667 prc_auc 0.80689[0m
[92maverage training of epoch 10: loss -0.82712 acc 0.33775 roc_auc 0.60922 prc_auc 0.78453[0m
[93maverage test of epoch 10: loss -0.84024 acc 0.32432 roc_auc 0.62667 prc_auc 0.82273[0m
[92maverage training of epoch 11: loss -0.90327 acc 0.33775 roc_auc 0.62529 prc_auc 0.79598[0m
[93maverage test of epoch 11: loss -0.91554 acc 0.32432 roc_auc 0.66667 prc_auc 0.83666[0m
[92maverage training of epoch 12: loss -0.97969 acc 0.33775 roc_auc 0.63647 prc_auc 0.80382[0m
[93maverage test of epoch 12: loss -0.99129 acc 0.32432 roc_auc 0.75667 prc_auc 0.86704[0m
[92maverage training of epoch 13: loss -1.05666 acc 0.33775 roc_auc 0.65078 prc_auc 0.81327[0m
[93maverage test of epoch 13: loss -1.06779 acc 0.32432 roc_auc 0.81167 prc_auc 0.88720[0m
[92maverage training of epoch 14: loss -1.13453 acc 0.33775 roc_auc 0.66902 prc_auc 0.82603[0m
[93maverage test of epoch 14: loss -1.14543 acc 0.32432 roc_auc 0.82667 prc_auc 0.90541[0m
[92maverage training of epoch 15: loss -1.21379 acc 0.33775 roc_auc 0.68235 prc_auc 0.83543[0m
[93maverage test of epoch 15: loss -1.22479 acc 0.32432 roc_auc 0.85667 prc_auc 0.91428[0m
[92maverage training of epoch 16: loss -1.29512 acc 0.33775 roc_auc 0.70333 prc_auc 0.84952[0m
[93maverage test of epoch 16: loss -1.30670 acc 0.32432 roc_auc 0.85333 prc_auc 0.91330[0m
[92maverage training of epoch 17: loss -1.37946 acc 0.33775 roc_auc 0.73118 prc_auc 0.86398[0m
[93maverage test of epoch 17: loss -1.39223 acc 0.32432 roc_auc 0.84667 prc_auc 0.91092[0m
[92maverage training of epoch 18: loss -1.46779 acc 0.33775 roc_auc 0.75824 prc_auc 0.88157[0m
[93maverage test of epoch 18: loss -1.48212 acc 0.32432 roc_auc 0.84667 prc_auc 0.91092[0m
[92maverage training of epoch 19: loss -1.56020 acc 0.33775 roc_auc 0.77765 prc_auc 0.89409[0m
[93maverage test of epoch 19: loss -1.57566 acc 0.32432 roc_auc 0.84667 prc_auc 0.91092[0m
[92maverage training of epoch 20: loss -1.65510 acc 0.33775 roc_auc 0.79706 prc_auc 0.90475[0m
[93maverage test of epoch 20: loss -1.67048 acc 0.32432 roc_auc 0.84667 prc_auc 0.91099[0m
[92maverage training of epoch 21: loss -1.74992 acc 0.33775 roc_auc 0.81510 prc_auc 0.91506[0m
[93maverage test of epoch 21: loss -1.76404 acc 0.32432 roc_auc 0.84667 prc_auc 0.91099[0m
[92maverage training of epoch 22: loss -1.84259 acc 0.33775 roc_auc 0.82686 prc_auc 0.92088[0m
[93maverage test of epoch 22: loss -1.85488 acc 0.32432 roc_auc 0.84667 prc_auc 0.91099[0m
[92maverage training of epoch 23: loss -1.93216 acc 0.33775 roc_auc 0.84039 prc_auc 0.92745[0m
[93maverage test of epoch 23: loss -1.94248 acc 0.32432 roc_auc 0.84667 prc_auc 0.91099[0m
[92maverage training of epoch 24: loss -2.01837 acc 0.33775 roc_auc 0.85529 prc_auc 0.93430[0m
[93maverage test of epoch 24: loss -2.02680 acc 0.32432 roc_auc 0.84667 prc_auc 0.91099[0m
[92maverage training of epoch 25: loss -2.10130 acc 0.33775 roc_auc 0.86039 prc_auc 0.93689[0m
[93maverage test of epoch 25: loss -2.10801 acc 0.32432 roc_auc 0.84667 prc_auc 0.91099[0m
[92maverage training of epoch 26: loss -2.18118 acc 0.33775 roc_auc 0.85137 prc_auc 0.93300[0m
[93maverage test of epoch 26: loss -2.18638 acc 0.32432 roc_auc 0.84667 prc_auc 0.91099[0m
[92maverage training of epoch 27: loss -2.25834 acc 0.33775 roc_auc 0.84451 prc_auc 0.93005[0m
[93maverage test of epoch 27: loss -2.26227 acc 0.32432 roc_auc 0.84667 prc_auc 0.91099[0m
[92maverage training of epoch 28: loss -2.33316 acc 0.33775 roc_auc 0.83333 prc_auc 0.92481[0m
[93maverage test of epoch 28: loss -2.33601 acc 0.32432 roc_auc 0.84333 prc_auc 0.90851[0m
[92maverage training of epoch 29: loss -2.40602 acc 0.33775 roc_auc 0.82275 prc_auc 0.91980[0m
[93maverage test of epoch 29: loss -2.40795 acc 0.32432 roc_auc 0.84000 prc_auc 0.90760[0m
[92maverage training of epoch 30: loss -2.47730 acc 0.33775 roc_auc 0.81039 prc_auc 0.91422[0m
[93maverage test of epoch 30: loss -2.47838 acc 0.32432 roc_auc 0.84333 prc_auc 0.90844[0m
[92maverage training of epoch 31: loss -2.54730 acc 0.33775 roc_auc 0.78765 prc_auc 0.90373[0m
[93maverage test of epoch 31: loss -2.54756 acc 0.32432 roc_auc 0.84333 prc_auc 0.90844[0m
[92maverage training of epoch 32: loss -2.61623 acc 0.33775 roc_auc 0.75598 prc_auc 0.88906[0m
[93maverage test of epoch 32: loss -2.61571 acc 0.32432 roc_auc 0.84333 prc_auc 0.90844[0m
[92maverage training of epoch 33: loss -2.68414 acc 0.33775 roc_auc 0.72804 prc_auc 0.87437[0m
[93maverage test of epoch 33: loss -2.68293 acc 0.32432 roc_auc 0.84333 prc_auc 0.90844[0m
[92maverage training of epoch 34: loss -2.75101 acc 0.33775 roc_auc 0.70314 prc_auc 0.85983[0m
[93maverage test of epoch 34: loss -2.74926 acc 0.32432 roc_auc 0.84000 prc_auc 0.90760[0m
[92maverage training of epoch 35: loss -2.81689 acc 0.33775 roc_auc 0.66902 prc_auc 0.84055[0m
[93maverage test of epoch 35: loss -2.81471 acc 0.32432 roc_auc 0.83833 prc_auc 0.90697[0m
[92maverage training of epoch 36: loss -2.88186 acc 0.33775 roc_auc 0.64490 prc_auc 0.82634[0m
[93maverage test of epoch 36: loss -2.87933 acc 0.32432 roc_auc 0.84667 prc_auc 0.91007[0m
[92maverage training of epoch 37: loss -2.94600 acc 0.33775 roc_auc 0.61039 prc_auc 0.80511[0m
[93maverage test of epoch 37: loss -2.94319 acc 0.32432 roc_auc 0.84333 prc_auc 0.90876[0m
[92maverage training of epoch 38: loss -3.00940 acc 0.33775 roc_auc 0.57667 prc_auc 0.78273[0m
[93maverage test of epoch 38: loss -3.00636 acc 0.32432 roc_auc 0.81333 prc_auc 0.89382[0m
[92maverage training of epoch 39: loss -3.07211 acc 0.33775 roc_auc 0.54588 prc_auc 0.75905[0m
[93maverage test of epoch 39: loss -3.06889 acc 0.32432 roc_auc 0.79667 prc_auc 0.88811[0mUsing backend: pytorch
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

[92maverage training of epoch 40: loss -3.13422 acc 0.33775 roc_auc 0.50882 prc_auc 0.72866[0m
[93maverage test of epoch 40: loss -3.13085 acc 0.32432 roc_auc 0.77500 prc_auc 0.86648[0m
[92maverage training of epoch 41: loss -3.19577 acc 0.33775 roc_auc 0.48176 prc_auc 0.69883[0m
[93maverage test of epoch 41: loss -3.19229 acc 0.32432 roc_auc 0.77333 prc_auc 0.86648[0m
[92maverage training of epoch 42: loss -3.25682 acc 0.33775 roc_auc 0.45020 prc_auc 0.66385[0m
[93maverage test of epoch 42: loss -3.25325 acc 0.32432 roc_auc 0.76167 prc_auc 0.86025[0m
[92maverage training of epoch 43: loss -3.31741 acc 0.33775 roc_auc 0.43108 prc_auc 0.64746[0m
[93maverage test of epoch 43: loss -3.31379 acc 0.32432 roc_auc 0.73833 prc_auc 0.85148[0m
[92maverage training of epoch 44: loss -3.37759 acc 0.33775 roc_auc 0.41608 prc_auc 0.62930[0m
[93maverage test of epoch 44: loss -3.37394 acc 0.32432 roc_auc 0.70667 prc_auc 0.84220[0m
[92maverage training of epoch 45: loss -3.43739 acc 0.33775 roc_auc 0.40373 prc_auc 0.61525[0m
[93maverage test of epoch 45: loss -3.43373 acc 0.32432 roc_auc 0.69000 prc_auc 0.83690[0m
[92maverage training of epoch 46: loss -3.49685 acc 0.33775 roc_auc 0.39392 prc_auc 0.60741[0m
[93maverage test of epoch 46: loss -3.49320 acc 0.32432 roc_auc 0.65667 prc_auc 0.82175[0m
[92maverage training of epoch 47: loss -3.55599 acc 0.33775 roc_auc 0.38922 prc_auc 0.60000[0m
[93maverage test of epoch 47: loss -3.55237 acc 0.32432 roc_auc 0.65167 prc_auc 0.81887[0m
[92maverage training of epoch 48: loss -3.61484 acc 0.33775 roc_auc 0.38412 prc_auc 0.59479[0m
[93maverage test of epoch 48: loss -3.61127 acc 0.32432 roc_auc 0.62000 prc_auc 0.80880[0m
[92maverage training of epoch 49: loss -3.67343 acc 0.33775 roc_auc 0.38098 prc_auc 0.58911[0m
[93maverage test of epoch 49: loss -3.66993 acc 0.32432 roc_auc 0.60667 prc_auc 0.80120[0m
[92maverage training of epoch 50: loss -3.73178 acc 0.33775 roc_auc 0.37725 prc_auc 0.58340[0m
[93maverage test of epoch 50: loss -3.72835 acc 0.32432 roc_auc 0.56833 prc_auc 0.78416[0m
[92maverage training of epoch 51: loss -3.78991 acc 0.33775 roc_auc 0.37529 prc_auc 0.58170[0m
[93maverage test of epoch 51: loss -3.78657 acc 0.32432 roc_auc 0.54500 prc_auc 0.75049[0m
[92maverage training of epoch 52: loss -3.84783 acc 0.33775 roc_auc 0.37353 prc_auc 0.58040[0m
[93maverage test of epoch 52: loss -3.84459 acc 0.32432 roc_auc 0.51667 prc_auc 0.73670[0m
[92maverage training of epoch 53: loss -3.90556 acc 0.33775 roc_auc 0.37255 prc_auc 0.57909[0m
[93maverage test of epoch 53: loss -3.90243 acc 0.32432 roc_auc 0.51167 prc_auc 0.73211[0m
[92maverage training of epoch 54: loss -3.96312 acc 0.33775 roc_auc 0.37137 prc_auc 0.57345[0m
[93maverage test of epoch 54: loss -3.96012 acc 0.32432 roc_auc 0.48833 prc_auc 0.72528[0m
[92maverage training of epoch 55: loss -4.02052 acc 0.33775 roc_auc 0.37098 prc_auc 0.56835[0m
[93maverage test of epoch 55: loss -4.01765 acc 0.32432 roc_auc 0.46833 prc_auc 0.71772[0m
[92maverage training of epoch 56: loss -4.07778 acc 0.33775 roc_auc 0.37078 prc_auc 0.56723[0m
[93maverage test of epoch 56: loss -4.07505 acc 0.32432 roc_auc 0.45667 prc_auc 0.71800[0m
[92maverage training of epoch 57: loss -4.13490 acc 0.33775 roc_auc 0.37059 prc_auc 0.56659[0m
[93maverage test of epoch 57: loss -4.13231 acc 0.32432 roc_auc 0.45167 prc_auc 0.71712[0m
[92maverage training of epoch 58: loss -4.19190 acc 0.33775 roc_auc 0.37000 prc_auc 0.56592[0m
[93maverage test of epoch 58: loss -4.18946 acc 0.32432 roc_auc 0.43000 prc_auc 0.68210[0m
[92maverage training of epoch 59: loss -4.24878 acc 0.33775 roc_auc 0.37020 prc_auc 0.56600[0m
[93maverage test of epoch 59: loss -4.24650 acc 0.32432 roc_auc 0.40333 prc_auc 0.65860[0m
[92maverage training of epoch 60: loss -4.30555 acc 0.33775 roc_auc 0.36961 prc_auc 0.56543[0m
[93maverage test of epoch 60: loss -4.30344 acc 0.32432 roc_auc 0.35500 prc_auc 0.63513[0m
[92maverage training of epoch 61: loss -4.36223 acc 0.33775 roc_auc 0.36980 prc_auc 0.56594[0m
[93maverage test of epoch 61: loss -4.36029 acc 0.32432 roc_auc 0.33667 prc_auc 0.62659[0m
[92maverage training of epoch 62: loss -4.41882 acc 0.33775 roc_auc 0.37020 prc_auc 0.56627[0m
[93maverage test of epoch 62: loss -4.41705 acc 0.32432 roc_auc 0.31667 prc_auc 0.63044[0m
[92maverage training of epoch 63: loss -4.47532 acc 0.33775 roc_auc 0.37020 prc_auc 0.56625[0m
[93maverage test of epoch 63: loss -4.47374 acc 0.32432 roc_auc 0.32000 prc_auc 0.62544[0m
[92maverage training of epoch 64: loss -4.53175 acc 0.33775 roc_auc 0.37020 prc_auc 0.56608[0m
[93maverage test of epoch 64: loss -4.53035 acc 0.32432 roc_auc 0.28333 prc_auc 0.60982[0m
[92maverage training of epoch 65: loss -4.58810 acc 0.33775 roc_auc 0.37039 prc_auc 0.56614[0m
[93maverage test of epoch 65: loss -4.58689 acc 0.32432 roc_auc 0.27833 prc_auc 0.60419[0m
[92maverage training of epoch 66: loss -4.64439 acc 0.33775 roc_auc 0.37059 prc_auc 0.56642[0m
[93maverage test of epoch 66: loss -4.64338 acc 0.32432 roc_auc 0.28333 prc_auc 0.60523[0m
[92maverage training of epoch 67: loss -4.70062 acc 0.33775 roc_auc 0.37078 prc_auc 0.56648[0m
[93maverage test of epoch 67: loss -4.69980 acc 0.32432 roc_auc 0.29000 prc_auc 0.59412[0m
[92maverage training of epoch 68: loss -4.75679 acc 0.33775 roc_auc 0.37127 prc_auc 0.56664[0m
[93maverage test of epoch 68: loss -4.75617 acc 0.32432 roc_auc 0.24667 prc_auc 0.56892[0m
[92maverage training of epoch 69: loss -4.81291 acc 0.33775 roc_auc 0.37186 prc_auc 0.56690[0m
[93maverage test of epoch 69: loss -4.81249 acc 0.32432 roc_auc 0.26500 prc_auc 0.58497[0m
[92maverage training of epoch 70: loss -4.86899 acc 0.33775 roc_auc 0.37235 prc_auc 0.56710[0m
[93maverage test of epoch 70: loss -4.86877 acc 0.32432 roc_auc 0.17500 prc_auc 0.53018[0m
[92maverage training of epoch 71: loss -4.92501 acc 0.33775 roc_auc 0.37255 prc_auc 0.56717[0m
[93maverage test of epoch 71: loss -4.92500 acc 0.32432 roc_auc 0.22333 prc_auc 0.55267[0m
[92maverage training of epoch 72: loss -4.98100 acc 0.33775 roc_auc 0.37294 prc_auc 0.56758[0m
[93maverage test of epoch 72: loss -4.98120 acc 0.32432 roc_auc 0.20500 prc_auc 0.54843[0m
[92maverage training of epoch 73: loss -5.03695 acc 0.33775 roc_auc 0.37333 prc_auc 0.56818[0m
[93maverage test of epoch 73: loss -5.03736 acc 0.32432 roc_auc 0.18833 prc_auc 0.54541[0m
[92maverage training of epoch 74: loss -5.09286 acc 0.33775 roc_auc 0.37333 prc_auc 0.56822[0m
[93maverage test of epoch 74: loss -5.09348 acc 0.32432 roc_auc 0.23500 prc_auc 0.56924[0m
[92maverage training of epoch 75: loss -5.14874 acc 0.33775 roc_auc 0.37392 prc_auc 0.56859[0m
[93maverage test of epoch 75: loss -5.14958 acc 0.32432 roc_auc 0.16667 prc_auc 0.54125[0m
[92maverage training of epoch 76: loss -5.20459 acc 0.33775 roc_auc 0.37422 prc_auc 0.56864[0m
[93maverage test of epoch 76: loss -5.20564 acc 0.32432 roc_auc 0.20833 prc_auc 0.55407[0m
[92maverage training of epoch 77: loss -5.26041 acc 0.33775 roc_auc 0.37451 prc_auc 0.56883[0m
[93maverage test of epoch 77: loss -5.26169 acc 0.32432 roc_auc 0.07333 prc_auc 0.54035[0m
[92maverage training of epoch 78: loss -5.31621 acc 0.33775 roc_auc 0.37471 prc_auc 0.56911[0m
[93maverage test of epoch 78: loss -5.31770 acc 0.32432 roc_auc 0.21500 prc_auc 0.56057[0m
[92maverage training of epoch 79: loss -5.37199 acc 0.33775 roc_auc 0.37412 prc_auc 0.56825[0m
[93maverage test of epoch 79: loss -5.37370 acc 0.32432 roc_auc 0.13500 prc_auc 0.53911[0m
[92maverage training of epoch 80: loss -5.42774 acc 0.33775 roc_auc 0.37451 prc_auc 0.56856[0m
[93maverage test of epoch 80: loss -5.42967 acc 0.32432 roc_auc 0.12833 prc_auc 0.53508[0m
[92maverage training of epoch 81: loss -5.48347 acc 0.33775 roc_auc 0.37471 prc_auc 0.56866[0m
[93maverage test of epoch 81: loss -5.48563 acc 0.32432 roc_auc 0.09833 prc_auc 0.53608[0m
[92maverage training of epoch 82: loss -5.53918 acc 0.33775 roc_auc 0.37480 prc_auc 0.56872[0m
[93maverage test of epoch 82: loss -5.54156 acc 0.32432 roc_auc 0.14667 prc_auc 0.54745[0m
[92maverage training of epoch 83: loss -5.59488 acc 0.33775 roc_auc 0.37471 prc_auc 0.56866[0m
[93maverage test of epoch 83: loss -5.59749 acc 0.32432 roc_auc 0.12833 prc_auc 0.56455[0m
[92maverage training of epoch 84: loss -5.65056 acc 0.33775 roc_auc 0.37480 prc_auc 0.56874[0m
[93maverage test of epoch 84: loss -5.65339 acc 0.32432 roc_auc 0.19333 prc_auc 0.58349[0m
[92maverage training of epoch 85: loss -5.70623 acc 0.33775 roc_auc 0.37471 prc_auc 0.56888[0m
[93maverage test of epoch 85: loss -5.70928 acc 0.32432 roc_auc 0.26667 prc_auc 0.57859[0m
[92maverage training of epoch 86: loss -5.76188 acc 0.33775 roc_auc 0.37490 prc_auc 0.56898[0m
[93maverage test of epoch 86: loss -5.76516 acc 0.32432 roc_auc 0.20667 prc_auc 0.56850[0m
[92maverage training of epoch 87: loss -5.81752 acc 0.33775 roc_auc 0.37490 prc_auc 0.56898[0m
[93maverage test of epoch 87: loss -5.82103 acc 0.32432 roc_auc 0.29000 prc_auc 0.59103[0m
[92maverage training of epoch 88: loss -5.87314 acc 0.33775 roc_auc 0.37510 prc_auc 0.56932[0m
[93maverage test of epoch 88: loss -5.87688 acc 0.32432 roc_auc 0.15667 prc_auc 0.57313[0m
[92maverage training of epoch 89: loss -5.92876 acc 0.33775 roc_auc 0.37510 prc_auc 0.56932[0m
[93maverage test of epoch 89: loss -5.93273 acc 0.32432 roc_auc 0.16667 prc_auc 0.57754[0m
[92maverage training of epoch 90: loss -5.98437 acc 0.33775 roc_auc 0.37510 prc_auc 0.56932[0m
[93maverage test of epoch 90: loss -5.98856 acc 0.32432 roc_auc 0.30333 prc_auc 0.60324[0m
[92maverage training of epoch 91: loss -6.03996 acc 0.33775 roc_auc 0.37529 prc_auc 0.56942[0m
[93maverage test of epoch 91: loss -6.04439 acc 0.32432 roc_auc 0.36000 prc_auc 0.65717[0m
[92maverage training of epoch 92: loss -6.09555 acc 0.33775 roc_auc 0.37549 prc_auc 0.56951[0m
[93maverage test of epoch 92: loss -6.10021 acc 0.32432 roc_auc 0.39333 prc_auc 0.62703[0m
[92maverage training of epoch 93: loss -6.15113 acc 0.33775 roc_auc 0.37549 prc_auc 0.56951[0m
[93maverage test of epoch 93: loss -6.15602 acc 0.32432 roc_auc 0.53500 prc_auc 0.70311[0m
[92maverage training of epoch 94: loss -6.20671 acc 0.33775 roc_auc 0.37549 prc_auc 0.56971[0m
[93maverage test of epoch 94: loss -6.21183 acc 0.32432 roc_auc 0.31333 prc_auc 0.64036[0m
[92maverage training of epoch 95: loss -6.26227 acc 0.33775 roc_auc 0.37549 prc_auc 0.56951[0m
[93maverage test of epoch 95: loss -6.26763 acc 0.32432 roc_auc 0.34000 prc_auc 0.61448[0m
[92maverage training of epoch 96: loss -6.31783 acc 0.33775 roc_auc 0.37549 prc_auc 0.56951[0m
[93maverage test of epoch 96: loss -6.32342 acc 0.32432 roc_auc 0.18000 prc_auc 0.59703[0m
[92maverage training of epoch 97: loss -6.37339 acc 0.33775 roc_auc 0.37549 prc_auc 0.56951[0m
[93maverage test of epoch 97: loss -6.37920 acc 0.32432 roc_auc 0.19833 prc_auc 0.56546[0m
[92maverage training of epoch 98: loss -6.42894 acc 0.33775 roc_auc 0.37549 prc_auc 0.56951[0m
[93maverage test of epoch 98: loss -6.43499 acc 0.32432 roc_auc 0.30000 prc_auc 0.59750[0m
[92maverage training of epoch 99: loss -6.48448 acc 0.33775 roc_auc 0.37549 prc_auc 0.57034[0m
[93maverage test of epoch 99: loss -6.49076 acc 0.32432 roc_auc 0.14333 prc_auc 0.56218[0m
Training model with dataset, testing using fold 4
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.01612 acc 0.66225 roc_auc 0.34980 prc_auc 0.57369[0m
[93maverage test of epoch 0: loss -0.11547 acc 0.67568 roc_auc 0.10667 prc_auc 0.55030[0m
[92maverage training of epoch 1: loss -0.14681 acc 0.66225 roc_auc 0.36039 prc_auc 0.58044[0m
[93maverage test of epoch 1: loss -0.26056 acc 0.67568 roc_auc 0.15000 prc_auc 0.53882[0m
[92maverage training of epoch 2: loss -0.30558 acc 0.66225 roc_auc 0.36431 prc_auc 0.58235[0m
[93maverage test of epoch 2: loss -0.43689 acc 0.67568 roc_auc 0.25000 prc_auc 0.58940[0m
[92maverage training of epoch 3: loss -0.49670 acc 0.66225 roc_auc 0.36980 prc_auc 0.59015[0m
[93maverage test of epoch 3: loss -0.64521 acc 0.67568 roc_auc 0.39000 prc_auc 0.68316[0m
[92maverage training of epoch 4: loss -0.71152 acc 0.66225 roc_auc 0.37706 prc_auc 0.59810[0m
[93maverage test of epoch 4: loss -0.86676 acc 0.67568 roc_auc 0.53667 prc_auc 0.77829[0m
[92maverage training of epoch 5: loss -0.92399 acc 0.66225 roc_auc 0.38392 prc_auc 0.60310[0m
[93maverage test of epoch 5: loss -1.07187 acc 0.67568 roc_auc 0.60000 prc_auc 0.82342[0m
[92maverage training of epoch 6: loss -1.11391 acc 0.66225 roc_auc 0.39216 prc_auc 0.60986[0m
[93maverage test of epoch 6: loss -1.24900 acc 0.67568 roc_auc 0.64333 prc_auc 0.84890[0m
[92maverage training of epoch 7: loss -1.27754 acc 0.66225 roc_auc 0.39784 prc_auc 0.61840[0m
[93maverage test of epoch 7: loss -1.40064 acc 0.67568 roc_auc 0.66000 prc_auc 0.85585[0m
[92maverage training of epoch 8: loss -1.41951 acc 0.66225 roc_auc 0.40353 prc_auc 0.62259[0m
[93maverage test of epoch 8: loss -1.53373 acc 0.67568 roc_auc 0.67333 prc_auc 0.85992[0m
[92maverage training of epoch 9: loss -1.54675 acc 0.66225 roc_auc 0.40980 prc_auc 0.62669[0m
[93maverage test of epoch 9: loss -1.65569 acc 0.67568 roc_auc 0.70000 prc_auc 0.87360[0m
[92maverage training of epoch 10: loss -1.66604 acc 0.66225 roc_auc 0.42275 prc_auc 0.63440[0m
[93maverage test of epoch 10: loss -1.77284 acc 0.67568 roc_auc 0.73333 prc_auc 0.89355[0m
[92maverage training of epoch 11: loss -1.78264 acc 0.66225 roc_auc 0.44431 prc_auc 0.65891[0m
[93maverage test of epoch 11: loss -1.88943 acc 0.67568 roc_auc 0.76000 prc_auc 0.90500[0m
[92maverage training of epoch 12: loss -1.89966 acc 0.66225 roc_auc 0.45539 prc_auc 0.66387[0m
[93maverage test of epoch 12: loss -2.00739 acc 0.67568 roc_auc 0.78000 prc_auc 0.91421[0m
[92maverage training of epoch 13: loss -2.01809 acc 0.66225 roc_auc 0.45843 prc_auc 0.66913[0m
[93maverage test of epoch 13: loss -2.12691 acc 0.67568 roc_auc 0.82000 prc_auc 0.92565[0m
[92maverage training of epoch 14: loss -2.13753 acc 0.66225 roc_auc 0.45833 prc_auc 0.66875[0m
[93maverage test of epoch 14: loss -2.24699 acc 0.67568 roc_auc 0.84000 prc_auc 0.93466[0m
[92maverage training of epoch 15: loss -2.25671 acc 0.66225 roc_auc 0.45775 prc_auc 0.66781[0m
[93maverage test of epoch 15: loss -2.36611 acc 0.67568 roc_auc 0.84333 prc_auc 0.93616[0m
[92maverage training of epoch 16: loss -2.37406 acc 0.66225 roc_auc 0.45608 prc_auc 0.66586[0m
[93maverage test of epoch 16: loss -2.48265 acc 0.67568 roc_auc 0.84500 prc_auc 0.93616[0m
[92maverage training of epoch 17: loss -2.48809 acc 0.66225 roc_auc 0.45265 prc_auc 0.66457[0m
[93maverage test of epoch 17: loss -2.59526 acc 0.67568 roc_auc 0.85667 prc_auc 0.94176[0m
[92maverage training of epoch 18: loss -2.59772 acc 0.66225 roc_auc 0.44980 prc_auc 0.66076[0m
[93maverage test of epoch 18: loss -2.70310 acc 0.67568 roc_auc 0.86833 prc_auc 0.94625[0m
[92maverage training of epoch 19: loss -2.70237 acc 0.66225 roc_auc 0.44765 prc_auc 0.65757[0m
[93maverage test of epoch 19: loss -2.80584 acc 0.67568 roc_auc 0.90167 prc_auc 0.95947[0m
[92maverage training of epoch 20: loss -2.80194 acc 0.66225 roc_auc 0.44559 prc_auc 0.65575[0m
[93maverage test of epoch 20: loss -2.90356 acc 0.67568 roc_auc 0.91167 prc_auc 0.96155[0m
[92maverage training of epoch 21: loss -2.89665 acc 0.66225 roc_auc 0.44157 prc_auc 0.65248[0m
[93maverage test of epoch 21: loss -2.99663 acc 0.67568 roc_auc 0.92833 prc_auc 0.96923[0m
[92maverage training of epoch 22: loss -2.98695 acc 0.66225 roc_auc 0.43373 prc_auc 0.64587[0m
[93maverage test of epoch 22: loss -3.08555 acc 0.67568 roc_auc 0.92333 prc_auc 0.96609[0m
[92maverage training of epoch 23: loss -3.07336 acc 0.66225 roc_auc 0.42922 prc_auc 0.64235[0m
[93maverage test of epoch 23: loss -3.17085 acc 0.67568 roc_auc 0.91500 prc_auc 0.96114[0m
[92maverage training of epoch 24: loss -3.15640 acc 0.66225 roc_auc 0.42382 prc_auc 0.63852[0m
[93maverage test of epoch 24: loss -3.25302 acc 0.67568 roc_auc 0.91667 prc_auc 0.96267[0m
[92maverage training of epoch 25: loss -3.23653 acc 0.66225 roc_auc 0.41853 prc_auc 0.63405[0m
[93maverage test of epoch 25: loss -3.33250 acc 0.67568 roc_auc 0.91167 prc_auc 0.95958[0m
[92maverage training of epoch 26: loss -3.31417 acc 0.66225 roc_auc 0.41539 prc_auc 0.63168[0m
[93maverage test of epoch 26: loss -3.40964 acc 0.67568 roc_auc 0.91333 prc_auc 0.95875[0m
[92maverage training of epoch 27: loss -3.38964 acc 0.66225 roc_auc 0.41294 prc_auc 0.62910[0m
[93maverage test of epoch 27: loss -3.48476 acc 0.67568 roc_auc 0.90333 prc_auc 0.95441[0m
[92maverage training of epoch 28: loss -3.46324 acc 0.66225 roc_auc 0.41029 prc_auc 0.62685[0m
[93maverage test of epoch 28: loss -3.55811 acc 0.67568 roc_auc 0.90667 prc_auc 0.95353[0m
[92maverage training of epoch 29: loss -3.53518 acc 0.66225 roc_auc 0.40804 prc_auc 0.62335[0m
[93maverage test of epoch 29: loss -3.62990 acc 0.67568 roc_auc 0.90833 prc_auc 0.95231[0m
[92maverage training of epoch 30: loss -3.60567 acc 0.66225 roc_auc 0.40539 prc_auc 0.62063[0m
[93maverage test of epoch 30: loss -3.70030 acc 0.67568 roc_auc 0.91000 prc_auc 0.95201[0m
[92maverage training of epoch 31: loss -3.67487 acc 0.66225 roc_auc 0.40216 prc_auc 0.61822[0m
[93maverage test of epoch 31: loss -3.76946 acc 0.67568 roc_auc 0.91167 prc_auc 0.95375[0m
[92maverage training of epoch 32: loss -3.74291 acc 0.66225 roc_auc 0.39931 prc_auc 0.61570[0m
[93maverage test of epoch 32: loss -3.83751 acc 0.67568 roc_auc 0.90667 prc_auc 0.94925[0m
[92maverage training of epoch 33: loss -3.80992 acc 0.66225 roc_auc 0.39627 prc_auc 0.61316[0m
[93maverage test of epoch 33: loss -3.90457 acc 0.67568 roc_auc 0.87500 prc_auc 0.93278[0m
[92maverage training of epoch 34: loss -3.87600 acc 0.66225 roc_auc 0.39461 prc_auc 0.61017[0m
[93maverage test of epoch 34: loss -3.97073 acc 0.67568 roc_auc 0.88667 prc_auc 0.93478[0m
[92maverage training of epoch 35: loss -3.94124 acc 0.66225 roc_auc 0.39196 prc_auc 0.60794[0m
[93maverage test of epoch 35: loss -4.03606 acc 0.67568 roc_auc 0.88833 prc_auc 0.92963[0m
[92maverage training of epoch 36: loss -4.00572 acc 0.66225 roc_auc 0.38980 prc_auc 0.60629[0m
[93maverage test of epoch 36: loss -4.10066 acc 0.67568 roc_auc 0.88333 prc_auc 0.92418[0m
[92maverage training of epoch 37: loss -4.06949 acc 0.66225 roc_auc 0.38765 prc_auc 0.60442[0m
[93maverage test of epoch 37: loss -4.16457 acc 0.67568 roc_auc 0.86000 prc_auc 0.91223[0m
[92maverage training of epoch 38: loss -4.13263 acc 0.66225 roc_auc 0.38657 prc_auc 0.60366[0m
[93maverage test of epoch 38: loss -4.22785 acc 0.67568 roc_auc 0.86667 prc_auc 0.91001[0m
[92maverage training of epoch 39: loss -4.19518 acc 0.66225 roc_auc 0.38559 prc_auc 0.60312[0m
[93maverage test of epoch 39: loss -4.29057 acc 0.67568 roc_auc 0.88000 prc_auc 0.91552[0m
[92maverage training of epoch 40: loss -4.25720 acc 0.66225 roc_auc 0.38451 prc_auc 0.60152[0m
[93maverage test of epoch 40: loss -4.35276 acc 0.67568 roc_auc 0.82333 prc_auc 0.88642[0m
[92maverage training of epoch 41: loss -4.31873 acc 0.66225 roc_auc 0.38471 prc_auc 0.60234[0m
[93maverage test of epoch 41: loss -4.41447 acc 0.67568 roc_auc 0.84333 prc_auc 0.89303[0m
[92maverage training of epoch 42: loss -4.37981 acc 0.66225 roc_auc 0.38422 prc_auc 0.60137[0m
[93maverage test of epoch 42: loss -4.47573 acc 0.67568 roc_auc 0.78667 prc_auc 0.86651[0m
[92maverage training of epoch 43: loss -4.44046 acc 0.66225 roc_auc 0.38353 prc_auc 0.60156[0m
[93maverage test of epoch 43: loss -4.53658 acc 0.67568 roc_auc 0.84000 prc_auc 0.88364[0m
[92maverage training of epoch 44: loss -4.50073 acc 0.66225 roc_auc 0.38324 prc_auc 0.60123[0m
[93maverage test of epoch 44: loss -4.59704 acc 0.67568 roc_auc 0.70000 prc_auc 0.81124[0m
[92maverage training of epoch 45: loss -4.56065 acc 0.66225 roc_auc 0.38382 prc_auc 0.60168[0m
[93maverage test of epoch 45: loss -4.65716 acc 0.67568 roc_auc 0.78000 prc_auc 0.85333[0m
[92maverage training of epoch 46: loss -4.62023 acc 0.66225 roc_auc 0.38304 prc_auc 0.59996[0m
[93maverage test of epoch 46: loss -4.71695 acc 0.67568 roc_auc 0.73333 prc_auc 0.81514[0m
[92maverage training of epoch 47: loss -4.67951 acc 0.66225 roc_auc 0.38235 prc_auc 0.59646[0m
[93maverage test of epoch 47: loss -4.77643 acc 0.67568 roc_auc 0.72000 prc_auc 0.80439[0m
[92maverage training of epoch 48: loss -4.73851 acc 0.66225 roc_auc 0.38167 prc_auc 0.59466[0m
[93maverage test of epoch 48: loss -4.83564 acc 0.67568 roc_auc 0.74000 prc_auc 0.83135[0m
[92maverage training of epoch 49: loss -4.79724 acc 0.66225 roc_auc 0.38098 prc_auc 0.59317[0m
[93maverage test of epoch 49: loss -4.89459 acc 0.67568 roc_auc 0.62000 prc_auc 0.73708[0m
[92maverage training of epoch 50: loss -4.85574 acc 0.66225 roc_auc 0.38157 prc_auc 0.59566[0m
[93maverage test of epoch 50: loss -4.95330 acc 0.67568 roc_auc 0.65333 prc_auc 0.75524[0m
[92maverage training of epoch 51: loss -4.91401 acc 0.66225 roc_auc 0.38196 prc_auc 0.59594[0m
[93maverage test of epoch 51: loss -5.01178 acc 0.67568 roc_auc 0.78000 prc_auc 0.85730[0m
[92maverage training of epoch 52: loss -4.97207 acc 0.66225 roc_auc 0.38245 prc_auc 0.59643[0m
[93maverage test of epoch 52: loss -5.07007 acc 0.67568 roc_auc 0.52167 prc_auc 0.68531[0m
[92maverage training of epoch 53: loss -5.02994 acc 0.66225 roc_auc 0.38225 prc_auc 0.59648[0m
[93maverage test of epoch 53: loss -5.12816 acc 0.67568 roc_auc 0.70000 prc_auc 0.80541[0m
[92maverage training of epoch 54: loss -5.08763 acc 0.66225 roc_auc 0.38284 prc_auc 0.59806[0m
[93maverage test of epoch 54: loss -5.18607 acc 0.67568 roc_auc 0.54333 prc_auc 0.69527[0m
[92maverage training of epoch 55: loss -5.14516 acc 0.66225 roc_auc 0.38304 prc_auc 0.59784[0m
[93maverage test of epoch 55: loss -5.24382 acc 0.67568 roc_auc 0.58667 prc_auc 0.71638[0m
[92maverage training of epoch 56: loss -5.20254 acc 0.66225 roc_auc 0.38363 prc_auc 0.59803[0m
[93maverage test of epoch 56: loss -5.30143 acc 0.67568 roc_auc 0.72000 prc_auc 0.81838[0m
[92maverage training of epoch 57: loss -5.25977 acc 0.66225 roc_auc 0.38206 prc_auc 0.59119[0m
[93maverage test of epoch 57: loss -5.35889 acc 0.67568 roc_auc 0.54000 prc_auc 0.70162[0m
[92maverage training of epoch 58: loss -5.31688 acc 0.66225 roc_auc 0.38010 prc_auc 0.58916[0m
[93maverage test of epoch 58: loss -5.41622 acc 0.67568 roc_auc 0.46000 prc_auc 0.65863[0m
[92maverage training of epoch 59: loss -5.37386 acc 0.66225 roc_auc 0.37931 prc_auc 0.58950[0m
[93maverage test of epoch 59: loss -5.47343 acc 0.67568 roc_auc 0.68000 prc_auc 0.79243[0m
[92maverage training of epoch 60: loss -5.43073 acc 0.66225 roc_auc 0.37833 prc_auc 0.58879[0m
[93maverage test of epoch 60: loss -5.53053 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 61: loss -5.48750 acc 0.66225 roc_auc 0.37873 prc_auc 0.58894[0m
[93maverage test of epoch 61: loss -5.58752 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 62: loss -5.54417 acc 0.66225 roc_auc 0.37912 prc_auc 0.58944[0m
[93maverage test of epoch 62: loss -5.64443 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 63: loss -5.60076 acc 0.66225 roc_auc 0.37902 prc_auc 0.58936[0m
[93maverage test of epoch 63: loss -5.70124 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 64: loss -5.65726 acc 0.66225 roc_auc 0.37892 prc_auc 0.58920[0m
[93maverage test of epoch 64: loss -5.75797 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 65: loss -5.71369 acc 0.66225 roc_auc 0.37873 prc_auc 0.58933[0m
[93maverage test of epoch 65: loss -5.81463 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 66: loss -5.77004 acc 0.66225 roc_auc 0.37853 prc_auc 0.58968[0m
[93maverage test of epoch 66: loss -5.87122 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 67: loss -5.82633 acc 0.66225 roc_auc 0.37843 prc_auc 0.58964[0m
[93maverage test of epoch 67: loss -5.92774 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 68: loss -5.88256 acc 0.66225 roc_auc 0.37784 prc_auc 0.58940[0m
[93maverage test of epoch 68: loss -5.98420 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 69: loss -5.93874 acc 0.66225 roc_auc 0.37824 prc_auc 0.58933[0m
[93maverage test of epoch 69: loss -6.04061 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 70: loss -5.99486 acc 0.66225 roc_auc 0.37814 prc_auc 0.59014[0m
[93maverage test of epoch 70: loss -6.09697 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 71: loss -6.05094 acc 0.66225 roc_auc 0.37833 prc_auc 0.59001[0m
[93maverage test of epoch 71: loss -6.15328 acc 0.67568 roc_auc 0.69333 prc_auc 0.77828[0m
[92maverage training of epoch 72: loss -6.10697 acc 0.66225 roc_auc 0.37804 prc_auc 0.58885[0m
[93maverage test of epoch 72: loss -6.20954 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 73: loss -6.16296 acc 0.66225 roc_auc 0.37804 prc_auc 0.58916[0m
[93maverage test of epoch 73: loss -6.26577 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 74: loss -6.21892 acc 0.66225 roc_auc 0.37843 prc_auc 0.59019[0m
[93maverage test of epoch 74: loss -6.32195 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 75: loss -6.27483 acc 0.66225 roc_auc 0.37863 prc_auc 0.59036[0m
[93maverage test of epoch 75: loss -6.37810 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 76: loss -6.33072 acc 0.66225 roc_auc 0.37902 prc_auc 0.59087[0m
[93maverage test of epoch 76: loss -6.43423 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 77: loss -6.38658 acc 0.66225 roc_auc 0.37745 prc_auc 0.58981[0m
[93maverage test of epoch 77: loss -6.49032 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 78: loss -6.44241 acc 0.66225 roc_auc 0.37814 prc_auc 0.59150[0m
[93maverage test of epoch 78: loss -6.54638 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 79: loss -6.49821 acc 0.66225 roc_auc 0.37794 prc_auc 0.59045[0m
[93maverage test of epoch 79: loss -6.60242 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 80: loss -6.55399 acc 0.66225 roc_auc 0.37775 prc_auc 0.59066[0m
[93maverage test of epoch 80: loss -6.65843 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 81: loss -6.60975 acc 0.66225 roc_auc 0.37725 prc_auc 0.59015[0m
[93maverage test of epoch 81: loss -6.71443 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 82: loss -6.66549 acc 0.66225 roc_auc 0.37716 prc_auc 0.58962[0m
[93maverage test of epoch 82: loss -6.77040 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 83: loss -6.72121 acc 0.66225 roc_auc 0.37775 prc_auc 0.59091[0m
[93maverage test of epoch 83: loss -6.82636 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 84: loss -6.77691 acc 0.66225 roc_auc 0.37765 prc_auc 0.59066[0m
[93maverage test of epoch 84: loss -6.88230 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 85: loss -6.83260 acc 0.66225 roc_auc 0.37716 prc_auc 0.58531[0m
[93maverage test of epoch 85: loss -6.93822 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 86: loss -6.88827 acc 0.66225 roc_auc 0.37598 prc_auc 0.58242[0m
[93maverage test of epoch 86: loss -6.99412 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 87: loss -6.94393 acc 0.66225 roc_auc 0.37578 prc_auc 0.58151[0m
[93maverage test of epoch 87: loss -7.05002 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 88: loss -6.99958 acc 0.66225 roc_auc 0.37627 prc_auc 0.58289[0m
[93maverage test of epoch 88: loss -7.10590 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 89: loss -7.05521 acc 0.66225 roc_auc 0.37539 prc_auc 0.57517[0m
[93maverage test of epoch 89: loss -7.16177 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 90: loss -7.11083 acc 0.66225 roc_auc 0.37500 prc_auc 0.57425[0m
[93maverage test of epoch 90: loss -7.21763 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 91: loss -7.16645 acc 0.66225 roc_auc 0.37500 prc_auc 0.57375[0m
[93maverage test of epoch 91: loss -7.27347 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 92: loss -7.22205 acc 0.66225 roc_auc 0.37431 prc_auc 0.57304[0m
[93maverage test of epoch 92: loss -7.32931 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 93: loss -7.27764 acc 0.66225 roc_auc 0.37324 prc_auc 0.57202[0m
[93maverage test of epoch 93: loss -7.38514 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 94: loss -7.33323 acc 0.66225 roc_auc 0.37353 prc_auc 0.57279[0m
[93maverage test of epoch 94: loss -7.44097 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 95: loss -7.38881 acc 0.66225 roc_auc 0.37363 prc_auc 0.57293[0m
[93maverage test of epoch 95: loss -7.49678 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 96: loss -7.44439 acc 0.66225 roc_auc 0.37235 prc_auc 0.57159[0m
[93maverage test of epoch 96: loss -7.55259 acc 0.67568 roc_auc 0.74000 prc_auc 0.83135[0m
[92maverage training of epoch 97: loss -7.49995 acc 0.66225 roc_auc 0.37294 prc_auc 0.57208[0m
[93maverage test of epoch 97: loss -7.60839 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 98: loss -7.55551 acc 0.66225 roc_auc 0.37235 prc_auc 0.57155[0m
[93maverage test of epoch 98: loss -7.66419 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 99: loss -7.61107 acc 0.66225 roc_auc 0.37206 prc_auc 0.57141[0m
[93maverage test of epoch 99: loss -7.71998 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
Run statistics: 
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 100)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 0)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))


Accuracy (avg): 0.53158 ROC_AUC (avg): 0.57297 PRC_AUC (avg): 0.74159 

Average forward propagation time taken(ms): 2.4609759378113756
Average backward propagation time taken(ms): 0.8649787262423787

