# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======






Params Turning Set
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 2)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 8)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 1)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))






args.base_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/
args.graph_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/
args.min_dfscode_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/
args.min_dfscode_tensor_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/
*** 1 train_index:  [  0   2   3   6   9  10  11  12  13  14  15  16  17  19  20  21  22  23
  24  26  27  30  31  32  33  35  36  38  39  41  42  43  44  45  46  47
  48  49  50  52  53  54  56  57  59  60  61  62  63  64  65  66  67  69
  70  71  72  73  74  76  77  78  79  80  81  82  83  84  87  88  89  91
  92  94  95  96  97  98 100 101 102 103 104 106 107 108 109 110 112 113
 115 116 117 118 119 120 121 122 124 125 126 127 129 130 131 132 134 136
 138 139 140 141 142 144 145 146 147 148 149 151 152 153 154 155 156 157
 158 159 160 161 164 165 166 167 169 170 171 172 173 175 176 177 178 179
 180 182 184 185 186 187]
*** 2 test_index:  [  1   4   5   7   8  18  25  28  29  34  37  40  51  55  58  68  75  85
  86  90  93  99 105 111 114 123 128 133 135 137 143 150 162 163 168 174
 181 183]
*** 1 train_index:  [  0   1   2   4   5   6   7   8  10  11  12  13  14  15  16  17  18  19
  20  21  22  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39
  40  41  43  44  45  46  49  50  51  52  53  54  55  56  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  74  75  76  77  79  80  81  82
  83  84  85  86  88  90  91  92  93  94  96  97  98  99 101 102 103 104
 105 106 107 111 112 113 114 115 120 121 122 123 124 125 126 128 129 130
 131 133 134 135 136 137 138 139 140 143 144 145 147 148 150 153 154 155
 156 159 162 163 164 165 167 168 169 170 171 172 173 174 175 177 178 179
 180 181 182 183 186 187]
*** 2 test_index:  [  3   9  23  31  42  47  48  57  72  73  78  87  89  95 100 108 109 110
 116 117 118 119 127 132 141 142 146 149 151 152 157 158 160 161 166 176
 184 185]
*** 1 train_index:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19
  20  23  24  25  26  28  29  31  32  34  35  36  37  38  40  41  42  43
  44  47  48  51  53  54  55  56  57  58  59  60  64  66  68  70  71  72
  73  74  75  76  77  78  79  81  82  84  85  86  87  88  89  90  92  93
  95  96  97  98  99 100 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134
 135 137 139 140 141 142 143 144 146 148 149 150 151 152 153 156 157 158
 159 160 161 162 163 165 166 167 168 169 171 172 173 174 175 176 177 178
 179 181 183 184 185 186]
*** 2 test_index:  [  0  16  21  22  27  30  33  39  45  46  49  50  52  61  62  63  65  67
  69  80  83  91  94 101 102 103 122 136 138 145 147 154 155 164 170 180
 182 187]
*** 1 train_index:  [  0   1   3   4   5   6   7   8   9  13  14  16  18  19  21  22  23  25
  26  27  28  29  30  31  33  34  36  37  38  39  40  42  43  45  46  47
  48  49  50  51  52  53  55  56  57  58  60  61  62  63  65  66  67  68
  69  71  72  73  75  76  77  78  79  80  81  83  84  85  86  87  88  89
  90  91  92  93  94  95  97  98  99 100 101 102 103 105 106 108 109 110
 111 113 114 115 116 117 118 119 120 121 122 123 125 127 128 131 132 133
 134 135 136 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152
 154 155 156 157 158 160 161 162 163 164 165 166 168 170 171 174 175 176
 180 181 182 183 184 185 187]
*** 2 test_index:  [  2  10  11  12  15  17  20  24  32  35  41  44  54  59  64  70  74  82
  96 104 107 112 124 126 129 130 139 153 159 167 169 172 173 177 178 179
 186]
*** 1 train_index:  [  0   1   2   3   4   5   7   8   9  10  11  12  15  16  17  18  20  21
  22  23  24  25  27  28  29  30  31  32  33  34  35  37  39  40  41  42
  44  45  46  47  48  49  50  51  52  54  55  57  58  59  61  62  63  64
  65  67  68  69  70  72  73  74  75  78  80  82  83  85  86  87  89  90
  91  93  94  95  96  99 100 101 102 103 104 105 107 108 109 110 111 112
 114 116 117 118 119 122 123 124 126 127 128 129 130 132 133 135 136 137
 138 139 141 142 143 145 146 147 149 150 151 152 153 154 155 157 158 159
 160 161 162 163 164 166 167 168 169 170 172 173 174 176 177 178 179 180
 181 182 183 184 185 186 187]
*** 2 test_index:  [  6  13  14  19  26  36  38  43  53  56  60  66  71  76  77  79  81  84
  88  92  97  98 106 113 115 120 121 125 131 134 140 144 148 156 165 171
 175]
{'base_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/',
 'batch_size': 1,
 'clean_temp': False,
 'clean_tensorboard': False,
 'current_dataset_path': None,
 'current_min_dfscode_path': None,
 'current_model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-20-57-02/',
 'current_processed_dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'current_temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-20-57-02/',
 'dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/datasets/',
 'device': device(type='cuda', index=0),
 'dfscode_rnn_dropout': 0.2,
 'embedding_size_dfscode_rnn': 8,
 'epochs': 50,
 'epochs_end': 10000,
 'epochs_save': 20,
 'epochs_validate': 1,
 'fname': 'DFScodeRNN_cls_LSTM_MUTAG',
 'gradient_clipping': True,
 'graph_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/',
 'graph_type': 'MUTAG',
 'graphgen_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/',
 'hidden_size_dfscode_rnn': 8,
 'load_device': device(type='cuda', index=0),
 'load_model': False,
 'load_model_path': '',
 'log_tensorboard': False,
 'lr': 0.0001,
 'max_prev_node': None,
 'min_dfscode_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/',
 'min_dfscode_tensor_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/',
 'note': 'DFScodeRNN_cls_LSTM',
 'num_graphs': None,
 'num_layers': 2,
 'num_workers': 8,
 'number_of_mlp_layer': 1,
 'rnn_type': 'LSTM',
 'save_model': False,
 'temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/',
 'tensorboard_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tensorboard/',
 'time': '2021-01-17-20-57-02',
 'weights': False}


graphgen args.__dict__: None




dataset_features: {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}




config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 50, 'learning_rate': 0.0001, 'seed': 1800, 'k_fold': 5, 'model': 'DFScodeRNN_cls_LSTM', 'dataset': 'MUTAG'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}}


Training a new model: DFScodeRNN_cls_LSTM
Training model with dataset, testing using fold 0
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 8, num_layers=2, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.37930 acc 0.33333 roc_auc 0.42080 prc_auc 0.62662[0m
[93maverage test of epoch 0: loss -0.60572 acc 0.34211 roc_auc 0.34154 prc_auc 0.62948[0m
[92maverage training of epoch 1: loss -0.87085 acc 0.33333 roc_auc 0.37420 prc_auc 0.60448[0m
[93maverage test of epoch 1: loss -1.10631 acc 0.34211 roc_auc 0.31385 prc_auc 0.57681[0m
[92maverage training of epoch 2: loss -1.32284 acc 0.34667 roc_auc 0.43360 prc_auc 0.66432[0m
[93maverage test of epoch 2: loss -1.54005 acc 0.39474 roc_auc 0.49846 prc_auc 0.75128[0m
[92maverage training of epoch 3: loss -1.74494 acc 0.42667 roc_auc 0.42340 prc_auc 0.64961[0m
[93maverage test of epoch 3: loss -1.99432 acc 0.42105 roc_auc 0.48308 prc_auc 0.67861[0m
[92maverage training of epoch 4: loss -2.20789 acc 0.56667 roc_auc 0.46820 prc_auc 0.63502[0m
[93maverage test of epoch 4: loss -2.44027 acc 0.57895 roc_auc 0.57538 prc_auc 0.76111[0m
[92maverage training of epoch 5: loss -2.66055 acc 0.60667 roc_auc 0.47480 prc_auc 0.68650[0m
[93maverage test of epoch 5: loss -2.90465 acc 0.63158 roc_auc 0.34769 prc_auc 0.57398[0m
[92maverage training of epoch 6: loss -3.14219 acc 0.65333 roc_auc 0.53760 prc_auc 0.69615[0m
[93maverage test of epoch 6: loss -3.37440 acc 0.63158 roc_auc 0.56000 prc_auc 0.67663[0m
[92maverage training of epoch 7: loss -3.63487 acc 0.66667 roc_auc 0.45460 prc_auc 0.64064[0m
[93maverage test of epoch 7: loss -3.99554 acc 0.65789 roc_auc 0.50769 prc_auc 0.67531[0m
[92maverage training of epoch 8: loss -4.40058 acc 0.66667 roc_auc 0.42760 prc_auc 0.64357[0m
[93maverage test of epoch 8: loss -4.75752 acc 0.65789 roc_auc 0.58769 prc_auc 0.76533[0m
[92maverage training of epoch 9: loss -5.16997 acc 0.66667 roc_auc 0.44120 prc_auc 0.62965[0m
[93maverage test of epoch 9: loss -5.53143 acc 0.65789 roc_auc 0.51692 prc_auc 0.67390[0m
[92maverage training of epoch 10: loss -5.89278 acc 0.66667 roc_auc 0.37300 prc_auc 0.61844[0m
[93maverage test of epoch 10: loss -6.29307 acc 0.65789 roc_auc 0.45538 prc_auc 0.61606[0m
[92maverage training of epoch 11: loss -6.66069 acc 0.66667 roc_auc 0.45140 prc_auc 0.65558[0m
[93maverage test of epoch 11: loss -7.00430 acc 0.65789 roc_auc 0.55385 prc_auc 0.70745[0m
[92maverage training of epoch 12: loss -7.37979 acc 0.66667 roc_auc 0.49500 prc_auc 0.71375[0m
[93maverage test of epoch 12: loss -7.72508 acc 0.65789 roc_auc 0.55692 prc_auc 0.76474[0m
[92maverage training of epoch 13: loss -8.10295 acc 0.66667 roc_auc 0.47000 prc_auc 0.65087[0m
[93maverage test of epoch 13: loss -8.38707 acc 0.65789 roc_auc 0.32308 prc_auc 0.55228[0m
[92maverage training of epoch 14: loss -8.78228 acc 0.66667 roc_auc 0.41700 prc_auc 0.63282[0m
[93maverage test of epoch 14: loss -9.09129 acc 0.65789 roc_auc 0.44308 prc_auc 0.61512[0m
[92maverage training of epoch 15: loss -9.46081 acc 0.66667 roc_auc 0.42220 prc_auc 0.60979[0m
[93maverage test of epoch 15: loss -9.76611 acc 0.65789 roc_auc 0.34154 prc_auc 0.59571[0m
[92maverage training of epoch 16: loss -10.14035 acc 0.66667 roc_auc 0.40240 prc_auc 0.59888[0m
[93maverage test of epoch 16: loss -10.42115 acc 0.65789 roc_auc 0.48923 prc_auc 0.66326[0m
[92maverage training of epoch 17: loss -10.79804 acc 0.66667 roc_auc 0.42500 prc_auc 0.63352[0m
[93maverage test of epoch 17: loss -11.10102 acc 0.65789 roc_auc 0.49538 prc_auc 0.61882[0m
[92maverage training of epoch 18: loss -11.44696 acc 0.66667 roc_auc 0.39780 prc_auc 0.63433[0m
[93maverage test of epoch 18: loss -11.74287 acc 0.65789 roc_auc 0.57538 prc_auc 0.71747[0m
[92maverage training of epoch 19: loss -12.10748 acc 0.66667 roc_auc 0.43260 prc_auc 0.64202[0m
[93maverage test of epoch 19: loss -12.37266 acc 0.65789 roc_auc 0.41846 prc_auc 0.64505[0m
[92maverage training of epoch 20: loss -12.76948 acc 0.66667 roc_auc 0.38160 prc_auc 0.60914[0m
[93maverage test of epoch 20: loss -13.03654 acc 0.65789 roc_auc 0.49077 prc_auc 0.66562[0m
[92maverage training of epoch 21: loss -13.42912 acc 0.66667 roc_auc 0.37420 prc_auc 0.59797[0m
[93maverage test of epoch 21: loss -13.69684 acc 0.65789 roc_auc 0.48000 prc_auc 0.67234[0m
[92maverage training of epoch 22: loss -14.09414 acc 0.66667 roc_auc 0.40280 prc_auc 0.61278[0m
[93maverage test of epoch 22: loss -14.36741 acc 0.65789 roc_auc 0.40615 prc_auc 0.58987[0m
[92maverage training of epoch 23: loss -14.76486 acc 0.66667 roc_auc 0.38950 prc_auc 0.61917[0m
[93maverage test of epoch 23: loss -15.05869 acc 0.65789 roc_auc 0.40615 prc_auc 0.60534[0m
[92maverage training of epoch 24: loss -15.45998 acc 0.66667 roc_auc 0.41100 prc_auc 0.62664[0m
[93maverage test of epoch 24: loss -15.76308 acc 0.65789 roc_auc 0.54308 prc_auc 0.70754[0m
[92maverage training of epoch 25: loss -16.14911 acc 0.66667 roc_auc 0.41060 prc_auc 0.61191[0m
[93maverage test of epoch 25: loss -16.43562 acc 0.65789 roc_auc 0.42308 prc_auc 0.62124[0m
[92maverage training of epoch 26: loss -16.85812 acc 0.66667 roc_auc 0.38620 prc_auc 0.60336[0m
[93maverage test of epoch 26: loss -17.15104 acc 0.65789 roc_auc 0.45231 prc_auc 0.65722[0m
[92maverage training of epoch 27: loss -17.56908 acc 0.66667 roc_auc 0.41000 prc_auc 0.61940[0m
[93maverage test of epoch 27: loss -17.85321 acc 0.65789 roc_auc 0.68769 prc_auc 0.79186[0m
[92maverage training of epoch 28: loss -18.30642 acc 0.66667 roc_auc 0.41200 prc_auc 0.61596[0m
[93maverage test of epoch 28: loss -18.58724 acc 0.65789 roc_auc 0.51846 prc_auc 0.71424[0m
[92maverage training of epoch 29: loss -19.03409 acc 0.66667 roc_auc 0.38130 prc_auc 0.59264[0m
[93maverage test of epoch 29: loss -19.35531 acc 0.65789 roc_auc 0.52615 prc_auc 0.67413[0m
[92maverage training of epoch 30: loss -19.79528 acc 0.66667 roc_auc 0.40020 prc_auc 0.61250[0m
[93maverage test of epoch 30: loss -20.10441 acc 0.65789 roc_auc 0.68154 prc_auc 0.79338[0m
[92maverage training of epoch 31: loss -20.55175 acc 0.66667 roc_auc 0.39890 prc_auc 0.61072[0m
[93maverage test of epoch 31: loss -20.85555 acc 0.65789 roc_auc 0.60154 prc_auc 0.69195[0m
[92maverage training of epoch 32: loss -21.33361 acc 0.66667 roc_auc 0.39170 prc_auc 0.60383[0m
[93maverage test of epoch 32: loss -21.64125 acc 0.65789 roc_auc 0.45385 prc_auc 0.71205[0m
[92maverage training of epoch 33: loss -22.12450 acc 0.66667 roc_auc 0.40550 prc_auc 0.61635[0m
[93maverage test of epoch 33: loss -22.43227 acc 0.65789 roc_auc 0.53077 prc_auc 0.67453[0m
[92maverage training of epoch 34: loss -22.92510 acc 0.66667 roc_auc 0.40000 prc_auc 0.60986[0m
[93maverage test of epoch 34: loss -23.22346 acc 0.65789 roc_auc 0.46615 prc_auc 0.63735[0m
[92maverage training of epoch 35: loss -23.74030 acc 0.66667 roc_auc 0.40040 prc_auc 0.61319[0m
[93maverage test of epoch 35: loss -24.05458 acc 0.65789 roc_auc 0.55538 prc_auc 0.71830[0m
[92maverage training of epoch 36: loss -24.56560 acc 0.66667 roc_auc 0.37410 prc_auc 0.58987[0m
[93maverage test of epoch 36: loss -24.90442 acc 0.65789 roc_auc 0.54000 prc_auc 0.67579[0m
[92maverage training of epoch 37: loss -25.41305 acc 0.66667 roc_auc 0.42080 prc_auc 0.61790[0m
[93maverage test of epoch 37: loss -25.73999 acc 0.65789 roc_auc 0.54154 prc_auc 0.68130[0m
[92maverage training of epoch 38: loss -26.27200 acc 0.66667 roc_auc 0.39450 prc_auc 0.60955[0m
[93maverage test of epoch 38: loss -26.58621 acc 0.65789 roc_auc 0.49077 prc_auc 0.65499[0m
[92maverage training of epoch 39: loss -27.13554 acc 0.66667 roc_auc 0.39930 prc_auc 0.61865[0m
[93maverage test of epoch 39: loss -27.46800 acc 0.65789 roc_auc 0.47231 prc_auc 0.64727[0m
[92maverage training of epoch 40: loss -28.02488 acc 0.66667 roc_auc 0.38300 prc_auc 0.60646[0m
[93maverage test of epoch 40: loss -28.34532 acc 0.65789 roc_auc 0.46000 prc_auc 0.64041[0m
[92maverage training of epoch 41: loss -28.92218 acc 0.66667 roc_auc 0.40340 prc_auc 0.62264[0m
[93maverage test of epoch 41: loss -29.24972 acc 0.65789 roc_auc 0.58923 prc_auc 0.70230[0m
[92maverage training of epoch 42: loss -29.82314 acc 0.66667 roc_auc 0.43300 prc_auc 0.63902[0m
[93maverage test of epoch 42: loss -30.16218 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -30.75029 acc 0.66667 roc_auc 0.44500 prc_auc 0.64349[0m
[93maverage test of epoch 43: loss -31.09292 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 44: loss -31.68451 acc 0.66667 roc_auc 0.46000 prc_auc 0.64944[0m
[93maverage test of epoch 44: loss -32.03154 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -32.62762 acc 0.66667 roc_auc 0.45000 prc_auc 0.64540[0m
[93maverage test of epoch 45: loss -32.97643 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -33.58828 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 46: loss -33.94407 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -34.56351 acc 0.66667 roc_auc 0.42000 prc_auc 0.63495[0m
[93maverage test of epoch 47: loss -34.91038 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -35.54893 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 48: loss -35.90596 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -36.54769 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 49: loss -36.90984 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 1
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 8, num_layers=2, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.81406 acc 0.33333 roc_auc 0.47220 prc_auc 0.64909[0m
[93maverage test of epoch 0: loss -1.09790 acc 0.39474 roc_auc 0.48308 prc_auc 0.68993[0m
[92maverage training of epoch 1: loss -1.35035 acc 0.32667 roc_auc 0.45920 prc_auc 0.62561[0m
[93maverage test of epoch 1: loss -1.63113 acc 0.34211 roc_auc 0.49846 prc_auc 0.73143[0m
[92maverage training of epoch 2: loss -1.96123 acc 0.33333 roc_auc 0.58960 prc_auc 0.72233[0m
[93maverage test of epoch 2: loss -2.30298 acc 0.34211 roc_auc 0.50769 prc_auc 0.72139[0m
[92maverage training of epoch 3: loss -2.63491 acc 0.33333 roc_auc 0.39600 prc_auc 0.60338[0m
[93maverage test of epoch 3: loss -2.98025 acc 0.34211 roc_auc 0.60308 prc_auc 0.75053[0m
[92maverage training of epoch 4: loss -3.24145 acc 0.33333 roc_auc 0.47120 prc_auc 0.65980[0m
[93maverage test of epoch 4: loss -3.50722 acc 0.34211 roc_auc 0.44923 prc_auc 0.63407[0m
[92maverage training of epoch 5: loss -3.79542 acc 0.33333 roc_auc 0.56300 prc_auc 0.68374[0m
[93maverage test of epoch 5: loss -4.06438 acc 0.34211 roc_auc 0.56000 prc_auc 0.71668[0m
[92maverage training of epoch 6: loss -4.31606 acc 0.33333 roc_auc 0.46340 prc_auc 0.65187[0m
[93maverage test of epoch 6: loss -4.60818 acc 0.34211 roc_auc 0.64308 prc_auc 0.74242[0m
[92maverage training of epoch 7: loss -4.84603 acc 0.33333 roc_auc 0.48700 prc_auc 0.68032[0m
[93maverage test of epoch 7: loss -5.15774 acc 0.34211 roc_auc 0.58154 prc_auc 0.79692[0m
[92maverage training of epoch 8: loss -5.42386 acc 0.33333 roc_auc 0.47700 prc_auc 0.64458[0m
[93maverage test of epoch 8: loss -5.74832 acc 0.34211 roc_auc 0.66154 prc_auc 0.80061[0m
[92maverage training of epoch 9: loss -6.10582 acc 0.33333 roc_auc 0.44720 prc_auc 0.62105[0m
[93maverage test of epoch 9: loss -6.52906 acc 0.34211 roc_auc 0.52615 prc_auc 0.66604[0m
[92maverage training of epoch 10: loss -6.98442 acc 0.44667 roc_auc 0.51120 prc_auc 0.70862[0m
[93maverage test of epoch 10: loss -7.47997 acc 0.65789 roc_auc 0.52308 prc_auc 0.68653[0m
[92maverage training of epoch 11: loss -7.97486 acc 0.66667 roc_auc 0.42700 prc_auc 0.61714[0m
[93maverage test of epoch 11: loss -8.47427 acc 0.65789 roc_auc 0.36308 prc_auc 0.60334[0m
[92maverage training of epoch 12: loss -8.90860 acc 0.66667 roc_auc 0.42580 prc_auc 0.61977[0m
[93maverage test of epoch 12: loss -9.37007 acc 0.65789 roc_auc 0.60923 prc_auc 0.75393[0m
[92maverage training of epoch 13: loss -9.71286 acc 0.66667 roc_auc 0.44220 prc_auc 0.61748[0m
[93maverage test of epoch 13: loss -10.09372 acc 0.65789 roc_auc 0.37538 prc_auc 0.59009[0m
[92maverage training of epoch 14: loss -10.41808 acc 0.66667 roc_auc 0.44260 prc_auc 0.61800[0m
[93maverage test of epoch 14: loss -10.77817 acc 0.65789 roc_auc 0.51385 prc_auc 0.73393[0m
[92maverage training of epoch 15: loss -11.10322 acc 0.66667 roc_auc 0.44240 prc_auc 0.61601[0m
[93maverage test of epoch 15: loss -11.45956 acc 0.65789 roc_auc 0.43077 prc_auc 0.66076[0m
[92maverage training of epoch 16: loss -11.77195 acc 0.66667 roc_auc 0.45720 prc_auc 0.63405[0m
[93maverage test of epoch 16: loss -12.09101 acc 0.65789 roc_auc 0.52615 prc_auc 0.75650[0m
[92maverage training of epoch 17: loss -12.40727 acc 0.66667 roc_auc 0.44700 prc_auc 0.63066[0m
[93maverage test of epoch 17: loss -12.70078 acc 0.65789 roc_auc 0.57231 prc_auc 0.71593[0m
[92maverage training of epoch 18: loss -13.03571 acc 0.66667 roc_auc 0.46320 prc_auc 0.64613[0m
[93maverage test of epoch 18: loss -13.37028 acc 0.65789 roc_auc 0.68923 prc_auc 0.81119[0m
[92maverage training of epoch 19: loss -13.68685 acc 0.66667 roc_auc 0.46280 prc_auc 0.65583[0m
[93maverage test of epoch 19: loss -14.00602 acc 0.65789 roc_auc 0.55385 prc_auc 0.76915[0m
[92maverage training of epoch 20: loss -14.34655 acc 0.66667 roc_auc 0.47640 prc_auc 0.66819[0m
[93maverage test of epoch 20: loss -14.64947 acc 0.65789 roc_auc 0.39846 prc_auc 0.63342[0m
[92maverage training of epoch 21: loss -14.99116 acc 0.66667 roc_auc 0.45120 prc_auc 0.64439[0m
[93maverage test of epoch 21: loss -15.32181 acc 0.65789 roc_auc 0.49846 prc_auc 0.66393[0m
[92maverage training of epoch 22: loss -15.64562 acc 0.66667 roc_auc 0.45380 prc_auc 0.64713[0m
[93maverage test of epoch 22: loss -15.98977 acc 0.65789 roc_auc 0.49231 prc_auc 0.66795[0m
[92maverage training of epoch 23: loss -16.31914 acc 0.66667 roc_auc 0.45460 prc_auc 0.64191[0m
[93maverage test of epoch 23: loss -16.64079 acc 0.65789 roc_auc 0.56615 prc_auc 0.76249[0m
[92maverage training of epoch 24: loss -16.99385 acc 0.66667 roc_auc 0.44690 prc_auc 0.63331[0m
[93maverage test of epoch 24: loss -17.31337 acc 0.65789 roc_auc 0.45846 prc_auc 0.65395[0m
[92maverage training of epoch 25: loss -17.67588 acc 0.66667 roc_auc 0.46400 prc_auc 0.65191[0m
[93maverage test of epoch 25: loss -18.01126 acc 0.65789 roc_auc 0.44923 prc_auc 0.64863[0m
[92maverage training of epoch 26: loss -18.36509 acc 0.66667 roc_auc 0.44180 prc_auc 0.62383[0m
[93maverage test of epoch 26: loss -18.69089 acc 0.65789 roc_auc 0.57231 prc_auc 0.74861[0m
[92maverage training of epoch 27: loss -19.07180 acc 0.66667 roc_auc 0.47000 prc_auc 0.65850[0m
[93maverage test of epoch 27: loss -19.39262 acc 0.65789 roc_auc 0.48923 prc_auc 0.67981[0m
[92maverage training of epoch 28: loss -19.78745 acc 0.66667 roc_auc 0.46720 prc_auc 0.64380[0m
[93maverage test of epoch 28: loss -20.13721 acc 0.65789 roc_auc 0.45846 prc_auc 0.67256[0m
[92maverage training of epoch 29: loss -20.51289 acc 0.66667 roc_auc 0.46640 prc_auc 0.64002[0m
[93maverage test of epoch 29: loss -20.85049 acc 0.65789 roc_auc 0.49692 prc_auc 0.69304[0m
[92maverage training of epoch 30: loss -21.23786 acc 0.66667 roc_auc 0.45780 prc_auc 0.64808[0m
[93maverage test of epoch 30: loss -21.58020 acc 0.65789 roc_auc 0.39385 prc_auc 0.57595[0m
[92maverage training of epoch 31: loss -21.98876 acc 0.66667 roc_auc 0.46460 prc_auc 0.64405[0m
[93maverage test of epoch 31: loss -22.33296 acc 0.65789 roc_auc 0.67385 prc_auc 0.80002[0m
[92maverage training of epoch 32: loss -22.75091 acc 0.66667 roc_auc 0.45600 prc_auc 0.64123[0m
[93maverage test of epoch 32: loss -23.11344 acc 0.65789 roc_auc 0.39692 prc_auc 0.65577[0m
[92maverage training of epoch 33: loss -23.51654 acc 0.66667 roc_auc 0.46500 prc_auc 0.63725[0m
[93maverage test of epoch 33: loss -23.87662 acc 0.65789 roc_auc 0.50000 prc_auc 0.69977[0m
[92maverage training of epoch 34: loss -24.30066 acc 0.66667 roc_auc 0.44900 prc_auc 0.63507[0m
[93maverage test of epoch 34: loss -24.65389 acc 0.65789 roc_auc 0.56923 prc_auc 0.75932[0m
[92maverage training of epoch 35: loss -25.08809 acc 0.66667 roc_auc 0.45760 prc_auc 0.64497[0m
[93maverage test of epoch 35: loss -25.45109 acc 0.65789 roc_auc 0.73846 prc_auc 0.81633[0m
[92maverage training of epoch 36: loss -25.88932 acc 0.66667 roc_auc 0.45860 prc_auc 0.64284[0m
[93maverage test of epoch 36: loss -26.27053 acc 0.65789 roc_auc 0.60308 prc_auc 0.76439[0m
[92maverage training of epoch 37: loss -26.70941 acc 0.66667 roc_auc 0.45920 prc_auc 0.64197[0m
[93maverage test of epoch 37: loss -27.08484 acc 0.65789 roc_auc 0.36769 prc_auc 0.61094[0m
[92maverage training of epoch 38: loss -27.53325 acc 0.66667 roc_auc 0.45980 prc_auc 0.63701[0m
[93maverage test of epoch 38: loss -27.90818 acc 0.65789 roc_auc 0.51846 prc_auc 0.67937[0m
[92maverage training of epoch 39: loss -28.37441 acc 0.66667 roc_auc 0.45440 prc_auc 0.63168[0m
[93maverage test of epoch 39: loss -28.76412 acc 0.65789 roc_auc 0.67846 prc_auc 0.77431[0m
[92maverage training of epoch 40: loss -29.22505 acc 0.66667 roc_auc 0.44260 prc_auc 0.63631[0m
[93maverage test of epoch 40: loss -29.61537 acc 0.65789 roc_auc 0.37385 prc_auc 0.58540[0m
[92maverage training of epoch 41: loss -30.08465 acc 0.66667 roc_auc 0.45510 prc_auc 0.63179[0m
[93maverage test of epoch 41: loss -30.47968 acc 0.65789 roc_auc 0.49077 prc_auc 0.63875[0m
[92maverage training of epoch 42: loss -30.96194 acc 0.66667 roc_auc 0.46350 prc_auc 0.65025[0m
[93maverage test of epoch 42: loss -31.35286 acc 0.65789 roc_auc 0.64923 prc_auc 0.80625[0m
[92maverage training of epoch 43: loss -31.84829 acc 0.66667 roc_auc 0.45430 prc_auc 0.63212[0m
[93maverage test of epoch 43: loss -32.24347 acc 0.65789 roc_auc 0.73692 prc_auc 0.80733[0m
[92maverage training of epoch 44: loss -32.74421 acc 0.66667 roc_auc 0.45250 prc_auc 0.63212[0m
[93maverage test of epoch 44: loss -33.14522 acc 0.65789 roc_auc 0.44308 prc_auc 0.63341[0m
[92maverage training of epoch 45: loss -33.64804 acc 0.66667 roc_auc 0.45970 prc_auc 0.63804[0m
[93maverage test of epoch 45: loss -34.05800 acc 0.65789 roc_auc 0.61692 prc_auc 0.70941[0m
[92maverage training of epoch 46: loss -34.56864 acc 0.66667 roc_auc 0.46080 prc_auc 0.63790[0m
[93maverage test of epoch 46: loss -34.97504 acc 0.65789 roc_auc 0.52000 prc_auc 0.67561[0m
[92maverage training of epoch 47: loss -35.49404 acc 0.66667 roc_auc 0.45680 prc_auc 0.63799[0m
[93maverage test of epoch 47: loss -35.90875 acc 0.65789 roc_auc 0.56462 prc_auc 0.70526[0m
[92maverage training of epoch 48: loss -36.43663 acc 0.66667 roc_auc 0.46650 prc_auc 0.65654[0m
[93maverage test of epoch 48: loss -36.85624 acc 0.65789 roc_auc 0.47385 prc_auc 0.62407[0m
[92maverage training of epoch 49: loss -37.38973 acc 0.66667 roc_auc 0.45000 prc_auc 0.62832[0m
[93maverage test of epoch 49: loss -37.80053 acc 0.65789 roc_auc 0.56462 prc_auc 0.69239[0m
Training model with dataset, testing using fold 2
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 8, num_layers=2, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.29668 acc 0.66667 roc_auc 0.55280 prc_auc 0.71647[0m
[93maverage test of epoch 0: loss -0.24343 acc 0.65789 roc_auc 0.68308 prc_auc 0.76831[0m
[92maverage training of epoch 1: loss -0.72283 acc 0.66667 roc_auc 0.50700 prc_auc 0.67112[0m
[93maverage test of epoch 1: loss -1.24336 acc 0.65789 roc_auc 0.67692 prc_auc 0.83762[0m
[92maverage training of epoch 2: loss -1.67179 acc 0.66667 roc_auc 0.39060 prc_auc 0.60041[0m
[93maverage test of epoch 2: loss -2.10677 acc 0.65789 roc_auc 0.41231 prc_auc 0.62835[0m
[92maverage training of epoch 3: loss -2.49477 acc 0.66667 roc_auc 0.44580 prc_auc 0.63359[0m
[93maverage test of epoch 3: loss -2.88153 acc 0.65789 roc_auc 0.78154 prc_auc 0.85805[0m
[92maverage training of epoch 4: loss -3.26788 acc 0.66667 roc_auc 0.43520 prc_auc 0.61062[0m
[93maverage test of epoch 4: loss -3.63962 acc 0.65789 roc_auc 0.54769 prc_auc 0.72071[0m
[92maverage training of epoch 5: loss -4.10531 acc 0.66667 roc_auc 0.47660 prc_auc 0.64979[0m
[93maverage test of epoch 5: loss -4.52398 acc 0.65789 roc_auc 0.50462 prc_auc 0.71512[0m
[92maverage training of epoch 6: loss -5.01382 acc 0.66667 roc_auc 0.42680 prc_auc 0.61162[0m
[93maverage test of epoch 6: loss -5.56152 acc 0.65789 roc_auc 0.44308 prc_auc 0.65453[0m
[92maverage training of epoch 7: loss -6.06001 acc 0.66667 roc_auc 0.41120 prc_auc 0.60882[0m
[93maverage test of epoch 7: loss -6.72896 acc 0.65789 roc_auc 0.62769 prc_auc 0.80427[0m
[92maverage training of epoch 8: loss -7.27160 acc 0.66667 roc_auc 0.49660 prc_auc 0.66276[0m
[93maverage test of epoch 8: loss -7.83130 acc 0.65789 roc_auc 0.58462 prc_auc 0.69122[0m
[92maverage training of epoch 9: loss -8.28061 acc 0.66667 roc_auc 0.50920 prc_auc 0.67161[0m
[93maverage test of epoch 9: loss -8.68017 acc 0.65789 roc_auc 0.60308 prc_auc 0.76367[0m
[92maverage training of epoch 10: loss -9.11930 acc 0.66667 roc_auc 0.41180 prc_auc 0.60861[0m
[93maverage test of epoch 10: loss -9.49629 acc 0.65789 roc_auc 0.43692 prc_auc 0.69327[0m
[92maverage training of epoch 11: loss -9.92323 acc 0.66667 roc_auc 0.49140 prc_auc 0.65767[0m
[93maverage test of epoch 11: loss -10.28802 acc 0.65789 roc_auc 0.62462 prc_auc 0.76443[0m
[92maverage training of epoch 12: loss -10.64063 acc 0.66667 roc_auc 0.41000 prc_auc 0.63764[0m
[93maverage test of epoch 12: loss -11.02930 acc 0.65789 roc_auc 0.54462 prc_auc 0.73457[0m
[92maverage training of epoch 13: loss -11.37408 acc 0.66667 roc_auc 0.46080 prc_auc 0.68224[0m
[93maverage test of epoch 13: loss -11.75601 acc 0.65789 roc_auc 0.44615 prc_auc 0.61632[0m
[92maverage training of epoch 14: loss -12.11531 acc 0.66667 roc_auc 0.42200 prc_auc 0.65070[0m
[93maverage test of epoch 14: loss -12.44598 acc 0.65789 roc_auc 0.54462 prc_auc 0.71707[0m
[92maverage training of epoch 15: loss -12.82800 acc 0.66667 roc_auc 0.45660 prc_auc 0.65717[0m
[93maverage test of epoch 15: loss -13.13903 acc 0.65789 roc_auc 0.61538 prc_auc 0.76714[0m
[92maverage training of epoch 16: loss -13.52859 acc 0.66667 roc_auc 0.41120 prc_auc 0.61459[0m
[93maverage test of epoch 16: loss -13.83511 acc 0.65789 roc_auc 0.45538 prc_auc 0.69594[0m
[92maverage training of epoch 17: loss -14.25785 acc 0.66667 roc_auc 0.39820 prc_auc 0.59513[0m
[93maverage test of epoch 17: loss -14.59144 acc 0.65789 roc_auc 0.55692 prc_auc 0.73097[0m
[92maverage training of epoch 18: loss -14.92583 acc 0.66667 roc_auc 0.43180 prc_auc 0.63817[0m
[93maverage test of epoch 18: loss -15.28803 acc 0.65789 roc_auc 0.60000 prc_auc 0.69140[0m
[92maverage training of epoch 19: loss -15.65813 acc 0.66667 roc_auc 0.42940 prc_auc 0.65337[0m
[93maverage test of epoch 19: loss -15.95274 acc 0.65789 roc_auc 0.30154 prc_auc 0.56025[0m
[92maverage training of epoch 20: loss -16.37189 acc 0.66667 roc_auc 0.41260 prc_auc 0.62749[0m
[93maverage test of epoch 20: loss -16.71769 acc 0.65789 roc_auc 0.48615 prc_auc 0.69551[0m
[92maverage training of epoch 21: loss -17.07091 acc 0.66667 roc_auc 0.43450 prc_auc 0.65864[0m
[93maverage test of epoch 21: loss -17.42569 acc 0.65789 roc_auc 0.49231 prc_auc 0.66752[0m
[92maverage training of epoch 22: loss -17.81496 acc 0.66667 roc_auc 0.46020 prc_auc 0.64129[0m
[93maverage test of epoch 22: loss -18.14276 acc 0.65789 roc_auc 0.31231 prc_auc 0.56428[0m
[92maverage training of epoch 23: loss -18.54121 acc 0.66667 roc_auc 0.44000 prc_auc 0.63372[0m
[93maverage test of epoch 23: loss -18.87010 acc 0.65789 roc_auc 0.43692 prc_auc 0.63979[0m
[92maverage training of epoch 24: loss -19.29239 acc 0.66667 roc_auc 0.43900 prc_auc 0.64170[0m
[93maverage test of epoch 24: loss -19.60285 acc 0.65789 roc_auc 0.47538 prc_auc 0.64569[0m
[92maverage training of epoch 25: loss -20.02675 acc 0.66667 roc_auc 0.43070 prc_auc 0.63761[0m
[93maverage test of epoch 25: loss -20.33936 acc 0.65789 roc_auc 0.54462 prc_auc 0.67578[0m
[92maverage training of epoch 26: loss -20.76910 acc 0.66667 roc_auc 0.42010 prc_auc 0.62960[0m
[93maverage test of epoch 26: loss -21.10171 acc 0.65789 roc_auc 0.59231 prc_auc 0.72681[0m
[92maverage training of epoch 27: loss -21.53077 acc 0.66667 roc_auc 0.44050 prc_auc 0.63405[0m
[93maverage test of epoch 27: loss -21.85692 acc 0.65789 roc_auc 0.32615 prc_auc 0.60566[0m
[92maverage training of epoch 28: loss -22.30075 acc 0.66667 roc_auc 0.44160 prc_auc 0.64597[0m
[93maverage test of epoch 28: loss -22.64080 acc 0.65789 roc_auc 0.41231 prc_auc 0.58724[0m
[92maverage training of epoch 29: loss -23.06770 acc 0.66667 roc_auc 0.42370 prc_auc 0.62682[0m
[93maverage test of epoch 29: loss -23.40087 acc 0.65789 roc_auc 0.55538 prc_auc 0.70598[0m
[92maverage training of epoch 30: loss -23.84594 acc 0.66667 roc_auc 0.45120 prc_auc 0.64421[0m
[93maverage test of epoch 30: loss -24.18140 acc 0.65789 roc_auc 0.51231 prc_auc 0.68239[0m
[92maverage training of epoch 31: loss -24.63908 acc 0.66667 roc_auc 0.43440 prc_auc 0.63662[0m
[93maverage test of epoch 31: loss -24.97585 acc 0.65789 roc_auc 0.46308 prc_auc 0.65988[0m
[92maverage training of epoch 32: loss -25.43785 acc 0.66667 roc_auc 0.43120 prc_auc 0.63287[0m
[93maverage test of epoch 32: loss -25.75299 acc 0.65789 roc_auc 0.41077 prc_auc 0.61938[0m
[92maverage training of epoch 33: loss -26.23685 acc 0.66667 roc_auc 0.43730 prc_auc 0.64930[0m
[93maverage test of epoch 33: loss -26.58261 acc 0.65789 roc_auc 0.26769 prc_auc 0.54419[0m
[92maverage training of epoch 34: loss -27.05253 acc 0.66667 roc_auc 0.42040 prc_auc 0.62279[0m
[93maverage test of epoch 34: loss -27.40030 acc 0.65789 roc_auc 0.61538 prc_auc 0.70663[0m
[92maverage training of epoch 35: loss -27.88077 acc 0.66667 roc_auc 0.44260 prc_auc 0.63372[0m
[93maverage test of epoch 35: loss -28.20788 acc 0.65789 roc_auc 0.42769 prc_auc 0.62533[0m
[92maverage training of epoch 36: loss -28.71402 acc 0.66667 roc_auc 0.42410 prc_auc 0.62960[0m
[93maverage test of epoch 36: loss -29.04670 acc 0.65789 roc_auc 0.64615 prc_auc 0.81644[0m
[92maverage training of epoch 37: loss -29.55487 acc 0.66667 roc_auc 0.43590 prc_auc 0.64201[0m
[93maverage test of epoch 37: loss -29.89523 acc 0.65789 roc_auc 0.55231 prc_auc 0.70454[0m
[92maverage training of epoch 38: loss -30.41488 acc 0.66667 roc_auc 0.43540 prc_auc 0.62935[0m
[93maverage test of epoch 38: loss -30.76525 acc 0.65789 roc_auc 0.58308 prc_auc 0.72649[0m
[92maverage training of epoch 39: loss -31.28271 acc 0.66667 roc_auc 0.43320 prc_auc 0.63465[0m
[93maverage test of epoch 39: loss -31.62764 acc 0.65789 roc_auc 0.27846 prc_auc 0.55932[0m
[92maverage training of epoch 40: loss -32.15585 acc 0.66667 roc_auc 0.43690 prc_auc 0.63079[0m
[93maverage test of epoch 40: loss -32.51487 acc 0.65789 roc_auc 0.54154 prc_auc 0.67764[0m
[92maverage training of epoch 41: loss -33.04173 acc 0.66667 roc_auc 0.42390 prc_auc 0.61572[0m
[93maverage test of epoch 41: loss -33.39944 acc 0.65789 roc_auc 0.61538 prc_auc 0.72781[0m
[92maverage training of epoch 42: loss -33.94248 acc 0.66667 roc_auc 0.45180 prc_auc 0.64184[0m
[93maverage test of epoch 42: loss -34.29057 acc 0.65789 roc_auc 0.40923 prc_auc 0.61903[0m
[92maverage training of epoch 43: loss -34.84923 acc 0.66667 roc_auc 0.43320 prc_auc 0.62438[0m
[93maverage test of epoch 43: loss -35.20332 acc 0.65789 roc_auc 0.49385 prc_auc 0.65540[0m
[92maverage training of epoch 44: loss -35.76958 acc 0.66667 roc_auc 0.43860 prc_auc 0.63302[0m
[93maverage test of epoch 44: loss -36.11501 acc 0.65789 roc_auc 0.54000 prc_auc 0.69118[0m
[92maverage training of epoch 45: loss -36.69440 acc 0.66667 roc_auc 0.43680 prc_auc 0.62730[0m
[93maverage test of epoch 45: loss -37.04966 acc 0.65789 roc_auc 0.54000 prc_auc 0.67641[0m
[92maverage training of epoch 46: loss -37.63171 acc 0.66667 roc_auc 0.42300 prc_auc 0.62098[0m
[93maverage test of epoch 46: loss -37.99285 acc 0.65789 roc_auc 0.57692 prc_auc 0.69444[0m
[92maverage training of epoch 47: loss -38.58253 acc 0.66667 roc_auc 0.40470 prc_auc 0.61648[0m
[93maverage test of epoch 47: loss -38.94774 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -39.54030 acc 0.66667 roc_auc 0.43390 prc_auc 0.63335[0m
[93maverage test of epoch 48: loss -39.89733 acc 0.65789 roc_auc 0.52308 prc_auc 0.66917[0m
[92maverage training of epoch 49: loss -40.51116 acc 0.66667 roc_auc 0.45040 prc_auc 0.64621[0m
[93maverage test of epoch 49: loss -40.88012 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 3
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 8, num_layers=2, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.13487 acc 0.66225 roc_auc 0.40176 prc_auc 0.60965[0m
[93maverage test of epoch 0: loss -0.03001 acc 0.67568 roc_auc 0.54667 prc_auc 0.75502[0m
[92maverage training of epoch 1: loss -0.11763 acc 0.66225 roc_auc 0.42627 prc_auc 0.60334[0m
[93maverage test of epoch 1: loss -0.26640 acc 0.67568 roc_auc 0.55667 prc_auc 0.75163[0m
[92maverage training of epoch 2: loss -0.31958 acc 0.66225 roc_auc 0.45000 prc_auc 0.63532[0m
[93maverage test of epoch 2: loss -0.40172 acc 0.67568 roc_auc 0.30000 prc_auc 0.60615[0m
[92maverage training of epoch 3: loss -0.45848 acc 0.66225 roc_auc 0.47647 prc_auc 0.65727[0m
[93maverage test of epoch 3: loss -0.55155 acc 0.67568 roc_auc 0.51667 prc_auc 0.67982[0m
[92maverage training of epoch 4: loss -0.59785 acc 0.66225 roc_auc 0.43627 prc_auc 0.63640[0m
[93maverage test of epoch 4: loss -0.70324 acc 0.67568 roc_auc 0.53667 prc_auc 0.69971[0m
[92maverage training of epoch 5: loss -0.73519 acc 0.66225 roc_auc 0.39784 prc_auc 0.59816[0m
[93maverage test of epoch 5: loss -0.85298 acc 0.67568 roc_auc 0.51333 prc_auc 0.71127[0m
[92maverage training of epoch 6: loss -0.89352 acc 0.66225 roc_auc 0.45392 prc_auc 0.64009[0m
[93maverage test of epoch 6: loss -0.99854 acc 0.67568 roc_auc 0.56667 prc_auc 0.75774[0m
[92maverage training of epoch 7: loss -1.04864 acc 0.66225 roc_auc 0.46529 prc_auc 0.62524[0m
[93maverage test of epoch 7: loss -1.16592 acc 0.67568 roc_auc 0.44667 prc_auc 0.70245[0m
[92maverage training of epoch 8: loss -1.20997 acc 0.66225 roc_auc 0.46490 prc_auc 0.66055[0m
[93maverage test of epoch 8: loss -1.32749 acc 0.67568 roc_auc 0.47000 prc_auc 0.69167[0m
[92maverage training of epoch 9: loss -1.38137 acc 0.66225 roc_auc 0.45529 prc_auc 0.65099[0m
[93maverage test of epoch 9: loss -1.51094 acc 0.67568 roc_auc 0.58000 prc_auc 0.77644[0m
[92maverage training of epoch 10: loss -1.55667 acc 0.66225 roc_auc 0.50549 prc_auc 0.69440[0m
[93maverage test of epoch 10: loss -1.68128 acc 0.67568 roc_auc 0.47667 prc_auc 0.65890[0m
[92maverage training of epoch 11: loss -1.74258 acc 0.66225 roc_auc 0.51824 prc_auc 0.71969[0m
[93maverage test of epoch 11: loss -1.86396 acc 0.67568 roc_auc 0.35667 prc_auc 0.59221[0m
[92maverage training of epoch 12: loss -1.93374 acc 0.66225 roc_auc 0.48412 prc_auc 0.66530[0m
[93maverage test of epoch 12: loss -2.08887 acc 0.67568 roc_auc 0.64667 prc_auc 0.78241[0m
[92maverage training of epoch 13: loss -2.12947 acc 0.66225 roc_auc 0.52686 prc_auc 0.71912[0m
[93maverage test of epoch 13: loss -2.29209 acc 0.67568 roc_auc 0.65000 prc_auc 0.82233[0m
[92maverage training of epoch 14: loss -2.33584 acc 0.66225 roc_auc 0.49529 prc_auc 0.66324[0m
[93maverage test of epoch 14: loss -2.49062 acc 0.67568 roc_auc 0.56000 prc_auc 0.75243[0m
[92maverage training of epoch 15: loss -2.54099 acc 0.66225 roc_auc 0.48078 prc_auc 0.64350[0m
[93maverage test of epoch 15: loss -2.70938 acc 0.67568 roc_auc 0.63000 prc_auc 0.79573[0m
[92maverage training of epoch 16: loss -2.74737 acc 0.66225 roc_auc 0.48892 prc_auc 0.70788[0m
[93maverage test of epoch 16: loss -2.90904 acc 0.67568 roc_auc 0.35333 prc_auc 0.62510[0m
[92maverage training of epoch 17: loss -2.97366 acc 0.66225 roc_auc 0.47843 prc_auc 0.65824[0m
[93maverage test of epoch 17: loss -3.12582 acc 0.67568 roc_auc 0.59333 prc_auc 0.69213[0m
[92maverage training of epoch 18: loss -3.19057 acc 0.66225 roc_auc 0.50373 prc_auc 0.65331[0m
[93maverage test of epoch 18: loss -3.37763 acc 0.67568 roc_auc 0.66667 prc_auc 0.82714[0m
[92maverage training of epoch 19: loss -3.43123 acc 0.66225 roc_auc 0.47745 prc_auc 0.69489[0m
[93maverage test of epoch 19: loss -3.61303 acc 0.67568 roc_auc 0.49667 prc_auc 0.69363[0m
[92maverage training of epoch 20: loss -3.67549 acc 0.66225 roc_auc 0.49804 prc_auc 0.63851[0m
[93maverage test of epoch 20: loss -3.86563 acc 0.67568 roc_auc 0.65333 prc_auc 0.81923[0m
[92maverage training of epoch 21: loss -3.93737 acc 0.66225 roc_auc 0.45137 prc_auc 0.65066[0m
[93maverage test of epoch 21: loss -4.13338 acc 0.67568 roc_auc 0.43000 prc_auc 0.63710[0m
[92maverage training of epoch 22: loss -4.21856 acc 0.66225 roc_auc 0.51745 prc_auc 0.66024[0m
[93maverage test of epoch 22: loss -4.43616 acc 0.67568 roc_auc 0.67667 prc_auc 0.83669[0m
[92maverage training of epoch 23: loss -4.50706 acc 0.66225 roc_auc 0.45814 prc_auc 0.62243[0m
[93maverage test of epoch 23: loss -4.72810 acc 0.67568 roc_auc 0.68667 prc_auc 0.81654[0m
[92maverage training of epoch 24: loss -4.80920 acc 0.66225 roc_auc 0.45069 prc_auc 0.67011[0m
[93maverage test of epoch 24: loss -5.02643 acc 0.67568 roc_auc 0.49667 prc_auc 0.67071[0m
[92maverage training of epoch 25: loss -5.11905 acc 0.66225 roc_auc 0.40941 prc_auc 0.60341[0m
[93maverage test of epoch 25: loss -5.36233 acc 0.67568 roc_auc 0.68333 prc_auc 0.79471[0m
[92maverage training of epoch 26: loss -5.44580 acc 0.66225 roc_auc 0.47765 prc_auc 0.64582[0m
[93maverage test of epoch 26: loss -5.69674 acc 0.67568 roc_auc 0.59000 prc_auc 0.74977[0m
[92maverage training of epoch 27: loss -5.76381 acc 0.66225 roc_auc 0.44549 prc_auc 0.63809[0m
[93maverage test of epoch 27: loss -6.02672 acc 0.67568 roc_auc 0.52667 prc_auc 0.78181[0m
[92maverage training of epoch 28: loss -6.12530 acc 0.66225 roc_auc 0.46186 prc_auc 0.65792[0m
[93maverage test of epoch 28: loss -6.39056 acc 0.67568 roc_auc 0.51000 prc_auc 0.70306[0m
[92maverage training of epoch 29: loss -6.48171 acc 0.66225 roc_auc 0.48824 prc_auc 0.67006[0m
[93maverage test of epoch 29: loss -6.77784 acc 0.67568 roc_auc 0.52000 prc_auc 0.66772[0m
[92maverage training of epoch 30: loss -6.86414 acc 0.66225 roc_auc 0.46225 prc_auc 0.65148[0m
[93maverage test of epoch 30: loss -7.15328 acc 0.67568 roc_auc 0.39667 prc_auc 0.65346[0m
[92maverage training of epoch 31: loss -7.26052 acc 0.66225 roc_auc 0.49039 prc_auc 0.67561[0m
[93maverage test of epoch 31: loss -7.55476 acc 0.67568 roc_auc 0.58167 prc_auc 0.76833[0m
[92maverage training of epoch 32: loss -7.65903 acc 0.66225 roc_auc 0.46078 prc_auc 0.64539[0m
[93maverage test of epoch 32: loss -7.97190 acc 0.67568 roc_auc 0.61500 prc_auc 0.78108[0m
[92maverage training of epoch 33: loss -8.06008 acc 0.66225 roc_auc 0.46382 prc_auc 0.63333[0m
[93maverage test of epoch 33: loss -8.38470 acc 0.67568 roc_auc 0.66000 prc_auc 0.84107[0m
[92maverage training of epoch 34: loss -8.47106 acc 0.66225 roc_auc 0.45588 prc_auc 0.63574[0m
[93maverage test of epoch 34: loss -8.77933 acc 0.67568 roc_auc 0.52333 prc_auc 0.78754[0m
[92maverage training of epoch 35: loss -8.88310 acc 0.66225 roc_auc 0.50549 prc_auc 0.66491[0m
[93maverage test of epoch 35: loss -9.20548 acc 0.67568 roc_auc 0.54833 prc_auc 0.71844[0m
[92maverage training of epoch 36: loss -9.29213 acc 0.66225 roc_auc 0.47500 prc_auc 0.66605[0m
[93maverage test of epoch 36: loss -9.61398 acc 0.67568 roc_auc 0.55667 prc_auc 0.73573[0m
[92maverage training of epoch 37: loss -9.71011 acc 0.66225 roc_auc 0.43922 prc_auc 0.61882[0m
[93maverage test of epoch 37: loss -10.04055 acc 0.67568 roc_auc 0.56000 prc_auc 0.69636[0m
[92maverage training of epoch 38: loss -10.12971 acc 0.66225 roc_auc 0.42824 prc_auc 0.60450[0m
[93maverage test of epoch 38: loss -10.47368 acc 0.67568 roc_auc 0.54333 prc_auc 0.72277[0m
[92maverage training of epoch 39: loss -10.56490 acc 0.66225 roc_auc 0.47235 prc_auc 0.66531[0m
[93maverage test of epoch 39: loss -10.89983 acc 0.67568 roc_auc 0.26667 prc_auc 0.55882[0m
[92maverage training of epoch 40: loss -11.00582 acc 0.66225 roc_auc 0.49029 prc_auc 0.65263[0m
[93maverage test of epoch 40: loss -11.33721 acc 0.67568 roc_auc 0.42167 prc_auc 0.67560[0m
[92maverage training of epoch 41: loss -11.44495 acc 0.66225 roc_auc 0.43284 prc_auc 0.61875[0m
[93maverage test of epoch 41: loss -11.79784 acc 0.67568 roc_auc 0.44000 prc_auc 0.67234[0m
[92maverage training of epoch 42: loss -11.89060 acc 0.66225 roc_auc 0.35627 prc_auc 0.56690[0m
[93maverage test of epoch 42: loss -12.25889 acc 0.67568 roc_auc 0.45000 prc_auc 0.63110[0m
[92maverage training of epoch 43: loss -12.36469 acc 0.66225 roc_auc 0.48265 prc_auc 0.64787[0m
[93maverage test of epoch 43: loss -12.71836 acc 0.67568 roc_auc 0.54833 prc_auc 0.72995[0m
[92maverage training of epoch 44: loss -12.82147 acc 0.66225 roc_auc 0.43500 prc_auc 0.61563[0m
[93maverage test of epoch 44: loss -13.20535 acc 0.67568 roc_auc 0.63000 prc_auc 0.79334[0m
[92maverage training of epoch 45: loss -13.29300 acc 0.66225 roc_auc 0.39500 prc_auc 0.59929[0m
[93maverage test of epoch 45: loss -13.67114 acc 0.67568 roc_auc 0.54167 prc_auc 0.73703[0m
[92maverage training of epoch 46: loss -13.77297 acc 0.66225 roc_auc 0.41275 prc_auc 0.59322[0m
[93maverage test of epoch 46: loss -14.16528 acc 0.67568 roc_auc 0.66500 prc_auc 0.81292[0m
[92maverage training of epoch 47: loss -14.26498 acc 0.66225 roc_auc 0.40314 prc_auc 0.59915[0m
[93maverage test of epoch 47: loss -14.64597 acc 0.67568 roc_auc 0.30833 prc_auc 0.57947[0m
[92maverage training of epoch 48: loss -14.75006 acc 0.66225 roc_auc 0.39088 prc_auc 0.60029[0m
[93maverage test of epoch 48: loss -15.15382 acc 0.67568 roc_auc 0.36167 prc_auc 0.62506[0m
[92maverage training of epoch 49: loss -15.26356 acc 0.66225 roc_auc 0.46431 prc_auc 0.62492[0m
[93maverage test of epoch 49: loss -15.67196 acc 0.67568 roc_auc 0.49833 prc_auc 0.66475[0m
Training model with dataset, testing using fold 4
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 8, num_layers=2, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.32686 acc 0.64238 roc_auc 0.52176 prc_auc 0.67803[0m
[93maverage test of epoch 0: loss -0.46794 acc 0.35135 roc_auc 0.44667 prc_auc 0.68659[0m
[92maverage training of epoch 1: loss -0.67605 acc 0.33775 roc_auc 0.52196 prc_auc 0.68259[0m
[93maverage test of epoch 1: loss -0.85367 acc 0.32432 roc_auc 0.49000 prc_auc 0.68385[0m
[92maverage training of epoch 2: loss -1.03302 acc 0.33775 roc_auc 0.60137 prc_auc 0.73504[0m
[93maverage test of epoch 2: loss -1.18518 acc 0.32432 roc_auc 0.52667 prc_auc 0.77216[0m
[92maverage training of epoch 3: loss -1.33524 acc 0.33775 roc_auc 0.46922 prc_auc 0.62837[0m
[93maverage test of epoch 3: loss -1.49618 acc 0.32432 roc_auc 0.43000 prc_auc 0.69323[0m
[92maverage training of epoch 4: loss -2.04691 acc 0.41060 roc_auc 0.43451 prc_auc 0.62054[0mUsing backend: pytorch

[93maverage test of epoch 4: loss -2.78682 acc 0.67568 roc_auc 0.40333 prc_auc 0.65427[0m
[92maverage training of epoch 5: loss -3.74224 acc 0.66225 roc_auc 0.47196 prc_auc 0.66288[0m
[93maverage test of epoch 5: loss -4.56055 acc 0.67568 roc_auc 0.51333 prc_auc 0.66856[0m
[92maverage training of epoch 6: loss -5.33684 acc 0.66225 roc_auc 0.46431 prc_auc 0.64674[0m
[93maverage test of epoch 6: loss -6.07855 acc 0.67568 roc_auc 0.57667 prc_auc 0.71597[0m
[92maverage training of epoch 7: loss -6.64665 acc 0.66225 roc_auc 0.46765 prc_auc 0.63580[0m
[93maverage test of epoch 7: loss -7.22841 acc 0.67568 roc_auc 0.64000 prc_auc 0.78951[0m
[92maverage training of epoch 8: loss -7.67733 acc 0.66225 roc_auc 0.48157 prc_auc 0.65410[0m
[93maverage test of epoch 8: loss -8.16137 acc 0.67568 roc_auc 0.43333 prc_auc 0.70431[0m
[92maverage training of epoch 9: loss -8.58882 acc 0.66225 roc_auc 0.48431 prc_auc 0.64949[0m
[93maverage test of epoch 9: loss -9.06573 acc 0.67568 roc_auc 0.49333 prc_auc 0.73056[0m
[92maverage training of epoch 10: loss -9.44459 acc 0.66225 roc_auc 0.50686 prc_auc 0.69905[0m
[93maverage test of epoch 10: loss -9.84474 acc 0.67568 roc_auc 0.41000 prc_auc 0.64473[0m
[92maverage training of epoch 11: loss -10.23371 acc 0.66225 roc_auc 0.50490 prc_auc 0.67911[0m
[93maverage test of epoch 11: loss -10.58626 acc 0.67568 roc_auc 0.59000 prc_auc 0.79391[0m
[92maverage training of epoch 12: loss -10.97957 acc 0.66225 roc_auc 0.53039 prc_auc 0.66638[0m
[93maverage test of epoch 12: loss -11.33986 acc 0.67568 roc_auc 0.49000 prc_auc 0.72738[0m
[92maverage training of epoch 13: loss -11.69013 acc 0.66225 roc_auc 0.46569 prc_auc 0.65105[0m
[93maverage test of epoch 13: loss -12.02963 acc 0.67568 roc_auc 0.38667 prc_auc 0.60391[0m
[92maverage training of epoch 14: loss -12.40614 acc 0.66225 roc_auc 0.46333 prc_auc 0.66721[0m
[93maverage test of epoch 14: loss -12.76585 acc 0.67568 roc_auc 0.46667 prc_auc 0.65095[0m
[92maverage training of epoch 15: loss -13.12394 acc 0.66225 roc_auc 0.48275 prc_auc 0.64894[0m
[93maverage test of epoch 15: loss -13.46457 acc 0.67568 roc_auc 0.62000 prc_auc 0.80643[0m
[92maverage training of epoch 16: loss -13.80362 acc 0.58278 roc_auc 0.43608 prc_auc 0.60593[0m
[93maverage test of epoch 16: loss -14.11716 acc 0.29730 roc_auc 0.39000 prc_auc 0.59654[0m
[92maverage training of epoch 17: loss -14.50072 acc 0.33113 roc_auc 0.43461 prc_auc 0.60154[0m
[93maverage test of epoch 17: loss -14.84705 acc 0.32432 roc_auc 0.63667 prc_auc 0.76553[0m
[92maverage training of epoch 18: loss -15.20053 acc 0.33775 roc_auc 0.44373 prc_auc 0.62957[0m
[93maverage test of epoch 18: loss -15.57762 acc 0.32432 roc_auc 0.37000 prc_auc 0.64040[0m
[92maverage training of epoch 19: loss -15.90873 acc 0.33775 roc_auc 0.44667 prc_auc 0.62220[0m
[93maverage test of epoch 19: loss -16.28142 acc 0.32432 roc_auc 0.36667 prc_auc 0.64106[0m
[92maverage training of epoch 20: loss -16.61737 acc 0.33775 roc_auc 0.41784 prc_auc 0.59948[0m
[93maverage test of epoch 20: loss -16.96283 acc 0.32432 roc_auc 0.40333 prc_auc 0.63264[0m
[92maverage training of epoch 21: loss -17.33680 acc 0.33775 roc_auc 0.45402 prc_auc 0.62816[0m
[93maverage test of epoch 21: loss -17.69367 acc 0.32432 roc_auc 0.37667 prc_auc 0.59663[0m
[92maverage training of epoch 22: loss -18.06367 acc 0.33775 roc_auc 0.43647 prc_auc 0.64958[0m
[93maverage test of epoch 22: loss -18.43347 acc 0.32432 roc_auc 0.46667 prc_auc 0.67677[0m
[92maverage training of epoch 23: loss -18.80082 acc 0.33775 roc_auc 0.42373 prc_auc 0.62183[0m
[93maverage test of epoch 23: loss -19.17499 acc 0.32432 roc_auc 0.49333 prc_auc 0.68458[0m
[92maverage training of epoch 24: loss -19.54572 acc 0.33775 roc_auc 0.42549 prc_auc 0.61229[0m
[93maverage test of epoch 24: loss -19.92902 acc 0.32432 roc_auc 0.29667 prc_auc 0.55381[0m
[92maverage training of epoch 25: loss -20.30490 acc 0.33775 roc_auc 0.39931 prc_auc 0.59982[0m
[93maverage test of epoch 25: loss -20.69089 acc 0.32432 roc_auc 0.38500 prc_auc 0.64986[0m
[92maverage training of epoch 26: loss -21.08599 acc 0.33775 roc_auc 0.38353 prc_auc 0.59086[0m
[93maverage test of epoch 26: loss -21.46007 acc 0.32432 roc_auc 0.44833 prc_auc 0.68504[0m
[92maverage training of epoch 27: loss -21.85636 acc 0.33775 roc_auc 0.40578 prc_auc 0.60227[0m
[93maverage test of epoch 27: loss -22.24553 acc 0.32432 roc_auc 0.53667 prc_auc 0.70962[0m
[92maverage training of epoch 28: loss -22.64767 acc 0.33775 roc_auc 0.41686 prc_auc 0.60391[0m
[93maverage test of epoch 28: loss -23.06474 acc 0.32432 roc_auc 0.53500 prc_auc 0.69267[0m
[92maverage training of epoch 29: loss -23.45169 acc 0.33775 roc_auc 0.39039 prc_auc 0.59625[0m
[93maverage test of epoch 29: loss -23.88074 acc 0.32432 roc_auc 0.59167 prc_auc 0.79197[0m
[92maverage training of epoch 30: loss -24.27735 acc 0.33775 roc_auc 0.40216 prc_auc 0.59591[0m
[93maverage test of epoch 30: loss -24.68393 acc 0.32432 roc_auc 0.43500 prc_auc 0.65388[0m
[92maverage training of epoch 31: loss -25.09729 acc 0.33775 roc_auc 0.38922 prc_auc 0.58615[0m
[93maverage test of epoch 31: loss -25.52175 acc 0.32432 roc_auc 0.50167 prc_auc 0.65911[0m
[92maverage training of epoch 32: loss -25.94054 acc 0.33775 roc_auc 0.38490 prc_auc 0.58496[0m
[93maverage test of epoch 32: loss -26.37593 acc 0.32432 roc_auc 0.39000 prc_auc 0.62453[0m
[92maverage training of epoch 33: loss -26.80117 acc 0.33775 roc_auc 0.39275 prc_auc 0.59364[0m
[93maverage test of epoch 33: loss -27.23282 acc 0.32432 roc_auc 0.63167 prc_auc 0.75714[0m
[92maverage training of epoch 34: loss -27.65694 acc 0.33775 roc_auc 0.38902 prc_auc 0.59751[0m
[93maverage test of epoch 34: loss -28.10637 acc 0.32432 roc_auc 0.55333 prc_auc 0.77427[0m
[92maverage training of epoch 35: loss -28.55178 acc 0.33775 roc_auc 0.39392 prc_auc 0.59819[0m
[93maverage test of epoch 35: loss -29.00965 acc 0.32432 roc_auc 0.62167 prc_auc 0.76075[0m
[92maverage training of epoch 36: loss -29.44015 acc 0.33775 roc_auc 0.39216 prc_auc 0.58685[0m
[93maverage test of epoch 36: loss -29.90177 acc 0.32432 roc_auc 0.35167 prc_auc 0.62266[0m
[92maverage training of epoch 37: loss -30.35160 acc 0.37748 roc_auc 0.39020 prc_auc 0.58876[0m
[93maverage test of epoch 37: loss -30.81789 acc 0.32432 roc_auc 0.50167 prc_auc 0.65699[0m
[92maverage training of epoch 38: loss -31.27708 acc 0.56954 roc_auc 0.39000 prc_auc 0.59031[0m
[93maverage test of epoch 38: loss -31.74910 acc 0.67568 roc_auc 0.36333 prc_auc 0.62291[0m
[92maverage training of epoch 39: loss -32.21551 acc 0.66225 roc_auc 0.37922 prc_auc 0.58394[0m
[93maverage test of epoch 39: loss -32.68497 acc 0.67568 roc_auc 0.67333 prc_auc 0.80336[0m
[92maverage training of epoch 40: loss -33.15927 acc 0.66225 roc_auc 0.38020 prc_auc 0.58237[0m
[93maverage test of epoch 40: loss -33.63947 acc 0.67568 roc_auc 0.38000 prc_auc 0.60294[0m
[92maverage training of epoch 41: loss -34.13122 acc 0.66225 roc_auc 0.37804 prc_auc 0.58233[0m
[93maverage test of epoch 41: loss -34.61446 acc 0.67568 roc_auc 0.58500 prc_auc 0.74433[0m
[92maverage training of epoch 42: loss -35.09625 acc 0.66225 roc_auc 0.37971 prc_auc 0.58370[0m
[93maverage test of epoch 42: loss -35.60148 acc 0.67568 roc_auc 0.39167 prc_auc 0.61442[0m
[92maverage training of epoch 43: loss -36.09490 acc 0.66225 roc_auc 0.37882 prc_auc 0.57944[0m
[93maverage test of epoch 43: loss -36.60811 acc 0.67568 roc_auc 0.55000 prc_auc 0.74225[0m
[92maverage training of epoch 44: loss -37.10144 acc 0.66225 roc_auc 0.38157 prc_auc 0.58211[0m
[93maverage test of epoch 44: loss -37.62298 acc 0.67568 roc_auc 0.34833 prc_auc 0.59840[0m
[92maverage training of epoch 45: loss -38.12444 acc 0.66225 roc_auc 0.37980 prc_auc 0.58603[0m
[93maverage test of epoch 45: loss -38.64984 acc 0.67568 roc_auc 0.28500 prc_auc 0.55222[0m
[92maverage training of epoch 46: loss -39.15824 acc 0.66225 roc_auc 0.37725 prc_auc 0.57735[0m
[93maverage test of epoch 46: loss -39.69149 acc 0.67568 roc_auc 0.47000 prc_auc 0.69376[0m
[92maverage training of epoch 47: loss -40.20949 acc 0.66225 roc_auc 0.37824 prc_auc 0.57748[0m
[93maverage test of epoch 47: loss -40.75219 acc 0.67568 roc_auc 0.56333 prc_auc 0.70576[0m
[92maverage training of epoch 48: loss -41.26871 acc 0.66225 roc_auc 0.38098 prc_auc 0.57931[0m
[93maverage test of epoch 48: loss -41.81891 acc 0.67568 roc_auc 0.47167 prc_auc 0.66462[0m
[92maverage training of epoch 49: loss -42.34965 acc 0.66225 roc_auc 0.37843 prc_auc 0.57771[0m
[93maverage test of epoch 49: loss -42.90906 acc 0.67568 roc_auc 0.51667 prc_auc 0.73304[0m
Run statistics: 
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 2)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 8)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 1)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))


Accuracy (avg): 0.66501 ROC_AUC (avg): 0.51592 PRC_AUC (avg): 0.68119 

Average forward propagation time taken(ms): 4.297274076737809
Average backward propagation time taken(ms): 1.580239935701439

