# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======






Params Turning Set
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 8)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 1)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))






args.base_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/
args.graph_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/
args.min_dfscode_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/
args.min_dfscode_tensor_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/
*** 1 train_index:  [  0   2   3   6   9  10  11  12  13  14  15  16  17  19  20  21  22  23
  24  26  27  30  31  32  33  35  36  38  39  41  42  43  44  45  46  47
  48  49  50  52  53  54  56  57  59  60  61  62  63  64  65  66  67  69
  70  71  72  73  74  76  77  78  79  80  81  82  83  84  87  88  89  91
  92  94  95  96  97  98 100 101 102 103 104 106 107 108 109 110 112 113
 115 116 117 118 119 120 121 122 124 125 126 127 129 130 131 132 134 136
 138 139 140 141 142 144 145 146 147 148 149 151 152 153 154 155 156 157
 158 159 160 161 164 165 166 167 169 170 171 172 173 175 176 177 178 179
 180 182 184 185 186 187]
*** 2 test_index:  [  1   4   5   7   8  18  25  28  29  34  37  40  51  55  58  68  75  85
  86  90  93  99 105 111 114 123 128 133 135 137 143 150 162 163 168 174
 181 183]
*** 1 train_index:  [  0   1   2   4   5   6   7   8  10  11  12  13  14  15  16  17  18  19
  20  21  22  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39
  40  41  43  44  45  46  49  50  51  52  53  54  55  56  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  74  75  76  77  79  80  81  82
  83  84  85  86  88  90  91  92  93  94  96  97  98  99 101 102 103 104
 105 106 107 111 112 113 114 115 120 121 122 123 124 125 126 128 129 130
 131 133 134 135 136 137 138 139 140 143 144 145 147 148 150 153 154 155
 156 159 162 163 164 165 167 168 169 170 171 172 173 174 175 177 178 179
 180 181 182 183 186 187]
*** 2 test_index:  [  3   9  23  31  42  47  48  57  72  73  78  87  89  95 100 108 109 110
 116 117 118 119 127 132 141 142 146 149 151 152 157 158 160 161 166 176
 184 185]
*** 1 train_index:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19
  20  23  24  25  26  28  29  31  32  34  35  36  37  38  40  41  42  43
  44  47  48  51  53  54  55  56  57  58  59  60  64  66  68  70  71  72
  73  74  75  76  77  78  79  81  82  84  85  86  87  88  89  90  92  93
  95  96  97  98  99 100 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134
 135 137 139 140 141 142 143 144 146 148 149 150 151 152 153 156 157 158
 159 160 161 162 163 165 166 167 168 169 171 172 173 174 175 176 177 178
 179 181 183 184 185 186]
*** 2 test_index:  [  0  16  21  22  27  30  33  39  45  46  49  50  52  61  62  63  65  67
  69  80  83  91  94 101 102 103 122 136 138 145 147 154 155 164 170 180
 182 187]
*** 1 train_index:  [  0   1   3   4   5   6   7   8   9  13  14  16  18  19  21  22  23  25
  26  27  28  29  30  31  33  34  36  37  38  39  40  42  43  45  46  47
  48  49  50  51  52  53  55  56  57  58  60  61  62  63  65  66  67  68
  69  71  72  73  75  76  77  78  79  80  81  83  84  85  86  87  88  89
  90  91  92  93  94  95  97  98  99 100 101 102 103 105 106 108 109 110
 111 113 114 115 116 117 118 119 120 121 122 123 125 127 128 131 132 133
 134 135 136 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152
 154 155 156 157 158 160 161 162 163 164 165 166 168 170 171 174 175 176
 180 181 182 183 184 185 187]
*** 2 test_index:  [  2  10  11  12  15  17  20  24  32  35  41  44  54  59  64  70  74  82
  96 104 107 112 124 126 129 130 139 153 159 167 169 172 173 177 178 179
 186]
*** 1 train_index:  [  0   1   2   3   4   5   7   8   9  10  11  12  15  16  17  18  20  21
  22  23  24  25  27  28  29  30  31  32  33  34  35  37  39  40  41  42
  44  45  46  47  48  49  50  51  52  54  55  57  58  59  61  62  63  64
  65  67  68  69  70  72  73  74  75  78  80  82  83  85  86  87  89  90
  91  93  94  95  96  99 100 101 102 103 104 105 107 108 109 110 111 112
 114 116 117 118 119 122 123 124 126 127 128 129 130 132 133 135 136 137
 138 139 141 142 143 145 146 147 149 150 151 152 153 154 155 157 158 159
 160 161 162 163 164 166 167 168 169 170 172 173 174 176 177 178 179 180
 181 182 183 184 185 186 187]
*** 2 test_index:  [  6  13  14  19  26  36  38  43  53  56  60  66  71  76  77  79  81  84
  88  92  97  98 106 113 115 120 121 125 131 134 140 144 148 156 165 171
 175]
{'base_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/',
 'batch_size': 1,
 'clean_temp': False,
 'clean_tensorboard': False,
 'current_dataset_path': None,
 'current_min_dfscode_path': None,
 'current_model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-17-38-51/',
 'current_processed_dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'current_temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-17-38-51/',
 'dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/datasets/',
 'device': device(type='cuda', index=0),
 'dfscode_rnn_dropout': 0.2,
 'embedding_size_dfscode_rnn': 8,
 'epochs': 50,
 'epochs_end': 10000,
 'epochs_save': 20,
 'epochs_validate': 1,
 'fname': 'DFScodeRNN_cls_LSTM_MUTAG',
 'gradient_clipping': True,
 'graph_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/',
 'graph_type': 'MUTAG',
 'graphgen_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/',
 'hidden_size_dfscode_rnn': 8,
 'load_device': device(type='cuda', index=0),
 'load_model': False,
 'load_model_path': '',
 'log_tensorboard': False,
 'lr': 0.0001,
 'max_prev_node': None,
 'min_dfscode_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/',
 'min_dfscode_tensor_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/',
 'note': 'DFScodeRNN_cls_LSTM',
 'num_graphs': None,
 'num_layers': 1,
 'num_workers': 8,
 'number_of_mlp_layer': 1,
 'rnn_type': 'LSTM',
 'save_model': False,
 'temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/',
 'tensorboard_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tensorboard/',
 'time': '2021-01-17-17-38-51',
 'weights': False}


graphgen args.__dict__: None




dataset_features: {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}




config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 50, 'learning_rate': 0.0001, 'seed': 1800, 'k_fold': 5, 'model': 'DFScodeRNN_cls_LSTM', 'dataset': 'MUTAG'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}}


Training a new model: DFScodeRNN_cls_LSTM
Training model with dataset, testing using fold 0
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 8, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.16459 acc 0.66667 roc_auc 0.30380 prc_auc 0.55026[0m
[93maverage test of epoch 0: loss -0.26427 acc 0.65789 roc_auc 0.18769 prc_auc 0.54101[0m
[92maverage training of epoch 1: loss -0.38890 acc 0.66667 roc_auc 0.32400 prc_auc 0.56348[0m
[93maverage test of epoch 1: loss -0.50919 acc 0.65789 roc_auc 0.22462 prc_auc 0.56786[0m
[92maverage training of epoch 2: loss -0.65527 acc 0.66667 roc_auc 0.34800 prc_auc 0.57518[0m
[93maverage test of epoch 2: loss -0.80028 acc 0.65789 roc_auc 0.40000 prc_auc 0.71349[0m
[92maverage training of epoch 3: loss -1.04954 acc 0.66667 roc_auc 0.39740 prc_auc 0.62001[0m
[93maverage test of epoch 3: loss -1.27866 acc 0.65789 roc_auc 0.74769 prc_auc 0.90192[0m
[92maverage training of epoch 4: loss -1.49922 acc 0.66667 roc_auc 0.36600 prc_auc 0.58574[0m
[93maverage test of epoch 4: loss -1.69771 acc 0.65789 roc_auc 0.60462 prc_auc 0.83299[0m
[92maverage training of epoch 5: loss -1.90414 acc 0.66667 roc_auc 0.28260 prc_auc 0.53861[0m
[93maverage test of epoch 5: loss -2.09519 acc 0.65789 roc_auc 0.14769 prc_auc 0.51475[0m
[92maverage training of epoch 6: loss -2.32318 acc 0.66667 roc_auc 0.23740 prc_auc 0.52053[0m
[93maverage test of epoch 6: loss -2.53614 acc 0.65789 roc_auc 0.12923 prc_auc 0.49040[0m
[92maverage training of epoch 7: loss -2.76683 acc 0.66667 roc_auc 0.25680 prc_auc 0.52415[0m
[93maverage test of epoch 7: loss -2.96704 acc 0.65789 roc_auc 0.12000 prc_auc 0.48415[0m
[92maverage training of epoch 8: loss -3.17747 acc 0.66667 roc_auc 0.31960 prc_auc 0.55596[0m
[93maverage test of epoch 8: loss -3.35810 acc 0.65789 roc_auc 0.11385 prc_auc 0.48204[0m
[92maverage training of epoch 9: loss -3.56015 acc 0.66667 roc_auc 0.34480 prc_auc 0.57198[0m
[93maverage test of epoch 9: loss -3.73154 acc 0.65789 roc_auc 0.12154 prc_auc 0.48314[0m
[92maverage training of epoch 10: loss -3.93207 acc 0.66667 roc_auc 0.36540 prc_auc 0.58437[0m
[93maverage test of epoch 10: loss -4.09930 acc 0.65789 roc_auc 0.12615 prc_auc 0.48525[0m
[92maverage training of epoch 11: loss -4.30166 acc 0.66667 roc_auc 0.37220 prc_auc 0.58990[0m
[93maverage test of epoch 11: loss -4.46757 acc 0.65789 roc_auc 0.13538 prc_auc 0.48714[0m
[92maverage training of epoch 12: loss -4.67145 acc 0.66667 roc_auc 0.37580 prc_auc 0.59263[0m
[93maverage test of epoch 12: loss -4.83444 acc 0.65789 roc_auc 0.18154 prc_auc 0.53870[0m
[92maverage training of epoch 13: loss -5.04367 acc 0.66667 roc_auc 0.37580 prc_auc 0.59354[0m
[93maverage test of epoch 13: loss -5.20913 acc 0.65789 roc_auc 0.18923 prc_auc 0.53441[0m
[92maverage training of epoch 14: loss -5.42684 acc 0.66667 roc_auc 0.37420 prc_auc 0.59318[0m
[93maverage test of epoch 14: loss -5.59578 acc 0.65789 roc_auc 0.13846 prc_auc 0.49353[0m
[92maverage training of epoch 15: loss -5.82270 acc 0.66667 roc_auc 0.37120 prc_auc 0.58878[0m
[93maverage test of epoch 15: loss -5.99376 acc 0.65789 roc_auc 0.14308 prc_auc 0.50174[0m
[92maverage training of epoch 16: loss -6.22801 acc 0.66667 roc_auc 0.36390 prc_auc 0.58429[0m
[93maverage test of epoch 16: loss -6.39738 acc 0.65789 roc_auc 0.14154 prc_auc 0.49792[0m
[92maverage training of epoch 17: loss -6.63528 acc 0.66667 roc_auc 0.36160 prc_auc 0.58111[0m
[93maverage test of epoch 17: loss -6.79919 acc 0.65789 roc_auc 0.13385 prc_auc 0.49309[0m
[92maverage training of epoch 18: loss -7.03791 acc 0.66667 roc_auc 0.36460 prc_auc 0.58215[0m
[93maverage test of epoch 18: loss -7.19545 acc 0.65789 roc_auc 0.13538 prc_auc 0.49219[0m
[92maverage training of epoch 19: loss -7.43430 acc 0.66667 roc_auc 0.37410 prc_auc 0.59711[0m
[93maverage test of epoch 19: loss -7.58685 acc 0.65789 roc_auc 0.14769 prc_auc 0.49248[0m
[92maverage training of epoch 20: loss -7.82657 acc 0.66667 roc_auc 0.38470 prc_auc 0.60482[0m
[93maverage test of epoch 20: loss -7.97601 acc 0.65789 roc_auc 0.20462 prc_auc 0.52093[0m
[92maverage training of epoch 21: loss -8.21755 acc 0.66667 roc_auc 0.39290 prc_auc 0.61531[0m
[93maverage test of epoch 21: loss -8.36527 acc 0.65789 roc_auc 0.25077 prc_auc 0.59913[0m
[92maverage training of epoch 22: loss -8.60929 acc 0.66667 roc_auc 0.39840 prc_auc 0.61857[0m
[93maverage test of epoch 22: loss -8.75621 acc 0.65789 roc_auc 0.23846 prc_auc 0.60217[0m
[92maverage training of epoch 23: loss -9.00333 acc 0.66667 roc_auc 0.40060 prc_auc 0.61920[0m
[93maverage test of epoch 23: loss -9.15019 acc 0.65789 roc_auc 0.28462 prc_auc 0.61580[0m
[92maverage training of epoch 24: loss -9.40086 acc 0.66667 roc_auc 0.40160 prc_auc 0.61959[0m
[93maverage test of epoch 24: loss -9.54819 acc 0.65789 roc_auc 0.49538 prc_auc 0.75931[0m
[92maverage training of epoch 25: loss -9.80272 acc 0.66667 roc_auc 0.40110 prc_auc 0.61952[0m
[93maverage test of epoch 25: loss -9.95092 acc 0.65789 roc_auc 0.50462 prc_auc 0.76009[0m
[92maverage training of epoch 26: loss -10.20956 acc 0.66667 roc_auc 0.39970 prc_auc 0.61816[0m
[93maverage test of epoch 26: loss -10.35892 acc 0.65789 roc_auc 0.51385 prc_auc 0.76355[0m
[92maverage training of epoch 27: loss -10.62187 acc 0.66667 roc_auc 0.39840 prc_auc 0.61656[0m
[93maverage test of epoch 27: loss -10.77263 acc 0.65789 roc_auc 0.61692 prc_auc 0.79333[0m
[92maverage training of epoch 28: loss -11.03995 acc 0.66667 roc_auc 0.39660 prc_auc 0.61538[0m
[93maverage test of epoch 28: loss -11.19217 acc 0.65789 roc_auc 0.62000 prc_auc 0.78560[0m
[92maverage training of epoch 29: loss -11.46387 acc 0.66667 roc_auc 0.39440 prc_auc 0.61331[0m
[93maverage test of epoch 29: loss -11.61768 acc 0.65789 roc_auc 0.70769 prc_auc 0.81994[0m
[92maverage training of epoch 30: loss -11.89388 acc 0.66667 roc_auc 0.39230 prc_auc 0.61202[0m
[93maverage test of epoch 30: loss -12.04944 acc 0.65789 roc_auc 0.73077 prc_auc 0.82178[0m
[92maverage training of epoch 31: loss -12.33024 acc 0.66667 roc_auc 0.39060 prc_auc 0.61154[0m
[93maverage test of epoch 31: loss -12.48769 acc 0.65789 roc_auc 0.71077 prc_auc 0.80478[0m
[92maverage training of epoch 32: loss -12.77319 acc 0.66667 roc_auc 0.38850 prc_auc 0.60875[0m
[93maverage test of epoch 32: loss -12.93267 acc 0.65789 roc_auc 0.74462 prc_auc 0.80463[0m
[92maverage training of epoch 33: loss -13.22294 acc 0.66667 roc_auc 0.38680 prc_auc 0.60733[0m
[93maverage test of epoch 33: loss -13.38456 acc 0.65789 roc_auc 0.63077 prc_auc 0.74054[0m
[92maverage training of epoch 34: loss -13.67968 acc 0.66667 roc_auc 0.38550 prc_auc 0.60725[0m
[93maverage test of epoch 34: loss -13.84354 acc 0.65789 roc_auc 0.81385 prc_auc 0.85026[0m
[92maverage training of epoch 35: loss -14.14358 acc 0.66667 roc_auc 0.38450 prc_auc 0.60665[0m
[93maverage test of epoch 35: loss -14.30979 acc 0.65789 roc_auc 0.60000 prc_auc 0.72632[0m
[92maverage training of epoch 36: loss -14.61481 acc 0.66667 roc_auc 0.38250 prc_auc 0.60390[0m
[93maverage test of epoch 36: loss -14.78345 acc 0.65789 roc_auc 0.64000 prc_auc 0.75368[0m
[92maverage training of epoch 37: loss -15.09352 acc 0.66667 roc_auc 0.38160 prc_auc 0.60341[0m
[93maverage test of epoch 37: loss -15.26468 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 38: loss -15.57978 acc 0.66667 roc_auc 0.37820 prc_auc 0.60146[0m
[93maverage test of epoch 38: loss -15.75334 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 39: loss -16.07314 acc 0.66667 roc_auc 0.37710 prc_auc 0.59905[0m
[93maverage test of epoch 39: loss -16.24891 acc 0.65789 roc_auc 0.60000 prc_auc 0.72632[0m
[92maverage training of epoch 40: loss -16.57345 acc 0.66667 roc_auc 0.37520 prc_auc 0.59641[0m
[93maverage test of epoch 40: loss -16.75155 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 41: loss -17.08095 acc 0.66667 roc_auc 0.37580 prc_auc 0.59704[0m
[93maverage test of epoch 41: loss -17.26153 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -17.59591 acc 0.66667 roc_auc 0.37480 prc_auc 0.59802[0m
[93maverage test of epoch 42: loss -17.77910 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -18.11855 acc 0.66667 roc_auc 0.37200 prc_auc 0.59212[0m
[93maverage test of epoch 43: loss -18.30446 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 44: loss -18.64907 acc 0.66667 roc_auc 0.36910 prc_auc 0.58821[0m
[93maverage test of epoch 44: loss -18.83781 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -19.18766 acc 0.66667 roc_auc 0.36930 prc_auc 0.58795[0m
[93maverage test of epoch 45: loss -19.37931 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -19.73447 acc 0.66667 roc_auc 0.36700 prc_auc 0.58585[0m
[93maverage test of epoch 46: loss -19.92912 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -20.28966 acc 0.66667 roc_auc 0.36760 prc_auc 0.58764[0m
[93maverage test of epoch 47: loss -20.48738 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -20.85335 acc 0.66667 roc_auc 0.37010 prc_auc 0.59097[0m
[93maverage test of epoch 48: loss -21.05422 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -21.42569 acc 0.66667 roc_auc 0.36530 prc_auc 0.59108[0m
[93maverage test of epoch 49: loss -21.62976 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 1
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 8, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -2.35300 acc 0.66667 roc_auc 0.44060 prc_auc 0.63385[0m
[93maverage test of epoch 0: loss -2.49434 acc 0.65789 roc_auc 0.37231 prc_auc 0.67766[0m
[92maverage training of epoch 1: loss -2.69254 acc 0.66667 roc_auc 0.45840 prc_auc 0.64918[0m
[93maverage test of epoch 1: loss -2.84465 acc 0.65789 roc_auc 0.43077 prc_auc 0.69338[0m
[92maverage training of epoch 2: loss -3.05541 acc 0.66667 roc_auc 0.45920 prc_auc 0.64631[0m
[93maverage test of epoch 2: loss -3.21653 acc 0.65789 roc_auc 0.32615 prc_auc 0.65207[0m
[92maverage training of epoch 3: loss -3.43841 acc 0.66667 roc_auc 0.46120 prc_auc 0.64773[0m
[93maverage test of epoch 3: loss -3.60857 acc 0.65789 roc_auc 0.30462 prc_auc 0.64514[0m
[92maverage training of epoch 4: loss -3.84184 acc 0.66667 roc_auc 0.46460 prc_auc 0.64939[0m
[93maverage test of epoch 4: loss -4.02228 acc 0.65789 roc_auc 0.30154 prc_auc 0.63735[0m
[92maverage training of epoch 5: loss -4.26781 acc 0.66667 roc_auc 0.46900 prc_auc 0.64659[0m
[93maverage test of epoch 5: loss -4.45937 acc 0.65789 roc_auc 0.53846 prc_auc 0.76526[0m
[92maverage training of epoch 6: loss -4.71792 acc 0.66667 roc_auc 0.47020 prc_auc 0.64837[0m
[93maverage test of epoch 6: loss -4.92027 acc 0.65789 roc_auc 0.67231 prc_auc 0.82823[0m
[92maverage training of epoch 7: loss -5.19236 acc 0.66667 roc_auc 0.47100 prc_auc 0.65640[0m
[93maverage test of epoch 7: loss -5.40421 acc 0.65789 roc_auc 0.83385 prc_auc 0.92129[0m
[92maverage training of epoch 8: loss -5.68805 acc 0.66667 roc_auc 0.47040 prc_auc 0.65611[0m
[93maverage test of epoch 8: loss -5.90608 acc 0.65789 roc_auc 0.89846 prc_auc 0.95218[0m
[92maverage training of epoch 9: loss -6.19827 acc 0.66667 roc_auc 0.47020 prc_auc 0.65610[0m
[93maverage test of epoch 9: loss -6.41838 acc 0.65789 roc_auc 0.90615 prc_auc 0.96168[0m
[92maverage training of epoch 10: loss -6.71580 acc 0.66667 roc_auc 0.47020 prc_auc 0.65606[0m
[93maverage test of epoch 10: loss -6.93473 acc 0.65789 roc_auc 0.83231 prc_auc 0.93180[0m
[92maverage training of epoch 11: loss -7.23517 acc 0.66667 roc_auc 0.47020 prc_auc 0.65606[0m
[93maverage test of epoch 11: loss -7.45058 acc 0.65789 roc_auc 0.72462 prc_auc 0.88926[0m
[92maverage training of epoch 12: loss -7.75250 acc 0.66667 roc_auc 0.47000 prc_auc 0.65602[0m
[93maverage test of epoch 12: loss -7.96273 acc 0.65789 roc_auc 0.64769 prc_auc 0.82001[0m
[92maverage training of epoch 13: loss -8.26561 acc 0.66667 roc_auc 0.47000 prc_auc 0.65602[0m
[93maverage test of epoch 13: loss -8.47004 acc 0.65789 roc_auc 0.69846 prc_auc 0.86080[0m
[92maverage training of epoch 14: loss -8.77392 acc 0.66667 roc_auc 0.47000 prc_auc 0.65602[0m
[93maverage test of epoch 14: loss -8.97236 acc 0.65789 roc_auc 0.59846 prc_auc 0.73938[0m
[92maverage training of epoch 15: loss -9.27708 acc 0.66667 roc_auc 0.46990 prc_auc 0.65597[0m
[93maverage test of epoch 15: loss -9.46884 acc 0.65789 roc_auc 0.70000 prc_auc 0.79474[0m
[92maverage training of epoch 16: loss -9.77411 acc 0.66667 roc_auc 0.46980 prc_auc 0.65597[0m
[93maverage test of epoch 16: loss -9.95921 acc 0.65789 roc_auc 0.60000 prc_auc 0.72632[0m
[92maverage training of epoch 17: loss -10.26564 acc 0.66667 roc_auc 0.46980 prc_auc 0.65597[0m
[93maverage test of epoch 17: loss -10.44450 acc 0.65789 roc_auc 0.60000 prc_auc 0.72632[0m
[92maverage training of epoch 18: loss -10.75244 acc 0.66667 roc_auc 0.46980 prc_auc 0.65597[0m
[93maverage test of epoch 18: loss -10.92548 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 19: loss -11.23562 acc 0.66667 roc_auc 0.46930 prc_auc 0.65503[0m
[93maverage test of epoch 19: loss -11.40354 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 20: loss -11.71649 acc 0.66667 roc_auc 0.46890 prc_auc 0.65272[0m
[93maverage test of epoch 20: loss -11.87989 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 21: loss -12.19624 acc 0.66667 roc_auc 0.46830 prc_auc 0.65251[0m
[93maverage test of epoch 21: loss -12.35572 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 22: loss -12.67599 acc 0.66667 roc_auc 0.46880 prc_auc 0.65601[0m
[93maverage test of epoch 22: loss -12.83210 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 23: loss -13.15674 acc 0.66667 roc_auc 0.46740 prc_auc 0.65021[0m
[93maverage test of epoch 23: loss -13.30992 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 24: loss -13.63930 acc 0.66667 roc_auc 0.46620 prc_auc 0.64595[0m
[93maverage test of epoch 24: loss -13.78994 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 25: loss -14.12439 acc 0.66667 roc_auc 0.46350 prc_auc 0.64586[0m
[93maverage test of epoch 25: loss -14.27282 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 26: loss -14.61263 acc 0.66667 roc_auc 0.46560 prc_auc 0.64781[0m
[93maverage test of epoch 26: loss -14.75914 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 27: loss -15.10457 acc 0.66667 roc_auc 0.46430 prc_auc 0.64693[0m
[93maverage test of epoch 27: loss -15.24941 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 28: loss -15.60066 acc 0.66667 roc_auc 0.46340 prc_auc 0.65111[0m
[93maverage test of epoch 28: loss -15.74407 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 29: loss -16.10135 acc 0.66667 roc_auc 0.46270 prc_auc 0.65159[0m
[93maverage test of epoch 29: loss -16.24351 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 30: loss -16.60699 acc 0.66667 roc_auc 0.47890 prc_auc 0.65521[0m
[93maverage test of epoch 30: loss -16.74808 acc 0.65789 roc_auc 0.52000 prc_auc 0.67158[0m
[92maverage training of epoch 31: loss -17.11794 acc 0.66667 roc_auc 0.46100 prc_auc 0.64674[0m
[93maverage test of epoch 31: loss -17.25810 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 32: loss -17.63448 acc 0.66667 roc_auc 0.47310 prc_auc 0.65736[0m
[93maverage test of epoch 32: loss -17.77384 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 33: loss -18.15686 acc 0.66667 roc_auc 0.50570 prc_auc 0.66995[0m
[93maverage test of epoch 33: loss -18.29554 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 34: loss -18.68534 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 34: loss -18.82343 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 35: loss -19.22011 acc 0.66667 roc_auc 0.48000 prc_auc 0.65793[0m
[93maverage test of epoch 35: loss -19.35769 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 36: loss -19.76138 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 36: loss -19.89853 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 37: loss -20.30930 acc 0.66667 roc_auc 0.50500 prc_auc 0.66896[0m
[93maverage test of epoch 37: loss -20.44607 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 38: loss -20.86403 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 38: loss -21.00049 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 39: loss -21.42571 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 39: loss -21.56189 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 40: loss -21.99448 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 40: loss -22.13043 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 41: loss -22.57045 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 41: loss -22.70622 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -23.15375 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 42: loss -23.28933 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -23.74443 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 43: loss -23.87987 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 44: loss -24.34263 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 44: loss -24.47795 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -24.94844 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 45: loss -25.08366 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -25.56194 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 46: loss -25.69706 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -26.18319 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 47: loss -26.31824 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -26.81227 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 48: loss -26.94722 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -27.44921 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 49: loss -27.58407 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 2
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 8, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.05504 acc 0.33333 roc_auc 0.39120 prc_auc 0.60301[0m
[93maverage test of epoch 0: loss -0.09672 acc 0.34211 roc_auc 0.88615 prc_auc 0.95503[0m
[92maverage training of epoch 1: loss -0.23140 acc 0.33333 roc_auc 0.44300 prc_auc 0.65962[0m
[93maverage test of epoch 1: loss -0.37757 acc 0.34211 roc_auc 0.93538 prc_auc 0.97114[0m
[92maverage training of epoch 2: loss -0.50872 acc 0.33333 roc_auc 0.51020 prc_auc 0.72040[0m
[93maverage test of epoch 2: loss -0.64946 acc 0.34211 roc_auc 0.95385 prc_auc 0.97918[0m
[92maverage training of epoch 3: loss -0.77769 acc 0.34000 roc_auc 0.55820 prc_auc 0.76858[0m
[93maverage test of epoch 3: loss -0.91417 acc 0.36842 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 4: loss -1.04177 acc 0.48667 roc_auc 0.57960 prc_auc 0.78246[0m
[93maverage test of epoch 4: loss -1.17355 acc 0.63158 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 5: loss -1.30691 acc 0.55333 roc_auc 0.64480 prc_auc 0.81857[0m
[93maverage test of epoch 5: loss -1.45368 acc 0.42105 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 6: loss -1.59821 acc 0.50000 roc_auc 0.59680 prc_auc 0.78910[0m
[93maverage test of epoch 6: loss -1.75259 acc 0.39474 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 7: loss -1.90246 acc 0.50000 roc_auc 0.56660 prc_auc 0.76949[0m
[93maverage test of epoch 7: loss -2.05940 acc 0.39474 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 8: loss -2.21080 acc 0.48667 roc_auc 0.55200 prc_auc 0.76369[0m
[93maverage test of epoch 8: loss -2.36933 acc 0.63158 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 9: loss -2.52544 acc 0.62000 roc_auc 0.54720 prc_auc 0.76083[0m
[93maverage test of epoch 9: loss -2.68975 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 10: loss -2.85374 acc 0.66667 roc_auc 0.54400 prc_auc 0.75992[0m
[93maverage test of epoch 10: loss -3.02658 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 11: loss -3.19973 acc 0.66667 roc_auc 0.54080 prc_auc 0.75755[0m
[93maverage test of epoch 11: loss -3.38158 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 12: loss -3.56272 acc 0.66667 roc_auc 0.54040 prc_auc 0.75729[0m
[93maverage test of epoch 12: loss -3.75199 acc 0.65789 roc_auc 0.95077 prc_auc 0.97808[0m
[92maverage training of epoch 13: loss -3.93983 acc 0.66667 roc_auc 0.54320 prc_auc 0.75756[0m
[93maverage test of epoch 13: loss -4.13567 acc 0.65789 roc_auc 0.94923 prc_auc 0.97644[0m
[92maverage training of epoch 14: loss -4.33003 acc 0.66667 roc_auc 0.54920 prc_auc 0.75899[0m
[93maverage test of epoch 14: loss -4.53311 acc 0.65789 roc_auc 0.94923 prc_auc 0.97644[0m
[92maverage training of epoch 15: loss -4.73635 acc 0.66667 roc_auc 0.54820 prc_auc 0.76099[0m
[93maverage test of epoch 15: loss -4.95043 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 16: loss -5.16762 acc 0.66667 roc_auc 0.55860 prc_auc 0.76542[0m
[93maverage test of epoch 16: loss -5.39877 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 17: loss -5.64465 acc 0.59333 roc_auc 0.58220 prc_auc 0.77241[0m
[93maverage test of epoch 17: loss -5.93109 acc 0.36842 roc_auc 0.94769 prc_auc 0.97644[0m
[92maverage training of epoch 18: loss -6.22357 acc 0.35333 roc_auc 0.59700 prc_auc 0.75526[0m
[93maverage test of epoch 18: loss -6.52938 acc 0.34211 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 19: loss -6.81724 acc 0.33333 roc_auc 0.56580 prc_auc 0.75437[0m
[93maverage test of epoch 19: loss -7.11955 acc 0.34211 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 20: loss -7.40377 acc 0.33333 roc_auc 0.53480 prc_auc 0.74427[0m
[93maverage test of epoch 20: loss -7.70544 acc 0.34211 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 21: loss -7.99058 acc 0.33333 roc_auc 0.51500 prc_auc 0.72388[0m
[93maverage test of epoch 21: loss -8.29510 acc 0.34211 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 22: loss -8.58271 acc 0.33333 roc_auc 0.49860 prc_auc 0.70289[0m
[93maverage test of epoch 22: loss -8.89071 acc 0.34211 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 23: loss -9.18184 acc 0.33333 roc_auc 0.48240 prc_auc 0.68538[0m
[93maverage test of epoch 23: loss -9.49441 acc 0.34211 roc_auc 0.95077 prc_auc 0.97751[0m
[92maverage training of epoch 24: loss -9.78997 acc 0.33333 roc_auc 0.47100 prc_auc 0.67155[0m
[93maverage test of epoch 24: loss -10.10773 acc 0.34211 roc_auc 0.94462 prc_auc 0.97394[0m
[92maverage training of epoch 25: loss -10.40822 acc 0.33333 roc_auc 0.45940 prc_auc 0.66251[0m
[93maverage test of epoch 25: loss -10.73146 acc 0.34211 roc_auc 0.94154 prc_auc 0.97221[0m
[92maverage training of epoch 26: loss -11.03717 acc 0.33333 roc_auc 0.44700 prc_auc 0.65416[0m
[93maverage test of epoch 26: loss -11.36603 acc 0.34211 roc_auc 0.93846 prc_auc 0.97094[0m
[92maverage training of epoch 27: loss -11.67714 acc 0.33333 roc_auc 0.43280 prc_auc 0.64063[0m
[93maverage test of epoch 27: loss -12.01168 acc 0.34211 roc_auc 0.93846 prc_auc 0.97094[0m
[92maverage training of epoch 28: loss -12.32836 acc 0.33333 roc_auc 0.41580 prc_auc 0.62527[0m
[93maverage test of epoch 28: loss -12.66863 acc 0.34211 roc_auc 0.93538 prc_auc 0.96933[0m
[92maverage training of epoch 29: loss -12.99101 acc 0.33333 roc_auc 0.40680 prc_auc 0.61823[0m
[93maverage test of epoch 29: loss -13.33707 acc 0.34211 roc_auc 0.93692 prc_auc 0.96923[0m
[92maverage training of epoch 30: loss -13.66533 acc 0.33333 roc_auc 0.40260 prc_auc 0.61543[0m
[93maverage test of epoch 30: loss -14.01726 acc 0.34211 roc_auc 0.93846 prc_auc 0.97072[0m
[92maverage training of epoch 31: loss -14.35158 acc 0.33333 roc_auc 0.39900 prc_auc 0.61201[0m
[93maverage test of epoch 31: loss -14.70948 acc 0.34211 roc_auc 0.92923 prc_auc 0.96260[0m
[92maverage training of epoch 32: loss -15.05005 acc 0.33333 roc_auc 0.39960 prc_auc 0.61256[0m
[93maverage test of epoch 32: loss -15.41404 acc 0.34211 roc_auc 0.91846 prc_auc 0.96045[0m
[92maverage training of epoch 33: loss -15.76109 acc 0.33333 roc_auc 0.39980 prc_auc 0.60468[0m
[93maverage test of epoch 33: loss -16.13131 acc 0.34211 roc_auc 0.90000 prc_auc 0.95233[0m
[92maverage training of epoch 34: loss -16.48505 acc 0.33333 roc_auc 0.39820 prc_auc 0.60069[0m
[93maverage test of epoch 34: loss -16.86165 acc 0.34211 roc_auc 0.89077 prc_auc 0.94720[0m
[92maverage training of epoch 35: loss -17.22232 acc 0.33333 roc_auc 0.39600 prc_auc 0.59825[0m
[93maverage test of epoch 35: loss -17.60547 acc 0.34211 roc_auc 0.85385 prc_auc 0.93163[0m
[92maverage training of epoch 36: loss -17.97328 acc 0.33333 roc_auc 0.39440 prc_auc 0.59660[0m
[93maverage test of epoch 36: loss -18.36315 acc 0.34211 roc_auc 0.86615 prc_auc 0.93289[0m
[92maverage training of epoch 37: loss -18.73834 acc 0.33333 roc_auc 0.39200 prc_auc 0.59440[0m
[93maverage test of epoch 37: loss -19.13508 acc 0.34211 roc_auc 0.91846 prc_auc 0.94429[0m
[92maverage training of epoch 38: loss -19.51788 acc 0.33333 roc_auc 0.39020 prc_auc 0.59409[0m
[93maverage test of epoch 38: loss -19.92167 acc 0.34211 roc_auc 0.90462 prc_auc 0.94111[0m
[92maverage training of epoch 39: loss -20.31229 acc 0.33333 roc_auc 0.38900 prc_auc 0.59295[0m
[93maverage test of epoch 39: loss -20.72329 acc 0.34211 roc_auc 0.84462 prc_auc 0.92342[0m
[92maverage training of epoch 40: loss -21.12195 acc 0.33333 roc_auc 0.38640 prc_auc 0.59165[0m
[93maverage test of epoch 40: loss -21.54031 acc 0.34211 roc_auc 0.86615 prc_auc 0.92728[0m
[92maverage training of epoch 41: loss -21.94721 acc 0.33333 roc_auc 0.38560 prc_auc 0.59087[0m
[93maverage test of epoch 41: loss -22.37309 acc 0.34211 roc_auc 0.86769 prc_auc 0.93259[0m
[92maverage training of epoch 42: loss -22.78842 acc 0.33333 roc_auc 0.38400 prc_auc 0.59002[0m
[93maverage test of epoch 42: loss -23.22195 acc 0.34211 roc_auc 0.90923 prc_auc 0.95969[0m
[92maverage training of epoch 43: loss -23.64592 acc 0.33333 roc_auc 0.38300 prc_auc 0.58963[0m
[93maverage test of epoch 43: loss -24.08723 acc 0.34211 roc_auc 0.93538 prc_auc 0.94750[0m
[92maverage training of epoch 44: loss -24.52000 acc 0.33333 roc_auc 0.38140 prc_auc 0.58816[0m
[93maverage test of epoch 44: loss -24.96922 acc 0.34211 roc_auc 0.79846 prc_auc 0.89914[0m
[92maverage training of epoch 45: loss -25.41094 acc 0.33333 roc_auc 0.38060 prc_auc 0.58741[0m
[93maverage test of epoch 45: loss -25.86802 acc 0.34211 roc_auc 0.85231 prc_auc 0.90514[0m
[92maverage training of epoch 46: loss -26.31845 acc 0.33333 roc_auc 0.38000 prc_auc 0.58684[0m
[93maverage test of epoch 46: loss -26.78316 acc 0.34211 roc_auc 0.88615 prc_auc 0.91922[0m
[92maverage training of epoch 47: loss -27.24241 acc 0.46000 roc_auc 0.37850 prc_auc 0.58515[0m
[93maverage test of epoch 47: loss -27.71483 acc 0.65789 roc_auc 0.77692 prc_auc 0.87755[0m
[92maverage training of epoch 48: loss -28.18301 acc 0.66667 roc_auc 0.37800 prc_auc 0.58453[0m
[93maverage test of epoch 48: loss -28.66321 acc 0.65789 roc_auc 0.75077 prc_auc 0.86032[0m
[92maverage training of epoch 49: loss -29.14043 acc 0.66667 roc_auc 0.37900 prc_auc 0.58482[0m
[93maverage test of epoch 49: loss -29.62832 acc 0.65789 roc_auc 0.81538 prc_auc 0.88778[0m
Training model with dataset, testing using fold 3
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 8, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.05420 acc 0.66225 roc_auc 0.54549 prc_auc 0.68640[0m
[93maverage test of epoch 0: loss -0.19321 acc 0.67568 roc_auc 0.24333 prc_auc 0.57791[0m
[92maverage training of epoch 1: loss -0.30777 acc 0.66225 roc_auc 0.48235 prc_auc 0.66697[0m
[93maverage test of epoch 1: loss -0.38737 acc 0.67568 roc_auc 0.19333 prc_auc 0.55737[0m
[92maverage training of epoch 2: loss -0.44843 acc 0.66225 roc_auc 0.42706 prc_auc 0.63178[0m
[93maverage test of epoch 2: loss -0.51862 acc 0.67568 roc_auc 0.19000 prc_auc 0.55642[0m
[92maverage training of epoch 3: loss -0.59110 acc 0.66225 roc_auc 0.42745 prc_auc 0.62666[0m
[93maverage test of epoch 3: loss -0.67512 acc 0.67568 roc_auc 0.14667 prc_auc 0.51323[0m
[92maverage training of epoch 4: loss -0.88850 acc 0.66225 roc_auc 0.53902 prc_auc 0.67724[0m
[93maverage test of epoch 4: loss -1.14884 acc 0.67568 roc_auc 0.19333 prc_auc 0.55724[0m
[92maverage training of epoch 5: loss -1.45039 acc 0.58278 roc_auc 0.55078 prc_auc 0.68810[0m
[93maverage test of epoch 5: loss -1.79860 acc 0.35135 roc_auc 0.67667 prc_auc 0.86528[0m
[92maverage training of epoch 6: loss -2.18382 acc 0.34437 roc_auc 0.57255 prc_auc 0.71614[0m
[93maverage test of epoch 6: loss -2.55429 acc 0.32432 roc_auc 0.85000 prc_auc 0.93224[0m
[92maverage training of epoch 7: loss -2.87463 acc 0.33775 roc_auc 0.57647 prc_auc 0.72189[0m
[93maverage test of epoch 7: loss -3.16748 acc 0.32432 roc_auc 0.78333 prc_auc 0.90668[0m
[92maverage training of epoch 8: loss -3.43779 acc 0.33775 roc_auc 0.57039 prc_auc 0.71348[0m
[93maverage test of epoch 8: loss -3.68943 acc 0.32432 roc_auc 0.72000 prc_auc 0.88073[0m
[92maverage training of epoch 9: loss -3.94108 acc 0.33775 roc_auc 0.56353 prc_auc 0.70620[0m
[93maverage test of epoch 9: loss -4.17542 acc 0.32432 roc_auc 0.67333 prc_auc 0.86294[0m
[92maverage training of epoch 10: loss -4.42010 acc 0.33775 roc_auc 0.55588 prc_auc 0.69417[0m
[93maverage test of epoch 10: loss -4.64608 acc 0.32432 roc_auc 0.62667 prc_auc 0.84033[0m
[92maverage training of epoch 11: loss -4.88844 acc 0.33775 roc_auc 0.54745 prc_auc 0.68799[0m
[93maverage test of epoch 11: loss -5.11003 acc 0.32432 roc_auc 0.61667 prc_auc 0.83148[0m
[92maverage training of epoch 12: loss -5.35234 acc 0.33775 roc_auc 0.53686 prc_auc 0.67747[0m
[93maverage test of epoch 12: loss -5.57134 acc 0.32432 roc_auc 0.60500 prc_auc 0.82699[0m
[92maverage training of epoch 13: loss -5.81415 acc 0.33775 roc_auc 0.53098 prc_auc 0.67315[0m
[93maverage test of epoch 13: loss -6.03116 acc 0.32432 roc_auc 0.58000 prc_auc 0.81318[0m
[92maverage training of epoch 14: loss -6.27532 acc 0.33775 roc_auc 0.52873 prc_auc 0.67240[0m
[93maverage test of epoch 14: loss -6.49127 acc 0.32432 roc_auc 0.57500 prc_auc 0.81151[0m
[92maverage training of epoch 15: loss -6.73742 acc 0.33775 roc_auc 0.52412 prc_auc 0.66828[0m
[93maverage test of epoch 15: loss -6.95295 acc 0.32432 roc_auc 0.56333 prc_auc 0.79927[0m
[92maverage training of epoch 16: loss -7.20153 acc 0.33775 roc_auc 0.51608 prc_auc 0.66084[0m
[93maverage test of epoch 16: loss -7.41714 acc 0.32432 roc_auc 0.53500 prc_auc 0.78550[0m
[92maverage training of epoch 17: loss -7.66851 acc 0.33775 roc_auc 0.50863 prc_auc 0.65639[0m
[93maverage test of epoch 17: loss -7.88462 acc 0.32432 roc_auc 0.50333 prc_auc 0.77251[0m
[92maverage training of epoch 18: loss -8.13905 acc 0.33775 roc_auc 0.50412 prc_auc 0.65259[0m
[93maverage test of epoch 18: loss -8.35601 acc 0.32432 roc_auc 0.46667 prc_auc 0.75649[0m
[92maverage training of epoch 19: loss -8.61374 acc 0.33775 roc_auc 0.49804 prc_auc 0.64888[0m
[93maverage test of epoch 19: loss -8.83185 acc 0.32432 roc_auc 0.44833 prc_auc 0.73851[0m
[92maverage training of epoch 20: loss -9.09304 acc 0.33775 roc_auc 0.49078 prc_auc 0.64467[0m
[93maverage test of epoch 20: loss -9.31253 acc 0.32432 roc_auc 0.44333 prc_auc 0.73553[0m
[92maverage training of epoch 21: loss -9.57726 acc 0.33775 roc_auc 0.48539 prc_auc 0.64289[0m
[93maverage test of epoch 21: loss -9.79823 acc 0.32432 roc_auc 0.39833 prc_auc 0.68779[0m
[92maverage training of epoch 22: loss -10.06633 acc 0.33775 roc_auc 0.47235 prc_auc 0.63709[0m
[93maverage test of epoch 22: loss -10.28862 acc 0.32432 roc_auc 0.36667 prc_auc 0.67523[0m
[92maverage training of epoch 23: loss -10.56000 acc 0.33775 roc_auc 0.45882 prc_auc 0.63056[0m
[93maverage test of epoch 23: loss -10.78372 acc 0.32432 roc_auc 0.32833 prc_auc 0.63958[0m
[92maverage training of epoch 24: loss -11.05845 acc 0.33775 roc_auc 0.45941 prc_auc 0.63103[0m
[93maverage test of epoch 24: loss -11.28377 acc 0.32432 roc_auc 0.27333 prc_auc 0.62300[0m
[92maverage training of epoch 25: loss -11.56205 acc 0.33775 roc_auc 0.45255 prc_auc 0.62695[0m
[93maverage test of epoch 25: loss -11.78929 acc 0.32432 roc_auc 0.23333 prc_auc 0.57917[0m
[92maverage training of epoch 26: loss -12.07137 acc 0.33775 roc_auc 0.44647 prc_auc 0.62494[0m
[93maverage test of epoch 26: loss -12.30086 acc 0.32432 roc_auc 0.23667 prc_auc 0.55844[0m
[92maverage training of epoch 27: loss -12.58695 acc 0.33775 roc_auc 0.44451 prc_auc 0.62272[0m
[93maverage test of epoch 27: loss -12.81896 acc 0.32432 roc_auc 0.26167 prc_auc 0.57757[0m
[92maverage training of epoch 28: loss -13.10924 acc 0.33775 roc_auc 0.44147 prc_auc 0.62300[0m
[93maverage test of epoch 28: loss -13.34402 acc 0.32432 roc_auc 0.25000 prc_auc 0.59454[0m
[92maverage training of epoch 29: loss -13.63863 acc 0.33775 roc_auc 0.43588 prc_auc 0.61814[0m
[93maverage test of epoch 29: loss -13.87639 acc 0.32432 roc_auc 0.26000 prc_auc 0.59535[0m
[92maverage training of epoch 30: loss -14.17545 acc 0.33775 roc_auc 0.43353 prc_auc 0.61788[0m
[93maverage test of epoch 30: loss -14.41638 acc 0.32432 roc_auc 0.25000 prc_auc 0.57037[0m
[92maverage training of epoch 31: loss -14.71999 acc 0.33775 roc_auc 0.43137 prc_auc 0.61331[0m
[93maverage test of epoch 31: loss -14.96425 acc 0.32432 roc_auc 0.25167 prc_auc 0.57435[0m
[92maverage training of epoch 32: loss -15.27249 acc 0.33775 roc_auc 0.42980 prc_auc 0.60595[0m
[93maverage test of epoch 32: loss -15.52023 acc 0.32432 roc_auc 0.18333 prc_auc 0.54600[0m
[92maverage training of epoch 33: loss -15.83317 acc 0.33775 roc_auc 0.42784 prc_auc 0.60324[0m
[93maverage test of epoch 33: loss -16.08452 acc 0.32432 roc_auc 0.32167 prc_auc 0.60719[0m
[92maverage training of epoch 34: loss -16.40222 acc 0.33775 roc_auc 0.42608 prc_auc 0.60259[0m
[93maverage test of epoch 34: loss -16.65730 acc 0.32432 roc_auc 0.26500 prc_auc 0.60452[0m
[92maverage training of epoch 35: loss -16.97979 acc 0.33775 roc_auc 0.42255 prc_auc 0.60234[0m
[93maverage test of epoch 35: loss -17.23870 acc 0.32432 roc_auc 0.35667 prc_auc 0.62948[0m
[92maverage training of epoch 36: loss -17.56602 acc 0.33775 roc_auc 0.41980 prc_auc 0.60355[0m
[93maverage test of epoch 36: loss -17.82886 acc 0.32432 roc_auc 0.44000 prc_auc 0.65183[0m
[92maverage training of epoch 37: loss -18.16103 acc 0.33775 roc_auc 0.41608 prc_auc 0.60268[0m
[93maverage test of epoch 37: loss -18.42789 acc 0.32432 roc_auc 0.25000 prc_auc 0.58215[0m
[92maverage training of epoch 38: loss -18.76493 acc 0.33775 roc_auc 0.41020 prc_auc 0.59972[0m
[93maverage test of epoch 38: loss -19.03589 acc 0.32432 roc_auc 0.35000 prc_auc 0.62758[0m
[92maverage training of epoch 39: loss -19.37781 acc 0.33775 roc_auc 0.40843 prc_auc 0.59979[0m
[93maverage test of epoch 39: loss -19.65296 acc 0.32432 roc_auc 0.17000 prc_auc 0.59120[0m
[92maverage training of epoch 40: loss -19.99975 acc 0.33775 roc_auc 0.40608 prc_auc 0.60043[0m
[93maverage test of epoch 40: loss -20.27914 acc 0.32432 roc_auc 0.31667 prc_auc 0.60878[0m
[92maverage training of epoch 41: loss -20.63082 acc 0.33775 roc_auc 0.40255 prc_auc 0.59851[0m
[93maverage test of epoch 41: loss -20.91452 acc 0.32432 roc_auc 0.46667 prc_auc 0.65836[0m
[92maverage training of epoch 42: loss -21.27107 acc 0.33775 roc_auc 0.40098 prc_auc 0.60042[0m
[93maverage test of epoch 42: loss -21.55915 acc 0.32432 roc_auc 0.36000 prc_auc 0.62887[0m
[92maverage training of epoch 43: loss -21.92055 acc 0.33775 roc_auc 0.39529 prc_auc 0.60247[0m
[93maverage test of epoch 43: loss -22.21306 acc 0.32432 roc_auc 0.33500 prc_auc 0.61112[0m
[92maverage training of epoch 44: loss -22.57931 acc 0.33775 roc_auc 0.38824 prc_auc 0.59673[0m
[93maverage test of epoch 44: loss -22.87631 acc 0.32432 roc_auc 0.33333 prc_auc 0.64227[0m
[92maverage training of epoch 45: loss -23.24739 acc 0.33775 roc_auc 0.38039 prc_auc 0.59468[0m
[93maverage test of epoch 45: loss -23.54889 acc 0.32432 roc_auc 0.44500 prc_auc 0.66403[0m
[92maverage training of epoch 46: loss -23.92463 acc 0.33775 roc_auc 0.37627 prc_auc 0.59309[0m
[93maverage test of epoch 46: loss -24.23049 acc 0.32432 roc_auc 0.49333 prc_auc 0.67291[0m
[92maverage training of epoch 47: loss -24.61059 acc 0.33775 roc_auc 0.37255 prc_auc 0.59180[0m
[93maverage test of epoch 47: loss -24.92046 acc 0.32432 roc_auc 0.58833 prc_auc 0.75453[0m
[92maverage training of epoch 48: loss -25.30459 acc 0.33775 roc_auc 0.36853 prc_auc 0.58936[0m
[93maverage test of epoch 48: loss -25.61844 acc 0.32432 roc_auc 0.45000 prc_auc 0.68074[0m
[92maverage training of epoch 49: loss -26.00666 acc 0.33775 roc_auc 0.37039 prc_auc 0.58969[0m
[93maverage test of epoch 49: loss -26.32461 acc 0.32432 roc_auc 0.64000 prc_auc 0.75487[0m
Training model with dataset, testing using fold 4
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 8, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=8, out_features=8, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=8, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.90372 acc 0.64901 roc_auc 0.55255 prc_auc 0.71437[0m
[93maverage test of epoch 0: loss 0.65184 acc 0.67568 roc_auc 0.35000 prc_auc 0.66080[0m
[92maverage training of epoch 1: loss 0.40839 acc 0.52318 roc_auc 0.47471 prc_auc 0.68375[0m
[93maverage test of epoch 1: loss 0.17254 acc 0.32432 roc_auc 0.41667 prc_auc 0.69396[0m
[92maverage training of epoch 2: loss 0.07502 acc 0.34437 roc_auc 0.38549 prc_auc 0.62347[0m
[93maverage test of epoch 2: loss -0.00147 acc 0.32432 roc_auc 0.25000 prc_auc 0.55296[0m
[92maverage training of epoch 3: loss -0.08102 acc 0.33775 roc_auc 0.32255 prc_auc 0.59349[0m
[93maverage test of epoch 3: loss -0.15799 acc 0.32432 roc_auc 0.17333 prc_auc 0.52591[0m
[92maverage training of epoch 4: loss -0.24282 acc 0.33775 roc_auc 0.28647 prc_auc 0.57120[0m
[93maverage test of epoch 4: loss -0.32580 acc 0.32432 roc_auc 0.13667 prc_auc 0.54522[0mUsing backend: pytorch
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

[92maverage training of epoch 5: loss -0.41913 acc 0.33775 roc_auc 0.25510 prc_auc 0.55669[0m
[93maverage test of epoch 5: loss -0.51087 acc 0.32432 roc_auc 0.12667 prc_auc 0.56303[0m
[92maverage training of epoch 6: loss -0.60925 acc 0.33775 roc_auc 0.23353 prc_auc 0.56034[0m
[93maverage test of epoch 6: loss -0.70519 acc 0.32432 roc_auc 0.15667 prc_auc 0.58572[0m
[92maverage training of epoch 7: loss -0.81386 acc 0.33775 roc_auc 0.31529 prc_auc 0.60423[0m
[93maverage test of epoch 7: loss -0.92466 acc 0.32432 roc_auc 0.19167 prc_auc 0.60756[0m
[92maverage training of epoch 8: loss -1.04784 acc 0.33775 roc_auc 0.35843 prc_auc 0.63289[0m
[93maverage test of epoch 8: loss -1.17574 acc 0.32432 roc_auc 0.17667 prc_auc 0.59623[0m
[92maverage training of epoch 9: loss -1.32332 acc 0.37086 roc_auc 0.37510 prc_auc 0.61177[0m
[93maverage test of epoch 9: loss -1.47749 acc 0.67568 roc_auc 0.40333 prc_auc 0.73732[0m
[92maverage training of epoch 10: loss -1.64606 acc 0.66225 roc_auc 0.44627 prc_auc 0.66608[0m
[93maverage test of epoch 10: loss -1.81698 acc 0.67568 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 11: loss -1.98213 acc 0.66225 roc_auc 0.47941 prc_auc 0.67472[0m
[93maverage test of epoch 11: loss -2.14686 acc 0.67568 roc_auc 0.93333 prc_auc 0.97304[0m
[92maverage training of epoch 12: loss -2.29969 acc 0.66225 roc_auc 0.46039 prc_auc 0.65583[0m
[93maverage test of epoch 12: loss -2.45530 acc 0.67568 roc_auc 0.87333 prc_auc 0.95069[0m
[92maverage training of epoch 13: loss -2.60036 acc 0.66225 roc_auc 0.43255 prc_auc 0.63764[0m
[93maverage test of epoch 13: loss -2.75195 acc 0.67568 roc_auc 0.62000 prc_auc 0.84905[0m
[92maverage training of epoch 14: loss -2.89203 acc 0.66225 roc_auc 0.40157 prc_auc 0.60883[0m
[93maverage test of epoch 14: loss -3.04186 acc 0.67568 roc_auc 0.19667 prc_auc 0.59822[0m
[92maverage training of epoch 15: loss -3.17925 acc 0.66225 roc_auc 0.37902 prc_auc 0.59636[0m
[93maverage test of epoch 15: loss -3.32911 acc 0.67568 roc_auc 0.16500 prc_auc 0.58856[0m
[92maverage training of epoch 16: loss -3.46427 acc 0.66225 roc_auc 0.37745 prc_auc 0.59715[0m
[93maverage test of epoch 16: loss -3.61465 acc 0.67568 roc_auc 0.20333 prc_auc 0.60125[0m
[92maverage training of epoch 17: loss -3.74841 acc 0.66225 roc_auc 0.37118 prc_auc 0.58530[0m
[93maverage test of epoch 17: loss -3.90022 acc 0.67568 roc_auc 0.17500 prc_auc 0.59129[0m
[92maverage training of epoch 18: loss -4.03254 acc 0.66225 roc_auc 0.36098 prc_auc 0.57982[0m
[93maverage test of epoch 18: loss -4.18557 acc 0.67568 roc_auc 0.13000 prc_auc 0.53483[0m
[92maverage training of epoch 19: loss -4.31740 acc 0.66225 roc_auc 0.36471 prc_auc 0.58383[0m
[93maverage test of epoch 19: loss -4.47251 acc 0.67568 roc_auc 0.13333 prc_auc 0.53650[0m
[92maverage training of epoch 20: loss -4.60347 acc 0.66225 roc_auc 0.35824 prc_auc 0.57782[0m
[93maverage test of epoch 20: loss -4.76065 acc 0.67568 roc_auc 0.13667 prc_auc 0.53841[0m
[92maverage training of epoch 21: loss -4.89127 acc 0.66225 roc_auc 0.35549 prc_auc 0.57733[0m
[93maverage test of epoch 21: loss -5.05067 acc 0.67568 roc_auc 0.14333 prc_auc 0.54233[0m
[92maverage training of epoch 22: loss -5.18115 acc 0.66225 roc_auc 0.34176 prc_auc 0.56613[0m
[93maverage test of epoch 22: loss -5.34293 acc 0.67568 roc_auc 0.13333 prc_auc 0.53674[0m
[92maverage training of epoch 23: loss -5.47357 acc 0.66225 roc_auc 0.34078 prc_auc 0.56505[0m
[93maverage test of epoch 23: loss -5.63785 acc 0.67568 roc_auc 0.12667 prc_auc 0.53377[0m
[92maverage training of epoch 24: loss -5.76882 acc 0.66225 roc_auc 0.33706 prc_auc 0.55456[0m
[93maverage test of epoch 24: loss -5.93545 acc 0.67568 roc_auc 0.12667 prc_auc 0.53377[0m
[92maverage training of epoch 25: loss -6.06715 acc 0.66225 roc_auc 0.32980 prc_auc 0.56056[0m
[93maverage test of epoch 25: loss -6.23686 acc 0.67568 roc_auc 0.13000 prc_auc 0.53522[0m
[92maverage training of epoch 26: loss -6.36952 acc 0.66225 roc_auc 0.33333 prc_auc 0.56216[0m
[93maverage test of epoch 26: loss -6.54220 acc 0.67568 roc_auc 0.12667 prc_auc 0.53377[0m
[92maverage training of epoch 27: loss -6.67615 acc 0.66225 roc_auc 0.33451 prc_auc 0.56669[0m
[93maverage test of epoch 27: loss -6.85250 acc 0.67568 roc_auc 0.13333 prc_auc 0.53674[0m
[92maverage training of epoch 28: loss -6.98831 acc 0.66225 roc_auc 0.34216 prc_auc 0.56545[0m
[93maverage test of epoch 28: loss -7.16891 acc 0.67568 roc_auc 0.14333 prc_auc 0.54233[0m
[92maverage training of epoch 29: loss -7.30762 acc 0.66225 roc_auc 0.35039 prc_auc 0.56916[0m
[93maverage test of epoch 29: loss -7.49341 acc 0.67568 roc_auc 0.14333 prc_auc 0.54233[0m
[92maverage training of epoch 30: loss -7.63617 acc 0.66225 roc_auc 0.35176 prc_auc 0.56111[0m
[93maverage test of epoch 30: loss -7.82833 acc 0.67568 roc_auc 0.13000 prc_auc 0.53522[0m
[92maverage training of epoch 31: loss -7.97575 acc 0.66225 roc_auc 0.35235 prc_auc 0.56070[0m
[93maverage test of epoch 31: loss -8.17445 acc 0.67568 roc_auc 0.12333 prc_auc 0.53209[0m
[92maverage training of epoch 32: loss -8.32566 acc 0.66225 roc_auc 0.35373 prc_auc 0.56117[0m
[93maverage test of epoch 32: loss -8.52984 acc 0.67568 roc_auc 0.08667 prc_auc 0.49366[0m
[92maverage training of epoch 33: loss -8.68367 acc 0.66225 roc_auc 0.35686 prc_auc 0.56589[0m
[93maverage test of epoch 33: loss -8.89230 acc 0.67568 roc_auc 0.07333 prc_auc 0.48872[0m
[92maverage training of epoch 34: loss -9.04785 acc 0.66225 roc_auc 0.36078 prc_auc 0.56922[0m
[93maverage test of epoch 34: loss -9.26022 acc 0.67568 roc_auc 0.07000 prc_auc 0.48739[0m
[92maverage training of epoch 35: loss -9.41700 acc 0.66225 roc_auc 0.36520 prc_auc 0.57368[0m
[93maverage test of epoch 35: loss -9.63288 acc 0.67568 roc_auc 0.06667 prc_auc 0.48630[0m
[92maverage training of epoch 36: loss -9.79068 acc 0.66225 roc_auc 0.36706 prc_auc 0.57530[0m
[93maverage test of epoch 36: loss -10.01011 acc 0.67568 roc_auc 0.06667 prc_auc 0.48630[0m
[92maverage training of epoch 37: loss -10.16888 acc 0.66225 roc_auc 0.37392 prc_auc 0.58290[0m
[93maverage test of epoch 37: loss -10.39199 acc 0.67568 roc_auc 0.05667 prc_auc 0.48606[0m
[92maverage training of epoch 38: loss -10.55160 acc 0.66225 roc_auc 0.37343 prc_auc 0.58112[0m
[93maverage test of epoch 38: loss -10.77832 acc 0.67568 roc_auc 0.05667 prc_auc 0.48442[0m
[92maverage training of epoch 39: loss -10.93883 acc 0.66225 roc_auc 0.37647 prc_auc 0.58317[0m
[93maverage test of epoch 39: loss -11.16935 acc 0.67568 roc_auc 0.05667 prc_auc 0.48501[0m
[92maverage training of epoch 40: loss -11.33092 acc 0.66225 roc_auc 0.37794 prc_auc 0.58459[0m
[93maverage test of epoch 40: loss -11.56530 acc 0.67568 roc_auc 0.05667 prc_auc 0.48755[0m
[92maverage training of epoch 41: loss -11.72794 acc 0.66225 roc_auc 0.38000 prc_auc 0.58655[0m
[93maverage test of epoch 41: loss -11.96635 acc 0.67568 roc_auc 0.05333 prc_auc 0.48741[0m
[92maverage training of epoch 42: loss -12.13011 acc 0.66225 roc_auc 0.38078 prc_auc 0.58719[0m
[93maverage test of epoch 42: loss -12.37265 acc 0.67568 roc_auc 0.05333 prc_auc 0.48371[0m
[92maverage training of epoch 43: loss -12.53759 acc 0.66225 roc_auc 0.38216 prc_auc 0.58806[0m
[93maverage test of epoch 43: loss -12.78435 acc 0.67568 roc_auc 0.05333 prc_auc 0.48849[0m
[92maverage training of epoch 44: loss -12.95051 acc 0.66225 roc_auc 0.38255 prc_auc 0.58840[0m
[93maverage test of epoch 44: loss -13.20158 acc 0.67568 roc_auc 0.06000 prc_auc 0.48882[0m
[92maverage training of epoch 45: loss -13.36898 acc 0.66225 roc_auc 0.38255 prc_auc 0.58842[0m
[93maverage test of epoch 45: loss -13.62443 acc 0.67568 roc_auc 0.06167 prc_auc 0.49176[0m
[92maverage training of epoch 46: loss -13.79309 acc 0.66225 roc_auc 0.38255 prc_auc 0.58855[0m
[93maverage test of epoch 46: loss -14.05300 acc 0.67568 roc_auc 0.06000 prc_auc 0.49178[0m
[92maverage training of epoch 47: loss -14.22294 acc 0.66225 roc_auc 0.38255 prc_auc 0.59017[0m
[93maverage test of epoch 47: loss -14.48736 acc 0.67568 roc_auc 0.06000 prc_auc 0.49870[0m
[92maverage training of epoch 48: loss -14.65858 acc 0.66225 roc_auc 0.38294 prc_auc 0.59057[0m
[93maverage test of epoch 48: loss -14.92757 acc 0.67568 roc_auc 0.06500 prc_auc 0.50046[0m
[92maverage training of epoch 49: loss -15.10010 acc 0.66225 roc_auc 0.38333 prc_auc 0.59119[0m
[93maverage test of epoch 49: loss -15.37371 acc 0.67568 roc_auc 0.07833 prc_auc 0.51292[0m
Run statistics: 
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 8)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 1)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))


Accuracy (avg): 0.59474 ROC_AUC (avg): 0.50674 PRC_AUC (avg): 0.69427 

Average forward propagation time taken(ms): 2.80637676495664
Average backward propagation time taken(ms): 0.9410325279312974

