# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======






Params Turning Set
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 0)
('lr', 0.0003)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))






args.base_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/
args.graph_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/
args.min_dfscode_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/
args.min_dfscode_tensor_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/
*** 1 train_index:  [  0   2   3   6   9  10  11  12  13  14  15  16  17  19  20  21  22  23
  24  26  27  30  31  32  33  35  36  38  39  41  42  43  44  45  46  47
  48  49  50  52  53  54  56  57  59  60  61  62  63  64  65  66  67  69
  70  71  72  73  74  76  77  78  79  80  81  82  83  84  87  88  89  91
  92  94  95  96  97  98 100 101 102 103 104 106 107 108 109 110 112 113
 115 116 117 118 119 120 121 122 124 125 126 127 129 130 131 132 134 136
 138 139 140 141 142 144 145 146 147 148 149 151 152 153 154 155 156 157
 158 159 160 161 164 165 166 167 169 170 171 172 173 175 176 177 178 179
 180 182 184 185 186 187]
*** 2 test_index:  [  1   4   5   7   8  18  25  28  29  34  37  40  51  55  58  68  75  85
  86  90  93  99 105 111 114 123 128 133 135 137 143 150 162 163 168 174
 181 183]
*** 1 train_index:  [  0   1   2   4   5   6   7   8  10  11  12  13  14  15  16  17  18  19
  20  21  22  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39
  40  41  43  44  45  46  49  50  51  52  53  54  55  56  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  74  75  76  77  79  80  81  82
  83  84  85  86  88  90  91  92  93  94  96  97  98  99 101 102 103 104
 105 106 107 111 112 113 114 115 120 121 122 123 124 125 126 128 129 130
 131 133 134 135 136 137 138 139 140 143 144 145 147 148 150 153 154 155
 156 159 162 163 164 165 167 168 169 170 171 172 173 174 175 177 178 179
 180 181 182 183 186 187]
*** 2 test_index:  [  3   9  23  31  42  47  48  57  72  73  78  87  89  95 100 108 109 110
 116 117 118 119 127 132 141 142 146 149 151 152 157 158 160 161 166 176
 184 185]
*** 1 train_index:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19
  20  23  24  25  26  28  29  31  32  34  35  36  37  38  40  41  42  43
  44  47  48  51  53  54  55  56  57  58  59  60  64  66  68  70  71  72
  73  74  75  76  77  78  79  81  82  84  85  86  87  88  89  90  92  93
  95  96  97  98  99 100 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134
 135 137 139 140 141 142 143 144 146 148 149 150 151 152 153 156 157 158
 159 160 161 162 163 165 166 167 168 169 171 172 173 174 175 176 177 178
 179 181 183 184 185 186]
*** 2 test_index:  [  0  16  21  22  27  30  33  39  45  46  49  50  52  61  62  63  65  67
  69  80  83  91  94 101 102 103 122 136 138 145 147 154 155 164 170 180
 182 187]
*** 1 train_index:  [  0   1   3   4   5   6   7   8   9  13  14  16  18  19  21  22  23  25
  26  27  28  29  30  31  33  34  36  37  38  39  40  42  43  45  46  47
  48  49  50  51  52  53  55  56  57  58  60  61  62  63  65  66  67  68
  69  71  72  73  75  76  77  78  79  80  81  83  84  85  86  87  88  89
  90  91  92  93  94  95  97  98  99 100 101 102 103 105 106 108 109 110
 111 113 114 115 116 117 118 119 120 121 122 123 125 127 128 131 132 133
 134 135 136 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152
 154 155 156 157 158 160 161 162 163 164 165 166 168 170 171 174 175 176
 180 181 182 183 184 185 187]
*** 2 test_index:  [  2  10  11  12  15  17  20  24  32  35  41  44  54  59  64  70  74  82
  96 104 107 112 124 126 129 130 139 153 159 167 169 172 173 177 178 179
 186]
*** 1 train_index:  [  0   1   2   3   4   5   7   8   9  10  11  12  15  16  17  18  20  21
  22  23  24  25  27  28  29  30  31  32  33  34  35  37  39  40  41  42
  44  45  46  47  48  49  50  51  52  54  55  57  58  59  61  62  63  64
  65  67  68  69  70  72  73  74  75  78  80  82  83  85  86  87  89  90
  91  93  94  95  96  99 100 101 102 103 104 105 107 108 109 110 111 112
 114 116 117 118 119 122 123 124 126 127 128 129 130 132 133 135 136 137
 138 139 141 142 143 145 146 147 149 150 151 152 153 154 155 157 158 159
 160 161 162 163 164 166 167 168 169 170 172 173 174 176 177 178 179 180
 181 182 183 184 185 186 187]
*** 2 test_index:  [  6  13  14  19  26  36  38  43  53  56  60  66  71  76  77  79  81  84
  88  92  97  98 106 113 115 120 121 125 131 134 140 144 148 156 165 171
 175]
{'base_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/',
 'batch_size': 1,
 'clean_temp': False,
 'clean_tensorboard': False,
 'current_dataset_path': None,
 'current_min_dfscode_path': None,
 'current_model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-16-42-57/',
 'current_processed_dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'current_temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-16-42-57/',
 'dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/datasets/',
 'device': device(type='cuda', index=0),
 'dfscode_rnn_dropout': 0.2,
 'embedding_size_dfscode_rnn': 8,
 'epochs': 50,
 'epochs_end': 10000,
 'epochs_save': 20,
 'epochs_validate': 1,
 'fname': 'DFScodeRNN_cls_LSTM_MUTAG',
 'gradient_clipping': True,
 'graph_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/',
 'graph_type': 'MUTAG',
 'graphgen_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/',
 'hidden_size_dfscode_rnn': 4,
 'load_device': device(type='cuda', index=0),
 'load_model': False,
 'load_model_path': '',
 'log_tensorboard': False,
 'lr': 0.0003,
 'max_prev_node': None,
 'min_dfscode_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/',
 'min_dfscode_tensor_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/',
 'note': 'DFScodeRNN_cls_LSTM',
 'num_graphs': None,
 'num_layers': 1,
 'num_workers': 8,
 'number_of_mlp_layer': 0,
 'rnn_type': 'LSTM',
 'save_model': False,
 'temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/',
 'tensorboard_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tensorboard/',
 'time': '2021-01-17-16-42-57',
 'weights': False}


graphgen args.__dict__: None




dataset_features: {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}




config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 50, 'learning_rate': 0.0003, 'seed': 1800, 'k_fold': 5, 'model': 'DFScodeRNN_cls_LSTM', 'dataset': 'MUTAG'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}}


Training a new model: DFScodeRNN_cls_LSTM
Training model with dataset, testing using fold 0
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss -0.08108 acc 0.33333 roc_auc 0.43760 prc_auc 0.67590[0m
[93maverage test of epoch 0: loss -0.25412 acc 0.34211 roc_auc 0.85846 prc_auc 0.93026[0m
[92maverage training of epoch 1: loss -0.44363 acc 0.33333 roc_auc 0.49600 prc_auc 0.72621[0m
[93maverage test of epoch 1: loss -0.65060 acc 0.34211 roc_auc 0.81846 prc_auc 0.91518[0m
[92maverage training of epoch 2: loss -0.82425 acc 0.36000 roc_auc 0.53200 prc_auc 0.75518[0m
[93maverage test of epoch 2: loss -0.99915 acc 0.39474 roc_auc 0.83692 prc_auc 0.92684[0m
[92maverage training of epoch 3: loss -1.14703 acc 0.48000 roc_auc 0.55200 prc_auc 0.77541[0m
[93maverage test of epoch 3: loss -1.31171 acc 0.65789 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 4: loss -1.45696 acc 0.66667 roc_auc 0.60400 prc_auc 0.80818[0m
[93maverage test of epoch 4: loss -1.61483 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 5: loss -1.74829 acc 0.66667 roc_auc 0.70180 prc_auc 0.85680[0m
[93maverage test of epoch 5: loss -1.88522 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 6: loss -2.02260 acc 0.66667 roc_auc 0.73000 prc_auc 0.86186[0m
[93maverage test of epoch 6: loss -2.15116 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 7: loss -2.28815 acc 0.66667 roc_auc 0.73560 prc_auc 0.85546[0m
[93maverage test of epoch 7: loss -2.40958 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 8: loss -2.55655 acc 0.66667 roc_auc 0.71640 prc_auc 0.84105[0m
[93maverage test of epoch 8: loss -2.68283 acc 0.65789 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 9: loss -2.83782 acc 0.66667 roc_auc 0.68900 prc_auc 0.82316[0m
[93maverage test of epoch 9: loss -2.96306 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 10: loss -3.11578 acc 0.66667 roc_auc 0.66700 prc_auc 0.80782[0m
[93maverage test of epoch 10: loss -3.23168 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 11: loss -3.37718 acc 0.66667 roc_auc 0.64760 prc_auc 0.79390[0m
[93maverage test of epoch 11: loss -3.48212 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 12: loss -3.62042 acc 0.66667 roc_auc 0.62580 prc_auc 0.77700[0m
[93maverage test of epoch 12: loss -3.71609 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 13: loss -3.84851 acc 0.66667 roc_auc 0.59900 prc_auc 0.75751[0m
[93maverage test of epoch 13: loss -3.93693 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 14: loss -4.06480 acc 0.66667 roc_auc 0.57440 prc_auc 0.74046[0m
[93maverage test of epoch 14: loss -4.14758 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 15: loss -4.27197 acc 0.66667 roc_auc 0.54840 prc_auc 0.72401[0m
[93maverage test of epoch 15: loss -4.35033 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 16: loss -4.47206 acc 0.66667 roc_auc 0.52100 prc_auc 0.70442[0m
[93maverage test of epoch 16: loss -4.54686 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 17: loss -4.66655 acc 0.66667 roc_auc 0.49720 prc_auc 0.68623[0m
[93maverage test of epoch 17: loss -4.73844 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 18: loss -4.85655 acc 0.66667 roc_auc 0.47160 prc_auc 0.66832[0m
[93maverage test of epoch 18: loss -4.92600 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 19: loss -5.04291 acc 0.66667 roc_auc 0.45020 prc_auc 0.65487[0m
[93maverage test of epoch 19: loss -5.11029 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 20: loss -5.22627 acc 0.66667 roc_auc 0.43340 prc_auc 0.64433[0m
[93maverage test of epoch 20: loss -5.29187 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 21: loss -5.40715 acc 0.66667 roc_auc 0.42260 prc_auc 0.63607[0m
[93maverage test of epoch 21: loss -5.47118 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 22: loss -5.58594 acc 0.66667 roc_auc 0.41560 prc_auc 0.63196[0m
[93maverage test of epoch 22: loss -5.64858 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 23: loss -5.76298 acc 0.66667 roc_auc 0.40960 prc_auc 0.62672[0m
[93maverage test of epoch 23: loss -5.82437 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 24: loss -5.93852 acc 0.66667 roc_auc 0.40260 prc_auc 0.62102[0m
[93maverage test of epoch 24: loss -5.99879 acc 0.65789 roc_auc 0.86462 prc_auc 0.93440[0m
[92maverage training of epoch 25: loss -6.11279 acc 0.66667 roc_auc 0.39830 prc_auc 0.61851[0m
[93maverage test of epoch 25: loss -6.17202 acc 0.65789 roc_auc 0.86462 prc_auc 0.93440[0m
[92maverage training of epoch 26: loss -6.28597 acc 0.66667 roc_auc 0.39170 prc_auc 0.61346[0m
[93maverage test of epoch 26: loss -6.34425 acc 0.65789 roc_auc 0.86462 prc_auc 0.93472[0m
[92maverage training of epoch 27: loss -6.45821 acc 0.66667 roc_auc 0.38840 prc_auc 0.60769[0m
[93maverage test of epoch 27: loss -6.51560 acc 0.65789 roc_auc 0.86154 prc_auc 0.92805[0m
[92maverage training of epoch 28: loss -6.62964 acc 0.66667 roc_auc 0.38450 prc_auc 0.60378[0m
[93maverage test of epoch 28: loss -6.68619 acc 0.65789 roc_auc 0.86000 prc_auc 0.92022[0m
[92maverage training of epoch 29: loss -6.80036 acc 0.66667 roc_auc 0.37990 prc_auc 0.59827[0m
[93maverage test of epoch 29: loss -6.85613 acc 0.65789 roc_auc 0.85538 prc_auc 0.91380[0m
[92maverage training of epoch 30: loss -6.97047 acc 0.66667 roc_auc 0.37720 prc_auc 0.59552[0m
[93maverage test of epoch 30: loss -7.02550 acc 0.65789 roc_auc 0.85077 prc_auc 0.90343[0m
[92maverage training of epoch 31: loss -7.14005 acc 0.66667 roc_auc 0.37370 prc_auc 0.59324[0m
[93maverage test of epoch 31: loss -7.19437 acc 0.65789 roc_auc 0.86615 prc_auc 0.93259[0m
[92maverage training of epoch 32: loss -7.30916 acc 0.66667 roc_auc 0.37300 prc_auc 0.59341[0m
[93maverage test of epoch 32: loss -7.36281 acc 0.65789 roc_auc 0.84923 prc_auc 0.90247[0m
[92maverage training of epoch 33: loss -7.47787 acc 0.66667 roc_auc 0.37140 prc_auc 0.59205[0m
[93maverage test of epoch 33: loss -7.53087 acc 0.65789 roc_auc 0.86462 prc_auc 0.92295[0m
[92maverage training of epoch 34: loss -7.64622 acc 0.66667 roc_auc 0.37030 prc_auc 0.58929[0m
[93maverage test of epoch 34: loss -7.69860 acc 0.65789 roc_auc 0.85846 prc_auc 0.91081[0m
[92maverage training of epoch 35: loss -7.81426 acc 0.66667 roc_auc 0.36910 prc_auc 0.58809[0m
[93maverage test of epoch 35: loss -7.86603 acc 0.65789 roc_auc 0.86769 prc_auc 0.93318[0m
[92maverage training of epoch 36: loss -7.98203 acc 0.66667 roc_auc 0.36820 prc_auc 0.58690[0m
[93maverage test of epoch 36: loss -8.03321 acc 0.65789 roc_auc 0.84923 prc_auc 0.90509[0m
[92maverage training of epoch 37: loss -8.14957 acc 0.66667 roc_auc 0.36760 prc_auc 0.58605[0m
[93maverage test of epoch 37: loss -8.20017 acc 0.65789 roc_auc 0.85385 prc_auc 0.90633[0m
[92maverage training of epoch 38: loss -8.31689 acc 0.66667 roc_auc 0.36760 prc_auc 0.58605[0m
[93maverage test of epoch 38: loss -8.36692 acc 0.65789 roc_auc 0.85692 prc_auc 0.91494[0m
[92maverage training of epoch 39: loss -8.48402 acc 0.66667 roc_auc 0.36700 prc_auc 0.58515[0m
[93maverage test of epoch 39: loss -8.53351 acc 0.65789 roc_auc 0.84769 prc_auc 0.89843[0m
[92maverage training of epoch 40: loss -8.65099 acc 0.66667 roc_auc 0.36610 prc_auc 0.58399[0m
[93maverage test of epoch 40: loss -8.69994 acc 0.65789 roc_auc 0.86000 prc_auc 0.91322[0m
[92maverage training of epoch 41: loss -8.81782 acc 0.66667 roc_auc 0.36570 prc_auc 0.58351[0m
[93maverage test of epoch 41: loss -8.86623 acc 0.65789 roc_auc 0.85231 prc_auc 0.90738[0m
[92maverage training of epoch 42: loss -8.98453 acc 0.66667 roc_auc 0.36550 prc_auc 0.58398[0m
[93maverage test of epoch 42: loss -9.03241 acc 0.65789 roc_auc 0.84769 prc_auc 0.90261[0m
[92maverage training of epoch 43: loss -9.15112 acc 0.66667 roc_auc 0.36380 prc_auc 0.56868[0m
[93maverage test of epoch 43: loss -9.19848 acc 0.65789 roc_auc 0.83077 prc_auc 0.88460[0m
[92maverage training of epoch 44: loss -9.31761 acc 0.66667 roc_auc 0.36150 prc_auc 0.56538[0m
[93maverage test of epoch 44: loss -9.36446 acc 0.65789 roc_auc 0.85692 prc_auc 0.91315[0m
[92maverage training of epoch 45: loss -9.48402 acc 0.66667 roc_auc 0.36050 prc_auc 0.56456[0m
[93maverage test of epoch 45: loss -9.53035 acc 0.65789 roc_auc 0.79846 prc_auc 0.85265[0m
[92maverage training of epoch 46: loss -9.65034 acc 0.66667 roc_auc 0.35980 prc_auc 0.56383[0m
[93maverage test of epoch 46: loss -9.69618 acc 0.65789 roc_auc 0.80154 prc_auc 0.84655[0m
[92maverage training of epoch 47: loss -9.81660 acc 0.66667 roc_auc 0.35920 prc_auc 0.56326[0m
[93maverage test of epoch 47: loss -9.86193 acc 0.65789 roc_auc 0.86769 prc_auc 0.88460[0m
[92maverage training of epoch 48: loss -9.98280 acc 0.66667 roc_auc 0.35880 prc_auc 0.56334[0m
[93maverage test of epoch 48: loss -10.02764 acc 0.65789 roc_auc 0.65692 prc_auc 0.73811[0m
[92maverage training of epoch 49: loss -10.14895 acc 0.66667 roc_auc 0.35890 prc_auc 0.56381[0m
[93maverage test of epoch 49: loss -10.19329 acc 0.65789 roc_auc 0.81077 prc_auc 0.84311[0m
Training model with dataset, testing using fold 1
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss 0.25330 acc 0.33333 roc_auc 0.57820 prc_auc 0.73535[0m
[93maverage test of epoch 0: loss 0.10208 acc 0.34211 roc_auc 0.72308 prc_auc 0.87159[0m
[92maverage training of epoch 1: loss -0.03479 acc 0.33333 roc_auc 0.54780 prc_auc 0.70513[0m
[93maverage test of epoch 1: loss -0.20896 acc 0.34211 roc_auc 0.24615 prc_auc 0.58395[0m
[92maverage training of epoch 2: loss -0.38432 acc 0.33333 roc_auc 0.47740 prc_auc 0.63825[0m
[93maverage test of epoch 2: loss -0.62911 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 3: loss -0.92459 acc 0.33333 roc_auc 0.44020 prc_auc 0.61732[0m
[93maverage test of epoch 3: loss -1.29161 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 4: loss -1.51519 acc 0.33333 roc_auc 0.35720 prc_auc 0.57404[0m
[93maverage test of epoch 4: loss -1.75346 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 5: loss -1.89108 acc 0.33333 roc_auc 0.18420 prc_auc 0.49693[0m
[93maverage test of epoch 5: loss -2.07375 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 6: loss -2.18484 acc 0.33333 roc_auc 0.16720 prc_auc 0.49416[0m
[93maverage test of epoch 6: loss -2.34768 acc 0.34211 roc_auc 0.11692 prc_auc 0.48512[0m
[92maverage training of epoch 7: loss -2.44368 acc 0.33333 roc_auc 0.26480 prc_auc 0.53204[0m
[93maverage test of epoch 7: loss -2.59462 acc 0.34211 roc_auc 0.11692 prc_auc 0.48512[0m
[92maverage training of epoch 8: loss -2.68000 acc 0.33333 roc_auc 0.32160 prc_auc 0.55384[0m
[93maverage test of epoch 8: loss -2.82208 acc 0.34211 roc_auc 0.11692 prc_auc 0.48512[0m
[92maverage training of epoch 9: loss -2.89933 acc 0.33333 roc_auc 0.35600 prc_auc 0.56652[0m
[93maverage test of epoch 9: loss -3.03407 acc 0.34211 roc_auc 0.11385 prc_auc 0.48477[0m
[92maverage training of epoch 10: loss -3.10490 acc 0.33333 roc_auc 0.37420 prc_auc 0.57403[0m
[93maverage test of epoch 10: loss -3.23325 acc 0.34211 roc_auc 0.12308 prc_auc 0.48679[0m
[92maverage training of epoch 11: loss -3.29895 acc 0.33333 roc_auc 0.38400 prc_auc 0.57783[0m
[93maverage test of epoch 11: loss -3.42164 acc 0.34211 roc_auc 0.12923 prc_auc 0.48859[0m
[92maverage training of epoch 12: loss -3.48328 acc 0.33333 roc_auc 0.39300 prc_auc 0.58090[0m
[93maverage test of epoch 12: loss -3.60100 acc 0.34211 roc_auc 0.13846 prc_auc 0.49031[0m
[92maverage training of epoch 13: loss -3.65950 acc 0.33333 roc_auc 0.39860 prc_auc 0.58270[0m
[93maverage test of epoch 13: loss -3.77289 acc 0.34211 roc_auc 0.14308 prc_auc 0.49170[0m
[92maverage training of epoch 14: loss -3.82904 acc 0.33333 roc_auc 0.40320 prc_auc 0.58452[0m
[93maverage test of epoch 14: loss -3.93874 acc 0.34211 roc_auc 0.16923 prc_auc 0.52970[0m
[92maverage training of epoch 15: loss -3.99327 acc 0.33333 roc_auc 0.41340 prc_auc 0.60262[0m
[93maverage test of epoch 15: loss -4.09997 acc 0.34211 roc_auc 0.21231 prc_auc 0.56552[0m
[92maverage training of epoch 16: loss -4.15358 acc 0.33333 roc_auc 0.42020 prc_auc 0.61275[0m
[93maverage test of epoch 16: loss -4.25809 acc 0.34211 roc_auc 0.26154 prc_auc 0.60112[0m
[92maverage training of epoch 17: loss -4.31160 acc 0.33333 roc_auc 0.42760 prc_auc 0.61857[0m
[93maverage test of epoch 17: loss -4.41512 acc 0.34211 roc_auc 0.27538 prc_auc 0.60339[0m
[92maverage training of epoch 18: loss -4.46979 acc 0.33333 roc_auc 0.43460 prc_auc 0.62570[0m
[93maverage test of epoch 18: loss -4.57447 acc 0.34211 roc_auc 0.31385 prc_auc 0.63487[0m
[92maverage training of epoch 19: loss -4.63276 acc 0.33333 roc_auc 0.44420 prc_auc 0.63460[0m
[93maverage test of epoch 19: loss -4.74316 acc 0.34211 roc_auc 0.34615 prc_auc 0.64814[0m
[92maverage training of epoch 20: loss -4.81106 acc 0.33333 roc_auc 0.45660 prc_auc 0.64543[0m
[93maverage test of epoch 20: loss -4.93848 acc 0.34211 roc_auc 0.49077 prc_auc 0.75768[0m
[92maverage training of epoch 21: loss -5.03294 acc 0.33333 roc_auc 0.47500 prc_auc 0.66400[0m
[93maverage test of epoch 21: loss -5.20558 acc 0.34211 roc_auc 0.75385 prc_auc 0.89532[0m
[92maverage training of epoch 22: loss -5.34667 acc 0.33333 roc_auc 0.55250 prc_auc 0.74674[0m
[93maverage test of epoch 22: loss -5.52378 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 23: loss -5.62578 acc 0.33333 roc_auc 0.59940 prc_auc 0.76339[0m
[93maverage test of epoch 23: loss -5.75334 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 24: loss -5.84501 acc 0.33333 roc_auc 0.56280 prc_auc 0.73486[0m
[93maverage test of epoch 24: loss -5.95972 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 25: loss -6.04719 acc 0.33333 roc_auc 0.53680 prc_auc 0.71404[0m
[93maverage test of epoch 25: loss -6.15527 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 26: loss -6.24031 acc 0.33333 roc_auc 0.51340 prc_auc 0.69500[0m
[93maverage test of epoch 26: loss -6.34399 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 27: loss -6.42752 acc 0.33333 roc_auc 0.49600 prc_auc 0.68003[0m
[93maverage test of epoch 27: loss -6.52796 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 28: loss -6.61053 acc 0.33333 roc_auc 0.48200 prc_auc 0.66656[0m
[93maverage test of epoch 28: loss -6.70845 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 29: loss -6.79041 acc 0.33333 roc_auc 0.46880 prc_auc 0.65131[0m
[93maverage test of epoch 29: loss -6.88628 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 30: loss -6.96787 acc 0.33333 roc_auc 0.45840 prc_auc 0.64372[0m
[93maverage test of epoch 30: loss -7.06203 acc 0.34211 roc_auc 0.88615 prc_auc 0.93236[0m
[92maverage training of epoch 31: loss -7.14342 acc 0.33333 roc_auc 0.45080 prc_auc 0.63541[0m
[93maverage test of epoch 31: loss -7.23611 acc 0.34211 roc_auc 0.88615 prc_auc 0.93236[0m
[92maverage training of epoch 32: loss -7.31743 acc 0.33333 roc_auc 0.44500 prc_auc 0.63092[0m
[93maverage test of epoch 32: loss -7.40883 acc 0.34211 roc_auc 0.88615 prc_auc 0.93403[0m
[92maverage training of epoch 33: loss -7.49019 acc 0.33333 roc_auc 0.44000 prc_auc 0.62636[0m
[93maverage test of epoch 33: loss -7.58045 acc 0.34211 roc_auc 0.88462 prc_auc 0.93091[0m
[92maverage training of epoch 34: loss -7.66192 acc 0.33333 roc_auc 0.43660 prc_auc 0.62290[0m
[93maverage test of epoch 34: loss -7.75114 acc 0.34211 roc_auc 0.88615 prc_auc 0.93288[0m
[92maverage training of epoch 35: loss -7.83278 acc 0.33333 roc_auc 0.43260 prc_auc 0.62067[0m
[93maverage test of epoch 35: loss -7.92106 acc 0.34211 roc_auc 0.88615 prc_auc 0.93262[0m
[92maverage training of epoch 36: loss -8.00293 acc 0.33333 roc_auc 0.43000 prc_auc 0.61897[0m
[93maverage test of epoch 36: loss -8.09034 acc 0.34211 roc_auc 0.87077 prc_auc 0.90572[0m
[92maverage training of epoch 37: loss -8.17247 acc 0.33333 roc_auc 0.42900 prc_auc 0.61871[0m
[93maverage test of epoch 37: loss -8.25906 acc 0.34211 roc_auc 0.87385 prc_auc 0.90596[0m
[92maverage training of epoch 38: loss -8.34149 acc 0.40000 roc_auc 0.42760 prc_auc 0.61806[0m
[93maverage test of epoch 38: loss -8.42732 acc 0.65789 roc_auc 0.86000 prc_auc 0.89709[0m
[92maverage training of epoch 39: loss -8.51007 acc 0.66667 roc_auc 0.42600 prc_auc 0.61747[0m
[93maverage test of epoch 39: loss -8.59518 acc 0.65789 roc_auc 0.86923 prc_auc 0.90021[0m
[92maverage training of epoch 40: loss -8.67827 acc 0.66667 roc_auc 0.42500 prc_auc 0.61682[0m
[93maverage test of epoch 40: loss -8.76270 acc 0.65789 roc_auc 0.85846 prc_auc 0.89199[0m
[92maverage training of epoch 41: loss -8.84615 acc 0.66667 roc_auc 0.42440 prc_auc 0.61662[0m
[93maverage test of epoch 41: loss -8.92992 acc 0.65789 roc_auc 0.84615 prc_auc 0.87049[0m
[92maverage training of epoch 42: loss -9.01375 acc 0.66667 roc_auc 0.42400 prc_auc 0.61653[0m
[93maverage test of epoch 42: loss -9.09690 acc 0.65789 roc_auc 0.79692 prc_auc 0.84841[0m
[92maverage training of epoch 43: loss -9.18112 acc 0.66667 roc_auc 0.42340 prc_auc 0.61137[0m
[93maverage test of epoch 43: loss -9.26365 acc 0.65789 roc_auc 0.87077 prc_auc 0.90063[0m
[92maverage training of epoch 44: loss -9.34828 acc 0.66667 roc_auc 0.42320 prc_auc 0.61131[0m
[93maverage test of epoch 44: loss -9.43021 acc 0.65789 roc_auc 0.88000 prc_auc 0.90513[0m
[92maverage training of epoch 45: loss -9.51525 acc 0.66667 roc_auc 0.42300 prc_auc 0.61127[0m
[93maverage test of epoch 45: loss -9.59661 acc 0.65789 roc_auc 0.87385 prc_auc 0.90318[0m
[92maverage training of epoch 46: loss -9.68207 acc 0.66667 roc_auc 0.42280 prc_auc 0.61121[0m
[93maverage test of epoch 46: loss -9.76287 acc 0.65789 roc_auc 0.79385 prc_auc 0.83556[0m
[92maverage training of epoch 47: loss -9.84876 acc 0.66667 roc_auc 0.42260 prc_auc 0.61116[0m
[93maverage test of epoch 47: loss -9.92900 acc 0.65789 roc_auc 0.82462 prc_auc 0.84843[0m
[92maverage training of epoch 48: loss -10.01533 acc 0.66667 roc_auc 0.42240 prc_auc 0.60983[0m
[93maverage test of epoch 48: loss -10.09503 acc 0.65789 roc_auc 0.83385 prc_auc 0.85841[0m
[92maverage training of epoch 49: loss -10.18179 acc 0.66667 roc_auc 0.42220 prc_auc 0.60979[0m
[93maverage test of epoch 49: loss -10.26096 acc 0.65789 roc_auc 0.82769 prc_auc 0.85119[0m
Training model with dataset, testing using fold 2
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss -0.78870 acc 0.33333 roc_auc 0.43600 prc_auc 0.63362[0m
[93maverage test of epoch 0: loss -1.00518 acc 0.34211 roc_auc 0.07385 prc_auc 0.47052[0m
[92maverage training of epoch 1: loss -1.19566 acc 0.37333 roc_auc 0.44440 prc_auc 0.64905[0m
[93maverage test of epoch 1: loss -1.39538 acc 0.65789 roc_auc 0.19077 prc_auc 0.54431[0m
[92maverage training of epoch 2: loss -1.58875 acc 0.66667 roc_auc 0.45240 prc_auc 0.65551[0m
[93maverage test of epoch 2: loss -1.79232 acc 0.65789 roc_auc 0.87077 prc_auc 0.94930[0m
[92maverage training of epoch 3: loss -2.06153 acc 0.66667 roc_auc 0.45620 prc_auc 0.66417[0m
[93maverage test of epoch 3: loss -2.38338 acc 0.65789 roc_auc 0.95538 prc_auc 0.98000[0m
[92maverage training of epoch 4: loss -2.70843 acc 0.66667 roc_auc 0.49800 prc_auc 0.69713[0m
[93maverage test of epoch 4: loss -2.93070 acc 0.65789 roc_auc 0.95692 prc_auc 0.98069[0m
[92maverage training of epoch 5: loss -3.11530 acc 0.66667 roc_auc 0.46320 prc_auc 0.66449[0m
[93maverage test of epoch 5: loss -3.25389 acc 0.65789 roc_auc 0.95692 prc_auc 0.98069[0m
[92maverage training of epoch 6: loss -3.40896 acc 0.66667 roc_auc 0.43280 prc_auc 0.64361[0m
[93maverage test of epoch 6: loss -3.52209 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 7: loss -3.66117 acc 0.66667 roc_auc 0.42380 prc_auc 0.64568[0m
[93maverage test of epoch 7: loss -3.75948 acc 0.65789 roc_auc 0.94462 prc_auc 0.97443[0m
[92maverage training of epoch 8: loss -3.88847 acc 0.66667 roc_auc 0.41280 prc_auc 0.63631[0m
[93maverage test of epoch 8: loss -3.97746 acc 0.65789 roc_auc 0.93846 prc_auc 0.97198[0m
[92maverage training of epoch 9: loss -4.09999 acc 0.66667 roc_auc 0.40420 prc_auc 0.62901[0m
[93maverage test of epoch 9: loss -4.18316 acc 0.65789 roc_auc 0.92154 prc_auc 0.96274[0m
[92maverage training of epoch 10: loss -4.30164 acc 0.66667 roc_auc 0.40360 prc_auc 0.62579[0m
[93maverage test of epoch 10: loss -4.38131 acc 0.65789 roc_auc 0.89231 prc_auc 0.95253[0m
[92maverage training of epoch 11: loss -4.49733 acc 0.66667 roc_auc 0.40420 prc_auc 0.62485[0m
[93maverage test of epoch 11: loss -4.57505 acc 0.65789 roc_auc 0.86462 prc_auc 0.94107[0m
[92maverage training of epoch 12: loss -4.68965 acc 0.66667 roc_auc 0.40620 prc_auc 0.62665[0m
[93maverage test of epoch 12: loss -4.76637 acc 0.65789 roc_auc 0.84923 prc_auc 0.93443[0m
[92maverage training of epoch 13: loss -4.88013 acc 0.66667 roc_auc 0.40580 prc_auc 0.62655[0m
[93maverage test of epoch 13: loss -4.95631 acc 0.65789 roc_auc 0.82154 prc_auc 0.92647[0m
[92maverage training of epoch 14: loss -5.06948 acc 0.66667 roc_auc 0.40240 prc_auc 0.62463[0m
[93maverage test of epoch 14: loss -5.14522 acc 0.65789 roc_auc 0.79846 prc_auc 0.91892[0m
[92maverage training of epoch 15: loss -5.25785 acc 0.66667 roc_auc 0.39880 prc_auc 0.62201[0m
[93maverage test of epoch 15: loss -5.33303 acc 0.65789 roc_auc 0.78154 prc_auc 0.90989[0m
[92maverage training of epoch 16: loss -5.44506 acc 0.66667 roc_auc 0.39530 prc_auc 0.61793[0m
[93maverage test of epoch 16: loss -5.51948 acc 0.65789 roc_auc 0.77077 prc_auc 0.90685[0m
[92maverage training of epoch 17: loss -5.63088 acc 0.66667 roc_auc 0.39360 prc_auc 0.61719[0m
[93maverage test of epoch 17: loss -5.70432 acc 0.65789 roc_auc 0.74923 prc_auc 0.89912[0m
[92maverage training of epoch 18: loss -5.81508 acc 0.66667 roc_auc 0.39460 prc_auc 0.61419[0m
[93maverage test of epoch 18: loss -5.88741 acc 0.65789 roc_auc 0.72000 prc_auc 0.88533[0m
[92maverage training of epoch 19: loss -5.99757 acc 0.66667 roc_auc 0.39600 prc_auc 0.61334[0m
[93maverage test of epoch 19: loss -6.06871 acc 0.65789 roc_auc 0.71385 prc_auc 0.88371[0m
[92maverage training of epoch 20: loss -6.17833 acc 0.66667 roc_auc 0.39240 prc_auc 0.60728[0m
[93maverage test of epoch 20: loss -6.24826 acc 0.65789 roc_auc 0.69231 prc_auc 0.86667[0m
[92maverage training of epoch 21: loss -6.35744 acc 0.66667 roc_auc 0.38700 prc_auc 0.59583[0m
[93maverage test of epoch 21: loss -6.42617 acc 0.65789 roc_auc 0.66769 prc_auc 0.84464[0m
[92maverage training of epoch 22: loss -6.53501 acc 0.66667 roc_auc 0.38360 prc_auc 0.59360[0m
[93maverage test of epoch 22: loss -6.60259 acc 0.65789 roc_auc 0.65077 prc_auc 0.82037[0m
[92maverage training of epoch 23: loss -6.71116 acc 0.66667 roc_auc 0.38340 prc_auc 0.59292[0m
[93maverage test of epoch 23: loss -6.77766 acc 0.65789 roc_auc 0.68308 prc_auc 0.84422[0m
[92maverage training of epoch 24: loss -6.88606 acc 0.66667 roc_auc 0.38480 prc_auc 0.59343[0m
[93maverage test of epoch 24: loss -6.95153 acc 0.65789 roc_auc 0.53692 prc_auc 0.73407[0m
[92maverage training of epoch 25: loss -7.05984 acc 0.66667 roc_auc 0.38300 prc_auc 0.59181[0m
[93maverage test of epoch 25: loss -7.12434 acc 0.65789 roc_auc 0.45231 prc_auc 0.64949[0m
[92maverage training of epoch 26: loss -7.23263 acc 0.66667 roc_auc 0.38330 prc_auc 0.59215[0m
[93maverage test of epoch 26: loss -7.29622 acc 0.65789 roc_auc 0.46462 prc_auc 0.67378[0m
[92maverage training of epoch 27: loss -7.40456 acc 0.66667 roc_auc 0.38260 prc_auc 0.59184[0m
[93maverage test of epoch 27: loss -7.46729 acc 0.65789 roc_auc 0.50000 prc_auc 0.69287[0m
[92maverage training of epoch 28: loss -7.57572 acc 0.66667 roc_auc 0.38180 prc_auc 0.58511[0m
[93maverage test of epoch 28: loss -7.63764 acc 0.65789 roc_auc 0.39077 prc_auc 0.61194[0m
[92maverage training of epoch 29: loss -7.74621 acc 0.66667 roc_auc 0.38000 prc_auc 0.57668[0m
[93maverage test of epoch 29: loss -7.80737 acc 0.65789 roc_auc 0.38000 prc_auc 0.61905[0m
[92maverage training of epoch 30: loss -7.91613 acc 0.66667 roc_auc 0.37960 prc_auc 0.57513[0m
[93maverage test of epoch 30: loss -7.97655 acc 0.65789 roc_auc 0.38000 prc_auc 0.60730[0m
[92maverage training of epoch 31: loss -8.08553 acc 0.66667 roc_auc 0.37810 prc_auc 0.57347[0m
[93maverage test of epoch 31: loss -8.14526 acc 0.65789 roc_auc 0.35846 prc_auc 0.61596[0m
[92maverage training of epoch 32: loss -8.25449 acc 0.66667 roc_auc 0.37750 prc_auc 0.57346[0m
[93maverage test of epoch 32: loss -8.31356 acc 0.65789 roc_auc 0.37385 prc_auc 0.60598[0m
[92maverage training of epoch 33: loss -8.42306 acc 0.66667 roc_auc 0.37740 prc_auc 0.57367[0m
[93maverage test of epoch 33: loss -8.48148 acc 0.65789 roc_auc 0.36000 prc_auc 0.60228[0m
[92maverage training of epoch 34: loss -8.59130 acc 0.66667 roc_auc 0.37780 prc_auc 0.57558[0m
[93maverage test of epoch 34: loss -8.64909 acc 0.65789 roc_auc 0.42000 prc_auc 0.62409[0m
[92maverage training of epoch 35: loss -8.75923 acc 0.66667 roc_auc 0.37750 prc_auc 0.57518[0m
[93maverage test of epoch 35: loss -8.81642 acc 0.65789 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 36: loss -8.92690 acc 0.66667 roc_auc 0.37760 prc_auc 0.57537[0m
[93maverage test of epoch 36: loss -8.98350 acc 0.65789 roc_auc 0.34000 prc_auc 0.59586[0m
[92maverage training of epoch 37: loss -9.09434 acc 0.66667 roc_auc 0.37760 prc_auc 0.57537[0m
[93maverage test of epoch 37: loss -9.15037 acc 0.65789 roc_auc 0.44462 prc_auc 0.63895[0m
[92maverage training of epoch 38: loss -9.26157 acc 0.66667 roc_auc 0.37730 prc_auc 0.57517[0m
[93maverage test of epoch 38: loss -9.31704 acc 0.65789 roc_auc 0.42000 prc_auc 0.62409[0m
[92maverage training of epoch 39: loss -9.42863 acc 0.66667 roc_auc 0.37690 prc_auc 0.57485[0m
[93maverage test of epoch 39: loss -9.48355 acc 0.65789 roc_auc 0.56000 prc_auc 0.69895[0m
[92maverage training of epoch 40: loss -9.59554 acc 0.66667 roc_auc 0.37690 prc_auc 0.57496[0m
[93maverage test of epoch 40: loss -9.64992 acc 0.65789 roc_auc 0.48769 prc_auc 0.65248[0m
[92maverage training of epoch 41: loss -9.76230 acc 0.66667 roc_auc 0.37690 prc_auc 0.57496[0m
[93maverage test of epoch 41: loss -9.81615 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -9.92895 acc 0.66667 roc_auc 0.37680 prc_auc 0.57458[0m
[93maverage test of epoch 42: loss -9.98228 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -10.09549 acc 0.66667 roc_auc 0.37670 prc_auc 0.57460[0m
[93maverage test of epoch 43: loss -10.14830 acc 0.65789 roc_auc 0.38000 prc_auc 0.60914[0m
[92maverage training of epoch 44: loss -10.26193 acc 0.66667 roc_auc 0.37670 prc_auc 0.57460[0m
[93maverage test of epoch 44: loss -10.31423 acc 0.65789 roc_auc 0.66000 prc_auc 0.76737[0m
[92maverage training of epoch 45: loss -10.42829 acc 0.66667 roc_auc 0.37670 prc_auc 0.57458[0m
[93maverage test of epoch 45: loss -10.48008 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -10.59458 acc 0.66667 roc_auc 0.37660 prc_auc 0.57460[0m
[93maverage test of epoch 46: loss -10.64587 acc 0.65789 roc_auc 0.72000 prc_auc 0.80842[0m
[92maverage training of epoch 47: loss -10.76081 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 47: loss -10.81159 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -10.92697 acc 0.66667 roc_auc 0.37670 prc_auc 0.57442[0m
[93maverage test of epoch 48: loss -10.97726 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -11.09309 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 49: loss -11.14288 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 3
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss -0.14893 acc 0.33775 roc_auc 0.41392 prc_auc 0.59278[0m
[93maverage test of epoch 0: loss -0.23816 acc 0.32432 roc_auc 0.10167 prc_auc 0.49676[0m
[92maverage training of epoch 1: loss -0.36725 acc 0.33775 roc_auc 0.44686 prc_auc 0.62361[0m
[93maverage test of epoch 1: loss -0.46140 acc 0.32432 roc_auc 0.23667 prc_auc 0.59860[0m
[92maverage training of epoch 2: loss -0.59604 acc 0.33775 roc_auc 0.50059 prc_auc 0.66061[0m
[93maverage test of epoch 2: loss -0.69007 acc 0.32432 roc_auc 0.38333 prc_auc 0.69233[0m
[92maverage training of epoch 3: loss -0.82870 acc 0.33775 roc_auc 0.55431 prc_auc 0.70705[0m
[93maverage test of epoch 3: loss -0.92118 acc 0.32432 roc_auc 0.63667 prc_auc 0.82797[0m
[92maverage training of epoch 4: loss -1.06569 acc 0.33775 roc_auc 0.60000 prc_auc 0.75252[0m
[93maverage test of epoch 4: loss -1.15886 acc 0.32432 roc_auc 0.82333 prc_auc 0.90243[0m
[92maverage training of epoch 5: loss -1.31221 acc 0.33775 roc_auc 0.63647 prc_auc 0.80104[0m
[93maverage test of epoch 5: loss -1.41087 acc 0.32432 roc_auc 0.84833 prc_auc 0.91092[0m
[92maverage training of epoch 6: loss -1.58135 acc 0.33775 roc_auc 0.67569 prc_auc 0.83109[0m
[93maverage test of epoch 6: loss -1.69654 acc 0.32432 roc_auc 0.85000 prc_auc 0.91183[0m
[92maverage training of epoch 7: loss -1.87901 acc 0.33775 roc_auc 0.73412 prc_auc 0.86685[0m
[93maverage test of epoch 7: loss -1.99412 acc 0.32432 roc_auc 0.84667 prc_auc 0.91099[0m
[92maverage training of epoch 8: loss -2.16423 acc 0.33775 roc_auc 0.75039 prc_auc 0.88463[0m
[93maverage test of epoch 8: loss -2.26401 acc 0.32432 roc_auc 0.84667 prc_auc 0.91092[0m
[92maverage training of epoch 9: loss -2.42123 acc 0.33775 roc_auc 0.68216 prc_auc 0.84592[0m
[93maverage test of epoch 9: loss -2.50845 acc 0.32432 roc_auc 0.84333 prc_auc 0.90866[0m
[92maverage training of epoch 10: loss -2.65652 acc 0.33775 roc_auc 0.57902 prc_auc 0.78201[0m
[93maverage test of epoch 10: loss -2.73514 acc 0.32432 roc_auc 0.85000 prc_auc 0.91125[0m
[92maverage training of epoch 11: loss -2.87613 acc 0.33775 roc_auc 0.47824 prc_auc 0.69589[0m
[93maverage test of epoch 11: loss -2.94883 acc 0.32432 roc_auc 0.70667 prc_auc 0.84243[0m
[92maverage training of epoch 12: loss -3.08440 acc 0.33775 roc_auc 0.40098 prc_auc 0.61374[0m
[93maverage test of epoch 12: loss -3.15299 acc 0.32432 roc_auc 0.53667 prc_auc 0.77245[0m
[92maverage training of epoch 13: loss -3.28439 acc 0.33775 roc_auc 0.37863 prc_auc 0.58636[0m
[93maverage test of epoch 13: loss -3.35011 acc 0.32432 roc_auc 0.40667 prc_auc 0.69478[0m
[92maverage training of epoch 14: loss -3.47819 acc 0.33775 roc_auc 0.37275 prc_auc 0.58089[0m
[93maverage test of epoch 14: loss -3.54190 acc 0.32432 roc_auc 0.27667 prc_auc 0.61834[0m
[92maverage training of epoch 15: loss -3.66724 acc 0.33775 roc_auc 0.37137 prc_auc 0.57290[0m
[93maverage test of epoch 15: loss -3.72953 acc 0.32432 roc_auc 0.19667 prc_auc 0.56945[0m
[92maverage training of epoch 16: loss -3.85256 acc 0.33775 roc_auc 0.37000 prc_auc 0.56962[0m
[93maverage test of epoch 16: loss -3.91386 acc 0.32432 roc_auc 0.15667 prc_auc 0.52329[0m
[92maverage training of epoch 17: loss -4.03489 acc 0.33775 roc_auc 0.36706 prc_auc 0.56419[0m
[93maverage test of epoch 17: loss -4.09552 acc 0.32432 roc_auc 0.12000 prc_auc 0.50330[0m
[92maverage training of epoch 18: loss -4.21478 acc 0.33775 roc_auc 0.36784 prc_auc 0.56469[0m
[93maverage test of epoch 18: loss -4.27500 acc 0.32432 roc_auc 0.10333 prc_auc 0.49893[0m
[92maverage training of epoch 19: loss -4.39267 acc 0.33775 roc_auc 0.36863 prc_auc 0.56464[0m
[93maverage test of epoch 19: loss -4.45268 acc 0.32432 roc_auc 0.09167 prc_auc 0.49547[0m
[92maverage training of epoch 20: loss -4.56891 acc 0.33775 roc_auc 0.37029 prc_auc 0.56581[0m
[93maverage test of epoch 20: loss -4.62884 acc 0.32432 roc_auc 0.09667 prc_auc 0.49947[0m
[92maverage training of epoch 21: loss -4.74375 acc 0.33775 roc_auc 0.37118 prc_auc 0.56626[0m
[93maverage test of epoch 21: loss -4.80374 acc 0.32432 roc_auc 0.09000 prc_auc 0.49387[0m
[92maverage training of epoch 22: loss -4.91743 acc 0.33775 roc_auc 0.37176 prc_auc 0.56665[0m
[93maverage test of epoch 22: loss -4.97758 acc 0.32432 roc_auc 0.08333 prc_auc 0.49789[0m
[92maverage training of epoch 23: loss -5.09012 acc 0.33775 roc_auc 0.37176 prc_auc 0.56646[0m
[93maverage test of epoch 23: loss -5.15050 acc 0.32432 roc_auc 0.08167 prc_auc 0.50284[0m
[92maverage training of epoch 24: loss -5.26196 acc 0.33775 roc_auc 0.37255 prc_auc 0.56702[0m
[93maverage test of epoch 24: loss -5.32266 acc 0.32432 roc_auc 0.08667 prc_auc 0.50010[0m
[92maverage training of epoch 25: loss -5.43309 acc 0.33775 roc_auc 0.37333 prc_auc 0.56758[0m
[93maverage test of epoch 25: loss -5.49415 acc 0.32432 roc_auc 0.08333 prc_auc 0.50527[0m
[92maverage training of epoch 26: loss -5.60360 acc 0.33775 roc_auc 0.37412 prc_auc 0.56838[0m
[93maverage test of epoch 26: loss -5.66507 acc 0.32432 roc_auc 0.08500 prc_auc 0.49953[0m
[92maverage training of epoch 27: loss -5.77358 acc 0.33775 roc_auc 0.37422 prc_auc 0.56841[0m
[93maverage test of epoch 27: loss -5.83551 acc 0.32432 roc_auc 0.09833 prc_auc 0.52162[0m
[92maverage training of epoch 28: loss -5.94310 acc 0.33775 roc_auc 0.37412 prc_auc 0.56847[0m
[93maverage test of epoch 28: loss -6.00552 acc 0.32432 roc_auc 0.09333 prc_auc 0.51589[0m
[92maverage training of epoch 29: loss -6.11223 acc 0.33775 roc_auc 0.37422 prc_auc 0.56826[0m
[93maverage test of epoch 29: loss -6.17518 acc 0.32432 roc_auc 0.08333 prc_auc 0.50858[0m
[92maverage training of epoch 30: loss -6.28102 acc 0.33775 roc_auc 0.37471 prc_auc 0.56866[0m
[93maverage test of epoch 30: loss -6.34451 acc 0.32432 roc_auc 0.10167 prc_auc 0.51462[0m
[92maverage training of epoch 31: loss -6.44952 acc 0.33775 roc_auc 0.37471 prc_auc 0.56868[0m
[93maverage test of epoch 31: loss -6.51357 acc 0.32432 roc_auc 0.09167 prc_auc 0.50871[0m
[92maverage training of epoch 32: loss -6.61776 acc 0.33775 roc_auc 0.37490 prc_auc 0.56898[0m
[93maverage test of epoch 32: loss -6.68240 acc 0.32432 roc_auc 0.11167 prc_auc 0.52143[0m
[92maverage training of epoch 33: loss -6.78578 acc 0.33775 roc_auc 0.37529 prc_auc 0.56942[0m
[93maverage test of epoch 33: loss -6.85102 acc 0.32432 roc_auc 0.09667 prc_auc 0.51960[0m
[92maverage training of epoch 34: loss -6.95361 acc 0.33775 roc_auc 0.37529 prc_auc 0.56942[0m
[93maverage test of epoch 34: loss -7.01946 acc 0.32432 roc_auc 0.11000 prc_auc 0.52275[0m
[92maverage training of epoch 35: loss -7.12126 acc 0.33775 roc_auc 0.37529 prc_auc 0.56942[0m
[93maverage test of epoch 35: loss -7.18775 acc 0.32432 roc_auc 0.10667 prc_auc 0.53618[0m
[92maverage training of epoch 36: loss -7.28877 acc 0.33775 roc_auc 0.37549 prc_auc 0.56951[0m
[93maverage test of epoch 36: loss -7.35589 acc 0.32432 roc_auc 0.14667 prc_auc 0.53791[0m
[92maverage training of epoch 37: loss -7.45616 acc 0.33775 roc_auc 0.37549 prc_auc 0.56951[0m
[93maverage test of epoch 37: loss -7.52392 acc 0.32432 roc_auc 0.15167 prc_auc 0.52960[0m
[92maverage training of epoch 38: loss -7.62343 acc 0.33775 roc_auc 0.37549 prc_auc 0.56951[0m
[93maverage test of epoch 38: loss -7.69185 acc 0.32432 roc_auc 0.16333 prc_auc 0.53901[0m
[92maverage training of epoch 39: loss -7.79060 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 39: loss -7.85968 acc 0.32432 roc_auc 0.14833 prc_auc 0.54509[0m
[92maverage training of epoch 40: loss -7.95768 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 40: loss -8.02743 acc 0.32432 roc_auc 0.22333 prc_auc 0.56021[0m
[92maverage training of epoch 41: loss -8.12469 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 41: loss -8.19511 acc 0.32432 roc_auc 0.22833 prc_auc 0.56335[0m
[92maverage training of epoch 42: loss -8.29164 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 42: loss -8.36273 acc 0.32432 roc_auc 0.26167 prc_auc 0.58598[0m
[92maverage training of epoch 43: loss -8.45853 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 43: loss -8.53030 acc 0.32432 roc_auc 0.25500 prc_auc 0.59794[0m
[92maverage training of epoch 44: loss -8.62536 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 44: loss -8.69782 acc 0.32432 roc_auc 0.33500 prc_auc 0.61112[0m
[92maverage training of epoch 45: loss -8.79215 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 45: loss -8.86530 acc 0.32432 roc_auc 0.24000 prc_auc 0.61369[0m
[92maverage training of epoch 46: loss -8.95890 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 46: loss -9.03274 acc 0.32432 roc_auc 0.28667 prc_auc 0.59655[0m
[92maverage training of epoch 47: loss -9.12562 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 47: loss -9.20014 acc 0.32432 roc_auc 0.52167 prc_auc 0.68637[0m
[92maverage training of epoch 48: loss -9.29231 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 48: loss -9.36752 acc 0.32432 roc_auc 0.58667 prc_auc 0.72188[0m
[92maverage training of epoch 49: loss -9.45896 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 49: loss -9.53488 acc 0.32432 roc_auc 0.38000 prc_auc 0.62284[0m
Training model with dataset, testing using fold 4
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss -0.14750 acc 0.66225 roc_auc 0.38549 prc_auc 0.59222[0m
[93maverage test of epoch 0: loss -0.42306 acc 0.67568 roc_auc 0.18000 prc_auc 0.52480[0m
[92maverage training of epoch 1: loss -0.68322 acc 0.66225 roc_auc 0.39490 prc_auc 0.60005[0m
[93maverage test of epoch 1: loss -1.05760 acc 0.67568 roc_auc 0.57333 prc_auc 0.80280[0m
[92maverage training of epoch 2: loss -1.26656 acc 0.66225 roc_auc 0.40098 prc_auc 0.60713[0m
[93maverage test of epoch 2: loss -1.54508 acc 0.67568 roc_auc 0.66000 prc_auc 0.85408[0m
[92maverage training of epoch 3: loss -1.67854 acc 0.66225 roc_auc 0.41863 prc_auc 0.62638[0m
[93maverage test of epoch 3: loss -1.90736 acc 0.67568 roc_auc 0.76000 prc_auc 0.90500[0m
[92maverage training of epoch 4: loss -2.03222 acc 0.66225 roc_auc 0.43529 prc_auc 0.64446[0m
[93maverage test of epoch 4: loss -2.26128 acc 0.67568 roc_auc 0.84000 prc_auc 0.93466[0m
[92maverage training of epoch 5: loss -2.38704 acc 0.66225 roc_auc 0.43696 prc_auc 0.64189[0m
[93maverage test of epoch 5: loss -2.61674 acc 0.67568 roc_auc 0.86000 prc_auc 0.94180[0m
[92maverage training of epoch 6: loss -2.72921 acc 0.66225 roc_auc 0.43049 prc_auc 0.64021[0m
[93maverage test of epoch 6: loss -2.94426 acc 0.67568 roc_auc 0.93333 prc_auc 0.97438[0m
[92maverage training of epoch 7: loss -3.03337 acc 0.66225 roc_auc 0.41520 prc_auc 0.62773[0mUsing backend: pytorch
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

[93maverage test of epoch 7: loss -3.22904 acc 0.67568 roc_auc 0.93000 prc_auc 0.97054[0m
[92maverage training of epoch 8: loss -3.29868 acc 0.66225 roc_auc 0.40725 prc_auc 0.62038[0m
[93maverage test of epoch 8: loss -3.48073 acc 0.67568 roc_auc 0.91500 prc_auc 0.96114[0m
[92maverage training of epoch 9: loss -3.53715 acc 0.66225 roc_auc 0.39951 prc_auc 0.61378[0m
[93maverage test of epoch 9: loss -3.71098 acc 0.67568 roc_auc 0.90333 prc_auc 0.95302[0m
[92maverage training of epoch 10: loss -3.75825 acc 0.66225 roc_auc 0.39284 prc_auc 0.60904[0m
[93maverage test of epoch 10: loss -3.92702 acc 0.67568 roc_auc 0.89333 prc_auc 0.94427[0m
[92maverage training of epoch 11: loss -3.96758 acc 0.66225 roc_auc 0.38725 prc_auc 0.60417[0m
[93maverage test of epoch 11: loss -4.13311 acc 0.67568 roc_auc 0.87667 prc_auc 0.93099[0m
[92maverage training of epoch 12: loss -4.16852 acc 0.66225 roc_auc 0.38451 prc_auc 0.60117[0m
[93maverage test of epoch 12: loss -4.33191 acc 0.67568 roc_auc 0.85333 prc_auc 0.91497[0m
[92maverage training of epoch 13: loss -4.36319 acc 0.66225 roc_auc 0.38304 prc_auc 0.60084[0m
[93maverage test of epoch 13: loss -4.52516 acc 0.67568 roc_auc 0.82333 prc_auc 0.89579[0m
[92maverage training of epoch 14: loss -4.55305 acc 0.66225 roc_auc 0.38314 prc_auc 0.60148[0m
[93maverage test of epoch 14: loss -4.71409 acc 0.67568 roc_auc 0.80000 prc_auc 0.88655[0m
[92maverage training of epoch 15: loss -4.73912 acc 0.66225 roc_auc 0.37765 prc_auc 0.58632[0m
[93maverage test of epoch 15: loss -4.89959 acc 0.67568 roc_auc 0.81333 prc_auc 0.88121[0m
[92maverage training of epoch 16: loss -4.92215 acc 0.66225 roc_auc 0.37627 prc_auc 0.58506[0m
[93maverage test of epoch 16: loss -5.08229 acc 0.67568 roc_auc 0.76667 prc_auc 0.85828[0m
[92maverage training of epoch 17: loss -5.10271 acc 0.66225 roc_auc 0.37627 prc_auc 0.58602[0m
[93maverage test of epoch 17: loss -5.26272 acc 0.67568 roc_auc 0.73000 prc_auc 0.83576[0m
[92maverage training of epoch 18: loss -5.28122 acc 0.66225 roc_auc 0.37696 prc_auc 0.58685[0m
[93maverage test of epoch 18: loss -5.44125 acc 0.67568 roc_auc 0.76000 prc_auc 0.83986[0m
[92maverage training of epoch 19: loss -5.45803 acc 0.66225 roc_auc 0.37765 prc_auc 0.58863[0m
[93maverage test of epoch 19: loss -5.61820 acc 0.67568 roc_auc 0.74000 prc_auc 0.83135[0m
[92maverage training of epoch 20: loss -5.63342 acc 0.66225 roc_auc 0.37716 prc_auc 0.58836[0m
[93maverage test of epoch 20: loss -5.79381 acc 0.67568 roc_auc 0.58667 prc_auc 0.71638[0m
[92maverage training of epoch 21: loss -5.80760 acc 0.66225 roc_auc 0.37686 prc_auc 0.58817[0m
[93maverage test of epoch 21: loss -5.96830 acc 0.67568 roc_auc 0.60667 prc_auc 0.72641[0m
[92maverage training of epoch 22: loss -5.98076 acc 0.66225 roc_auc 0.37755 prc_auc 0.58974[0m
[93maverage test of epoch 22: loss -6.14182 acc 0.67568 roc_auc 0.69333 prc_auc 0.77828[0m
[92maverage training of epoch 23: loss -6.15305 acc 0.66225 roc_auc 0.37794 prc_auc 0.58999[0m
[93maverage test of epoch 23: loss -6.31452 acc 0.67568 roc_auc 0.56500 prc_auc 0.70560[0m
[92maverage training of epoch 24: loss -6.32458 acc 0.66225 roc_auc 0.37843 prc_auc 0.59030[0m
[93maverage test of epoch 24: loss -6.48652 acc 0.67568 roc_auc 0.48000 prc_auc 0.66703[0m
[92maverage training of epoch 25: loss -6.49548 acc 0.66225 roc_auc 0.37804 prc_auc 0.58961[0m
[93maverage test of epoch 25: loss -6.65790 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 26: loss -6.66581 acc 0.66225 roc_auc 0.37853 prc_auc 0.59060[0m
[93maverage test of epoch 26: loss -6.82876 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 27: loss -6.83567 acc 0.66225 roc_auc 0.37833 prc_auc 0.59107[0m
[93maverage test of epoch 27: loss -6.99916 acc 0.67568 roc_auc 0.58667 prc_auc 0.71638[0m
[92maverage training of epoch 28: loss -7.00511 acc 0.66225 roc_auc 0.37804 prc_auc 0.59110[0m
[93maverage test of epoch 28: loss -7.16917 acc 0.67568 roc_auc 0.48000 prc_auc 0.66703[0m
[92maverage training of epoch 29: loss -7.17418 acc 0.66225 roc_auc 0.37784 prc_auc 0.59052[0m
[93maverage test of epoch 29: loss -7.33884 acc 0.67568 roc_auc 0.60667 prc_auc 0.72641[0m
[92maverage training of epoch 30: loss -7.34294 acc 0.66225 roc_auc 0.37775 prc_auc 0.59022[0m
[93maverage test of epoch 30: loss -7.50820 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 31: loss -7.51143 acc 0.66225 roc_auc 0.37735 prc_auc 0.58967[0m
[93maverage test of epoch 31: loss -7.67730 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 32: loss -7.67967 acc 0.66225 roc_auc 0.37647 prc_auc 0.58393[0m
[93maverage test of epoch 32: loss -7.84617 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 33: loss -7.84770 acc 0.66225 roc_auc 0.37618 prc_auc 0.58247[0m
[93maverage test of epoch 33: loss -8.01485 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 34: loss -8.01555 acc 0.66225 roc_auc 0.37627 prc_auc 0.58234[0m
[93maverage test of epoch 34: loss -8.18334 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 35: loss -8.18324 acc 0.66225 roc_auc 0.37539 prc_auc 0.57465[0m
[93maverage test of epoch 35: loss -8.35168 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 36: loss -8.35078 acc 0.66225 roc_auc 0.37451 prc_auc 0.57288[0m
[93maverage test of epoch 36: loss -8.51989 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 37: loss -8.51820 acc 0.66225 roc_auc 0.37333 prc_auc 0.57150[0m
[93maverage test of epoch 37: loss -8.68798 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 38: loss -8.68551 acc 0.66225 roc_auc 0.37343 prc_auc 0.57108[0m
[93maverage test of epoch 38: loss -8.85596 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 39: loss -8.85273 acc 0.66225 roc_auc 0.37304 prc_auc 0.57055[0m
[93maverage test of epoch 39: loss -9.02385 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 40: loss -9.01985 acc 0.66225 roc_auc 0.37304 prc_auc 0.57120[0m
[93maverage test of epoch 40: loss -9.19167 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 41: loss -9.18691 acc 0.66225 roc_auc 0.37265 prc_auc 0.57137[0m
[93maverage test of epoch 41: loss -9.35940 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 42: loss -9.35390 acc 0.66225 roc_auc 0.37216 prc_auc 0.57085[0m
[93maverage test of epoch 42: loss -9.52708 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 43: loss -9.52082 acc 0.66225 roc_auc 0.37216 prc_auc 0.57042[0m
[93maverage test of epoch 43: loss -9.69470 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 44: loss -9.68770 acc 0.66225 roc_auc 0.37196 prc_auc 0.57059[0m
[93maverage test of epoch 44: loss -9.86227 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 45: loss -9.85453 acc 0.66225 roc_auc 0.37118 prc_auc 0.56998[0m
[93maverage test of epoch 45: loss -10.02979 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 46: loss -10.02132 acc 0.66225 roc_auc 0.37098 prc_auc 0.56994[0m
[93maverage test of epoch 46: loss -10.19728 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 47: loss -10.18808 acc 0.66225 roc_auc 0.37078 prc_auc 0.56919[0m
[93maverage test of epoch 47: loss -10.36473 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 48: loss -10.35480 acc 0.66225 roc_auc 0.37088 prc_auc 0.57076[0m
[93maverage test of epoch 48: loss -10.53215 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 49: loss -10.52149 acc 0.66225 roc_auc 0.37098 prc_auc 0.57062[0m
[93maverage test of epoch 49: loss -10.69954 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
Run statistics: 
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 0)
('lr', 0.0003)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))


Accuracy (avg): 0.59474 ROC_AUC (avg): 0.60369 PRC_AUC (avg): 0.73014 

Average forward propagation time taken(ms): 2.453618720714701
Average backward propagation time taken(ms): 0.8678684745925512

