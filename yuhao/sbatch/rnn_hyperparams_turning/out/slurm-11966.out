# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======






Params Turning Set
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 100)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 16)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 0)
('lr', 0.0003)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))






args.base_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/
args.graph_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/
args.min_dfscode_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/
args.min_dfscode_tensor_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/
*** 1 train_index:  [  0   2   3   6   9  10  11  12  13  14  15  16  17  19  20  21  22  23
  24  26  27  30  31  32  33  35  36  38  39  41  42  43  44  45  46  47
  48  49  50  52  53  54  56  57  59  60  61  62  63  64  65  66  67  69
  70  71  72  73  74  76  77  78  79  80  81  82  83  84  87  88  89  91
  92  94  95  96  97  98 100 101 102 103 104 106 107 108 109 110 112 113
 115 116 117 118 119 120 121 122 124 125 126 127 129 130 131 132 134 136
 138 139 140 141 142 144 145 146 147 148 149 151 152 153 154 155 156 157
 158 159 160 161 164 165 166 167 169 170 171 172 173 175 176 177 178 179
 180 182 184 185 186 187]
*** 2 test_index:  [  1   4   5   7   8  18  25  28  29  34  37  40  51  55  58  68  75  85
  86  90  93  99 105 111 114 123 128 133 135 137 143 150 162 163 168 174
 181 183]
*** 1 train_index:  [  0   1   2   4   5   6   7   8  10  11  12  13  14  15  16  17  18  19
  20  21  22  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39
  40  41  43  44  45  46  49  50  51  52  53  54  55  56  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  74  75  76  77  79  80  81  82
  83  84  85  86  88  90  91  92  93  94  96  97  98  99 101 102 103 104
 105 106 107 111 112 113 114 115 120 121 122 123 124 125 126 128 129 130
 131 133 134 135 136 137 138 139 140 143 144 145 147 148 150 153 154 155
 156 159 162 163 164 165 167 168 169 170 171 172 173 174 175 177 178 179
 180 181 182 183 186 187]
*** 2 test_index:  [  3   9  23  31  42  47  48  57  72  73  78  87  89  95 100 108 109 110
 116 117 118 119 127 132 141 142 146 149 151 152 157 158 160 161 166 176
 184 185]
*** 1 train_index:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19
  20  23  24  25  26  28  29  31  32  34  35  36  37  38  40  41  42  43
  44  47  48  51  53  54  55  56  57  58  59  60  64  66  68  70  71  72
  73  74  75  76  77  78  79  81  82  84  85  86  87  88  89  90  92  93
  95  96  97  98  99 100 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134
 135 137 139 140 141 142 143 144 146 148 149 150 151 152 153 156 157 158
 159 160 161 162 163 165 166 167 168 169 171 172 173 174 175 176 177 178
 179 181 183 184 185 186]
*** 2 test_index:  [  0  16  21  22  27  30  33  39  45  46  49  50  52  61  62  63  65  67
  69  80  83  91  94 101 102 103 122 136 138 145 147 154 155 164 170 180
 182 187]
*** 1 train_index:  [  0   1   3   4   5   6   7   8   9  13  14  16  18  19  21  22  23  25
  26  27  28  29  30  31  33  34  36  37  38  39  40  42  43  45  46  47
  48  49  50  51  52  53  55  56  57  58  60  61  62  63  65  66  67  68
  69  71  72  73  75  76  77  78  79  80  81  83  84  85  86  87  88  89
  90  91  92  93  94  95  97  98  99 100 101 102 103 105 106 108 109 110
 111 113 114 115 116 117 118 119 120 121 122 123 125 127 128 131 132 133
 134 135 136 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152
 154 155 156 157 158 160 161 162 163 164 165 166 168 170 171 174 175 176
 180 181 182 183 184 185 187]
*** 2 test_index:  [  2  10  11  12  15  17  20  24  32  35  41  44  54  59  64  70  74  82
  96 104 107 112 124 126 129 130 139 153 159 167 169 172 173 177 178 179
 186]
*** 1 train_index:  [  0   1   2   3   4   5   7   8   9  10  11  12  15  16  17  18  20  21
  22  23  24  25  27  28  29  30  31  32  33  34  35  37  39  40  41  42
  44  45  46  47  48  49  50  51  52  54  55  57  58  59  61  62  63  64
  65  67  68  69  70  72  73  74  75  78  80  82  83  85  86  87  89  90
  91  93  94  95  96  99 100 101 102 103 104 105 107 108 109 110 111 112
 114 116 117 118 119 122 123 124 126 127 128 129 130 132 133 135 136 137
 138 139 141 142 143 145 146 147 149 150 151 152 153 154 155 157 158 159
 160 161 162 163 164 166 167 168 169 170 172 173 174 176 177 178 179 180
 181 182 183 184 185 186 187]
*** 2 test_index:  [  6  13  14  19  26  36  38  43  53  56  60  66  71  76  77  79  81  84
  88  92  97  98 106 113 115 120 121 125 131 134 140 144 148 156 165 171
 175]
{'base_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/',
 'batch_size': 1,
 'clean_temp': False,
 'clean_tensorboard': False,
 'current_dataset_path': None,
 'current_min_dfscode_path': None,
 'current_model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/DFScodeRNN_cls_LSTM_MUTAG_2021-01-18-02-34-34/',
 'current_processed_dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'current_temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/DFScodeRNN_cls_LSTM_MUTAG_2021-01-18-02-34-34/',
 'dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/datasets/',
 'device': device(type='cuda', index=0),
 'dfscode_rnn_dropout': 0.2,
 'embedding_size_dfscode_rnn': 16,
 'epochs': 100,
 'epochs_end': 10000,
 'epochs_save': 20,
 'epochs_validate': 1,
 'fname': 'DFScodeRNN_cls_LSTM_MUTAG',
 'gradient_clipping': True,
 'graph_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/',
 'graph_type': 'MUTAG',
 'graphgen_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/',
 'hidden_size_dfscode_rnn': 4,
 'load_device': device(type='cuda', index=0),
 'load_model': False,
 'load_model_path': '',
 'log_tensorboard': False,
 'lr': 0.0003,
 'max_prev_node': None,
 'min_dfscode_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/',
 'min_dfscode_tensor_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/',
 'note': 'DFScodeRNN_cls_LSTM',
 'num_graphs': None,
 'num_layers': 1,
 'num_workers': 8,
 'number_of_mlp_layer': 0,
 'rnn_type': 'LSTM',
 'save_model': False,
 'temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/',
 'tensorboard_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tensorboard/',
 'time': '2021-01-18-02-34-34',
 'weights': False}


graphgen args.__dict__: None




dataset_features: {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}




config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 100, 'learning_rate': 0.0003, 'seed': 1800, 'k_fold': 5, 'model': 'DFScodeRNN_cls_LSTM', 'dataset': 'MUTAG'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}}


Training a new model: DFScodeRNN_cls_LSTM
Training model with dataset, testing using fold 0
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss 0.03830 acc 0.33333 roc_auc 0.56640 prc_auc 0.72384[0m
[93maverage test of epoch 0: loss -0.14409 acc 0.34211 roc_auc 0.88923 prc_auc 0.94599[0m
[92maverage training of epoch 1: loss -0.29529 acc 0.33333 roc_auc 0.55080 prc_auc 0.73964[0m
[93maverage test of epoch 1: loss -0.48110 acc 0.34211 roc_auc 0.88000 prc_auc 0.94441[0m
[92maverage training of epoch 2: loss -0.68006 acc 0.33333 roc_auc 0.52380 prc_auc 0.72491[0m
[93maverage test of epoch 2: loss -0.92241 acc 0.34211 roc_auc 0.90154 prc_auc 0.95504[0m
[92maverage training of epoch 3: loss -1.11401 acc 0.35333 roc_auc 0.55480 prc_auc 0.75463[0m
[93maverage test of epoch 3: loss -1.37962 acc 0.39474 roc_auc 0.89846 prc_auc 0.95557[0m
[92maverage training of epoch 4: loss -1.50804 acc 0.66000 roc_auc 0.72620 prc_auc 0.85502[0m
[93maverage test of epoch 4: loss -1.72478 acc 0.78947 roc_auc 0.89846 prc_auc 0.95390[0m
[92maverage training of epoch 5: loss -1.88806 acc 0.80667 roc_auc 0.83620 prc_auc 0.89844[0m
[93maverage test of epoch 5: loss -2.11621 acc 0.81579 roc_auc 0.87692 prc_auc 0.95033[0m
[92maverage training of epoch 6: loss -2.32469 acc 0.86000 roc_auc 0.84460 prc_auc 0.88875[0m
[93maverage test of epoch 6: loss -2.44332 acc 0.81579 roc_auc 0.88615 prc_auc 0.95366[0m
[92maverage training of epoch 7: loss -2.66502 acc 0.87333 roc_auc 0.84380 prc_auc 0.88585[0m
[93maverage test of epoch 7: loss -2.70534 acc 0.81579 roc_auc 0.88615 prc_auc 0.95366[0m
[92maverage training of epoch 8: loss -2.93217 acc 0.88000 roc_auc 0.83900 prc_auc 0.88130[0m
[93maverage test of epoch 8: loss -2.97737 acc 0.84211 roc_auc 0.88923 prc_auc 0.95442[0m
[92maverage training of epoch 9: loss -3.16324 acc 0.88000 roc_auc 0.82880 prc_auc 0.87258[0m
[93maverage test of epoch 9: loss -3.14412 acc 0.81579 roc_auc 0.88308 prc_auc 0.95213[0m
[92maverage training of epoch 10: loss -3.38028 acc 0.88000 roc_auc 0.83160 prc_auc 0.86345[0m
[93maverage test of epoch 10: loss -3.39819 acc 0.84211 roc_auc 0.89846 prc_auc 0.95735[0m
[92maverage training of epoch 11: loss -3.48577 acc 0.84000 roc_auc 0.79420 prc_auc 0.84954[0m
[93maverage test of epoch 11: loss -3.59426 acc 0.84211 roc_auc 0.90154 prc_auc 0.95910[0m
[92maverage training of epoch 12: loss -3.59728 acc 0.80667 roc_auc 0.70320 prc_auc 0.78318[0m
[93maverage test of epoch 12: loss -3.78002 acc 0.84211 roc_auc 0.88615 prc_auc 0.95247[0m
[92maverage training of epoch 13: loss -3.97509 acc 0.88667 roc_auc 0.83800 prc_auc 0.87766[0m
[93maverage test of epoch 13: loss -3.89716 acc 0.81579 roc_auc 0.89538 prc_auc 0.95423[0m
[92maverage training of epoch 14: loss -4.18899 acc 0.89333 roc_auc 0.84580 prc_auc 0.88237[0m
[93maverage test of epoch 14: loss -4.07858 acc 0.81579 roc_auc 0.89538 prc_auc 0.95483[0m
[92maverage training of epoch 15: loss -4.35824 acc 0.88667 roc_auc 0.84740 prc_auc 0.88283[0m
[93maverage test of epoch 15: loss -4.32568 acc 0.84211 roc_auc 0.90769 prc_auc 0.96191[0m
[92maverage training of epoch 16: loss -4.52343 acc 0.88000 roc_auc 0.84360 prc_auc 0.88095[0m
[93maverage test of epoch 16: loss -4.49704 acc 0.84211 roc_auc 0.90154 prc_auc 0.95915[0m
[92maverage training of epoch 17: loss -4.70187 acc 0.88000 roc_auc 0.82240 prc_auc 0.85456[0m
[93maverage test of epoch 17: loss -4.67579 acc 0.84211 roc_auc 0.89846 prc_auc 0.95662[0m
[92maverage training of epoch 18: loss -4.85474 acc 0.87333 roc_auc 0.79280 prc_auc 0.83590[0m
[93maverage test of epoch 18: loss -4.77478 acc 0.81579 roc_auc 0.90462 prc_auc 0.95920[0m
[92maverage training of epoch 19: loss -4.99843 acc 0.86667 roc_auc 0.75360 prc_auc 0.78966[0m
[93maverage test of epoch 19: loss -4.93412 acc 0.81579 roc_auc 0.90462 prc_auc 0.96070[0m
[92maverage training of epoch 20: loss -5.11455 acc 0.84667 roc_auc 0.80790 prc_auc 0.85554[0m
[93maverage test of epoch 20: loss -5.17896 acc 0.84211 roc_auc 0.90462 prc_auc 0.95959[0m
[92maverage training of epoch 21: loss -5.44593 acc 0.90000 roc_auc 0.84500 prc_auc 0.87467[0m
[93maverage test of epoch 21: loss -5.26453 acc 0.81579 roc_auc 0.91077 prc_auc 0.96163[0m
[92maverage training of epoch 22: loss -5.38680 acc 0.82667 roc_auc 0.65340 prc_auc 0.71488[0m
[93maverage test of epoch 22: loss -4.93863 acc 0.65789 roc_auc 0.87538 prc_auc 0.94014[0m
[92maverage training of epoch 23: loss -5.05968 acc 0.66667 roc_auc 0.37580 prc_auc 0.59221[0m
[93maverage test of epoch 23: loss -5.11304 acc 0.65789 roc_auc 0.87077 prc_auc 0.93530[0m
[92maverage training of epoch 24: loss -5.23435 acc 0.66667 roc_auc 0.37460 prc_auc 0.59041[0m
[93maverage test of epoch 24: loss -5.28680 acc 0.65789 roc_auc 0.88000 prc_auc 0.93539[0m
[92maverage training of epoch 25: loss -5.40821 acc 0.66667 roc_auc 0.37300 prc_auc 0.58947[0m
[93maverage test of epoch 25: loss -5.45979 acc 0.65789 roc_auc 0.88308 prc_auc 0.93154[0m
[92maverage training of epoch 26: loss -5.58135 acc 0.66667 roc_auc 0.37220 prc_auc 0.58938[0m
[93maverage test of epoch 26: loss -5.63211 acc 0.65789 roc_auc 0.88154 prc_auc 0.93203[0m
[92maverage training of epoch 27: loss -5.75385 acc 0.66667 roc_auc 0.37180 prc_auc 0.58904[0m
[93maverage test of epoch 27: loss -5.80382 acc 0.65789 roc_auc 0.87385 prc_auc 0.91122[0m
[92maverage training of epoch 28: loss -5.92578 acc 0.66667 roc_auc 0.37100 prc_auc 0.58876[0m
[93maverage test of epoch 28: loss -5.97499 acc 0.65789 roc_auc 0.88000 prc_auc 0.90915[0m
[92maverage training of epoch 29: loss -6.09718 acc 0.66667 roc_auc 0.37040 prc_auc 0.58854[0m
[93maverage test of epoch 29: loss -6.14567 acc 0.65789 roc_auc 0.87846 prc_auc 0.90125[0m
[92maverage training of epoch 30: loss -6.26812 acc 0.66667 roc_auc 0.36960 prc_auc 0.58830[0m
[93maverage test of epoch 30: loss -6.31589 acc 0.65789 roc_auc 0.88462 prc_auc 0.90934[0m
[92maverage training of epoch 31: loss -6.43861 acc 0.66667 roc_auc 0.36910 prc_auc 0.58804[0m
[93maverage test of epoch 31: loss -6.48570 acc 0.65789 roc_auc 0.86769 prc_auc 0.89637[0m
[92maverage training of epoch 32: loss -6.60871 acc 0.66667 roc_auc 0.36880 prc_auc 0.58856[0m
[93maverage test of epoch 32: loss -6.65512 acc 0.65789 roc_auc 0.84154 prc_auc 0.87393[0m
[92maverage training of epoch 33: loss -6.77843 acc 0.66667 roc_auc 0.36830 prc_auc 0.58822[0m
[93maverage test of epoch 33: loss -6.82419 acc 0.65789 roc_auc 0.81846 prc_auc 0.85741[0m
[92maverage training of epoch 34: loss -6.94781 acc 0.66667 roc_auc 0.36800 prc_auc 0.58725[0m
[93maverage test of epoch 34: loss -6.99293 acc 0.65789 roc_auc 0.76769 prc_auc 0.81867[0m
[92maverage training of epoch 35: loss -7.11688 acc 0.66667 roc_auc 0.36750 prc_auc 0.58727[0m
[93maverage test of epoch 35: loss -7.16138 acc 0.65789 roc_auc 0.75077 prc_auc 0.82298[0m
[92maverage training of epoch 36: loss -7.28566 acc 0.66667 roc_auc 0.36710 prc_auc 0.58727[0m
[93maverage test of epoch 36: loss -7.32954 acc 0.65789 roc_auc 0.72615 prc_auc 0.79384[0m
[92maverage training of epoch 37: loss -7.45417 acc 0.66667 roc_auc 0.36680 prc_auc 0.58682[0m
[93maverage test of epoch 37: loss -7.49746 acc 0.65789 roc_auc 0.69077 prc_auc 0.77651[0m
[92maverage training of epoch 38: loss -7.62244 acc 0.66667 roc_auc 0.36650 prc_auc 0.58690[0m
[93maverage test of epoch 38: loss -7.66513 acc 0.65789 roc_auc 0.63385 prc_auc 0.74699[0m
[92maverage training of epoch 39: loss -7.79048 acc 0.66667 roc_auc 0.36630 prc_auc 0.58679[0m
[93maverage test of epoch 39: loss -7.83260 acc 0.65789 roc_auc 0.64769 prc_auc 0.76930[0m
[92maverage training of epoch 40: loss -7.95831 acc 0.66667 roc_auc 0.36650 prc_auc 0.58702[0m
[93maverage test of epoch 40: loss -7.99986 acc 0.65789 roc_auc 0.49692 prc_auc 0.65675[0m
[92maverage training of epoch 41: loss -8.12596 acc 0.66667 roc_auc 0.36650 prc_auc 0.58708[0m
[93maverage test of epoch 41: loss -8.16694 acc 0.65789 roc_auc 0.44769 prc_auc 0.63997[0m
[92maverage training of epoch 42: loss -8.29343 acc 0.66667 roc_auc 0.36560 prc_auc 0.58419[0m
[93maverage test of epoch 42: loss -8.33386 acc 0.65789 roc_auc 0.45538 prc_auc 0.63867[0m
[92maverage training of epoch 43: loss -8.46074 acc 0.66667 roc_auc 0.36520 prc_auc 0.58432[0m
[93maverage test of epoch 43: loss -8.50063 acc 0.65789 roc_auc 0.52154 prc_auc 0.66895[0m
[92maverage training of epoch 44: loss -8.62791 acc 0.66667 roc_auc 0.36490 prc_auc 0.58315[0m
[93maverage test of epoch 44: loss -8.66726 acc 0.65789 roc_auc 0.51231 prc_auc 0.66351[0m
[92maverage training of epoch 45: loss -8.79494 acc 0.66667 roc_auc 0.36420 prc_auc 0.58355[0m
[93maverage test of epoch 45: loss -8.83375 acc 0.65789 roc_auc 0.52000 prc_auc 0.67158[0m
[92maverage training of epoch 46: loss -8.96185 acc 0.66667 roc_auc 0.36440 prc_auc 0.58324[0m
[93maverage test of epoch 46: loss -9.00014 acc 0.65789 roc_auc 0.51077 prc_auc 0.66281[0m
[92maverage training of epoch 47: loss -9.12865 acc 0.66667 roc_auc 0.36470 prc_auc 0.58351[0m
[93maverage test of epoch 47: loss -9.16642 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -9.29535 acc 0.66667 roc_auc 0.36450 prc_auc 0.58287[0m
[93maverage test of epoch 48: loss -9.33260 acc 0.65789 roc_auc 0.35846 prc_auc 0.60256[0m
[92maverage training of epoch 49: loss -9.46196 acc 0.66667 roc_auc 0.36430 prc_auc 0.58229[0m
[93maverage test of epoch 49: loss -9.49870 acc 0.65789 roc_auc 0.50154 prc_auc 0.65860[0m
[92maverage training of epoch 50: loss -9.62849 acc 0.66667 roc_auc 0.36440 prc_auc 0.58215[0m
[93maverage test of epoch 50: loss -9.66471 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 51: loss -9.79494 acc 0.66667 roc_auc 0.36440 prc_auc 0.58236[0m
[93maverage test of epoch 51: loss -9.83066 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 52: loss -9.96132 acc 0.66667 roc_auc 0.36430 prc_auc 0.58205[0m
[93maverage test of epoch 52: loss -9.99654 acc 0.65789 roc_auc 0.44000 prc_auc 0.63209[0m
[92maverage training of epoch 53: loss -10.12764 acc 0.66667 roc_auc 0.36390 prc_auc 0.58253[0m
[93maverage test of epoch 53: loss -10.16236 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 54: loss -10.29391 acc 0.66667 roc_auc 0.36410 prc_auc 0.57934[0m
[93maverage test of epoch 54: loss -10.32813 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 55: loss -10.46012 acc 0.66667 roc_auc 0.36250 prc_auc 0.56641[0m
[93maverage test of epoch 55: loss -10.49385 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 56: loss -10.62628 acc 0.66667 roc_auc 0.36040 prc_auc 0.56584[0m
[93maverage test of epoch 56: loss -10.65952 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 57: loss -10.79241 acc 0.66667 roc_auc 0.35970 prc_auc 0.56358[0m
[93maverage test of epoch 57: loss -10.82515 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 58: loss -10.95849 acc 0.66667 roc_auc 0.35910 prc_auc 0.56375[0m
[93maverage test of epoch 58: loss -10.99075 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 59: loss -11.12454 acc 0.66667 roc_auc 0.35860 prc_auc 0.56334[0m
[93maverage test of epoch 59: loss -11.15631 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 60: loss -11.29056 acc 0.66667 roc_auc 0.35840 prc_auc 0.56283[0m
[93maverage test of epoch 60: loss -11.32185 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 61: loss -11.45655 acc 0.66667 roc_auc 0.35850 prc_auc 0.56350[0m
[93maverage test of epoch 61: loss -11.48735 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 62: loss -11.62252 acc 0.66667 roc_auc 0.35840 prc_auc 0.56361[0m
[93maverage test of epoch 62: loss -11.65283 acc 0.65789 roc_auc 0.36000 prc_auc 0.60228[0m
[92maverage training of epoch 63: loss -11.78846 acc 0.66667 roc_auc 0.35880 prc_auc 0.56397[0m
[93maverage test of epoch 63: loss -11.81829 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 64: loss -11.95437 acc 0.66667 roc_auc 0.35860 prc_auc 0.56331[0m
[93maverage test of epoch 64: loss -11.98373 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 65: loss -12.12028 acc 0.66667 roc_auc 0.35850 prc_auc 0.56340[0m
[93maverage test of epoch 65: loss -12.14915 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 66: loss -12.28616 acc 0.66667 roc_auc 0.35800 prc_auc 0.56328[0m
[93maverage test of epoch 66: loss -12.31455 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 67: loss -12.45202 acc 0.66667 roc_auc 0.35790 prc_auc 0.56276[0m
[93maverage test of epoch 67: loss -12.47993 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 68: loss -12.61787 acc 0.66667 roc_auc 0.35820 prc_auc 0.56233[0m
[93maverage test of epoch 68: loss -12.64531 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 69: loss -12.78371 acc 0.66667 roc_auc 0.35760 prc_auc 0.56291[0m
[93maverage test of epoch 69: loss -12.81067 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 70: loss -12.94954 acc 0.66667 roc_auc 0.35790 prc_auc 0.56384[0m
[93maverage test of epoch 70: loss -12.97602 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 71: loss -13.11535 acc 0.66667 roc_auc 0.35770 prc_auc 0.56313[0m
[93maverage test of epoch 71: loss -13.14136 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 72: loss -13.28116 acc 0.66667 roc_auc 0.35770 prc_auc 0.56283[0m
[93maverage test of epoch 72: loss -13.30668 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 73: loss -13.44695 acc 0.66667 roc_auc 0.35810 prc_auc 0.56306[0m
[93maverage test of epoch 73: loss -13.47200 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 74: loss -13.61274 acc 0.66667 roc_auc 0.35760 prc_auc 0.56211[0m
[93maverage test of epoch 74: loss -13.63731 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 75: loss -13.77852 acc 0.66667 roc_auc 0.35800 prc_auc 0.56236[0m
[93maverage test of epoch 75: loss -13.80262 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 76: loss -13.94429 acc 0.66667 roc_auc 0.35710 prc_auc 0.56278[0m
[93maverage test of epoch 76: loss -13.96792 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 77: loss -14.11006 acc 0.66667 roc_auc 0.35810 prc_auc 0.56481[0m
[93maverage test of epoch 77: loss -14.13321 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 78: loss -14.27582 acc 0.66667 roc_auc 0.35770 prc_auc 0.56264[0m
[93maverage test of epoch 78: loss -14.29849 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 79: loss -14.44158 acc 0.66667 roc_auc 0.35720 prc_auc 0.56223[0m
[93maverage test of epoch 79: loss -14.46378 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 80: loss -14.60733 acc 0.66667 roc_auc 0.35780 prc_auc 0.56432[0m
[93maverage test of epoch 80: loss -14.62905 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 81: loss -14.77308 acc 0.66667 roc_auc 0.35820 prc_auc 0.56321[0m
[93maverage test of epoch 81: loss -14.79433 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 82: loss -14.93882 acc 0.66667 roc_auc 0.35790 prc_auc 0.56467[0m
[93maverage test of epoch 82: loss -14.95959 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 83: loss -15.10456 acc 0.66667 roc_auc 0.35730 prc_auc 0.56281[0m
[93maverage test of epoch 83: loss -15.12486 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 84: loss -15.27030 acc 0.66667 roc_auc 0.35780 prc_auc 0.56293[0m
[93maverage test of epoch 84: loss -15.29012 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 85: loss -15.43603 acc 0.66667 roc_auc 0.35820 prc_auc 0.56437[0m
[93maverage test of epoch 85: loss -15.45538 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 86: loss -15.60176 acc 0.66667 roc_auc 0.35800 prc_auc 0.56263[0m
[93maverage test of epoch 86: loss -15.62064 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 87: loss -15.76749 acc 0.66667 roc_auc 0.35840 prc_auc 0.56325[0m
[93maverage test of epoch 87: loss -15.78589 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 88: loss -15.93321 acc 0.66667 roc_auc 0.35870 prc_auc 0.56344[0m
[93maverage test of epoch 88: loss -15.95114 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 89: loss -16.09893 acc 0.66667 roc_auc 0.35850 prc_auc 0.56372[0m
[93maverage test of epoch 89: loss -16.11639 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 90: loss -16.26466 acc 0.66667 roc_auc 0.35850 prc_auc 0.56291[0m
[93maverage test of epoch 90: loss -16.28164 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 91: loss -16.43038 acc 0.66667 roc_auc 0.35830 prc_auc 0.56467[0m
[93maverage test of epoch 91: loss -16.44689 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 92: loss -16.59609 acc 0.66667 roc_auc 0.35760 prc_auc 0.56488[0m
[93maverage test of epoch 92: loss -16.61213 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 93: loss -16.76181 acc 0.66667 roc_auc 0.35850 prc_auc 0.56494[0m
[93maverage test of epoch 93: loss -16.77738 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 94: loss -16.92753 acc 0.66667 roc_auc 0.35800 prc_auc 0.56435[0m
[93maverage test of epoch 94: loss -16.94262 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 95: loss -17.09324 acc 0.66667 roc_auc 0.35770 prc_auc 0.56456[0m
[93maverage test of epoch 95: loss -17.10786 acc 0.65789 roc_auc 0.63231 prc_auc 0.72428[0m
[92maverage training of epoch 96: loss -17.25895 acc 0.66667 roc_auc 0.35750 prc_auc 0.56371[0m
[93maverage test of epoch 96: loss -17.27310 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 97: loss -17.42467 acc 0.66667 roc_auc 0.35940 prc_auc 0.56578[0m
[93maverage test of epoch 97: loss -17.43834 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 98: loss -17.59038 acc 0.66667 roc_auc 0.35820 prc_auc 0.56527[0m
[93maverage test of epoch 98: loss -17.60358 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 99: loss -17.75609 acc 0.66667 roc_auc 0.35830 prc_auc 0.56472[0m
[93maverage test of epoch 99: loss -17.76882 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 1
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss -0.31554 acc 0.33333 roc_auc 0.42100 prc_auc 0.62727[0m
[93maverage test of epoch 0: loss -0.71555 acc 0.34211 roc_auc 0.25538 prc_auc 0.59912[0m
[92maverage training of epoch 1: loss -1.05111 acc 0.40000 roc_auc 0.47040 prc_auc 0.66312[0m
[93maverage test of epoch 1: loss -1.51799 acc 0.65789 roc_auc 0.59538 prc_auc 0.82801[0m
[92maverage training of epoch 2: loss -1.98598 acc 0.66667 roc_auc 0.51600 prc_auc 0.70481[0m
[93maverage test of epoch 2: loss -2.43576 acc 0.65789 roc_auc 0.79692 prc_auc 0.89651[0m
[92maverage training of epoch 3: loss -2.70593 acc 0.66667 roc_auc 0.59260 prc_auc 0.76672[0m
[93maverage test of epoch 3: loss -2.92723 acc 0.65789 roc_auc 0.81231 prc_auc 0.90709[0m
[92maverage training of epoch 4: loss -3.10180 acc 0.66667 roc_auc 0.58040 prc_auc 0.75407[0m
[93maverage test of epoch 4: loss -3.25103 acc 0.65789 roc_auc 0.80923 prc_auc 0.90476[0m
[92maverage training of epoch 5: loss -3.39480 acc 0.66667 roc_auc 0.55160 prc_auc 0.73641[0m
[93maverage test of epoch 5: loss -3.51695 acc 0.65789 roc_auc 0.81231 prc_auc 0.90709[0m
[92maverage training of epoch 6: loss -3.64705 acc 0.66667 roc_auc 0.52560 prc_auc 0.71648[0m
[93maverage test of epoch 6: loss -3.75521 acc 0.65789 roc_auc 0.80923 prc_auc 0.90553[0m
[92maverage training of epoch 7: loss -3.87798 acc 0.66667 roc_auc 0.49480 prc_auc 0.68596[0m
[93maverage test of epoch 7: loss -3.97763 acc 0.65789 roc_auc 0.80615 prc_auc 0.90220[0m
[92maverage training of epoch 8: loss -4.09599 acc 0.66667 roc_auc 0.47400 prc_auc 0.66128[0m
[93maverage test of epoch 8: loss -4.18990 acc 0.65789 roc_auc 0.80615 prc_auc 0.90220[0m
[92maverage training of epoch 9: loss -4.30535 acc 0.66667 roc_auc 0.45800 prc_auc 0.64684[0m
[93maverage test of epoch 9: loss -4.39500 acc 0.65789 roc_auc 0.80615 prc_auc 0.90254[0m
[92maverage training of epoch 10: loss -4.50838 acc 0.66667 roc_auc 0.44560 prc_auc 0.63721[0m
[93maverage test of epoch 10: loss -4.59462 acc 0.65789 roc_auc 0.80615 prc_auc 0.90254[0m
[92maverage training of epoch 11: loss -4.70642 acc 0.66667 roc_auc 0.43660 prc_auc 0.62395[0m
[93maverage test of epoch 11: loss -4.78975 acc 0.65789 roc_auc 0.80615 prc_auc 0.90254[0m
[92maverage training of epoch 12: loss -4.90028 acc 0.66667 roc_auc 0.43310 prc_auc 0.61843[0m
[93maverage test of epoch 12: loss -4.98103 acc 0.65789 roc_auc 0.80615 prc_auc 0.90254[0m
[92maverage training of epoch 13: loss -5.09055 acc 0.66667 roc_auc 0.43050 prc_auc 0.61326[0m
[93maverage test of epoch 13: loss -5.16896 acc 0.65789 roc_auc 0.80615 prc_auc 0.90058[0m
[92maverage training of epoch 14: loss -5.27767 acc 0.66667 roc_auc 0.42680 prc_auc 0.60735[0m
[93maverage test of epoch 14: loss -5.35393 acc 0.65789 roc_auc 0.80462 prc_auc 0.90058[0m
[92maverage training of epoch 15: loss -5.46201 acc 0.66667 roc_auc 0.42560 prc_auc 0.60548[0m
[93maverage test of epoch 15: loss -5.53632 acc 0.65789 roc_auc 0.80308 prc_auc 0.89391[0m
[92maverage training of epoch 16: loss -5.64392 acc 0.66667 roc_auc 0.42460 prc_auc 0.60076[0m
[93maverage test of epoch 16: loss -5.71644 acc 0.65789 roc_auc 0.80154 prc_auc 0.89410[0m
[92maverage training of epoch 17: loss -5.82371 acc 0.66667 roc_auc 0.42300 prc_auc 0.59964[0m
[93maverage test of epoch 17: loss -5.89460 acc 0.65789 roc_auc 0.80308 prc_auc 0.89276[0m
[92maverage training of epoch 18: loss -6.00165 acc 0.66667 roc_auc 0.42120 prc_auc 0.59927[0m
[93maverage test of epoch 18: loss -6.07104 acc 0.65789 roc_auc 0.80000 prc_auc 0.89276[0m
[92maverage training of epoch 19: loss -6.17797 acc 0.66667 roc_auc 0.42110 prc_auc 0.60006[0m
[93maverage test of epoch 19: loss -6.24600 acc 0.65789 roc_auc 0.80615 prc_auc 0.89825[0m
[92maverage training of epoch 20: loss -6.35291 acc 0.66667 roc_auc 0.42140 prc_auc 0.60205[0m
[93maverage test of epoch 20: loss -6.41969 acc 0.65789 roc_auc 0.80615 prc_auc 0.89682[0m
[92maverage training of epoch 21: loss -6.52665 acc 0.66667 roc_auc 0.42150 prc_auc 0.60336[0m
[93maverage test of epoch 21: loss -6.59227 acc 0.65789 roc_auc 0.80615 prc_auc 0.89349[0m
[92maverage training of epoch 22: loss -6.69935 acc 0.66667 roc_auc 0.42100 prc_auc 0.60433[0m
[93maverage test of epoch 22: loss -6.76390 acc 0.65789 roc_auc 0.79692 prc_auc 0.88931[0m
[92maverage training of epoch 23: loss -6.87115 acc 0.66667 roc_auc 0.42180 prc_auc 0.60685[0m
[93maverage test of epoch 23: loss -6.93471 acc 0.65789 roc_auc 0.88154 prc_auc 0.92363[0m
[92maverage training of epoch 24: loss -7.04217 acc 0.66667 roc_auc 0.42240 prc_auc 0.60825[0m
[93maverage test of epoch 24: loss -7.10481 acc 0.65789 roc_auc 0.87077 prc_auc 0.91167[0m
[92maverage training of epoch 25: loss -7.21252 acc 0.66667 roc_auc 0.42310 prc_auc 0.61136[0m
[93maverage test of epoch 25: loss -7.27429 acc 0.65789 roc_auc 0.88154 prc_auc 0.92164[0m
[92maverage training of epoch 26: loss -7.38229 acc 0.66667 roc_auc 0.42320 prc_auc 0.61143[0m
[93maverage test of epoch 26: loss -7.44324 acc 0.65789 roc_auc 0.87385 prc_auc 0.90528[0m
[92maverage training of epoch 27: loss -7.55155 acc 0.66667 roc_auc 0.42300 prc_auc 0.61138[0m
[93maverage test of epoch 27: loss -7.61173 acc 0.65789 roc_auc 0.87231 prc_auc 0.90910[0m
[92maverage training of epoch 28: loss -7.72037 acc 0.66667 roc_auc 0.42280 prc_auc 0.61108[0m
[93maverage test of epoch 28: loss -7.77981 acc 0.65789 roc_auc 0.88308 prc_auc 0.91777[0m
[92maverage training of epoch 29: loss -7.88881 acc 0.66667 roc_auc 0.42250 prc_auc 0.61103[0m
[93maverage test of epoch 29: loss -7.94755 acc 0.65789 roc_auc 0.88154 prc_auc 0.91510[0m
[92maverage training of epoch 30: loss -8.05692 acc 0.66667 roc_auc 0.42250 prc_auc 0.61103[0m
[93maverage test of epoch 30: loss -8.11498 acc 0.65789 roc_auc 0.88154 prc_auc 0.91556[0m
[92maverage training of epoch 31: loss -8.22473 acc 0.66667 roc_auc 0.42250 prc_auc 0.61103[0m
[93maverage test of epoch 31: loss -8.28214 acc 0.65789 roc_auc 0.86462 prc_auc 0.89079[0m
[92maverage training of epoch 32: loss -8.39230 acc 0.66667 roc_auc 0.42240 prc_auc 0.61098[0m
[93maverage test of epoch 32: loss -8.44908 acc 0.65789 roc_auc 0.85538 prc_auc 0.88000[0m
[92maverage training of epoch 33: loss -8.55964 acc 0.66667 roc_auc 0.42210 prc_auc 0.60965[0m
[93maverage test of epoch 33: loss -8.61581 acc 0.65789 roc_auc 0.86462 prc_auc 0.89079[0m
[92maverage training of epoch 34: loss -8.72679 acc 0.66667 roc_auc 0.42200 prc_auc 0.60960[0m
[93maverage test of epoch 34: loss -8.78237 acc 0.65789 roc_auc 0.87077 prc_auc 0.89689[0m
[92maverage training of epoch 35: loss -8.89377 acc 0.66667 roc_auc 0.42180 prc_auc 0.60960[0m
[93maverage test of epoch 35: loss -8.94877 acc 0.65789 roc_auc 0.86462 prc_auc 0.89079[0m
[92maverage training of epoch 36: loss -9.06060 acc 0.66667 roc_auc 0.42180 prc_auc 0.60960[0m
[93maverage test of epoch 36: loss -9.11504 acc 0.65789 roc_auc 0.87077 prc_auc 0.89689[0m
[92maverage training of epoch 37: loss -9.22731 acc 0.66667 roc_auc 0.42180 prc_auc 0.60963[0m
[93maverage test of epoch 37: loss -9.28118 acc 0.65789 roc_auc 0.82462 prc_auc 0.84889[0m
[92maverage training of epoch 38: loss -9.39389 acc 0.66667 roc_auc 0.42180 prc_auc 0.60960[0m
[93maverage test of epoch 38: loss -9.44723 acc 0.65789 roc_auc 0.89231 prc_auc 0.90963[0m
[92maverage training of epoch 39: loss -9.56038 acc 0.66667 roc_auc 0.42160 prc_auc 0.60877[0m
[93maverage test of epoch 39: loss -9.61317 acc 0.65789 roc_auc 0.82000 prc_auc 0.84667[0m
[92maverage training of epoch 40: loss -9.72678 acc 0.66667 roc_auc 0.42160 prc_auc 0.60813[0m
[93maverage test of epoch 40: loss -9.77905 acc 0.65789 roc_auc 0.88462 prc_auc 0.90133[0m
[92maverage training of epoch 41: loss -9.89310 acc 0.66667 roc_auc 0.42150 prc_auc 0.60718[0m
[93maverage test of epoch 41: loss -9.94484 acc 0.65789 roc_auc 0.82000 prc_auc 0.84667[0m
[92maverage training of epoch 42: loss -10.05935 acc 0.66667 roc_auc 0.42140 prc_auc 0.60718[0m
[93maverage test of epoch 42: loss -10.11058 acc 0.65789 roc_auc 0.83692 prc_auc 0.85806[0m
[92maverage training of epoch 43: loss -10.22555 acc 0.66667 roc_auc 0.42120 prc_auc 0.60647[0m
[93maverage test of epoch 43: loss -10.27626 acc 0.65789 roc_auc 0.77385 prc_auc 0.81889[0m
[92maverage training of epoch 44: loss -10.39169 acc 0.66667 roc_auc 0.42100 prc_auc 0.60591[0m
[93maverage test of epoch 44: loss -10.44189 acc 0.65789 roc_auc 0.82154 prc_auc 0.84961[0m
[92maverage training of epoch 45: loss -10.55778 acc 0.66667 roc_auc 0.42060 prc_auc 0.60510[0m
[93maverage test of epoch 45: loss -10.60748 acc 0.65789 roc_auc 0.76154 prc_auc 0.82530[0m
[92maverage training of epoch 46: loss -10.72383 acc 0.66667 roc_auc 0.42070 prc_auc 0.60517[0m
[93maverage test of epoch 46: loss -10.77303 acc 0.65789 roc_auc 0.76615 prc_auc 0.81514[0m
[92maverage training of epoch 47: loss -10.88985 acc 0.66667 roc_auc 0.42060 prc_auc 0.60502[0m
[93maverage test of epoch 47: loss -10.93855 acc 0.65789 roc_auc 0.75846 prc_auc 0.81158[0m
[92maverage training of epoch 48: loss -11.05583 acc 0.66667 roc_auc 0.42070 prc_auc 0.60517[0m
[93maverage test of epoch 48: loss -11.10404 acc 0.65789 roc_auc 0.78769 prc_auc 0.82194[0m
[92maverage training of epoch 49: loss -11.22178 acc 0.66667 roc_auc 0.42050 prc_auc 0.60491[0m
[93maverage test of epoch 49: loss -11.26950 acc 0.65789 roc_auc 0.57538 prc_auc 0.70162[0m
[92maverage training of epoch 50: loss -11.38771 acc 0.66667 roc_auc 0.42060 prc_auc 0.60510[0m
[93maverage test of epoch 50: loss -11.43494 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 51: loss -11.55362 acc 0.66667 roc_auc 0.42050 prc_auc 0.60491[0m
[93maverage test of epoch 51: loss -11.60035 acc 0.65789 roc_auc 0.72462 prc_auc 0.80436[0m
[92maverage training of epoch 52: loss -11.71950 acc 0.66667 roc_auc 0.42040 prc_auc 0.60455[0m
[93maverage test of epoch 52: loss -11.76575 acc 0.65789 roc_auc 0.82308 prc_auc 0.84667[0m
[92maverage training of epoch 53: loss -11.88537 acc 0.66667 roc_auc 0.42040 prc_auc 0.60466[0m
[93maverage test of epoch 53: loss -11.93113 acc 0.65789 roc_auc 0.76923 prc_auc 0.82737[0m
[92maverage training of epoch 54: loss -12.05122 acc 0.66667 roc_auc 0.42030 prc_auc 0.60458[0m
[93maverage test of epoch 54: loss -12.09650 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 55: loss -12.21706 acc 0.66667 roc_auc 0.42040 prc_auc 0.60463[0m
[93maverage test of epoch 55: loss -12.26186 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 56: loss -12.38289 acc 0.66667 roc_auc 0.42030 prc_auc 0.60458[0m
[93maverage test of epoch 56: loss -12.42721 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 57: loss -12.54870 acc 0.66667 roc_auc 0.42030 prc_auc 0.60458[0m
[93maverage test of epoch 57: loss -12.59254 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 58: loss -12.71451 acc 0.66667 roc_auc 0.42030 prc_auc 0.60458[0m
[93maverage test of epoch 58: loss -12.75786 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 59: loss -12.88030 acc 0.66667 roc_auc 0.42020 prc_auc 0.60458[0m
[93maverage test of epoch 59: loss -12.92318 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 60: loss -13.04609 acc 0.66667 roc_auc 0.42030 prc_auc 0.60455[0m
[93maverage test of epoch 60: loss -13.08849 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 61: loss -13.21187 acc 0.66667 roc_auc 0.42030 prc_auc 0.60497[0m
[93maverage test of epoch 61: loss -13.25379 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 62: loss -13.37764 acc 0.66667 roc_auc 0.42030 prc_auc 0.60494[0m
[93maverage test of epoch 62: loss -13.41908 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 63: loss -13.54341 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 63: loss -13.58437 acc 0.65789 roc_auc 0.68154 prc_auc 0.75647[0m
[92maverage training of epoch 64: loss -13.70917 acc 0.66667 roc_auc 0.42020 prc_auc 0.60458[0m
[93maverage test of epoch 64: loss -13.74966 acc 0.65789 roc_auc 0.54462 prc_auc 0.69663[0m
[92maverage training of epoch 65: loss -13.87493 acc 0.66667 roc_auc 0.42030 prc_auc 0.60461[0m
[93maverage test of epoch 65: loss -13.91494 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 66: loss -14.04068 acc 0.66667 roc_auc 0.42020 prc_auc 0.60497[0m
[93maverage test of epoch 66: loss -14.08022 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 67: loss -14.20643 acc 0.66667 roc_auc 0.42030 prc_auc 0.60500[0m
[93maverage test of epoch 67: loss -14.24549 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 68: loss -14.37217 acc 0.66667 roc_auc 0.42020 prc_auc 0.60500[0m
[93maverage test of epoch 68: loss -14.41076 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 69: loss -14.53792 acc 0.66667 roc_auc 0.42040 prc_auc 0.60497[0m
[93maverage test of epoch 69: loss -14.57603 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 70: loss -14.70366 acc 0.66667 roc_auc 0.42030 prc_auc 0.60497[0m
[93maverage test of epoch 70: loss -14.74129 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 71: loss -14.86939 acc 0.66667 roc_auc 0.42020 prc_auc 0.60500[0m
[93maverage test of epoch 71: loss -14.90656 acc 0.65789 roc_auc 0.58000 prc_auc 0.71263[0m
[92maverage training of epoch 72: loss -15.03513 acc 0.66667 roc_auc 0.42010 prc_auc 0.60553[0m
[93maverage test of epoch 72: loss -15.07182 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 73: loss -15.20086 acc 0.66667 roc_auc 0.42020 prc_auc 0.60511[0m
[93maverage test of epoch 73: loss -15.23708 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 74: loss -15.36660 acc 0.66667 roc_auc 0.42040 prc_auc 0.60553[0m
[93maverage test of epoch 74: loss -15.40234 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 75: loss -15.53233 acc 0.66667 roc_auc 0.42020 prc_auc 0.60545[0m
[93maverage test of epoch 75: loss -15.56760 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 76: loss -15.69806 acc 0.66667 roc_auc 0.42030 prc_auc 0.60460[0m
[93maverage test of epoch 76: loss -15.73285 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 77: loss -15.86378 acc 0.66667 roc_auc 0.42010 prc_auc 0.60497[0m
[93maverage test of epoch 77: loss -15.89811 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 78: loss -16.02951 acc 0.66667 roc_auc 0.42060 prc_auc 0.60550[0m
[93maverage test of epoch 78: loss -16.06336 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 79: loss -16.19524 acc 0.66667 roc_auc 0.42030 prc_auc 0.60497[0m
[93maverage test of epoch 79: loss -16.22862 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 80: loss -16.36097 acc 0.66667 roc_auc 0.42070 prc_auc 0.60550[0m
[93maverage test of epoch 80: loss -16.39387 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 81: loss -16.52669 acc 0.66667 roc_auc 0.42000 prc_auc 0.60507[0m
[93maverage test of epoch 81: loss -16.55912 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 82: loss -16.69241 acc 0.66667 roc_auc 0.42050 prc_auc 0.60502[0m
[93maverage test of epoch 82: loss -16.72437 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 83: loss -16.85814 acc 0.66667 roc_auc 0.42030 prc_auc 0.60537[0m
[93maverage test of epoch 83: loss -16.88962 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 84: loss -17.02386 acc 0.66667 roc_auc 0.42070 prc_auc 0.60502[0m
[93maverage test of epoch 84: loss -17.05487 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 85: loss -17.18958 acc 0.66667 roc_auc 0.42050 prc_auc 0.60468[0m
[93maverage test of epoch 85: loss -17.22012 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 86: loss -17.35531 acc 0.66667 roc_auc 0.42060 prc_auc 0.60474[0m
[93maverage test of epoch 86: loss -17.38537 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 87: loss -17.52103 acc 0.66667 roc_auc 0.42010 prc_auc 0.60535[0m
[93maverage test of epoch 87: loss -17.55062 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 88: loss -17.68675 acc 0.66667 roc_auc 0.42050 prc_auc 0.60470[0m
[93maverage test of epoch 88: loss -17.71586 acc 0.65789 roc_auc 0.27385 prc_auc 0.59600[0m
[92maverage training of epoch 89: loss -17.85247 acc 0.66667 roc_auc 0.42070 prc_auc 0.60550[0m
[93maverage test of epoch 89: loss -17.88111 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 90: loss -18.01819 acc 0.66667 roc_auc 0.42050 prc_auc 0.60539[0m
[93maverage test of epoch 90: loss -18.04636 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 91: loss -18.18391 acc 0.66667 roc_auc 0.42010 prc_auc 0.60485[0m
[93maverage test of epoch 91: loss -18.21160 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 92: loss -18.34962 acc 0.66667 roc_auc 0.42020 prc_auc 0.60558[0m
[93maverage test of epoch 92: loss -18.37685 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 93: loss -18.51534 acc 0.66667 roc_auc 0.42040 prc_auc 0.60560[0m
[93maverage test of epoch 93: loss -18.54209 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 94: loss -18.68106 acc 0.66667 roc_auc 0.42040 prc_auc 0.60536[0m
[93maverage test of epoch 94: loss -18.70734 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 95: loss -18.84677 acc 0.66667 roc_auc 0.42050 prc_auc 0.60480[0m
[93maverage test of epoch 95: loss -18.87258 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 96: loss -19.01249 acc 0.66667 roc_auc 0.42060 prc_auc 0.60493[0m
[93maverage test of epoch 96: loss -19.03782 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 97: loss -19.17821 acc 0.66667 roc_auc 0.42060 prc_auc 0.60544[0m
[93maverage test of epoch 97: loss -19.20307 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 98: loss -19.34392 acc 0.66667 roc_auc 0.42030 prc_auc 0.60485[0m
[93maverage test of epoch 98: loss -19.36831 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 99: loss -19.50964 acc 0.66667 roc_auc 0.42060 prc_auc 0.60577[0m
[93maverage test of epoch 99: loss -19.53355 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 2
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss 0.17349 acc 0.34000 roc_auc 0.39420 prc_auc 0.61630[0m
[93maverage test of epoch 0: loss 0.02484 acc 0.63158 roc_auc 0.53846 prc_auc 0.80401[0m
[92maverage training of epoch 1: loss -0.10909 acc 0.66000 roc_auc 0.42680 prc_auc 0.64033[0m
[93maverage test of epoch 1: loss -0.24028 acc 0.65789 roc_auc 0.78462 prc_auc 0.91657[0m
[92maverage training of epoch 2: loss -0.38437 acc 0.66667 roc_auc 0.45980 prc_auc 0.66777[0m
[93maverage test of epoch 2: loss -0.52612 acc 0.65789 roc_auc 0.94923 prc_auc 0.97679[0m
[92maverage training of epoch 3: loss -0.73433 acc 0.66667 roc_auc 0.47940 prc_auc 0.68102[0m
[93maverage test of epoch 3: loss -0.96111 acc 0.65789 roc_auc 0.95385 prc_auc 0.97950[0m
[92maverage training of epoch 4: loss -1.26023 acc 0.66667 roc_auc 0.49060 prc_auc 0.69547[0m
[93maverage test of epoch 4: loss -1.53197 acc 0.65789 roc_auc 0.95385 prc_auc 0.97950[0m
[92maverage training of epoch 5: loss -1.79985 acc 0.66667 roc_auc 0.49780 prc_auc 0.69444[0m
[93maverage test of epoch 5: loss -2.02070 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 6: loss -2.26622 acc 0.66667 roc_auc 0.47960 prc_auc 0.68504[0m
[93maverage test of epoch 6: loss -2.47321 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 7: loss -2.71188 acc 0.66667 roc_auc 0.48680 prc_auc 0.69082[0m
[93maverage test of epoch 7: loss -2.88548 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 8: loss -3.07384 acc 0.66667 roc_auc 0.51780 prc_auc 0.71358[0m
[93maverage test of epoch 8: loss -3.19255 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 9: loss -3.35295 acc 0.66667 roc_auc 0.51900 prc_auc 0.70928[0m
[93maverage test of epoch 9: loss -3.44652 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 10: loss -3.59558 acc 0.66667 roc_auc 0.51200 prc_auc 0.70161[0m
[93maverage test of epoch 10: loss -3.67679 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 11: loss -3.81934 acc 0.66667 roc_auc 0.50080 prc_auc 0.69190[0m
[93maverage test of epoch 11: loss -3.89267 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 12: loss -4.03087 acc 0.66667 roc_auc 0.48460 prc_auc 0.67827[0m
[93maverage test of epoch 12: loss -4.09856 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 13: loss -4.23366 acc 0.66667 roc_auc 0.46710 prc_auc 0.66447[0m
[93maverage test of epoch 13: loss -4.29707 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 14: loss -4.42988 acc 0.66667 roc_auc 0.45310 prc_auc 0.65531[0m
[93maverage test of epoch 14: loss -4.48991 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 15: loss -4.62101 acc 0.66667 roc_auc 0.44480 prc_auc 0.65039[0m
[93maverage test of epoch 15: loss -4.67828 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 16: loss -4.80808 acc 0.66667 roc_auc 0.44010 prc_auc 0.64704[0m
[93maverage test of epoch 16: loss -4.86306 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 17: loss -4.99188 acc 0.66667 roc_auc 0.43440 prc_auc 0.64391[0m
[93maverage test of epoch 17: loss -5.04491 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 18: loss -5.17299 acc 0.66667 roc_auc 0.42890 prc_auc 0.63961[0m
[93maverage test of epoch 18: loss -5.22433 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 19: loss -5.35187 acc 0.66667 roc_auc 0.42240 prc_auc 0.62959[0m
[93maverage test of epoch 19: loss -5.40173 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 20: loss -5.52890 acc 0.66667 roc_auc 0.41190 prc_auc 0.62052[0m
[93maverage test of epoch 20: loss -5.57744 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 21: loss -5.70435 acc 0.66667 roc_auc 0.40470 prc_auc 0.61520[0m
[93maverage test of epoch 21: loss -5.75172 acc 0.65789 roc_auc 0.95077 prc_auc 0.97663[0m
[92maverage training of epoch 22: loss -5.87849 acc 0.66667 roc_auc 0.40070 prc_auc 0.61119[0m
[93maverage test of epoch 22: loss -5.92479 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 23: loss -6.05149 acc 0.66667 roc_auc 0.39860 prc_auc 0.61093[0m
[93maverage test of epoch 23: loss -6.09681 acc 0.65789 roc_auc 0.95077 prc_auc 0.97663[0m
[92maverage training of epoch 24: loss -6.22354 acc 0.66667 roc_auc 0.39700 prc_auc 0.60971[0m
[93maverage test of epoch 24: loss -6.26795 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 25: loss -6.39476 acc 0.66667 roc_auc 0.39570 prc_auc 0.60873[0m
[93maverage test of epoch 25: loss -6.43833 acc 0.65789 roc_auc 0.95077 prc_auc 0.97663[0m
[92maverage training of epoch 26: loss -6.56527 acc 0.66667 roc_auc 0.39380 prc_auc 0.60777[0m
[93maverage test of epoch 26: loss -6.60805 acc 0.65789 roc_auc 0.95077 prc_auc 0.97663[0m
[92maverage training of epoch 27: loss -6.73517 acc 0.66667 roc_auc 0.39150 prc_auc 0.60667[0m
[93maverage test of epoch 27: loss -6.77720 acc 0.65789 roc_auc 0.95385 prc_auc 0.97808[0m
[92maverage training of epoch 28: loss -6.90454 acc 0.66667 roc_auc 0.39010 prc_auc 0.60563[0m
[93maverage test of epoch 28: loss -6.94587 acc 0.65789 roc_auc 0.95077 prc_auc 0.97663[0m
[92maverage training of epoch 29: loss -7.07345 acc 0.66667 roc_auc 0.38850 prc_auc 0.60349[0m
[93maverage test of epoch 29: loss -7.11410 acc 0.65789 roc_auc 0.95385 prc_auc 0.97663[0m
[92maverage training of epoch 30: loss -7.24197 acc 0.66667 roc_auc 0.38830 prc_auc 0.60238[0m
[93maverage test of epoch 30: loss -7.28197 acc 0.65789 roc_auc 0.94923 prc_auc 0.97524[0m
[92maverage training of epoch 31: loss -7.41013 acc 0.66667 roc_auc 0.38750 prc_auc 0.60144[0m
[93maverage test of epoch 31: loss -7.44951 acc 0.65789 roc_auc 0.94923 prc_auc 0.97264[0m
[92maverage training of epoch 32: loss -7.57800 acc 0.66667 roc_auc 0.38700 prc_auc 0.60132[0m
[93maverage test of epoch 32: loss -7.61678 acc 0.65789 roc_auc 0.95385 prc_auc 0.97298[0m
[92maverage training of epoch 33: loss -7.74561 acc 0.66667 roc_auc 0.38630 prc_auc 0.60051[0m
[93maverage test of epoch 33: loss -7.78380 acc 0.65789 roc_auc 0.96154 prc_auc 0.97536[0m
[92maverage training of epoch 34: loss -7.91299 acc 0.66667 roc_auc 0.38590 prc_auc 0.59989[0m
[93maverage test of epoch 34: loss -7.95060 acc 0.65789 roc_auc 0.95385 prc_auc 0.97298[0m
[92maverage training of epoch 35: loss -8.08017 acc 0.66667 roc_auc 0.38540 prc_auc 0.59965[0m
[93maverage test of epoch 35: loss -8.11722 acc 0.65789 roc_auc 0.93538 prc_auc 0.95520[0m
[92maverage training of epoch 36: loss -8.24717 acc 0.66667 roc_auc 0.38490 prc_auc 0.59652[0m
[93maverage test of epoch 36: loss -8.28368 acc 0.65789 roc_auc 0.84462 prc_auc 0.87016[0m
[92maverage training of epoch 37: loss -8.41403 acc 0.66667 roc_auc 0.38440 prc_auc 0.59678[0m
[93maverage test of epoch 37: loss -8.44999 acc 0.65789 roc_auc 0.92462 prc_auc 0.94060[0m
[92maverage training of epoch 38: loss -8.58074 acc 0.66667 roc_auc 0.38420 prc_auc 0.59673[0m
[93maverage test of epoch 38: loss -8.61617 acc 0.65789 roc_auc 0.84462 prc_auc 0.86648[0m
[92maverage training of epoch 39: loss -8.74734 acc 0.66667 roc_auc 0.38400 prc_auc 0.59711[0m
[93maverage test of epoch 39: loss -8.78225 acc 0.65789 roc_auc 0.84308 prc_auc 0.87222[0m
[92maverage training of epoch 40: loss -8.91383 acc 0.66667 roc_auc 0.38340 prc_auc 0.59213[0m
[93maverage test of epoch 40: loss -8.94823 acc 0.65789 roc_auc 0.88154 prc_auc 0.90708[0m
[92maverage training of epoch 41: loss -9.08024 acc 0.66667 roc_auc 0.38280 prc_auc 0.58949[0m
[93maverage test of epoch 41: loss -9.11412 acc 0.65789 roc_auc 0.86462 prc_auc 0.87965[0m
[92maverage training of epoch 42: loss -9.24656 acc 0.66667 roc_auc 0.38260 prc_auc 0.58940[0m
[93maverage test of epoch 42: loss -9.27993 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -9.41282 acc 0.66667 roc_auc 0.38120 prc_auc 0.58858[0m
[93maverage test of epoch 43: loss -9.44569 acc 0.65789 roc_auc 0.67231 prc_auc 0.74632[0m
[92maverage training of epoch 44: loss -9.57901 acc 0.66667 roc_auc 0.38050 prc_auc 0.58721[0m
[93maverage test of epoch 44: loss -9.61138 acc 0.65789 roc_auc 0.88000 prc_auc 0.91789[0m
[92maverage training of epoch 45: loss -9.74514 acc 0.66667 roc_auc 0.38050 prc_auc 0.58642[0m
[93maverage test of epoch 45: loss -9.77702 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -9.91123 acc 0.66667 roc_auc 0.38060 prc_auc 0.58685[0m
[93maverage test of epoch 46: loss -9.94261 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -10.07728 acc 0.66667 roc_auc 0.38030 prc_auc 0.58224[0m
[93maverage test of epoch 47: loss -10.10817 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -10.24329 acc 0.66667 roc_auc 0.37930 prc_auc 0.57854[0m
[93maverage test of epoch 48: loss -10.27369 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -10.40926 acc 0.66667 roc_auc 0.37910 prc_auc 0.57792[0m
[93maverage test of epoch 49: loss -10.43918 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 50: loss -10.57521 acc 0.66667 roc_auc 0.37840 prc_auc 0.57734[0m
[93maverage test of epoch 50: loss -10.60464 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 51: loss -10.74113 acc 0.66667 roc_auc 0.37900 prc_auc 0.57876[0m
[93maverage test of epoch 51: loss -10.77007 acc 0.65789 roc_auc 0.84308 prc_auc 0.87222[0m
[92maverage training of epoch 52: loss -10.90702 acc 0.66667 roc_auc 0.37870 prc_auc 0.57754[0m
[93maverage test of epoch 52: loss -10.93548 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 53: loss -11.07289 acc 0.66667 roc_auc 0.37860 prc_auc 0.57727[0m
[93maverage test of epoch 53: loss -11.10088 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 54: loss -11.23875 acc 0.66667 roc_auc 0.37840 prc_auc 0.57699[0m
[93maverage test of epoch 54: loss -11.26625 acc 0.65789 roc_auc 0.84308 prc_auc 0.87222[0m
[92maverage training of epoch 55: loss -11.40459 acc 0.66667 roc_auc 0.37790 prc_auc 0.57668[0m
[93maverage test of epoch 55: loss -11.43161 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 56: loss -11.57042 acc 0.66667 roc_auc 0.37800 prc_auc 0.57721[0m
[93maverage test of epoch 56: loss -11.59696 acc 0.65789 roc_auc 0.82615 prc_auc 0.84917[0m
[92maverage training of epoch 57: loss -11.73623 acc 0.66667 roc_auc 0.37770 prc_auc 0.57733[0m
[93maverage test of epoch 57: loss -11.76230 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 58: loss -11.90203 acc 0.66667 roc_auc 0.37670 prc_auc 0.57604[0m
[93maverage test of epoch 58: loss -11.92762 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 59: loss -12.06782 acc 0.66667 roc_auc 0.37720 prc_auc 0.57639[0m
[93maverage test of epoch 59: loss -12.09293 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 60: loss -12.23360 acc 0.66667 roc_auc 0.37690 prc_auc 0.57603[0m
[93maverage test of epoch 60: loss -12.25824 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 61: loss -12.39937 acc 0.66667 roc_auc 0.37690 prc_auc 0.57596[0m
[93maverage test of epoch 61: loss -12.42353 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 62: loss -12.56514 acc 0.66667 roc_auc 0.37740 prc_auc 0.57649[0m
[93maverage test of epoch 62: loss -12.58882 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 63: loss -12.73090 acc 0.66667 roc_auc 0.37680 prc_auc 0.57590[0m
[93maverage test of epoch 63: loss -12.75411 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 64: loss -12.89665 acc 0.66667 roc_auc 0.37730 prc_auc 0.57713[0m
[93maverage test of epoch 64: loss -12.91938 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 65: loss -13.06240 acc 0.66667 roc_auc 0.37690 prc_auc 0.57797[0m
[93maverage test of epoch 65: loss -13.08466 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 66: loss -13.22814 acc 0.66667 roc_auc 0.37670 prc_auc 0.57642[0m
[93maverage test of epoch 66: loss -13.24992 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 67: loss -13.39387 acc 0.66667 roc_auc 0.37660 prc_auc 0.57684[0m
[93maverage test of epoch 67: loss -13.41519 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 68: loss -13.55961 acc 0.66667 roc_auc 0.37680 prc_auc 0.57757[0m
[93maverage test of epoch 68: loss -13.58045 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 69: loss -13.72534 acc 0.66667 roc_auc 0.37630 prc_auc 0.57340[0m
[93maverage test of epoch 69: loss -13.74570 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 70: loss -13.89106 acc 0.66667 roc_auc 0.37720 prc_auc 0.57809[0m
[93maverage test of epoch 70: loss -13.91095 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 71: loss -14.05679 acc 0.66667 roc_auc 0.37820 prc_auc 0.57657[0m
[93maverage test of epoch 71: loss -14.07620 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 72: loss -14.22251 acc 0.66667 roc_auc 0.37650 prc_auc 0.57459[0m
[93maverage test of epoch 72: loss -14.24145 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 73: loss -14.38823 acc 0.66667 roc_auc 0.37810 prc_auc 0.57571[0m
[93maverage test of epoch 73: loss -14.40670 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 74: loss -14.55394 acc 0.66667 roc_auc 0.37780 prc_auc 0.57697[0m
[93maverage test of epoch 74: loss -14.57194 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 75: loss -14.71966 acc 0.66667 roc_auc 0.37780 prc_auc 0.57755[0m
[93maverage test of epoch 75: loss -14.73718 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 76: loss -14.88537 acc 0.66667 roc_auc 0.37690 prc_auc 0.57588[0m
[93maverage test of epoch 76: loss -14.90242 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 77: loss -15.05108 acc 0.66667 roc_auc 0.37820 prc_auc 0.57445[0m
[93maverage test of epoch 77: loss -15.06766 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 78: loss -15.21679 acc 0.66667 roc_auc 0.37720 prc_auc 0.57900[0m
[93maverage test of epoch 78: loss -15.23289 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 79: loss -15.38250 acc 0.66667 roc_auc 0.37920 prc_auc 0.57898[0m
[93maverage test of epoch 79: loss -15.39813 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 80: loss -15.54821 acc 0.66667 roc_auc 0.37630 prc_auc 0.57743[0m
[93maverage test of epoch 80: loss -15.56336 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 81: loss -15.71391 acc 0.66667 roc_auc 0.37720 prc_auc 0.57738[0m
[93maverage test of epoch 81: loss -15.72860 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 82: loss -15.87962 acc 0.66667 roc_auc 0.37670 prc_auc 0.57448[0m
[93maverage test of epoch 82: loss -15.89383 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 83: loss -16.04532 acc 0.66667 roc_auc 0.37740 prc_auc 0.57504[0m
[93maverage test of epoch 83: loss -16.05906 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 84: loss -16.21103 acc 0.66667 roc_auc 0.37490 prc_auc 0.57646[0m
[93maverage test of epoch 84: loss -16.22429 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 85: loss -16.37673 acc 0.66667 roc_auc 0.37770 prc_auc 0.57692[0m
[93maverage test of epoch 85: loss -16.38952 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 86: loss -16.54243 acc 0.66667 roc_auc 0.37720 prc_auc 0.57852[0m
[93maverage test of epoch 86: loss -16.55475 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 87: loss -16.70813 acc 0.66667 roc_auc 0.37760 prc_auc 0.57512[0m
[93maverage test of epoch 87: loss -16.71998 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 88: loss -16.87383 acc 0.66667 roc_auc 0.37690 prc_auc 0.57633[0m
[93maverage test of epoch 88: loss -16.88521 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 89: loss -17.03953 acc 0.66667 roc_auc 0.37850 prc_auc 0.58105[0m
[93maverage test of epoch 89: loss -17.05044 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 90: loss -17.20523 acc 0.66667 roc_auc 0.37860 prc_auc 0.57812[0m
[93maverage test of epoch 90: loss -17.21566 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 91: loss -17.37093 acc 0.66667 roc_auc 0.37810 prc_auc 0.57968[0m
[93maverage test of epoch 91: loss -17.38089 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 92: loss -17.53663 acc 0.66667 roc_auc 0.37920 prc_auc 0.57890[0m
[93maverage test of epoch 92: loss -17.54611 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 93: loss -17.70233 acc 0.66667 roc_auc 0.38060 prc_auc 0.58388[0m
[93maverage test of epoch 93: loss -17.71134 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 94: loss -17.86803 acc 0.66667 roc_auc 0.37600 prc_auc 0.57883[0m
[93maverage test of epoch 94: loss -17.87656 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 95: loss -18.03372 acc 0.66667 roc_auc 0.37570 prc_auc 0.57696[0m
[93maverage test of epoch 95: loss -18.04179 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 96: loss -18.19942 acc 0.66667 roc_auc 0.37900 prc_auc 0.58189[0m
[93maverage test of epoch 96: loss -18.20701 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 97: loss -18.36512 acc 0.66667 roc_auc 0.37870 prc_auc 0.58202[0m
[93maverage test of epoch 97: loss -18.37224 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 98: loss -18.53081 acc 0.66667 roc_auc 0.37740 prc_auc 0.58249[0m
[93maverage test of epoch 98: loss -18.53746 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 99: loss -18.69651 acc 0.66667 roc_auc 0.37720 prc_auc 0.57885[0m
[93maverage test of epoch 99: loss -18.70268 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 3
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss -0.03426 acc 0.66225 roc_auc 0.47961 prc_auc 0.63633[0m
[93maverage test of epoch 0: loss -0.10886 acc 0.67568 roc_auc 0.51333 prc_auc 0.78854[0m
[92maverage training of epoch 1: loss -0.18136 acc 0.59603 roc_auc 0.56314 prc_auc 0.70558[0m
[93maverage test of epoch 1: loss -0.26182 acc 0.35135 roc_auc 0.94000 prc_auc 0.97577[0m
[92maverage training of epoch 2: loss -0.34327 acc 0.36424 roc_auc 0.66784 prc_auc 0.80859[0m
[93maverage test of epoch 2: loss -0.41528 acc 0.35135 roc_auc 0.87667 prc_auc 0.93583[0m
[92maverage training of epoch 3: loss -0.50167 acc 0.35099 roc_auc 0.76392 prc_auc 0.88469[0m
[93maverage test of epoch 3: loss -0.57391 acc 0.35135 roc_auc 0.86667 prc_auc 0.92599[0m
[92maverage training of epoch 4: loss -0.68368 acc 0.35099 roc_auc 0.81569 prc_auc 0.91310[0m
[93maverage test of epoch 4: loss -0.77007 acc 0.35135 roc_auc 0.86333 prc_auc 0.92366[0m
[92maverage training of epoch 5: loss -0.88859 acc 0.35099 roc_auc 0.74667 prc_auc 0.88735[0m
[93maverage test of epoch 5: loss -0.98218 acc 0.32432 roc_auc 0.86667 prc_auc 0.93436[0m
[92maverage training of epoch 6: loss -1.10403 acc 0.33775 roc_auc 0.55422 prc_auc 0.77154[0m
[93maverage test of epoch 6: loss -1.19912 acc 0.32432 roc_auc 0.87333 prc_auc 0.93345[0m
[92maverage training of epoch 7: loss -1.31542 acc 0.33775 roc_auc 0.43000 prc_auc 0.65189[0m
[93maverage test of epoch 7: loss -1.40718 acc 0.32432 roc_auc 0.81000 prc_auc 0.88706[0m
[92maverage training of epoch 8: loss -1.52470 acc 0.33775 roc_auc 0.40000 prc_auc 0.62036[0m
[93maverage test of epoch 8: loss -1.62071 acc 0.32432 roc_auc 0.81000 prc_auc 0.88807[0m
[92maverage training of epoch 9: loss -1.74177 acc 0.33775 roc_auc 0.39451 prc_auc 0.60760[0m
[93maverage test of epoch 9: loss -1.84174 acc 0.32432 roc_auc 0.82000 prc_auc 0.89096[0m
[92maverage training of epoch 10: loss -1.96151 acc 0.33775 roc_auc 0.39020 prc_auc 0.60153[0m
[93maverage test of epoch 10: loss -2.06038 acc 0.32432 roc_auc 0.82667 prc_auc 0.89488[0m
[92maverage training of epoch 11: loss -2.17563 acc 0.33775 roc_auc 0.38373 prc_auc 0.59076[0m
[93maverage test of epoch 11: loss -2.27139 acc 0.32432 roc_auc 0.84333 prc_auc 0.89975[0m
[92maverage training of epoch 12: loss -2.38187 acc 0.33775 roc_auc 0.37667 prc_auc 0.56966[0m
[93maverage test of epoch 12: loss -2.47463 acc 0.32432 roc_auc 0.80000 prc_auc 0.87637[0m
[92maverage training of epoch 13: loss -2.58098 acc 0.33775 roc_auc 0.37451 prc_auc 0.56774[0m
[93maverage test of epoch 13: loss -2.67134 acc 0.32432 roc_auc 0.79333 prc_auc 0.86519[0m
[92maverage training of epoch 14: loss -2.77424 acc 0.33775 roc_auc 0.37431 prc_auc 0.56776[0m
[93maverage test of epoch 14: loss -2.86283 acc 0.32432 roc_auc 0.79333 prc_auc 0.86549[0m
[92maverage training of epoch 15: loss -2.96285 acc 0.33775 roc_auc 0.37373 prc_auc 0.56753[0m
[93maverage test of epoch 15: loss -3.05016 acc 0.32432 roc_auc 0.79833 prc_auc 0.87418[0m
[92maverage training of epoch 16: loss -3.14773 acc 0.33775 roc_auc 0.37373 prc_auc 0.56768[0m
[93maverage test of epoch 16: loss -3.23415 acc 0.32432 roc_auc 0.79333 prc_auc 0.86360[0m
[92maverage training of epoch 17: loss -3.32961 acc 0.33775 roc_auc 0.37392 prc_auc 0.56795[0m
[93maverage test of epoch 17: loss -3.41543 acc 0.32432 roc_auc 0.80000 prc_auc 0.86590[0m
[92maverage training of epoch 18: loss -3.50905 acc 0.33775 roc_auc 0.37451 prc_auc 0.56845[0m
[93maverage test of epoch 18: loss -3.59452 acc 0.32432 roc_auc 0.80000 prc_auc 0.86585[0m
[92maverage training of epoch 19: loss -3.68649 acc 0.33775 roc_auc 0.37431 prc_auc 0.56844[0m
[93maverage test of epoch 19: loss -3.77178 acc 0.32432 roc_auc 0.79667 prc_auc 0.86501[0m
[92maverage training of epoch 20: loss -3.86227 acc 0.33775 roc_auc 0.37490 prc_auc 0.56913[0m
[93maverage test of epoch 20: loss -3.94754 acc 0.32432 roc_auc 0.80333 prc_auc 0.86730[0m
[92maverage training of epoch 21: loss -4.03667 acc 0.33775 roc_auc 0.37549 prc_auc 0.56932[0m
[93maverage test of epoch 21: loss -4.12203 acc 0.32432 roc_auc 0.79000 prc_auc 0.86185[0m
[92maverage training of epoch 22: loss -4.20992 acc 0.33775 roc_auc 0.37549 prc_auc 0.56946[0m
[93maverage test of epoch 22: loss -4.29547 acc 0.32432 roc_auc 0.79667 prc_auc 0.86420[0m
[92maverage training of epoch 23: loss -4.38219 acc 0.33775 roc_auc 0.37529 prc_auc 0.56920[0m
[93maverage test of epoch 23: loss -4.46802 acc 0.32432 roc_auc 0.79667 prc_auc 0.86371[0m
[92maverage training of epoch 24: loss -4.55364 acc 0.33775 roc_auc 0.37529 prc_auc 0.56920[0m
[93maverage test of epoch 24: loss -4.63980 acc 0.32432 roc_auc 0.81833 prc_auc 0.88382[0m
[92maverage training of epoch 25: loss -4.72439 acc 0.33775 roc_auc 0.37529 prc_auc 0.56920[0m
[93maverage test of epoch 25: loss -4.81095 acc 0.32432 roc_auc 0.79833 prc_auc 0.84538[0m
[92maverage training of epoch 26: loss -4.89455 acc 0.47682 roc_auc 0.37549 prc_auc 0.56979[0m
[93maverage test of epoch 26: loss -4.98155 acc 0.67568 roc_auc 0.80667 prc_auc 0.85407[0m
[92maverage training of epoch 27: loss -5.06420 acc 0.66225 roc_auc 0.37529 prc_auc 0.56942[0m
[93maverage test of epoch 27: loss -5.15168 acc 0.67568 roc_auc 0.78167 prc_auc 0.83174[0m
[92maverage training of epoch 28: loss -5.23342 acc 0.66225 roc_auc 0.37529 prc_auc 0.56942[0m
[93maverage test of epoch 28: loss -5.32140 acc 0.67568 roc_auc 0.75667 prc_auc 0.82622[0m
[92maverage training of epoch 29: loss -5.40226 acc 0.66225 roc_auc 0.37549 prc_auc 0.56951[0m
[93maverage test of epoch 29: loss -5.49079 acc 0.67568 roc_auc 0.80833 prc_auc 0.86512[0m
[92maverage training of epoch 30: loss -5.57079 acc 0.66225 roc_auc 0.37549 prc_auc 0.56951[0m
[93maverage test of epoch 30: loss -5.65987 acc 0.67568 roc_auc 0.71000 prc_auc 0.78675[0m
[92maverage training of epoch 31: loss -5.73903 acc 0.66225 roc_auc 0.37569 prc_auc 0.56982[0m
[93maverage test of epoch 31: loss -5.82870 acc 0.67568 roc_auc 0.76833 prc_auc 0.83524[0m
[92maverage training of epoch 32: loss -5.90704 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 32: loss -5.99731 acc 0.67568 roc_auc 0.73667 prc_auc 0.81535[0m
[92maverage training of epoch 33: loss -6.07485 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 33: loss -6.16573 acc 0.67568 roc_auc 0.66000 prc_auc 0.76569[0m
[92maverage training of epoch 34: loss -6.24248 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 34: loss -6.33399 acc 0.67568 roc_auc 0.67500 prc_auc 0.77073[0m
[92maverage training of epoch 35: loss -6.40995 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 35: loss -6.50210 acc 0.67568 roc_auc 0.67000 prc_auc 0.76487[0m
[92maverage training of epoch 36: loss -6.57730 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 36: loss -6.67010 acc 0.67568 roc_auc 0.69833 prc_auc 0.81578[0m
[92maverage training of epoch 37: loss -6.74453 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 37: loss -6.83798 acc 0.67568 roc_auc 0.82500 prc_auc 0.89323[0m
[92maverage training of epoch 38: loss -6.91166 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0mUsing backend: pytorch
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

[93maverage test of epoch 38: loss -7.00577 acc 0.67568 roc_auc 0.78833 prc_auc 0.85127[0m
[92maverage training of epoch 39: loss -7.07870 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 39: loss -7.17348 acc 0.67568 roc_auc 0.64833 prc_auc 0.76824[0m
[92maverage training of epoch 40: loss -7.24566 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 40: loss -7.34112 acc 0.67568 roc_auc 0.76167 prc_auc 0.83122[0m
[92maverage training of epoch 41: loss -7.41257 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 41: loss -7.50870 acc 0.67568 roc_auc 0.70500 prc_auc 0.81571[0m
[92maverage training of epoch 42: loss -7.57941 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 42: loss -7.67623 acc 0.67568 roc_auc 0.82000 prc_auc 0.88203[0m
[92maverage training of epoch 43: loss -7.74621 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 43: loss -7.84371 acc 0.67568 roc_auc 0.51667 prc_auc 0.70936[0m
[92maverage training of epoch 44: loss -7.91296 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 44: loss -8.01115 acc 0.67568 roc_auc 0.66833 prc_auc 0.77100[0m
[92maverage training of epoch 45: loss -8.07967 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 45: loss -8.17855 acc 0.67568 roc_auc 0.76333 prc_auc 0.85159[0m
[92maverage training of epoch 46: loss -8.24635 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 46: loss -8.34593 acc 0.67568 roc_auc 0.60000 prc_auc 0.73814[0m
[92maverage training of epoch 47: loss -8.41301 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 47: loss -8.51328 acc 0.67568 roc_auc 0.68500 prc_auc 0.84103[0m
[92maverage training of epoch 48: loss -8.57964 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 48: loss -8.68060 acc 0.67568 roc_auc 0.26333 prc_auc 0.61089[0m
[92maverage training of epoch 49: loss -8.74624 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 49: loss -8.84790 acc 0.67568 roc_auc 0.66833 prc_auc 0.77975[0m
[92maverage training of epoch 50: loss -8.91283 acc 0.66225 roc_auc 0.37569 prc_auc 0.56964[0m
[93maverage test of epoch 50: loss -9.01519 acc 0.67568 roc_auc 0.66000 prc_auc 0.76703[0m
[92maverage training of epoch 51: loss -9.07940 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 51: loss -9.18246 acc 0.67568 roc_auc 0.38000 prc_auc 0.65036[0m
[92maverage training of epoch 52: loss -9.24595 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 52: loss -9.34972 acc 0.67568 roc_auc 0.62000 prc_auc 0.74600[0m
[92maverage training of epoch 53: loss -9.41250 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 53: loss -9.51696 acc 0.67568 roc_auc 0.46000 prc_auc 0.65903[0m
[92maverage training of epoch 54: loss -9.57903 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 54: loss -9.68419 acc 0.67568 roc_auc 0.81167 prc_auc 0.86625[0m
[92maverage training of epoch 55: loss -9.74555 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 55: loss -9.85142 acc 0.67568 roc_auc 0.80000 prc_auc 0.85476[0m
[92maverage training of epoch 56: loss -9.91206 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 56: loss -10.01863 acc 0.67568 roc_auc 0.40000 prc_auc 0.64569[0m
[92maverage training of epoch 57: loss -10.07856 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 57: loss -10.18584 acc 0.67568 roc_auc 0.68333 prc_auc 0.79339[0m
[92maverage training of epoch 58: loss -10.24506 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 58: loss -10.35305 acc 0.67568 roc_auc 0.42000 prc_auc 0.64303[0m
[92maverage training of epoch 59: loss -10.41155 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 59: loss -10.52024 acc 0.67568 roc_auc 0.62167 prc_auc 0.75503[0m
[92maverage training of epoch 60: loss -10.57804 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 60: loss -10.68743 acc 0.67568 roc_auc 0.42167 prc_auc 0.64350[0m
[92maverage training of epoch 61: loss -10.74452 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 61: loss -10.85462 acc 0.67568 roc_auc 0.32000 prc_auc 0.63849[0m
[92maverage training of epoch 62: loss -10.91100 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 62: loss -11.02181 acc 0.67568 roc_auc 0.73167 prc_auc 0.81072[0m
[92maverage training of epoch 63: loss -11.07747 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 63: loss -11.18899 acc 0.67568 roc_auc 0.79333 prc_auc 0.85023[0m
[92maverage training of epoch 64: loss -11.24394 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 64: loss -11.35616 acc 0.67568 roc_auc 0.69667 prc_auc 0.79879[0m
[92maverage training of epoch 65: loss -11.41041 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 65: loss -11.52334 acc 0.67568 roc_auc 0.62000 prc_auc 0.74541[0m
[92maverage training of epoch 66: loss -11.57687 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 66: loss -11.69051 acc 0.67568 roc_auc 0.64833 prc_auc 0.81416[0m
[92maverage training of epoch 67: loss -11.74334 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 67: loss -11.85768 acc 0.67568 roc_auc 0.80167 prc_auc 0.84882[0m
[92maverage training of epoch 68: loss -11.90980 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 68: loss -12.02485 acc 0.67568 roc_auc 0.85167 prc_auc 0.88575[0m
[92maverage training of epoch 69: loss -12.07625 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 69: loss -12.19201 acc 0.67568 roc_auc 0.51333 prc_auc 0.71683[0m
[92maverage training of epoch 70: loss -12.24271 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 70: loss -12.35918 acc 0.67568 roc_auc 0.48000 prc_auc 0.67311[0m
[92maverage training of epoch 71: loss -12.40917 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 71: loss -12.52634 acc 0.67568 roc_auc 0.76000 prc_auc 0.83527[0m
[92maverage training of epoch 72: loss -12.57562 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 72: loss -12.69350 acc 0.67568 roc_auc 0.78667 prc_auc 0.83800[0m
[92maverage training of epoch 73: loss -12.74207 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 73: loss -12.86066 acc 0.67568 roc_auc 0.22000 prc_auc 0.64449[0m
[92maverage training of epoch 74: loss -12.90852 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 74: loss -13.02782 acc 0.67568 roc_auc 0.18000 prc_auc 0.64185[0m
[92maverage training of epoch 75: loss -13.07497 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 75: loss -13.19498 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 76: loss -13.24142 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 76: loss -13.36213 acc 0.67568 roc_auc 0.18000 prc_auc 0.64185[0m
[92maverage training of epoch 77: loss -13.40787 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 77: loss -13.52929 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 78: loss -13.57432 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 78: loss -13.69644 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 79: loss -13.74076 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 79: loss -13.86360 acc 0.67568 roc_auc 0.76833 prc_auc 0.82405[0m
[92maverage training of epoch 80: loss -13.90721 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 80: loss -14.03075 acc 0.67568 roc_auc 0.70000 prc_auc 0.79751[0m
[92maverage training of epoch 81: loss -14.07366 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 81: loss -14.19791 acc 0.67568 roc_auc 0.68000 prc_auc 0.78032[0m
[92maverage training of epoch 82: loss -14.24010 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 82: loss -14.36506 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 83: loss -14.40655 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 83: loss -14.53222 acc 0.67568 roc_auc 0.64000 prc_auc 0.75076[0m
[92maverage training of epoch 84: loss -14.57299 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 84: loss -14.69937 acc 0.67568 roc_auc 0.32000 prc_auc 0.62790[0m
[92maverage training of epoch 85: loss -14.73944 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 85: loss -14.86652 acc 0.67568 roc_auc 0.54000 prc_auc 0.70162[0m
[92maverage training of epoch 86: loss -14.90588 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 86: loss -15.03368 acc 0.67568 roc_auc 0.28000 prc_auc 0.65859[0m
[92maverage training of epoch 87: loss -15.07233 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 87: loss -15.20083 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 88: loss -15.23877 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 88: loss -15.36798 acc 0.67568 roc_auc 0.66000 prc_auc 0.77268[0m
[92maverage training of epoch 89: loss -15.40522 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 89: loss -15.53514 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 90: loss -15.57166 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 90: loss -15.70229 acc 0.67568 roc_auc 0.66000 prc_auc 0.75703[0m
[92maverage training of epoch 91: loss -15.73811 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 91: loss -15.86944 acc 0.67568 roc_auc 0.38000 prc_auc 0.67495[0m
[92maverage training of epoch 92: loss -15.90455 acc 0.66225 roc_auc 0.37578 prc_auc 0.56962[0m
[93maverage test of epoch 92: loss -16.03659 acc 0.67568 roc_auc 0.44000 prc_auc 0.65720[0m
[92maverage training of epoch 93: loss -16.07099 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 93: loss -16.20374 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 94: loss -16.23744 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 94: loss -16.37090 acc 0.67568 roc_auc 0.54000 prc_auc 0.69996[0m
[92maverage training of epoch 95: loss -16.40388 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 95: loss -16.53805 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 96: loss -16.57032 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 96: loss -16.70520 acc 0.67568 roc_auc 0.64000 prc_auc 0.76649[0m
[92maverage training of epoch 97: loss -16.73677 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 97: loss -16.87235 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 98: loss -16.90321 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 98: loss -17.03950 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 99: loss -17.06965 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 99: loss -17.20665 acc 0.67568 roc_auc 0.54000 prc_auc 0.70162[0m
Training model with dataset, testing using fold 4
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss -0.61953 acc 0.66225 roc_auc 0.42137 prc_auc 0.63177[0m
[93maverage test of epoch 0: loss -0.73787 acc 0.67568 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 1: loss -0.84207 acc 0.66225 roc_auc 0.43549 prc_auc 0.66114[0m
[93maverage test of epoch 1: loss -0.96082 acc 0.67568 roc_auc 0.92667 prc_auc 0.97089[0m
[92maverage training of epoch 2: loss -1.06204 acc 0.66225 roc_auc 0.44608 prc_auc 0.65865[0m
[93maverage test of epoch 2: loss -1.18263 acc 0.67568 roc_auc 0.92667 prc_auc 0.97089[0m
[92maverage training of epoch 3: loss -1.28160 acc 0.66225 roc_auc 0.46353 prc_auc 0.67190[0m
[93maverage test of epoch 3: loss -1.40494 acc 0.67568 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 4: loss -1.50149 acc 0.66225 roc_auc 0.48627 prc_auc 0.69258[0m
[93maverage test of epoch 4: loss -1.62746 acc 0.67568 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 5: loss -1.72055 acc 0.66225 roc_auc 0.49451 prc_auc 0.70130[0m
[93maverage test of epoch 5: loss -1.84804 acc 0.67568 roc_auc 0.93000 prc_auc 0.97264[0m
[92maverage training of epoch 6: loss -1.93753 acc 0.66225 roc_auc 0.48725 prc_auc 0.69015[0m
[93maverage test of epoch 6: loss -2.06634 acc 0.67568 roc_auc 0.93000 prc_auc 0.97264[0m
[92maverage training of epoch 7: loss -2.15281 acc 0.66225 roc_auc 0.47627 prc_auc 0.67876[0m
[93maverage test of epoch 7: loss -2.28313 acc 0.67568 roc_auc 0.92667 prc_auc 0.97135[0m
[92maverage training of epoch 8: loss -2.36633 acc 0.66225 roc_auc 0.46804 prc_auc 0.67138[0m
[93maverage test of epoch 8: loss -2.49764 acc 0.67568 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 9: loss -2.57698 acc 0.66225 roc_auc 0.45882 prc_auc 0.66296[0m
[93maverage test of epoch 9: loss -2.70854 acc 0.67568 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 10: loss -2.78364 acc 0.66225 roc_auc 0.45020 prc_auc 0.65579[0m
[93maverage test of epoch 10: loss -2.91486 acc 0.67568 roc_auc 0.93167 prc_auc 0.97235[0m
[92maverage training of epoch 11: loss -2.98566 acc 0.66225 roc_auc 0.43745 prc_auc 0.64496[0m
[93maverage test of epoch 11: loss -3.11622 acc 0.67568 roc_auc 0.93333 prc_auc 0.97390[0m
[92maverage training of epoch 12: loss -3.18295 acc 0.66225 roc_auc 0.42627 prc_auc 0.63680[0m
[93maverage test of epoch 12: loss -3.31275 acc 0.67568 roc_auc 0.93333 prc_auc 0.97377[0m
[92maverage training of epoch 13: loss -3.37578 acc 0.66225 roc_auc 0.41735 prc_auc 0.63068[0m
[93maverage test of epoch 13: loss -3.50490 acc 0.67568 roc_auc 0.93333 prc_auc 0.97377[0m
[92maverage training of epoch 14: loss -3.56465 acc 0.66225 roc_auc 0.40824 prc_auc 0.62537[0m
[93maverage test of epoch 14: loss -3.69322 acc 0.67568 roc_auc 0.93167 prc_auc 0.97104[0m
[92maverage training of epoch 15: loss -3.75010 acc 0.66225 roc_auc 0.40373 prc_auc 0.62301[0m
[93maverage test of epoch 15: loss -3.87829 acc 0.67568 roc_auc 0.93167 prc_auc 0.97104[0m
[92maverage training of epoch 16: loss -3.93265 acc 0.66225 roc_auc 0.39706 prc_auc 0.61720[0m
[93maverage test of epoch 16: loss -4.06062 acc 0.67568 roc_auc 0.93333 prc_auc 0.97264[0m
[92maverage training of epoch 17: loss -4.11277 acc 0.66225 roc_auc 0.39039 prc_auc 0.60601[0m
[93maverage test of epoch 17: loss -4.24066 acc 0.67568 roc_auc 0.93167 prc_auc 0.97104[0m
[92maverage training of epoch 18: loss -4.29085 acc 0.66225 roc_auc 0.38657 prc_auc 0.59877[0m
[93maverage test of epoch 18: loss -4.41877 acc 0.67568 roc_auc 0.93167 prc_auc 0.97104[0m
[92maverage training of epoch 19: loss -4.46721 acc 0.66225 roc_auc 0.38529 prc_auc 0.59869[0m
[93maverage test of epoch 19: loss -4.59528 acc 0.67568 roc_auc 0.93167 prc_auc 0.97104[0m
[92maverage training of epoch 20: loss -4.64214 acc 0.66225 roc_auc 0.38275 prc_auc 0.59681[0m
[93maverage test of epoch 20: loss -4.77044 acc 0.67568 roc_auc 0.93667 prc_auc 0.97264[0m
[92maverage training of epoch 21: loss -4.81586 acc 0.66225 roc_auc 0.38176 prc_auc 0.59553[0m
[93maverage test of epoch 21: loss -4.94446 acc 0.67568 roc_auc 0.93333 prc_auc 0.96845[0m
[92maverage training of epoch 22: loss -4.98856 acc 0.66225 roc_auc 0.38088 prc_auc 0.59301[0m
[93maverage test of epoch 22: loss -5.11752 acc 0.67568 roc_auc 0.93000 prc_auc 0.96845[0m
[92maverage training of epoch 23: loss -5.16039 acc 0.66225 roc_auc 0.38108 prc_auc 0.59321[0m
[93maverage test of epoch 23: loss -5.28977 acc 0.67568 roc_auc 0.93333 prc_auc 0.96845[0m
[92maverage training of epoch 24: loss -5.33149 acc 0.66225 roc_auc 0.38020 prc_auc 0.59282[0m
[93maverage test of epoch 24: loss -5.46132 acc 0.67568 roc_auc 0.93000 prc_auc 0.96958[0m
[92maverage training of epoch 25: loss -5.50195 acc 0.66225 roc_auc 0.37941 prc_auc 0.59130[0m
[93maverage test of epoch 25: loss -5.63228 acc 0.67568 roc_auc 0.93667 prc_auc 0.97264[0m
[92maverage training of epoch 26: loss -5.67188 acc 0.66225 roc_auc 0.37941 prc_auc 0.59243[0m
[93maverage test of epoch 26: loss -5.80273 acc 0.67568 roc_auc 0.93667 prc_auc 0.97264[0m
[92maverage training of epoch 27: loss -5.84135 acc 0.66225 roc_auc 0.37863 prc_auc 0.59121[0m
[93maverage test of epoch 27: loss -5.97275 acc 0.67568 roc_auc 0.93667 prc_auc 0.97264[0m
[92maverage training of epoch 28: loss -6.01043 acc 0.66225 roc_auc 0.37863 prc_auc 0.59131[0m
[93maverage test of epoch 28: loss -6.14239 acc 0.67568 roc_auc 0.93333 prc_auc 0.96845[0m
[92maverage training of epoch 29: loss -6.17916 acc 0.66225 roc_auc 0.37873 prc_auc 0.59094[0m
[93maverage test of epoch 29: loss -6.31172 acc 0.67568 roc_auc 0.93333 prc_auc 0.96845[0m
[92maverage training of epoch 30: loss -6.34760 acc 0.66225 roc_auc 0.37667 prc_auc 0.58256[0m
[93maverage test of epoch 30: loss -6.48076 acc 0.67568 roc_auc 0.93333 prc_auc 0.96845[0m
[92maverage training of epoch 31: loss -6.51579 acc 0.66225 roc_auc 0.37657 prc_auc 0.58241[0m
[93maverage test of epoch 31: loss -6.64957 acc 0.67568 roc_auc 0.93333 prc_auc 0.96875[0m
[92maverage training of epoch 32: loss -6.68375 acc 0.66225 roc_auc 0.37608 prc_auc 0.57484[0m
[93maverage test of epoch 32: loss -6.81816 acc 0.67568 roc_auc 0.94000 prc_auc 0.96875[0m
[92maverage training of epoch 33: loss -6.85153 acc 0.66225 roc_auc 0.37510 prc_auc 0.57304[0m
[93maverage test of epoch 33: loss -6.98658 acc 0.67568 roc_auc 0.93333 prc_auc 0.96998[0m
[92maverage training of epoch 34: loss -7.01914 acc 0.66225 roc_auc 0.37363 prc_auc 0.57135[0m
[93maverage test of epoch 34: loss -7.15484 acc 0.67568 roc_auc 0.94333 prc_auc 0.96875[0m
[92maverage training of epoch 35: loss -7.18661 acc 0.66225 roc_auc 0.37353 prc_auc 0.57131[0m
[93maverage test of epoch 35: loss -7.32297 acc 0.67568 roc_auc 0.93000 prc_auc 0.95690[0m
[92maverage training of epoch 36: loss -7.35395 acc 0.66225 roc_auc 0.37304 prc_auc 0.57097[0m
[93maverage test of epoch 36: loss -7.49098 acc 0.67568 roc_auc 0.91667 prc_auc 0.94505[0m
[92maverage training of epoch 37: loss -7.52118 acc 0.66225 roc_auc 0.37294 prc_auc 0.57091[0m
[93maverage test of epoch 37: loss -7.65888 acc 0.67568 roc_auc 0.89333 prc_auc 0.93850[0m
[92maverage training of epoch 38: loss -7.68832 acc 0.66225 roc_auc 0.37245 prc_auc 0.57052[0m
[93maverage test of epoch 38: loss -7.82669 acc 0.67568 roc_auc 0.91333 prc_auc 0.93912[0m
[92maverage training of epoch 39: loss -7.85538 acc 0.66225 roc_auc 0.37225 prc_auc 0.57023[0m
[93maverage test of epoch 39: loss -7.99443 acc 0.67568 roc_auc 0.92000 prc_auc 0.95750[0m
[92maverage training of epoch 40: loss -8.02236 acc 0.66225 roc_auc 0.37206 prc_auc 0.56997[0m
[93maverage test of epoch 40: loss -8.16210 acc 0.67568 roc_auc 0.90000 prc_auc 0.93703[0m
[92maverage training of epoch 41: loss -8.18928 acc 0.66225 roc_auc 0.37118 prc_auc 0.56927[0m
[93maverage test of epoch 41: loss -8.32970 acc 0.67568 roc_auc 0.82667 prc_auc 0.89147[0m
[92maverage training of epoch 42: loss -8.35615 acc 0.66225 roc_auc 0.37078 prc_auc 0.56895[0m
[93maverage test of epoch 42: loss -8.49726 acc 0.67568 roc_auc 0.92000 prc_auc 0.94811[0m
[92maverage training of epoch 43: loss -8.52296 acc 0.66225 roc_auc 0.37088 prc_auc 0.56914[0m
[93maverage test of epoch 43: loss -8.66477 acc 0.67568 roc_auc 0.79333 prc_auc 0.83776[0m
[92maverage training of epoch 44: loss -8.68974 acc 0.66225 roc_auc 0.37078 prc_auc 0.56897[0m
[93maverage test of epoch 44: loss -8.83224 acc 0.67568 roc_auc 0.66667 prc_auc 0.75758[0m
[92maverage training of epoch 45: loss -8.85648 acc 0.66225 roc_auc 0.37078 prc_auc 0.56895[0m
[93maverage test of epoch 45: loss -8.99967 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 46: loss -9.02318 acc 0.66225 roc_auc 0.37069 prc_auc 0.56877[0m
[93maverage test of epoch 46: loss -9.16707 acc 0.67568 roc_auc 0.90000 prc_auc 0.93514[0m
[92maverage training of epoch 47: loss -9.18986 acc 0.66225 roc_auc 0.37039 prc_auc 0.56861[0m
[93maverage test of epoch 47: loss -9.33445 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 48: loss -9.35651 acc 0.66225 roc_auc 0.37049 prc_auc 0.56861[0m
[93maverage test of epoch 48: loss -9.50180 acc 0.67568 roc_auc 0.54167 prc_auc 0.69444[0m
[92maverage training of epoch 49: loss -9.52314 acc 0.66225 roc_auc 0.37039 prc_auc 0.56849[0m
[93maverage test of epoch 49: loss -9.66913 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 50: loss -9.68975 acc 0.66225 roc_auc 0.37029 prc_auc 0.56849[0m
[93maverage test of epoch 50: loss -9.83644 acc 0.67568 roc_auc 0.70000 prc_auc 0.80541[0m
[92maverage training of epoch 51: loss -9.85634 acc 0.66225 roc_auc 0.37020 prc_auc 0.56849[0m
[93maverage test of epoch 51: loss -10.00374 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 52: loss -10.02292 acc 0.66225 roc_auc 0.37029 prc_auc 0.56849[0m
[93maverage test of epoch 52: loss -10.17102 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 53: loss -10.18949 acc 0.66225 roc_auc 0.37029 prc_auc 0.56849[0m
[93maverage test of epoch 53: loss -10.33829 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 54: loss -10.35604 acc 0.66225 roc_auc 0.37029 prc_auc 0.56831[0m
[93maverage test of epoch 54: loss -10.50554 acc 0.67568 roc_auc 0.71333 prc_auc 0.78999[0m
[92maverage training of epoch 55: loss -10.52258 acc 0.66225 roc_auc 0.37000 prc_auc 0.56832[0m
[93maverage test of epoch 55: loss -10.67279 acc 0.67568 roc_auc 0.64000 prc_auc 0.76649[0m
[92maverage training of epoch 56: loss -10.68911 acc 0.66225 roc_auc 0.37020 prc_auc 0.56834[0m
[93maverage test of epoch 56: loss -10.84003 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 57: loss -10.85564 acc 0.66225 roc_auc 0.37000 prc_auc 0.56821[0m
[93maverage test of epoch 57: loss -11.00726 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 58: loss -11.02215 acc 0.66225 roc_auc 0.36990 prc_auc 0.56821[0m
[93maverage test of epoch 58: loss -11.17448 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 59: loss -11.18866 acc 0.66225 roc_auc 0.36990 prc_auc 0.56804[0m
[93maverage test of epoch 59: loss -11.34169 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 60: loss -11.35516 acc 0.66225 roc_auc 0.36980 prc_auc 0.56801[0m
[93maverage test of epoch 60: loss -11.50890 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 61: loss -11.52166 acc 0.66225 roc_auc 0.36980 prc_auc 0.56804[0m
[93maverage test of epoch 61: loss -11.67610 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 62: loss -11.68815 acc 0.66225 roc_auc 0.36980 prc_auc 0.56801[0m
[93maverage test of epoch 62: loss -11.84330 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 63: loss -11.85463 acc 0.66225 roc_auc 0.36990 prc_auc 0.56804[0m
[93maverage test of epoch 63: loss -12.01049 acc 0.67568 roc_auc 0.75167 prc_auc 0.80977[0m
[92maverage training of epoch 64: loss -12.02112 acc 0.66225 roc_auc 0.36990 prc_auc 0.56842[0m
[93maverage test of epoch 64: loss -12.17768 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 65: loss -12.18759 acc 0.66225 roc_auc 0.36980 prc_auc 0.56804[0m
[93maverage test of epoch 65: loss -12.34486 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 66: loss -12.35407 acc 0.66225 roc_auc 0.37000 prc_auc 0.56821[0m
[93maverage test of epoch 66: loss -12.51205 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 67: loss -12.52054 acc 0.66225 roc_auc 0.36980 prc_auc 0.56804[0m
[93maverage test of epoch 67: loss -12.67923 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 68: loss -12.68701 acc 0.66225 roc_auc 0.36990 prc_auc 0.56823[0m
[93maverage test of epoch 68: loss -12.84640 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 69: loss -12.85348 acc 0.66225 roc_auc 0.36980 prc_auc 0.56825[0m
[93maverage test of epoch 69: loss -13.01358 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 70: loss -13.01994 acc 0.66225 roc_auc 0.37000 prc_auc 0.56849[0m
[93maverage test of epoch 70: loss -13.18075 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 71: loss -13.18641 acc 0.66225 roc_auc 0.36990 prc_auc 0.56804[0m
[93maverage test of epoch 71: loss -13.34792 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 72: loss -13.35287 acc 0.66225 roc_auc 0.37000 prc_auc 0.56836[0m
[93maverage test of epoch 72: loss -13.51509 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 73: loss -13.51933 acc 0.66225 roc_auc 0.37000 prc_auc 0.56864[0m
[93maverage test of epoch 73: loss -13.68226 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 74: loss -13.68579 acc 0.66225 roc_auc 0.36990 prc_auc 0.56819[0m
[93maverage test of epoch 74: loss -13.84943 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 75: loss -13.85225 acc 0.66225 roc_auc 0.36990 prc_auc 0.56849[0m
[93maverage test of epoch 75: loss -14.01659 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 76: loss -14.01871 acc 0.66225 roc_auc 0.37000 prc_auc 0.56864[0m
[93maverage test of epoch 76: loss -14.18376 acc 0.67568 roc_auc 0.66000 prc_auc 0.77946[0m
[92maverage training of epoch 77: loss -14.18517 acc 0.66225 roc_auc 0.36990 prc_auc 0.56847[0m
[93maverage test of epoch 77: loss -14.35093 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 78: loss -14.35162 acc 0.66225 roc_auc 0.36980 prc_auc 0.56838[0m
[93maverage test of epoch 78: loss -14.51809 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 79: loss -14.51808 acc 0.66225 roc_auc 0.36990 prc_auc 0.56840[0m
[93maverage test of epoch 79: loss -14.68525 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 80: loss -14.68453 acc 0.66225 roc_auc 0.37000 prc_auc 0.56831[0m
[93maverage test of epoch 80: loss -14.85241 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 81: loss -14.85099 acc 0.66225 roc_auc 0.36980 prc_auc 0.56962[0m
[93maverage test of epoch 81: loss -15.01958 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 82: loss -15.01744 acc 0.66225 roc_auc 0.36990 prc_auc 0.56822[0m
[93maverage test of epoch 82: loss -15.18674 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 83: loss -15.18389 acc 0.66225 roc_auc 0.36961 prc_auc 0.56889[0m
[93maverage test of epoch 83: loss -15.35390 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 84: loss -15.35034 acc 0.66225 roc_auc 0.37000 prc_auc 0.56930[0m
[93maverage test of epoch 84: loss -15.52106 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 85: loss -15.51680 acc 0.66225 roc_auc 0.36980 prc_auc 0.56852[0m
[93maverage test of epoch 85: loss -15.68822 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 86: loss -15.68325 acc 0.66225 roc_auc 0.36961 prc_auc 0.56874[0m
[93maverage test of epoch 86: loss -15.85538 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 87: loss -15.84970 acc 0.66225 roc_auc 0.36971 prc_auc 0.56928[0m
[93maverage test of epoch 87: loss -16.02254 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 88: loss -16.01615 acc 0.66225 roc_auc 0.36971 prc_auc 0.56925[0m
[93maverage test of epoch 88: loss -16.18969 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 89: loss -16.18260 acc 0.66225 roc_auc 0.36990 prc_auc 0.56831[0m
[93maverage test of epoch 89: loss -16.35685 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 90: loss -16.34905 acc 0.66225 roc_auc 0.36971 prc_auc 0.56790[0m
[93maverage test of epoch 90: loss -16.52401 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 91: loss -16.51550 acc 0.66225 roc_auc 0.36980 prc_auc 0.56935[0m
[93maverage test of epoch 91: loss -16.69117 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 92: loss -16.68195 acc 0.66225 roc_auc 0.37000 prc_auc 0.56989[0m
[93maverage test of epoch 92: loss -16.85833 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 93: loss -16.84840 acc 0.66225 roc_auc 0.36971 prc_auc 0.56911[0m
[93maverage test of epoch 93: loss -17.02549 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 94: loss -17.01486 acc 0.66225 roc_auc 0.36980 prc_auc 0.56988[0m
[93maverage test of epoch 94: loss -17.19265 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 95: loss -17.18131 acc 0.66225 roc_auc 0.36951 prc_auc 0.56725[0m
[93maverage test of epoch 95: loss -17.35981 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 96: loss -17.34776 acc 0.66225 roc_auc 0.37000 prc_auc 0.56930[0m
[93maverage test of epoch 96: loss -17.52697 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 97: loss -17.51421 acc 0.66225 roc_auc 0.36971 prc_auc 0.56919[0m
[93maverage test of epoch 97: loss -17.69413 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 98: loss -17.68066 acc 0.66225 roc_auc 0.36931 prc_auc 0.56917[0m
[93maverage test of epoch 98: loss -17.86129 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 99: loss -17.84711 acc 0.66225 roc_auc 0.36961 prc_auc 0.56933[0m
[93maverage test of epoch 99: loss -18.02845 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
Run statistics: 
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 100)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 16)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 0)
('lr', 0.0003)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))


Accuracy (avg): 0.66501 ROC_AUC (avg): 0.508 PRC_AUC (avg): 0.6702 

Average forward propagation time taken(ms): 2.4660184061477075
Average backward propagation time taken(ms): 0.8666757630457742

