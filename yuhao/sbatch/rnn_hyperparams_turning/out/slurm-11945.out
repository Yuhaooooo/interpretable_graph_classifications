# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======






Params Turning Set
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 100)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 2)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))






args.base_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/
args.graph_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/
args.min_dfscode_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/
args.min_dfscode_tensor_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/
*** 1 train_index:  [  0   2   3   6   9  10  11  12  13  14  15  16  17  19  20  21  22  23
  24  26  27  30  31  32  33  35  36  38  39  41  42  43  44  45  46  47
  48  49  50  52  53  54  56  57  59  60  61  62  63  64  65  66  67  69
  70  71  72  73  74  76  77  78  79  80  81  82  83  84  87  88  89  91
  92  94  95  96  97  98 100 101 102 103 104 106 107 108 109 110 112 113
 115 116 117 118 119 120 121 122 124 125 126 127 129 130 131 132 134 136
 138 139 140 141 142 144 145 146 147 148 149 151 152 153 154 155 156 157
 158 159 160 161 164 165 166 167 169 170 171 172 173 175 176 177 178 179
 180 182 184 185 186 187]
*** 2 test_index:  [  1   4   5   7   8  18  25  28  29  34  37  40  51  55  58  68  75  85
  86  90  93  99 105 111 114 123 128 133 135 137 143 150 162 163 168 174
 181 183]
*** 1 train_index:  [  0   1   2   4   5   6   7   8  10  11  12  13  14  15  16  17  18  19
  20  21  22  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39
  40  41  43  44  45  46  49  50  51  52  53  54  55  56  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  74  75  76  77  79  80  81  82
  83  84  85  86  88  90  91  92  93  94  96  97  98  99 101 102 103 104
 105 106 107 111 112 113 114 115 120 121 122 123 124 125 126 128 129 130
 131 133 134 135 136 137 138 139 140 143 144 145 147 148 150 153 154 155
 156 159 162 163 164 165 167 168 169 170 171 172 173 174 175 177 178 179
 180 181 182 183 186 187]
*** 2 test_index:  [  3   9  23  31  42  47  48  57  72  73  78  87  89  95 100 108 109 110
 116 117 118 119 127 132 141 142 146 149 151 152 157 158 160 161 166 176
 184 185]
*** 1 train_index:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19
  20  23  24  25  26  28  29  31  32  34  35  36  37  38  40  41  42  43
  44  47  48  51  53  54  55  56  57  58  59  60  64  66  68  70  71  72
  73  74  75  76  77  78  79  81  82  84  85  86  87  88  89  90  92  93
  95  96  97  98  99 100 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134
 135 137 139 140 141 142 143 144 146 148 149 150 151 152 153 156 157 158
 159 160 161 162 163 165 166 167 168 169 171 172 173 174 175 176 177 178
 179 181 183 184 185 186]
*** 2 test_index:  [  0  16  21  22  27  30  33  39  45  46  49  50  52  61  62  63  65  67
  69  80  83  91  94 101 102 103 122 136 138 145 147 154 155 164 170 180
 182 187]
*** 1 train_index:  [  0   1   3   4   5   6   7   8   9  13  14  16  18  19  21  22  23  25
  26  27  28  29  30  31  33  34  36  37  38  39  40  42  43  45  46  47
  48  49  50  51  52  53  55  56  57  58  60  61  62  63  65  66  67  68
  69  71  72  73  75  76  77  78  79  80  81  83  84  85  86  87  88  89
  90  91  92  93  94  95  97  98  99 100 101 102 103 105 106 108 109 110
 111 113 114 115 116 117 118 119 120 121 122 123 125 127 128 131 132 133
 134 135 136 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152
 154 155 156 157 158 160 161 162 163 164 165 166 168 170 171 174 175 176
 180 181 182 183 184 185 187]
*** 2 test_index:  [  2  10  11  12  15  17  20  24  32  35  41  44  54  59  64  70  74  82
  96 104 107 112 124 126 129 130 139 153 159 167 169 172 173 177 178 179
 186]
*** 1 train_index:  [  0   1   2   3   4   5   7   8   9  10  11  12  15  16  17  18  20  21
  22  23  24  25  27  28  29  30  31  32  33  34  35  37  39  40  41  42
  44  45  46  47  48  49  50  51  52  54  55  57  58  59  61  62  63  64
  65  67  68  69  70  72  73  74  75  78  80  82  83  85  86  87  89  90
  91  93  94  95  96  99 100 101 102 103 104 105 107 108 109 110 111 112
 114 116 117 118 119 122 123 124 126 127 128 129 130 132 133 135 136 137
 138 139 141 142 143 145 146 147 149 150 151 152 153 154 155 157 158 159
 160 161 162 163 164 166 167 168 169 170 172 173 174 176 177 178 179 180
 181 182 183 184 185 186 187]
*** 2 test_index:  [  6  13  14  19  26  36  38  43  53  56  60  66  71  76  77  79  81  84
  88  92  97  98 106 113 115 120 121 125 131 134 140 144 148 156 165 171
 175]
{'base_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/',
 'batch_size': 1,
 'clean_temp': False,
 'clean_tensorboard': False,
 'current_dataset_path': None,
 'current_min_dfscode_path': None,
 'current_model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/DFScodeRNN_cls_LSTM_MUTAG_2021-01-18-00-33-07/',
 'current_processed_dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'current_temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/DFScodeRNN_cls_LSTM_MUTAG_2021-01-18-00-33-07/',
 'dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/datasets/',
 'device': device(type='cuda', index=0),
 'dfscode_rnn_dropout': 0.2,
 'embedding_size_dfscode_rnn': 8,
 'epochs': 100,
 'epochs_end': 10000,
 'epochs_save': 20,
 'epochs_validate': 1,
 'fname': 'DFScodeRNN_cls_LSTM_MUTAG',
 'gradient_clipping': True,
 'graph_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/',
 'graph_type': 'MUTAG',
 'graphgen_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/',
 'hidden_size_dfscode_rnn': 4,
 'load_device': device(type='cuda', index=0),
 'load_model': False,
 'load_model_path': '',
 'log_tensorboard': False,
 'lr': 0.0001,
 'max_prev_node': None,
 'min_dfscode_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/',
 'min_dfscode_tensor_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/',
 'note': 'DFScodeRNN_cls_LSTM',
 'num_graphs': None,
 'num_layers': 1,
 'num_workers': 8,
 'number_of_mlp_layer': 2,
 'rnn_type': 'LSTM',
 'save_model': False,
 'temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/',
 'tensorboard_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tensorboard/',
 'time': '2021-01-18-00-33-07',
 'weights': False}


graphgen args.__dict__: None




dataset_features: {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}




config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 100, 'learning_rate': 0.0001, 'seed': 1800, 'k_fold': 5, 'model': 'DFScodeRNN_cls_LSTM', 'dataset': 'MUTAG'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}}


Training a new model: DFScodeRNN_cls_LSTM
Training model with dataset, testing using fold 0
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=4, bias=True)
    (ReLU1): ReLU()
    (Linear2): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.02287 acc 0.33333 roc_auc 0.49860 prc_auc 0.69388[0m
[93maverage test of epoch 0: loss 0.00086 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 1: loss 0.00077 acc 0.33333 roc_auc 0.36900 prc_auc 0.58794[0m
[93maverage test of epoch 1: loss -0.01012 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 2: loss -0.01043 acc 0.33333 roc_auc 0.36790 prc_auc 0.58711[0m
[93maverage test of epoch 2: loss -0.02112 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 3: loss -0.02157 acc 0.33333 roc_auc 0.35970 prc_auc 0.56465[0m
[93maverage test of epoch 3: loss -0.03213 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 4: loss -0.03261 acc 0.33333 roc_auc 0.35850 prc_auc 0.56306[0m
[93maverage test of epoch 4: loss -0.04314 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 5: loss -0.04365 acc 0.33333 roc_auc 0.35830 prc_auc 0.56291[0m
[93maverage test of epoch 5: loss -0.05415 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 6: loss -0.05469 acc 0.33333 roc_auc 0.35780 prc_auc 0.56257[0m
[93maverage test of epoch 6: loss -0.06516 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 7: loss -0.06573 acc 0.33333 roc_auc 0.35760 prc_auc 0.56233[0m
[93maverage test of epoch 7: loss -0.07618 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 8: loss -0.07678 acc 0.33333 roc_auc 0.35760 prc_auc 0.56226[0m
[93maverage test of epoch 8: loss -0.08719 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 9: loss -0.08782 acc 0.33333 roc_auc 0.35750 prc_auc 0.56208[0m
[93maverage test of epoch 9: loss -0.09821 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 10: loss -0.09887 acc 0.33333 roc_auc 0.35740 prc_auc 0.56208[0m
[93maverage test of epoch 10: loss -0.10922 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 11: loss -0.10992 acc 0.33333 roc_auc 0.35730 prc_auc 0.56199[0m
[93maverage test of epoch 11: loss -0.12024 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 12: loss -0.12096 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 12: loss -0.13125 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 13: loss -0.13201 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 13: loss -0.14227 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 14: loss -0.14306 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 14: loss -0.15328 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 15: loss -0.15410 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 15: loss -0.16430 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 16: loss -0.16515 acc 0.33333 roc_auc 0.35700 prc_auc 0.56196[0m
[93maverage test of epoch 16: loss -0.17531 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 17: loss -0.17620 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 17: loss -0.18633 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 18: loss -0.18725 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 18: loss -0.19735 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 19: loss -0.19830 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 19: loss -0.20836 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 20: loss -0.20934 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 20: loss -0.21938 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 21: loss -0.22039 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 21: loss -0.23040 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 22: loss -0.23144 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 22: loss -0.24141 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 23: loss -0.24249 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 23: loss -0.25243 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 24: loss -0.25354 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 24: loss -0.26345 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 25: loss -0.26458 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 25: loss -0.27446 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 26: loss -0.27563 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 26: loss -0.28548 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 27: loss -0.28668 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 27: loss -0.29650 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 28: loss -0.29773 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 28: loss -0.30751 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 29: loss -0.30878 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 29: loss -0.31853 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 30: loss -0.31982 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 30: loss -0.32955 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 31: loss -0.33087 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 31: loss -0.34056 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 32: loss -0.34192 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 32: loss -0.35158 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 33: loss -0.35297 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 33: loss -0.36260 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 34: loss -0.36402 acc 0.33333 roc_auc 0.35710 prc_auc 0.56188[0m
[93maverage test of epoch 34: loss -0.37361 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 35: loss -0.37506 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 35: loss -0.38463 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 36: loss -0.38611 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 36: loss -0.39565 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 37: loss -0.39716 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 37: loss -0.40666 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 38: loss -0.40821 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 38: loss -0.41768 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 39: loss -0.41926 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 39: loss -0.42870 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 40: loss -0.43031 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 40: loss -0.43971 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 41: loss -0.44135 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 41: loss -0.45073 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -0.45240 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 42: loss -0.46175 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -0.46345 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 43: loss -0.47276 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 44: loss -0.47450 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 44: loss -0.48378 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -0.48555 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 45: loss -0.49479 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -0.49659 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 46: loss -0.50581 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -0.50764 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 47: loss -0.51683 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -0.51869 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 48: loss -0.52784 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -0.52974 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 49: loss -0.53886 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 50: loss -0.54079 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 50: loss -0.54988 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 51: loss -0.55183 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 51: loss -0.56089 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 52: loss -0.56288 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 52: loss -0.57191 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 53: loss -0.57393 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 53: loss -0.58293 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 54: loss -0.58498 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 54: loss -0.59394 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 55: loss -0.59603 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 55: loss -0.60496 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 56: loss -0.60707 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 56: loss -0.61598 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 57: loss -0.61812 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 57: loss -0.62699 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 58: loss -0.62917 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 58: loss -0.63801 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 59: loss -0.64022 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 59: loss -0.64903 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 60: loss -0.65127 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 60: loss -0.66004 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 61: loss -0.66231 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 61: loss -0.67106 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 62: loss -0.67336 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 62: loss -0.68207 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 63: loss -0.68441 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 63: loss -0.69309 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 64: loss -0.69546 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 64: loss -0.70411 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 65: loss -0.70651 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 65: loss -0.71512 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 66: loss -0.71755 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 66: loss -0.72614 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 67: loss -0.72860 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 67: loss -0.73716 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 68: loss -0.73965 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 68: loss -0.74817 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 69: loss -0.75070 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 69: loss -0.75919 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 70: loss -0.76174 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 70: loss -0.77021 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 71: loss -0.77279 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 71: loss -0.78122 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 72: loss -0.78384 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 72: loss -0.79224 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 73: loss -0.79489 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 73: loss -0.80326 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 74: loss -0.80594 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 74: loss -0.81427 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 75: loss -0.81698 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 75: loss -0.82529 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 76: loss -0.82803 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 76: loss -0.83630 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 77: loss -0.83908 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 77: loss -0.84732 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 78: loss -0.85013 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 78: loss -0.85834 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 79: loss -0.86118 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 79: loss -0.86935 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 80: loss -0.87222 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 80: loss -0.88037 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 81: loss -0.88327 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 81: loss -0.89139 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 82: loss -0.89432 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 82: loss -0.90240 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 83: loss -0.90537 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 83: loss -0.91342 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 84: loss -0.91641 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 84: loss -0.92443 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 85: loss -0.92746 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 85: loss -0.93545 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 86: loss -0.93851 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 86: loss -0.94647 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 87: loss -0.94956 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 87: loss -0.95748 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 88: loss -0.96060 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 88: loss -0.96850 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 89: loss -0.97165 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 89: loss -0.97952 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 90: loss -0.98270 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 90: loss -0.99053 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 91: loss -0.99375 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 91: loss -1.00155 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 92: loss -1.00480 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 92: loss -1.01256 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 93: loss -1.01584 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 93: loss -1.02358 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 94: loss -1.02689 acc 0.33333 roc_auc 0.35700 prc_auc 0.56188[0m
[93maverage test of epoch 94: loss -1.03460 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 95: loss -1.04024 acc 0.33333 roc_auc 0.37900 prc_auc 0.59104[0m
[93maverage test of epoch 95: loss -1.05566 acc 0.34211 roc_auc 0.85385 prc_auc 0.91135[0m
[92maverage training of epoch 96: loss -1.07673 acc 0.33333 roc_auc 0.39860 prc_auc 0.60441[0m
[93maverage test of epoch 96: loss -1.10755 acc 0.34211 roc_auc 0.89385 prc_auc 0.94338[0m
[92maverage training of epoch 97: loss -1.14442 acc 0.33333 roc_auc 0.40400 prc_auc 0.61040[0m
[93maverage test of epoch 97: loss -1.19146 acc 0.34211 roc_auc 0.87538 prc_auc 0.91696[0m
[92maverage training of epoch 98: loss -1.24089 acc 0.33333 roc_auc 0.40830 prc_auc 0.61553[0m
[93maverage test of epoch 98: loss -1.29671 acc 0.34211 roc_auc 0.86615 prc_auc 0.93342[0m
[92maverage training of epoch 99: loss -1.35352 acc 0.33333 roc_auc 0.41400 prc_auc 0.62221[0m
[93maverage test of epoch 99: loss -1.41784 acc 0.34211 roc_auc 0.86154 prc_auc 0.93174[0m
Training model with dataset, testing using fold 1
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=4, bias=True)
    (ReLU1): ReLU()
    (Linear2): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.19881 acc 0.33333 roc_auc 0.41100 prc_auc 0.61619[0m
[93maverage test of epoch 0: loss 0.17473 acc 0.34211 roc_auc 0.38462 prc_auc 0.71668[0m
[92maverage training of epoch 1: loss 0.12639 acc 0.33333 roc_auc 0.43440 prc_auc 0.62519[0m
[93maverage test of epoch 1: loss 0.11110 acc 0.34211 roc_auc 0.43077 prc_auc 0.74226[0m
[92maverage training of epoch 2: loss 0.07618 acc 0.42000 roc_auc 0.45200 prc_auc 0.64334[0m
[93maverage test of epoch 2: loss 0.05590 acc 0.63158 roc_auc 0.56615 prc_auc 0.81194[0m
[92maverage training of epoch 3: loss 0.02801 acc 0.65333 roc_auc 0.47140 prc_auc 0.66333[0m
[93maverage test of epoch 3: loss 0.00695 acc 0.65789 roc_auc 0.70154 prc_auc 0.87818[0m
[92maverage training of epoch 4: loss -0.01713 acc 0.66667 roc_auc 0.48660 prc_auc 0.65807[0m
[93maverage test of epoch 4: loss -0.02779 acc 0.65789 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 5: loss -0.03887 acc 0.66667 roc_auc 0.41040 prc_auc 0.59983[0m
[93maverage test of epoch 5: loss -0.03965 acc 0.65789 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 6: loss -0.05021 acc 0.66667 roc_auc 0.41060 prc_auc 0.59987[0m
[93maverage test of epoch 6: loss -0.05148 acc 0.65789 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 7: loss -0.06142 acc 0.66667 roc_auc 0.41170 prc_auc 0.60014[0m
[93maverage test of epoch 7: loss -0.06288 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 8: loss -0.07257 acc 0.66667 roc_auc 0.42030 prc_auc 0.60488[0m
[93maverage test of epoch 8: loss -0.07389 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 9: loss -0.08362 acc 0.66667 roc_auc 0.42030 prc_auc 0.60485[0m
[93maverage test of epoch 9: loss -0.08491 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 10: loss -0.09467 acc 0.66667 roc_auc 0.42010 prc_auc 0.60497[0m
[93maverage test of epoch 10: loss -0.09593 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 11: loss -0.10571 acc 0.66667 roc_auc 0.42020 prc_auc 0.60497[0m
[93maverage test of epoch 11: loss -0.10694 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 12: loss -0.11676 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 12: loss -0.11796 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 13: loss -0.12781 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 13: loss -0.12898 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 14: loss -0.13886 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 14: loss -0.13999 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 15: loss -0.14991 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 15: loss -0.15101 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 16: loss -0.16096 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 16: loss -0.16203 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 17: loss -0.17201 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 17: loss -0.17305 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 18: loss -0.18305 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 18: loss -0.18406 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 19: loss -0.19410 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 19: loss -0.19508 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 20: loss -0.20515 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 20: loss -0.20610 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 21: loss -0.21620 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 21: loss -0.21712 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 22: loss -0.22725 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 22: loss -0.22813 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 23: loss -0.23830 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 23: loss -0.23915 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 24: loss -0.24935 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 24: loss -0.25017 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 25: loss -0.26040 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 25: loss -0.26119 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 26: loss -0.27145 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 26: loss -0.27220 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 27: loss -0.28249 acc 0.66667 roc_auc 0.42000 prc_auc 0.60458[0m
[93maverage test of epoch 27: loss -0.28322 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 28: loss -0.29354 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 28: loss -0.29424 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 29: loss -0.30459 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 29: loss -0.30526 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 30: loss -0.31564 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 30: loss -0.31627 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 31: loss -0.32669 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 31: loss -0.32729 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 32: loss -0.33774 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 32: loss -0.33831 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 33: loss -0.34879 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 33: loss -0.34933 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 34: loss -0.35984 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 34: loss -0.36034 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 35: loss -0.37089 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 35: loss -0.37136 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 36: loss -0.38194 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 36: loss -0.38238 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 37: loss -0.39299 acc 0.66667 roc_auc 0.42020 prc_auc 0.60463[0m
[93maverage test of epoch 37: loss -0.39340 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 38: loss -0.40404 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 38: loss -0.40442 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 39: loss -0.41508 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 39: loss -0.41543 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 40: loss -0.42613 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 40: loss -0.42645 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 41: loss -0.43718 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 41: loss -0.43747 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -0.44823 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 42: loss -0.44849 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -0.45928 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 43: loss -0.45950 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 44: loss -0.47033 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 44: loss -0.47052 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -0.48138 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 45: loss -0.48154 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -0.49243 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 46: loss -0.49256 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -0.50348 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 47: loss -0.50357 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -0.51453 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 48: loss -0.51459 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -0.52558 acc 0.66667 roc_auc 0.42020 prc_auc 0.60463[0m
[93maverage test of epoch 49: loss -0.52561 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 50: loss -0.53662 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 50: loss -0.53663 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 51: loss -0.54767 acc 0.66667 roc_auc 0.42030 prc_auc 0.60463[0m
[93maverage test of epoch 51: loss -0.54764 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 52: loss -0.55872 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 52: loss -0.55866 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 53: loss -0.56977 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 53: loss -0.56968 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 54: loss -0.58082 acc 0.66667 roc_auc 0.42020 prc_auc 0.60463[0m
[93maverage test of epoch 54: loss -0.58070 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 55: loss -0.59187 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 55: loss -0.59172 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 56: loss -0.60292 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 56: loss -0.60273 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 57: loss -0.61397 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 57: loss -0.61375 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 58: loss -0.62502 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 58: loss -0.62477 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 59: loss -0.63607 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 59: loss -0.63579 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 60: loss -0.64712 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 60: loss -0.64680 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 61: loss -0.65816 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 61: loss -0.65782 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 62: loss -0.66921 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 62: loss -0.66884 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 63: loss -0.68026 acc 0.66667 roc_auc 0.42020 prc_auc 0.60463[0m
[93maverage test of epoch 63: loss -0.67986 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 64: loss -0.69131 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 64: loss -0.69087 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 65: loss -0.70236 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 65: loss -0.70189 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 66: loss -0.71341 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 66: loss -0.71291 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 67: loss -0.72446 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 67: loss -0.72393 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 68: loss -0.73551 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 68: loss -0.73494 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 69: loss -0.74656 acc 0.66667 roc_auc 0.42020 prc_auc 0.60455[0m
[93maverage test of epoch 69: loss -0.74596 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 70: loss -0.75761 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 70: loss -0.75698 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 71: loss -0.76865 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 71: loss -0.76799 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 72: loss -0.77970 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 72: loss -0.77901 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 73: loss -0.79075 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 73: loss -0.79003 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 74: loss -0.80180 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 74: loss -0.80105 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 75: loss -0.81285 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 75: loss -0.81206 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 76: loss -0.82390 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 76: loss -0.82308 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 77: loss -0.83495 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 77: loss -0.83410 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 78: loss -0.84599 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 78: loss -0.84511 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 79: loss -0.85704 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 79: loss -0.85613 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 80: loss -0.86809 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 80: loss -0.86715 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 81: loss -0.87914 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 81: loss -0.87817 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 82: loss -0.89019 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 82: loss -0.88918 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 83: loss -0.90124 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 83: loss -0.90020 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 84: loss -0.91229 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 84: loss -0.91122 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 85: loss -0.92333 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 85: loss -0.92223 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 86: loss -0.93438 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 86: loss -0.93325 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 87: loss -0.94543 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 87: loss -0.94427 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 88: loss -0.95648 acc 0.66667 roc_auc 0.42000 prc_auc 0.60458[0m
[93maverage test of epoch 88: loss -0.95529 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 89: loss -0.96753 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 89: loss -0.96630 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 90: loss -0.97858 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 90: loss -0.97732 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 91: loss -0.98963 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 91: loss -0.98834 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 92: loss -1.00067 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 92: loss -0.99935 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 93: loss -1.01172 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 93: loss -1.01037 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 94: loss -1.02277 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 94: loss -1.02139 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 95: loss -1.03382 acc 0.66667 roc_auc 0.42020 prc_auc 0.60455[0m
[93maverage test of epoch 95: loss -1.03240 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 96: loss -1.04487 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 96: loss -1.04342 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 97: loss -1.05592 acc 0.66667 roc_auc 0.42010 prc_auc 0.60458[0m
[93maverage test of epoch 97: loss -1.05444 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 98: loss -1.06696 acc 0.66667 roc_auc 0.42000 prc_auc 0.60458[0m
[93maverage test of epoch 98: loss -1.06546 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 99: loss -1.07801 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 99: loss -1.07647 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 2
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=4, bias=True)
    (ReLU1): ReLU()
    (Linear2): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.73213 acc 0.66667 roc_auc 0.44860 prc_auc 0.63953[0m
[93maverage test of epoch 0: loss 0.69862 acc 0.65789 roc_auc 0.56154 prc_auc 0.80624[0m
[92maverage training of epoch 1: loss 0.68152 acc 0.66667 roc_auc 0.39070 prc_auc 0.59852[0m
[93maverage test of epoch 1: loss 0.66775 acc 0.65789 roc_auc 0.60000 prc_auc 0.81108[0m
[92maverage training of epoch 2: loss 0.65158 acc 0.66667 roc_auc 0.40370 prc_auc 0.61305[0m
[93maverage test of epoch 2: loss 0.63851 acc 0.65789 roc_auc 0.56769 prc_auc 0.81062[0m
[92maverage training of epoch 3: loss 0.62267 acc 0.66667 roc_auc 0.40200 prc_auc 0.61503[0m
[93maverage test of epoch 3: loss 0.61021 acc 0.65789 roc_auc 0.57077 prc_auc 0.81999[0m
[92maverage training of epoch 4: loss 0.59464 acc 0.66667 roc_auc 0.40170 prc_auc 0.59977[0m
[93maverage test of epoch 4: loss 0.58264 acc 0.65789 roc_auc 0.56308 prc_auc 0.80868[0m
[92maverage training of epoch 5: loss 0.56743 acc 0.66667 roc_auc 0.40760 prc_auc 0.63762[0m
[93maverage test of epoch 5: loss 0.55593 acc 0.65789 roc_auc 0.56154 prc_auc 0.80858[0m
[92maverage training of epoch 6: loss 0.54089 acc 0.66667 roc_auc 0.40600 prc_auc 0.60528[0m
[93maverage test of epoch 6: loss 0.52988 acc 0.65789 roc_auc 0.57231 prc_auc 0.82058[0m
[92maverage training of epoch 7: loss 0.51510 acc 0.66667 roc_auc 0.40560 prc_auc 0.62144[0m
[93maverage test of epoch 7: loss 0.50451 acc 0.65789 roc_auc 0.58462 prc_auc 0.82148[0m
[92maverage training of epoch 8: loss 0.49004 acc 0.66667 roc_auc 0.40240 prc_auc 0.60867[0m
[93maverage test of epoch 8: loss 0.47981 acc 0.65789 roc_auc 0.63077 prc_auc 0.82761[0m
[92maverage training of epoch 9: loss 0.46554 acc 0.66667 roc_auc 0.39260 prc_auc 0.59969[0m
[93maverage test of epoch 9: loss 0.45569 acc 0.65789 roc_auc 0.60462 prc_auc 0.81940[0m
[92maverage training of epoch 10: loss 0.44165 acc 0.66667 roc_auc 0.39280 prc_auc 0.60394[0m
[93maverage test of epoch 10: loss 0.43227 acc 0.65789 roc_auc 0.62615 prc_auc 0.83061[0m
[92maverage training of epoch 11: loss 0.41822 acc 0.66667 roc_auc 0.37820 prc_auc 0.59674[0m
[93maverage test of epoch 11: loss 0.40916 acc 0.65789 roc_auc 0.54923 prc_auc 0.78048[0m
[92maverage training of epoch 12: loss 0.37684 acc 0.66667 roc_auc 0.52200 prc_auc 0.67080[0m
[93maverage test of epoch 12: loss 0.32847 acc 0.65789 roc_auc 0.05692 prc_auc 0.46586[0m
[92maverage training of epoch 13: loss 0.27161 acc 0.66667 roc_auc 0.51440 prc_auc 0.66524[0m
[93maverage test of epoch 13: loss 0.23056 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 14: loss 0.22430 acc 0.66667 roc_auc 0.37690 prc_auc 0.57465[0m
[93maverage test of epoch 14: loss 0.21955 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 15: loss 0.21325 acc 0.66667 roc_auc 0.37690 prc_auc 0.57465[0m
[93maverage test of epoch 15: loss 0.20853 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 16: loss 0.20220 acc 0.66667 roc_auc 0.37680 prc_auc 0.57485[0m
[93maverage test of epoch 16: loss 0.19752 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 17: loss 0.19115 acc 0.66667 roc_auc 0.37670 prc_auc 0.57480[0m
[93maverage test of epoch 17: loss 0.18650 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 18: loss 0.18011 acc 0.66667 roc_auc 0.37690 prc_auc 0.57463[0m
[93maverage test of epoch 18: loss 0.17548 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 19: loss 0.16906 acc 0.66667 roc_auc 0.37680 prc_auc 0.57460[0m
[93maverage test of epoch 19: loss 0.16447 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 20: loss 0.15801 acc 0.66667 roc_auc 0.37670 prc_auc 0.57442[0m
[93maverage test of epoch 20: loss 0.15345 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 21: loss 0.14696 acc 0.66667 roc_auc 0.37670 prc_auc 0.57442[0m
[93maverage test of epoch 21: loss 0.14244 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 22: loss 0.13591 acc 0.66667 roc_auc 0.37680 prc_auc 0.57460[0m
[93maverage test of epoch 22: loss 0.13142 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 23: loss 0.12487 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 23: loss 0.12040 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 24: loss 0.11382 acc 0.66667 roc_auc 0.37670 prc_auc 0.57442[0m
[93maverage test of epoch 24: loss 0.10939 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 25: loss 0.10277 acc 0.66667 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 25: loss 0.09837 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 26: loss 0.09172 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 26: loss 0.08735 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 27: loss 0.08067 acc 0.66667 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 27: loss 0.07634 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 28: loss 0.06963 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 28: loss 0.06532 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 29: loss 0.05858 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 29: loss 0.05430 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 30: loss 0.04753 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 30: loss 0.04329 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 31: loss 0.03648 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 31: loss 0.03227 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 32: loss 0.02543 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 32: loss 0.02125 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 33: loss 0.01438 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 33: loss 0.01024 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 34: loss 0.00334 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 34: loss -0.00078 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 35: loss -0.00771 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 35: loss -0.01180 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 36: loss -0.01876 acc 0.66667 roc_auc 0.37660 prc_auc 0.57448[0m
[93maverage test of epoch 36: loss -0.02281 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 37: loss -0.02981 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 37: loss -0.03383 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 38: loss -0.04086 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 38: loss -0.04485 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 39: loss -0.05190 acc 0.66667 roc_auc 0.37650 prc_auc 0.57448[0m
[93maverage test of epoch 39: loss -0.05586 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 40: loss -0.06295 acc 0.66667 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 40: loss -0.06688 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 41: loss -0.07400 acc 0.66667 roc_auc 0.37640 prc_auc 0.57445[0m
[93maverage test of epoch 41: loss -0.07790 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -0.08505 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 42: loss -0.08891 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -0.09610 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 43: loss -0.09993 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 44: loss -0.10714 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 44: loss -0.11095 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -0.11819 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 45: loss -0.12196 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -0.12924 acc 0.66667 roc_auc 0.37640 prc_auc 0.57439[0m
[93maverage test of epoch 46: loss -0.13298 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -0.14029 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 47: loss -0.14400 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -0.15134 acc 0.66667 roc_auc 0.37650 prc_auc 0.57448[0m
[93maverage test of epoch 48: loss -0.15501 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -0.16238 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 49: loss -0.16603 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 50: loss -0.17343 acc 0.66667 roc_auc 0.37650 prc_auc 0.57448[0m
[93maverage test of epoch 50: loss -0.17705 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 51: loss -0.18448 acc 0.66667 roc_auc 0.37640 prc_auc 0.57439[0m
[93maverage test of epoch 51: loss -0.18806 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 52: loss -0.19553 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 52: loss -0.19908 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 53: loss -0.20658 acc 0.66667 roc_auc 0.37640 prc_auc 0.57448[0m
[93maverage test of epoch 53: loss -0.21009 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 54: loss -0.21762 acc 0.66667 roc_auc 0.37640 prc_auc 0.57439[0m
[93maverage test of epoch 54: loss -0.22111 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 55: loss -0.22867 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 55: loss -0.23213 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 56: loss -0.23972 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 56: loss -0.24314 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 57: loss -0.25077 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 57: loss -0.25416 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 58: loss -0.26181 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 58: loss -0.26518 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 59: loss -0.27286 acc 0.66667 roc_auc 0.37640 prc_auc 0.57448[0m
[93maverage test of epoch 59: loss -0.27619 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 60: loss -0.28391 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 60: loss -0.28721 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 61: loss -0.29496 acc 0.66667 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 61: loss -0.29822 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 62: loss -0.30601 acc 0.66667 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 62: loss -0.30924 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 63: loss -0.31705 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 63: loss -0.32026 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 64: loss -0.32810 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 64: loss -0.33127 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 65: loss -0.33915 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 65: loss -0.34229 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 66: loss -0.35020 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 66: loss -0.35331 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 67: loss -0.36125 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 67: loss -0.36432 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 68: loss -0.37229 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 68: loss -0.37534 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 69: loss -0.38334 acc 0.66667 roc_auc 0.37640 prc_auc 0.57439[0m
[93maverage test of epoch 69: loss -0.38636 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 70: loss -0.39439 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 70: loss -0.39737 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 71: loss -0.40544 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 71: loss -0.40839 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 72: loss -0.41648 acc 0.66667 roc_auc 0.37650 prc_auc 0.57445[0m
[93maverage test of epoch 72: loss -0.41940 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 73: loss -0.42753 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 73: loss -0.43042 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 74: loss -0.43858 acc 0.66667 roc_auc 0.37650 prc_auc 0.57448[0m
[93maverage test of epoch 74: loss -0.44144 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 75: loss -0.44963 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 75: loss -0.45245 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 76: loss -0.46068 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 76: loss -0.46347 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 77: loss -0.47172 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 77: loss -0.47449 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 78: loss -0.48277 acc 0.66667 roc_auc 0.37640 prc_auc 0.57439[0m
[93maverage test of epoch 78: loss -0.48550 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 79: loss -0.49382 acc 0.66667 roc_auc 0.37650 prc_auc 0.57445[0m
[93maverage test of epoch 79: loss -0.49652 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 80: loss -0.50487 acc 0.66667 roc_auc 0.37670 prc_auc 0.57448[0m
[93maverage test of epoch 80: loss -0.50754 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 81: loss -0.51592 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 81: loss -0.51855 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 82: loss -0.52696 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 82: loss -0.52957 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 83: loss -0.53801 acc 0.66667 roc_auc 0.37670 prc_auc 0.57448[0m
[93maverage test of epoch 83: loss -0.54059 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 84: loss -0.54906 acc 0.66667 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 84: loss -0.55160 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 85: loss -0.56011 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 85: loss -0.56262 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 86: loss -0.57116 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 86: loss -0.57363 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 87: loss -0.58220 acc 0.66667 roc_auc 0.37660 prc_auc 0.57448[0m
[93maverage test of epoch 87: loss -0.58465 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 88: loss -0.59325 acc 0.66667 roc_auc 0.37660 prc_auc 0.57448[0m
[93maverage test of epoch 88: loss -0.59567 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 89: loss -0.60430 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 89: loss -0.60668 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 90: loss -0.61535 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 90: loss -0.61770 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 91: loss -0.62640 acc 0.66667 roc_auc 0.37650 prc_auc 0.57445[0m
[93maverage test of epoch 91: loss -0.62872 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 92: loss -0.63744 acc 0.66667 roc_auc 0.37640 prc_auc 0.57439[0m
[93maverage test of epoch 92: loss -0.63973 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 93: loss -0.64849 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 93: loss -0.65075 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 94: loss -0.65954 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 94: loss -0.66177 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 95: loss -0.67059 acc 0.66667 roc_auc 0.37640 prc_auc 0.57439[0m
[93maverage test of epoch 95: loss -0.67278 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 96: loss -0.68163 acc 0.66667 roc_auc 0.37650 prc_auc 0.57448[0m
[93maverage test of epoch 96: loss -0.68380 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 97: loss -0.69268 acc 0.66667 roc_auc 0.37650 prc_auc 0.57445[0m
[93maverage test of epoch 97: loss -0.69481 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 98: loss -0.70373 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 98: loss -0.70583 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 99: loss -0.71478 acc 0.66667 roc_auc 0.37640 prc_auc 0.57448[0m
[93maverage test of epoch 99: loss -0.71685 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 3
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=4, bias=True)
    (ReLU1): ReLU()
    (Linear2): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.03701 acc 0.66225 roc_auc 0.54275 prc_auc 0.68392[0m
[93maverage test of epoch 0: loss -0.07086 acc 0.67568 roc_auc 0.57667 prc_auc 0.75829[0m
[92maverage training of epoch 1: loss -0.07833 acc 0.66225 roc_auc 0.53451 prc_auc 0.67583[0m
[93maverage test of epoch 1: loss -0.10835 acc 0.67568 roc_auc 0.56333 prc_auc 0.72795[0m
[92maverage training of epoch 2: loss -0.10744 acc 0.66225 roc_auc 0.43353 prc_auc 0.61394[0m
[93maverage test of epoch 2: loss -0.12863 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 3: loss -0.12664 acc 0.66225 roc_auc 0.43882 prc_auc 0.61149[0m
[93maverage test of epoch 3: loss -0.14755 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 4: loss -0.14542 acc 0.66225 roc_auc 0.43961 prc_auc 0.61358[0m
[93maverage test of epoch 4: loss -0.16609 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 5: loss -0.16384 acc 0.66225 roc_auc 0.43922 prc_auc 0.61919[0m
[93maverage test of epoch 5: loss -0.18427 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 6: loss -0.18193 acc 0.66225 roc_auc 0.43775 prc_auc 0.61641[0m
[93maverage test of epoch 6: loss -0.20215 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 7: loss -0.19968 acc 0.66225 roc_auc 0.43931 prc_auc 0.61846[0m
[93maverage test of epoch 7: loss -0.21915 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 8: loss -0.21496 acc 0.66225 roc_auc 0.37745 prc_auc 0.57001[0m
[93maverage test of epoch 8: loss -0.23335 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 9: loss -0.22905 acc 0.66225 roc_auc 0.37588 prc_auc 0.57031[0m
[93maverage test of epoch 9: loss -0.24746 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 10: loss -0.24307 acc 0.66225 roc_auc 0.37490 prc_auc 0.56828[0m
[93maverage test of epoch 10: loss -0.26153 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 11: loss -0.25709 acc 0.66225 roc_auc 0.37382 prc_auc 0.56752[0m
[93maverage test of epoch 11: loss -0.27563 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 12: loss -0.27118 acc 0.66225 roc_auc 0.37578 prc_auc 0.57225[0m
[93maverage test of epoch 12: loss -0.28983 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 13: loss -0.28541 acc 0.66225 roc_auc 0.37559 prc_auc 0.57433[0m
[93maverage test of epoch 13: loss -0.30421 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 14: loss -0.29988 acc 0.66225 roc_auc 0.37412 prc_auc 0.57472[0m
[93maverage test of epoch 14: loss -0.31889 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 15: loss -0.31469 acc 0.66225 roc_auc 0.37176 prc_auc 0.57399[0m
[93maverage test of epoch 15: loss -0.33396 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 16: loss -0.32995 acc 0.66225 roc_auc 0.36990 prc_auc 0.57445[0m
[93maverage test of epoch 16: loss -0.34952 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 17: loss -0.34575 acc 0.66225 roc_auc 0.37029 prc_auc 0.57601[0m
[93maverage test of epoch 17: loss -0.36566 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 18: loss -0.36216 acc 0.66225 roc_auc 0.36951 prc_auc 0.57578[0m
[93maverage test of epoch 18: loss -0.38246 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 19: loss -0.37926 acc 0.66225 roc_auc 0.36912 prc_auc 0.57584[0m
[93maverage test of epoch 19: loss -0.39996 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 20: loss -0.39706 acc 0.66225 roc_auc 0.36971 prc_auc 0.57595[0m
[93maverage test of epoch 20: loss -0.41817 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 21: loss -0.41558 acc 0.66225 roc_auc 0.36980 prc_auc 0.57532[0m
[93maverage test of epoch 21: loss -0.43710 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 22: loss -0.43480 acc 0.66225 roc_auc 0.37010 prc_auc 0.57427[0m
[93maverage test of epoch 22: loss -0.45672 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 23: loss -0.45470 acc 0.66225 roc_auc 0.37078 prc_auc 0.57444[0m
[93maverage test of epoch 23: loss -0.47701 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 24: loss -0.47526 acc 0.66225 roc_auc 0.37088 prc_auc 0.57357[0m
[93maverage test of epoch 24: loss -0.49795 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 25: loss -0.49644 acc 0.66225 roc_auc 0.37373 prc_auc 0.57483[0m
[93maverage test of epoch 25: loss -0.51950 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 26: loss -0.51821 acc 0.66225 roc_auc 0.37431 prc_auc 0.57472[0m
[93maverage test of epoch 26: loss -0.54162 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 27: loss -0.54055 acc 0.66225 roc_auc 0.37412 prc_auc 0.57453[0m
[93maverage test of epoch 27: loss -0.56430 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 28: loss -0.56343 acc 0.66225 roc_auc 0.37549 prc_auc 0.57445[0m
[93maverage test of epoch 28: loss -0.58751 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 29: loss -0.58682 acc 0.66225 roc_auc 0.37529 prc_auc 0.57285[0m
[93maverage test of epoch 29: loss -0.61123 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 30: loss -0.61071 acc 0.66225 roc_auc 0.37588 prc_auc 0.57241[0m
[93maverage test of epoch 30: loss -0.63543 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 31: loss -0.63507 acc 0.66225 roc_auc 0.37510 prc_auc 0.57080[0m
[93maverage test of epoch 31: loss -0.66011 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 32: loss -0.65990 acc 0.66225 roc_auc 0.37294 prc_auc 0.56711[0m
[93maverage test of epoch 32: loss -0.68524 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 33: loss -0.68518 acc 0.66225 roc_auc 0.37314 prc_auc 0.56697[0m
[93maverage test of epoch 33: loss -0.71082 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 34: loss -0.71089 acc 0.66225 roc_auc 0.37353 prc_auc 0.56720[0m
[93maverage test of epoch 34: loss -0.73683 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 35: loss -0.73703 acc 0.66225 roc_auc 0.37412 prc_auc 0.56780[0m
[93maverage test of epoch 35: loss -0.76327 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 36: loss -0.76359 acc 0.66225 roc_auc 0.37431 prc_auc 0.56817[0m
[93maverage test of epoch 36: loss -0.79013 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0mUsing backend: pytorch
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

[92maverage training of epoch 37: loss -0.79057 acc 0.66225 roc_auc 0.37529 prc_auc 0.56910[0m
[93maverage test of epoch 37: loss -0.81740 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 38: loss -0.81795 acc 0.66225 roc_auc 0.37578 prc_auc 0.56962[0m
[93maverage test of epoch 38: loss -0.84508 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 39: loss -0.84574 acc 0.66225 roc_auc 0.37627 prc_auc 0.57045[0m
[93maverage test of epoch 39: loss -0.87315 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 40: loss -0.87392 acc 0.66225 roc_auc 0.37588 prc_auc 0.57050[0m
[93maverage test of epoch 40: loss -0.90163 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 41: loss -0.90249 acc 0.66225 roc_auc 0.37647 prc_auc 0.57037[0m
[93maverage test of epoch 41: loss -0.93050 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 42: loss -0.93146 acc 0.66225 roc_auc 0.37706 prc_auc 0.57060[0m
[93maverage test of epoch 42: loss -0.95975 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 43: loss -0.96081 acc 0.66225 roc_auc 0.37667 prc_auc 0.57059[0m
[93maverage test of epoch 43: loss -0.98940 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 44: loss -0.99055 acc 0.66225 roc_auc 0.37686 prc_auc 0.57024[0m
[93maverage test of epoch 44: loss -1.01943 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 45: loss -1.02067 acc 0.66225 roc_auc 0.37696 prc_auc 0.56972[0m
[93maverage test of epoch 45: loss -1.04985 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 46: loss -1.05117 acc 0.66225 roc_auc 0.37735 prc_auc 0.56986[0m
[93maverage test of epoch 46: loss -1.08064 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 47: loss -1.08204 acc 0.66225 roc_auc 0.37775 prc_auc 0.57004[0m
[93maverage test of epoch 47: loss -1.11181 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 48: loss -1.11330 acc 0.66225 roc_auc 0.37843 prc_auc 0.57078[0m
[93maverage test of epoch 48: loss -1.14337 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 49: loss -1.14493 acc 0.66225 roc_auc 0.37804 prc_auc 0.57052[0m
[93maverage test of epoch 49: loss -1.17529 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 50: loss -1.17693 acc 0.66225 roc_auc 0.37843 prc_auc 0.57069[0m
[93maverage test of epoch 50: loss -1.20760 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 51: loss -1.20931 acc 0.66225 roc_auc 0.37824 prc_auc 0.57057[0m
[93maverage test of epoch 51: loss -1.24027 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 52: loss -1.29088 acc 0.66225 roc_auc 0.41353 prc_auc 0.60378[0m
[93maverage test of epoch 52: loss -1.39611 acc 0.67568 roc_auc 0.34833 prc_auc 0.70172[0m
[92maverage training of epoch 53: loss -1.47400 acc 0.66225 roc_auc 0.42608 prc_auc 0.61804[0m
[93maverage test of epoch 53: loss -1.59230 acc 0.67568 roc_auc 0.85667 prc_auc 0.91893[0m
[92maverage training of epoch 54: loss -1.68711 acc 0.66225 roc_auc 0.43431 prc_auc 0.63519[0m
[93maverage test of epoch 54: loss -1.82296 acc 0.67568 roc_auc 0.85000 prc_auc 0.91448[0m
[92maverage training of epoch 55: loss -1.92337 acc 0.66225 roc_auc 0.44490 prc_auc 0.65784[0m
[93maverage test of epoch 55: loss -2.06060 acc 0.67568 roc_auc 0.85000 prc_auc 0.91471[0m
[92maverage training of epoch 56: loss -2.15770 acc 0.66225 roc_auc 0.44157 prc_auc 0.64854[0m
[93maverage test of epoch 56: loss -2.29293 acc 0.67568 roc_auc 0.85333 prc_auc 0.91893[0m
[92maverage training of epoch 57: loss -2.38967 acc 0.66225 roc_auc 0.41941 prc_auc 0.63497[0m
[93maverage test of epoch 57: loss -2.52728 acc 0.67568 roc_auc 0.85333 prc_auc 0.91916[0m
[92maverage training of epoch 58: loss -2.62551 acc 0.66225 roc_auc 0.41922 prc_auc 0.62101[0m
[93maverage test of epoch 58: loss -2.76413 acc 0.67568 roc_auc 0.85333 prc_auc 0.91939[0m
[92maverage training of epoch 59: loss -2.86353 acc 0.66225 roc_auc 0.43549 prc_auc 0.61767[0m
[93maverage test of epoch 59: loss -3.00887 acc 0.67568 roc_auc 0.85333 prc_auc 0.91918[0m
[92maverage training of epoch 60: loss -3.11481 acc 0.66225 roc_auc 0.43608 prc_auc 0.61857[0m
[93maverage test of epoch 60: loss -3.27035 acc 0.67568 roc_auc 0.85333 prc_auc 0.91963[0m
[92maverage training of epoch 61: loss -3.38625 acc 0.66225 roc_auc 0.43892 prc_auc 0.62056[0m
[93maverage test of epoch 61: loss -3.55684 acc 0.67568 roc_auc 0.85333 prc_auc 0.91977[0m
[92maverage training of epoch 62: loss -3.69072 acc 0.66225 roc_auc 0.44059 prc_auc 0.62267[0m
[93maverage test of epoch 62: loss -3.88717 acc 0.67568 roc_auc 0.85500 prc_auc 0.92060[0m
[92maverage training of epoch 63: loss -4.05147 acc 0.66225 roc_auc 0.44314 prc_auc 0.62512[0m
[93maverage test of epoch 63: loss -4.28466 acc 0.67568 roc_auc 0.85500 prc_auc 0.91957[0m
[92maverage training of epoch 64: loss -4.47426 acc 0.66225 roc_auc 0.44392 prc_auc 0.62572[0m
[93maverage test of epoch 64: loss -4.72988 acc 0.67568 roc_auc 0.85333 prc_auc 0.91936[0m
[92maverage training of epoch 65: loss -4.92290 acc 0.66225 roc_auc 0.44353 prc_auc 0.62533[0m
[93maverage test of epoch 65: loss -5.18275 acc 0.67568 roc_auc 0.85500 prc_auc 0.91931[0m
[92maverage training of epoch 66: loss -5.37260 acc 0.66225 roc_auc 0.44333 prc_auc 0.62516[0m
[93maverage test of epoch 66: loss -5.63408 acc 0.67568 roc_auc 0.85500 prc_auc 0.92063[0m
[92maverage training of epoch 67: loss -5.82196 acc 0.66225 roc_auc 0.44353 prc_auc 0.62521[0m
[93maverage test of epoch 67: loss -6.08747 acc 0.67568 roc_auc 0.85333 prc_auc 0.92084[0m
[92maverage training of epoch 68: loss -6.27600 acc 0.66225 roc_auc 0.44333 prc_auc 0.62516[0m
[93maverage test of epoch 68: loss -6.54803 acc 0.67568 roc_auc 0.85833 prc_auc 0.92119[0m
[92maverage training of epoch 69: loss -6.73910 acc 0.66225 roc_auc 0.44314 prc_auc 0.62496[0m
[93maverage test of epoch 69: loss -7.01945 acc 0.67568 roc_auc 0.85167 prc_auc 0.92021[0m
[92maverage training of epoch 70: loss -7.21406 acc 0.66225 roc_auc 0.44314 prc_auc 0.62496[0m
[93maverage test of epoch 70: loss -7.50388 acc 0.67568 roc_auc 0.85667 prc_auc 0.91960[0m
[92maverage training of epoch 71: loss -7.70299 acc 0.66225 roc_auc 0.44275 prc_auc 0.62438[0m
[93maverage test of epoch 71: loss -8.00340 acc 0.67568 roc_auc 0.85667 prc_auc 0.92092[0m
[92maverage training of epoch 72: loss -8.20773 acc 0.66225 roc_auc 0.44255 prc_auc 0.62433[0m
[93maverage test of epoch 72: loss -8.51962 acc 0.67568 roc_auc 0.85333 prc_auc 0.92055[0m
[92maverage training of epoch 73: loss -8.72973 acc 0.66225 roc_auc 0.44137 prc_auc 0.62311[0m
[93maverage test of epoch 73: loss -9.05386 acc 0.67568 roc_auc 0.86167 prc_auc 0.92140[0m
[92maverage training of epoch 74: loss -9.27023 acc 0.66225 roc_auc 0.44118 prc_auc 0.62310[0m
[93maverage test of epoch 74: loss -9.60729 acc 0.67568 roc_auc 0.86167 prc_auc 0.92157[0m
[92maverage training of epoch 75: loss -9.83033 acc 0.66225 roc_auc 0.44078 prc_auc 0.62273[0m
[93maverage test of epoch 75: loss -10.18096 acc 0.67568 roc_auc 0.85667 prc_auc 0.91783[0m
[92maverage training of epoch 76: loss -10.41102 acc 0.66225 roc_auc 0.44039 prc_auc 0.62265[0m
[93maverage test of epoch 76: loss -10.77583 acc 0.67568 roc_auc 0.86500 prc_auc 0.92251[0m
[92maverage training of epoch 77: loss -11.01284 acc 0.66225 roc_auc 0.44020 prc_auc 0.62259[0m
[93maverage test of epoch 77: loss -11.39180 acc 0.67568 roc_auc 0.85167 prc_auc 0.91363[0m
[92maverage training of epoch 78: loss -11.63556 acc 0.66225 roc_auc 0.43961 prc_auc 0.62231[0m
[93maverage test of epoch 78: loss -12.02902 acc 0.67568 roc_auc 0.87167 prc_auc 0.91974[0m
[92maverage training of epoch 79: loss -12.27991 acc 0.66225 roc_auc 0.43980 prc_auc 0.62248[0m
[93maverage test of epoch 79: loss -12.68861 acc 0.67568 roc_auc 0.86333 prc_auc 0.91022[0m
[92maverage training of epoch 80: loss -12.94703 acc 0.66225 roc_auc 0.43990 prc_auc 0.62248[0m
[93maverage test of epoch 80: loss -13.37165 acc 0.67568 roc_auc 0.84167 prc_auc 0.88514[0m
[92maverage training of epoch 81: loss -13.63793 acc 0.66225 roc_auc 0.43980 prc_auc 0.62254[0m
[93maverage test of epoch 81: loss -14.07914 acc 0.67568 roc_auc 0.86000 prc_auc 0.89252[0m
[92maverage training of epoch 82: loss -14.35361 acc 0.66225 roc_auc 0.43941 prc_auc 0.62239[0m
[93maverage test of epoch 82: loss -14.81206 acc 0.67568 roc_auc 0.68000 prc_auc 0.78703[0m
[92maverage training of epoch 83: loss -15.09499 acc 0.66225 roc_auc 0.43951 prc_auc 0.62239[0m
[93maverage test of epoch 83: loss -15.57129 acc 0.67568 roc_auc 0.64000 prc_auc 0.76649[0m
[92maverage training of epoch 84: loss -15.86296 acc 0.66225 roc_auc 0.43951 prc_auc 0.62239[0m
[93maverage test of epoch 84: loss -16.35773 acc 0.67568 roc_auc 0.70000 prc_auc 0.79751[0m
[92maverage training of epoch 85: loss -16.65838 acc 0.66225 roc_auc 0.43971 prc_auc 0.62247[0m
[93maverage test of epoch 85: loss -17.17224 acc 0.67568 roc_auc 0.84000 prc_auc 0.87854[0m
[92maverage training of epoch 86: loss -17.48200 acc 0.66225 roc_auc 0.43971 prc_auc 0.62247[0m
[93maverage test of epoch 86: loss -18.01530 acc 0.67568 roc_auc 0.84000 prc_auc 0.87252[0m
[92maverage training of epoch 87: loss -18.33361 acc 0.66225 roc_auc 0.43971 prc_auc 0.62247[0m
[93maverage test of epoch 87: loss -18.88608 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 88: loss -19.21233 acc 0.66225 roc_auc 0.44010 prc_auc 0.62287[0m
[93maverage test of epoch 88: loss -19.78370 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 89: loss -20.11759 acc 0.66225 roc_auc 0.43980 prc_auc 0.62265[0m
[93maverage test of epoch 89: loss -20.70810 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 90: loss -21.04988 acc 0.66225 roc_auc 0.43971 prc_auc 0.62243[0m
[93maverage test of epoch 90: loss -21.66024 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 91: loss -22.01024 acc 0.66225 roc_auc 0.43990 prc_auc 0.62268[0m
[93maverage test of epoch 91: loss -22.64111 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 92: loss -22.99967 acc 0.66225 roc_auc 0.43873 prc_auc 0.62097[0m
[93maverage test of epoch 92: loss -23.65179 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 93: loss -24.01915 acc 0.66225 roc_auc 0.43892 prc_auc 0.62122[0m
[93maverage test of epoch 93: loss -24.69310 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 94: loss -25.06912 acc 0.66225 roc_auc 0.43863 prc_auc 0.62267[0m
[93maverage test of epoch 94: loss -25.76485 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 95: loss -26.14940 acc 0.66225 roc_auc 0.43686 prc_auc 0.62029[0m
[93maverage test of epoch 95: loss -26.86733 acc 0.67568 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 96: loss -27.26063 acc 0.66225 roc_auc 0.43598 prc_auc 0.62046[0m
[93maverage test of epoch 96: loss -28.00135 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 97: loss -28.40353 acc 0.66225 roc_auc 0.44000 prc_auc 0.62784[0m
[93maverage test of epoch 97: loss -29.16753 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 98: loss -29.57877 acc 0.66225 roc_auc 0.44078 prc_auc 0.63305[0m
[93maverage test of epoch 98: loss -30.36654 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 99: loss -30.78664 acc 0.66225 roc_auc 0.43853 prc_auc 0.62455[0m
[93maverage test of epoch 99: loss -31.59817 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
Training model with dataset, testing using fold 4
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=4, bias=True)
    (ReLU1): ReLU()
    (Linear2): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.14949 acc 0.33775 roc_auc 0.39794 prc_auc 0.60670[0m
[93maverage test of epoch 0: loss -0.16729 acc 0.32432 roc_auc 0.93167 prc_auc 0.96936[0m
[92maverage training of epoch 1: loss -0.19315 acc 0.33775 roc_auc 0.38765 prc_auc 0.59932[0m
[93maverage test of epoch 1: loss -0.21035 acc 0.32432 roc_auc 0.92667 prc_auc 0.96674[0m
[92maverage training of epoch 2: loss -0.23607 acc 0.33775 roc_auc 0.39167 prc_auc 0.60543[0m
[93maverage test of epoch 2: loss -0.25777 acc 0.32432 roc_auc 0.92667 prc_auc 0.96814[0m
[92maverage training of epoch 3: loss -0.28399 acc 0.33775 roc_auc 0.43824 prc_auc 0.64780[0m
[93maverage test of epoch 3: loss -0.30071 acc 0.32432 roc_auc 0.62000 prc_auc 0.75351[0m
[92maverage training of epoch 4: loss -0.32188 acc 0.33775 roc_auc 0.42402 prc_auc 0.63621[0m
[93maverage test of epoch 4: loss -0.33675 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 5: loss -0.35731 acc 0.33775 roc_auc 0.42157 prc_auc 0.63503[0m
[93maverage test of epoch 5: loss -0.37234 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 6: loss -0.39254 acc 0.33775 roc_auc 0.42078 prc_auc 0.63456[0m
[93maverage test of epoch 6: loss -0.40819 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 7: loss -0.42818 acc 0.33775 roc_auc 0.41941 prc_auc 0.63406[0m
[93maverage test of epoch 7: loss -0.44452 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 8: loss -0.46438 acc 0.33775 roc_auc 0.41471 prc_auc 0.62410[0m
[93maverage test of epoch 8: loss -0.48152 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 9: loss -0.50201 acc 0.33775 roc_auc 0.41647 prc_auc 0.61776[0m
[93maverage test of epoch 9: loss -0.51933 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 10: loss -0.54018 acc 0.33775 roc_auc 0.42118 prc_auc 0.62886[0m
[93maverage test of epoch 10: loss -0.55783 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 11: loss -0.57947 acc 0.33775 roc_auc 0.43196 prc_auc 0.64014[0m
[93maverage test of epoch 11: loss -0.59706 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 12: loss -0.61933 acc 0.33775 roc_auc 0.43667 prc_auc 0.64763[0m
[93maverage test of epoch 12: loss -0.63702 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 13: loss -0.65981 acc 0.33775 roc_auc 0.43667 prc_auc 0.64774[0m
[93maverage test of epoch 13: loss -0.67773 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 14: loss -0.70089 acc 0.34437 roc_auc 0.42824 prc_auc 0.63082[0m
[93maverage test of epoch 14: loss -0.71918 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 15: loss -0.74272 acc 0.35099 roc_auc 0.42275 prc_auc 0.62761[0m
[93maverage test of epoch 15: loss -0.76139 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 16: loss -0.78528 acc 0.35099 roc_auc 0.42157 prc_auc 0.62720[0m
[93maverage test of epoch 16: loss -0.80434 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 17: loss -0.82862 acc 0.35099 roc_auc 0.41980 prc_auc 0.62668[0m
[93maverage test of epoch 17: loss -0.84805 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 18: loss -0.87286 acc 0.41722 roc_auc 0.42059 prc_auc 0.62699[0m
[93maverage test of epoch 18: loss -0.89251 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 19: loss -1.01589 acc 0.66225 roc_auc 0.42235 prc_auc 0.62117[0m
[93maverage test of epoch 19: loss -1.18771 acc 0.67568 roc_auc 0.19667 prc_auc 0.59145[0m
[92maverage training of epoch 20: loss -1.32083 acc 0.66225 roc_auc 0.41529 prc_auc 0.61625[0m
[93maverage test of epoch 20: loss -1.47286 acc 0.67568 roc_auc 0.26333 prc_auc 0.63048[0m
[92maverage training of epoch 21: loss -1.59650 acc 0.66225 roc_auc 0.41235 prc_auc 0.60787[0m
[93maverage test of epoch 21: loss -1.75580 acc 0.67568 roc_auc 0.31333 prc_auc 0.65854[0m
[92maverage training of epoch 22: loss -1.88058 acc 0.66225 roc_auc 0.40941 prc_auc 0.60642[0m
[93maverage test of epoch 22: loss -2.05421 acc 0.67568 roc_auc 0.21667 prc_auc 0.59783[0m
[92maverage training of epoch 23: loss -2.19244 acc 0.66225 roc_auc 0.40941 prc_auc 0.60605[0m
[93maverage test of epoch 23: loss -2.39208 acc 0.67568 roc_auc 0.18000 prc_auc 0.55635[0m
[92maverage training of epoch 24: loss -2.54725 acc 0.66225 roc_auc 0.41020 prc_auc 0.60622[0m
[93maverage test of epoch 24: loss -2.77491 acc 0.67568 roc_auc 0.09333 prc_auc 0.49863[0m
[92maverage training of epoch 25: loss -2.94370 acc 0.66225 roc_auc 0.41216 prc_auc 0.60720[0m
[93maverage test of epoch 25: loss -3.19479 acc 0.67568 roc_auc 0.23000 prc_auc 0.60151[0m
[92maverage training of epoch 26: loss -3.37213 acc 0.66225 roc_auc 0.41608 prc_auc 0.61752[0m
[93maverage test of epoch 26: loss -3.63953 acc 0.67568 roc_auc 0.35000 prc_auc 0.70636[0m
[92maverage training of epoch 27: loss -3.81569 acc 0.66225 roc_auc 0.41922 prc_auc 0.62056[0m
[93maverage test of epoch 27: loss -4.08896 acc 0.67568 roc_auc 0.53667 prc_auc 0.81049[0m
[92maverage training of epoch 28: loss -4.25485 acc 0.66225 roc_auc 0.42392 prc_auc 0.62346[0m
[93maverage test of epoch 28: loss -4.52541 acc 0.67568 roc_auc 0.74167 prc_auc 0.89220[0m
[92maverage training of epoch 29: loss -4.67697 acc 0.66225 roc_auc 0.42706 prc_auc 0.62851[0m
[93maverage test of epoch 29: loss -4.94225 acc 0.67568 roc_auc 0.93667 prc_auc 0.97102[0m
[92maverage training of epoch 30: loss -5.08017 acc 0.66225 roc_auc 0.43098 prc_auc 0.63514[0m
[93maverage test of epoch 30: loss -5.34213 acc 0.67568 roc_auc 0.92833 prc_auc 0.97135[0m
[92maverage training of epoch 31: loss -5.46938 acc 0.66225 roc_auc 0.43235 prc_auc 0.63602[0m
[93maverage test of epoch 31: loss -5.73091 acc 0.67568 roc_auc 0.92833 prc_auc 0.97135[0m
[92maverage training of epoch 32: loss -5.85073 acc 0.66225 roc_auc 0.43255 prc_auc 0.63630[0m
[93maverage test of epoch 32: loss -6.11482 acc 0.67568 roc_auc 0.92667 prc_auc 0.96988[0m
[92maverage training of epoch 33: loss -6.22954 acc 0.66225 roc_auc 0.43235 prc_auc 0.63615[0m
[93maverage test of epoch 33: loss -6.49825 acc 0.67568 roc_auc 0.92833 prc_auc 0.96988[0m
[92maverage training of epoch 34: loss -6.60946 acc 0.66225 roc_auc 0.43216 prc_auc 0.63532[0m
[93maverage test of epoch 34: loss -6.88410 acc 0.67568 roc_auc 0.92833 prc_auc 0.96988[0m
[92maverage training of epoch 35: loss -6.99287 acc 0.66225 roc_auc 0.43157 prc_auc 0.63543[0m
[93maverage test of epoch 35: loss -7.27433 acc 0.67568 roc_auc 0.93167 prc_auc 0.96958[0m
[92maverage training of epoch 36: loss -7.38140 acc 0.66225 roc_auc 0.43020 prc_auc 0.63263[0m
[93maverage test of epoch 36: loss -7.67039 acc 0.67568 roc_auc 0.93333 prc_auc 0.96875[0m
[92maverage training of epoch 37: loss -7.77624 acc 0.66225 roc_auc 0.42863 prc_auc 0.62794[0m
[93maverage test of epoch 37: loss -8.07337 acc 0.67568 roc_auc 0.94333 prc_auc 0.96875[0m
[92maverage training of epoch 38: loss -8.17832 acc 0.66225 roc_auc 0.42755 prc_auc 0.62766[0m
[93maverage test of epoch 38: loss -8.48404 acc 0.67568 roc_auc 0.91667 prc_auc 0.95548[0m
[92maverage training of epoch 39: loss -8.58840 acc 0.66225 roc_auc 0.42657 prc_auc 0.62492[0m
[93maverage test of epoch 39: loss -8.90309 acc 0.67568 roc_auc 0.90500 prc_auc 0.95032[0m
[92maverage training of epoch 40: loss -9.00706 acc 0.66225 roc_auc 0.42578 prc_auc 0.62507[0m
[93maverage test of epoch 40: loss -9.33104 acc 0.67568 roc_auc 0.90167 prc_auc 0.93834[0m
[92maverage training of epoch 41: loss -9.43485 acc 0.66225 roc_auc 0.42520 prc_auc 0.62418[0m
[93maverage test of epoch 41: loss -9.76850 acc 0.67568 roc_auc 0.89333 prc_auc 0.93120[0m
[92maverage training of epoch 42: loss -9.87228 acc 0.66225 roc_auc 0.42520 prc_auc 0.62418[0m
[93maverage test of epoch 42: loss -10.21589 acc 0.67568 roc_auc 0.86000 prc_auc 0.89868[0m
[92maverage training of epoch 43: loss -10.31979 acc 0.66225 roc_auc 0.42500 prc_auc 0.62437[0m
[93maverage test of epoch 43: loss -10.67368 acc 0.67568 roc_auc 0.84000 prc_auc 0.89622[0m
[92maverage training of epoch 44: loss -10.77784 acc 0.66225 roc_auc 0.42480 prc_auc 0.62439[0m
[93maverage test of epoch 44: loss -11.14229 acc 0.67568 roc_auc 0.86000 prc_auc 0.90919[0m
[92maverage training of epoch 45: loss -11.24684 acc 0.66225 roc_auc 0.42451 prc_auc 0.62427[0m
[93maverage test of epoch 45: loss -11.62213 acc 0.67568 roc_auc 0.54167 prc_auc 0.69444[0m
[92maverage training of epoch 46: loss -11.72719 acc 0.66225 roc_auc 0.42490 prc_auc 0.62340[0m
[93maverage test of epoch 46: loss -12.11361 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 47: loss -12.21922 acc 0.66225 roc_auc 0.42451 prc_auc 0.62423[0m
[93maverage test of epoch 47: loss -12.61709 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 48: loss -12.72327 acc 0.66225 roc_auc 0.42392 prc_auc 0.62352[0m
[93maverage test of epoch 48: loss -13.13295 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 49: loss -13.23977 acc 0.66225 roc_auc 0.42304 prc_auc 0.62216[0m
[93maverage test of epoch 49: loss -13.66156 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 50: loss -13.76903 acc 0.66225 roc_auc 0.41922 prc_auc 0.61859[0m
[93maverage test of epoch 50: loss -14.20307 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 51: loss -14.31067 acc 0.66225 roc_auc 0.42206 prc_auc 0.62173[0m
[93maverage test of epoch 51: loss -14.75632 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 52: loss -14.86376 acc 0.66225 roc_auc 0.42176 prc_auc 0.62143[0m
[93maverage test of epoch 52: loss -15.32113 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 53: loss -15.42855 acc 0.66225 roc_auc 0.41363 prc_auc 0.61923[0m
[93maverage test of epoch 53: loss -15.89802 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 54: loss -16.00558 acc 0.66225 roc_auc 0.42127 prc_auc 0.62896[0m
[93maverage test of epoch 54: loss -16.48752 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 55: loss -16.59538 acc 0.66225 roc_auc 0.43608 prc_auc 0.63311[0m
[93maverage test of epoch 55: loss -17.09019 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 56: loss -17.19844 acc 0.66225 roc_auc 0.42696 prc_auc 0.62724[0m
[93maverage test of epoch 56: loss -17.70643 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 57: loss -17.81518 acc 0.66225 roc_auc 0.43627 prc_auc 0.63600[0m
[93maverage test of epoch 57: loss -18.33673 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 58: loss -18.44610 acc 0.66225 roc_auc 0.45863 prc_auc 0.64438[0m
[93maverage test of epoch 58: loss -18.98157 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 59: loss -19.09157 acc 0.66225 roc_auc 0.45284 prc_auc 0.64209[0m
[93maverage test of epoch 59: loss -19.64122 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 60: loss -19.75192 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 60: loss -20.31614 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 61: loss -20.42760 acc 0.66225 roc_auc 0.42608 prc_auc 0.63242[0m
[93maverage test of epoch 61: loss -21.00670 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 62: loss -21.11894 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 62: loss -21.71323 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 63: loss -21.82625 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 63: loss -22.43601 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 64: loss -22.54979 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 64: loss -23.17531 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 65: loss -23.28983 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 65: loss -23.93133 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 66: loss -24.04652 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 66: loss -24.70427 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 67: loss -24.82013 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 67: loss -25.49443 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 68: loss -25.61088 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 68: loss -26.30193 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 69: loss -26.41891 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 69: loss -27.12696 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 70: loss -27.24439 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 70: loss -27.96959 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 71: loss -28.08736 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 71: loss -28.82995 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 72: loss -28.94798 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 72: loss -29.70810 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 73: loss -29.82557 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 73: loss -30.60189 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 74: loss -30.71801 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 74: loss -31.51046 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 75: loss -31.62542 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 75: loss -32.43432 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 76: loss -32.54822 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 76: loss -33.37387 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 77: loss -33.48675 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 77: loss -34.32941 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 78: loss -34.44133 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 78: loss -35.30125 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 79: loss -35.41223 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 79: loss -36.28964 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 80: loss -36.39967 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 80: loss -37.29482 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 81: loss -37.40395 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 81: loss -38.31709 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 82: loss -38.42531 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 82: loss -39.35667 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 83: loss -39.46398 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 83: loss -40.41380 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 84: loss -40.52020 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 84: loss -41.48873 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 85: loss -41.59425 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 85: loss -42.58172 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 86: loss -42.68630 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 86: loss -43.69292 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 87: loss -43.79652 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 87: loss -44.82250 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 88: loss -44.92511 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 88: loss -45.97067 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 89: loss -46.07224 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 89: loss -47.13756 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 90: loss -47.23796 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 90: loss -48.32326 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 91: loss -48.42251 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 91: loss -49.52802 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 92: loss -49.62615 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 92: loss -50.75209 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 93: loss -50.84899 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 93: loss -51.99562 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 94: loss -52.09493 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 94: loss -53.27723 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 95: loss -53.40162 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 95: loss -54.62437 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 96: loss -54.75209 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 96: loss -55.99872 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 97: loss -56.12455 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 97: loss -57.39273 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 98: loss -57.51645 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 98: loss -58.80684 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 99: loss -58.92949 acc 0.66225 roc_auc 0.50000 prc_auc 0.66225[0m
[93maverage test of epoch 99: loss -60.24311 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
Run statistics: 
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 100)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 2)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))


Accuracy (avg): 0.60185 ROC_AUC (avg): 0.57231 PRC_AUC (avg): 0.71978 

Average forward propagation time taken(ms): 3.094148088528943
Average backward propagation time taken(ms): 0.9909966320585188

