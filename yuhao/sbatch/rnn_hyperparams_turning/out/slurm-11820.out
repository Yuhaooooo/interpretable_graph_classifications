# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======






Params Turning Set
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 1)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))






args.base_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/
args.graph_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/
args.min_dfscode_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/
args.min_dfscode_tensor_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/
*** 1 train_index:  [  0   2   3   6   9  10  11  12  13  14  15  16  17  19  20  21  22  23
  24  26  27  30  31  32  33  35  36  38  39  41  42  43  44  45  46  47
  48  49  50  52  53  54  56  57  59  60  61  62  63  64  65  66  67  69
  70  71  72  73  74  76  77  78  79  80  81  82  83  84  87  88  89  91
  92  94  95  96  97  98 100 101 102 103 104 106 107 108 109 110 112 113
 115 116 117 118 119 120 121 122 124 125 126 127 129 130 131 132 134 136
 138 139 140 141 142 144 145 146 147 148 149 151 152 153 154 155 156 157
 158 159 160 161 164 165 166 167 169 170 171 172 173 175 176 177 178 179
 180 182 184 185 186 187]
*** 2 test_index:  [  1   4   5   7   8  18  25  28  29  34  37  40  51  55  58  68  75  85
  86  90  93  99 105 111 114 123 128 133 135 137 143 150 162 163 168 174
 181 183]
*** 1 train_index:  [  0   1   2   4   5   6   7   8  10  11  12  13  14  15  16  17  18  19
  20  21  22  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39
  40  41  43  44  45  46  49  50  51  52  53  54  55  56  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  74  75  76  77  79  80  81  82
  83  84  85  86  88  90  91  92  93  94  96  97  98  99 101 102 103 104
 105 106 107 111 112 113 114 115 120 121 122 123 124 125 126 128 129 130
 131 133 134 135 136 137 138 139 140 143 144 145 147 148 150 153 154 155
 156 159 162 163 164 165 167 168 169 170 171 172 173 174 175 177 178 179
 180 181 182 183 186 187]
*** 2 test_index:  [  3   9  23  31  42  47  48  57  72  73  78  87  89  95 100 108 109 110
 116 117 118 119 127 132 141 142 146 149 151 152 157 158 160 161 166 176
 184 185]
*** 1 train_index:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19
  20  23  24  25  26  28  29  31  32  34  35  36  37  38  40  41  42  43
  44  47  48  51  53  54  55  56  57  58  59  60  64  66  68  70  71  72
  73  74  75  76  77  78  79  81  82  84  85  86  87  88  89  90  92  93
  95  96  97  98  99 100 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134
 135 137 139 140 141 142 143 144 146 148 149 150 151 152 153 156 157 158
 159 160 161 162 163 165 166 167 168 169 171 172 173 174 175 176 177 178
 179 181 183 184 185 186]
*** 2 test_index:  [  0  16  21  22  27  30  33  39  45  46  49  50  52  61  62  63  65  67
  69  80  83  91  94 101 102 103 122 136 138 145 147 154 155 164 170 180
 182 187]
*** 1 train_index:  [  0   1   3   4   5   6   7   8   9  13  14  16  18  19  21  22  23  25
  26  27  28  29  30  31  33  34  36  37  38  39  40  42  43  45  46  47
  48  49  50  51  52  53  55  56  57  58  60  61  62  63  65  66  67  68
  69  71  72  73  75  76  77  78  79  80  81  83  84  85  86  87  88  89
  90  91  92  93  94  95  97  98  99 100 101 102 103 105 106 108 109 110
 111 113 114 115 116 117 118 119 120 121 122 123 125 127 128 131 132 133
 134 135 136 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152
 154 155 156 157 158 160 161 162 163 164 165 166 168 170 171 174 175 176
 180 181 182 183 184 185 187]
*** 2 test_index:  [  2  10  11  12  15  17  20  24  32  35  41  44  54  59  64  70  74  82
  96 104 107 112 124 126 129 130 139 153 159 167 169 172 173 177 178 179
 186]
*** 1 train_index:  [  0   1   2   3   4   5   7   8   9  10  11  12  15  16  17  18  20  21
  22  23  24  25  27  28  29  30  31  32  33  34  35  37  39  40  41  42
  44  45  46  47  48  49  50  51  52  54  55  57  58  59  61  62  63  64
  65  67  68  69  70  72  73  74  75  78  80  82  83  85  86  87  89  90
  91  93  94  95  96  99 100 101 102 103 104 105 107 108 109 110 111 112
 114 116 117 118 119 122 123 124 126 127 128 129 130 132 133 135 136 137
 138 139 141 142 143 145 146 147 149 150 151 152 153 154 155 157 158 159
 160 161 162 163 164 166 167 168 169 170 172 173 174 176 177 178 179 180
 181 182 183 184 185 186 187]
*** 2 test_index:  [  6  13  14  19  26  36  38  43  53  56  60  66  71  76  77  79  81  84
  88  92  97  98 106 113 115 120 121 125 131 134 140 144 148 156 165 171
 175]
{'base_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/',
 'batch_size': 1,
 'clean_temp': False,
 'clean_tensorboard': False,
 'current_dataset_path': None,
 'current_min_dfscode_path': None,
 'current_model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-16-54-07/',
 'current_processed_dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'current_temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-16-54-07/',
 'dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/datasets/',
 'device': device(type='cuda', index=0),
 'dfscode_rnn_dropout': 0.2,
 'embedding_size_dfscode_rnn': 8,
 'epochs': 50,
 'epochs_end': 10000,
 'epochs_save': 20,
 'epochs_validate': 1,
 'fname': 'DFScodeRNN_cls_LSTM_MUTAG',
 'gradient_clipping': True,
 'graph_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/',
 'graph_type': 'MUTAG',
 'graphgen_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/',
 'hidden_size_dfscode_rnn': 4,
 'load_device': device(type='cuda', index=0),
 'load_model': False,
 'load_model_path': '',
 'log_tensorboard': False,
 'lr': 0.0001,
 'max_prev_node': None,
 'min_dfscode_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/',
 'min_dfscode_tensor_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/',
 'note': 'DFScodeRNN_cls_LSTM',
 'num_graphs': None,
 'num_layers': 1,
 'num_workers': 8,
 'number_of_mlp_layer': 1,
 'rnn_type': 'LSTM',
 'save_model': False,
 'temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/',
 'tensorboard_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tensorboard/',
 'time': '2021-01-17-16-54-07',
 'weights': False}


graphgen args.__dict__: None




dataset_features: {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}




config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 50, 'learning_rate': 0.0001, 'seed': 1800, 'k_fold': 5, 'model': 'DFScodeRNN_cls_LSTM', 'dataset': 'MUTAG'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}}


Training a new model: DFScodeRNN_cls_LSTM
Training model with dataset, testing using fold 0
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.25479 acc 0.33333 roc_auc 0.57700 prc_auc 0.73097[0m
[93maverage test of epoch 0: loss 0.22593 acc 0.34211 roc_auc 0.26615 prc_auc 0.62352[0m
[92maverage training of epoch 1: loss 0.14527 acc 0.33333 roc_auc 0.54760 prc_auc 0.71848[0m
[93maverage test of epoch 1: loss -0.11152 acc 0.34211 roc_auc 0.80923 prc_auc 0.91291[0m
[92maverage training of epoch 2: loss -0.41480 acc 0.33333 roc_auc 0.57680 prc_auc 0.72633[0m
[93maverage test of epoch 2: loss -0.69539 acc 0.34211 roc_auc 0.52308 prc_auc 0.79152[0m
[92maverage training of epoch 3: loss -0.93966 acc 0.33333 roc_auc 0.58160 prc_auc 0.73303[0m
[93maverage test of epoch 3: loss -1.18831 acc 0.34211 roc_auc 0.48000 prc_auc 0.76943[0m
[92maverage training of epoch 4: loss -1.41349 acc 0.33333 roc_auc 0.58480 prc_auc 0.73421[0m
[93maverage test of epoch 4: loss -1.65152 acc 0.34211 roc_auc 0.40154 prc_auc 0.71838[0m
[92maverage training of epoch 5: loss -1.84305 acc 0.33333 roc_auc 0.59600 prc_auc 0.74240[0m
[93maverage test of epoch 5: loss -2.04838 acc 0.34211 roc_auc 0.48308 prc_auc 0.77129[0m
[92maverage training of epoch 6: loss -2.23693 acc 0.33333 roc_auc 0.59400 prc_auc 0.74460[0m
[93maverage test of epoch 6: loss -2.44993 acc 0.34211 roc_auc 0.80154 prc_auc 0.90634[0m
[92maverage training of epoch 7: loss -2.65411 acc 0.33333 roc_auc 0.58840 prc_auc 0.74125[0m
[93maverage test of epoch 7: loss -2.89074 acc 0.34211 roc_auc 0.69077 prc_auc 0.87243[0m
[92maverage training of epoch 8: loss -3.13056 acc 0.33333 roc_auc 0.58620 prc_auc 0.73551[0m
[93maverage test of epoch 8: loss -3.41352 acc 0.34211 roc_auc 0.52923 prc_auc 0.79973[0m
[92maverage training of epoch 9: loss -3.71383 acc 0.33333 roc_auc 0.58760 prc_auc 0.73595[0m
[93maverage test of epoch 9: loss -4.07068 acc 0.34211 roc_auc 0.21385 prc_auc 0.56965[0m
[92maverage training of epoch 10: loss -4.45295 acc 0.33333 roc_auc 0.58960 prc_auc 0.73738[0m
[93maverage test of epoch 10: loss -4.89984 acc 0.34211 roc_auc 0.47385 prc_auc 0.76671[0m
[92maverage training of epoch 11: loss -5.35071 acc 0.33333 roc_auc 0.59580 prc_auc 0.74400[0m
[93maverage test of epoch 11: loss -5.85087 acc 0.34211 roc_auc 0.69846 prc_auc 0.86127[0m
[92maverage training of epoch 12: loss -6.29763 acc 0.33333 roc_auc 0.60820 prc_auc 0.75724[0m
[93maverage test of epoch 12: loss -6.76828 acc 0.34211 roc_auc 0.84308 prc_auc 0.91506[0m
[92maverage training of epoch 13: loss -7.15928 acc 0.33333 roc_auc 0.61760 prc_auc 0.76920[0m
[93maverage test of epoch 13: loss -7.56709 acc 0.34211 roc_auc 0.85846 prc_auc 0.92975[0m
[92maverage training of epoch 14: loss -7.89948 acc 0.33333 roc_auc 0.62040 prc_auc 0.77657[0m
[93maverage test of epoch 14: loss -8.24732 acc 0.34211 roc_auc 0.85846 prc_auc 0.92975[0m
[92maverage training of epoch 15: loss -8.53003 acc 0.33333 roc_auc 0.61870 prc_auc 0.77614[0m
[93maverage test of epoch 15: loss -8.83065 acc 0.34211 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 16: loss -9.07781 acc 0.33333 roc_auc 0.60780 prc_auc 0.76826[0m
[93maverage test of epoch 16: loss -9.34621 acc 0.34211 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 17: loss -9.56958 acc 0.33333 roc_auc 0.59100 prc_auc 0.76018[0m
[93maverage test of epoch 17: loss -9.81684 acc 0.34211 roc_auc 0.82923 prc_auc 0.92102[0m
[92maverage training of epoch 18: loss -10.02445 acc 0.33333 roc_auc 0.55730 prc_auc 0.72848[0m
[93maverage test of epoch 18: loss -10.25782 acc 0.34211 roc_auc 0.83385 prc_auc 0.92488[0m
[92maverage training of epoch 19: loss -10.45465 acc 0.33333 roc_auc 0.49570 prc_auc 0.67830[0m
[93maverage test of epoch 19: loss -10.67853 acc 0.34211 roc_auc 0.83077 prc_auc 0.92180[0m
[92maverage training of epoch 20: loss -10.86769 acc 0.33333 roc_auc 0.47140 prc_auc 0.64919[0m
[93maverage test of epoch 20: loss -11.08489 acc 0.34211 roc_auc 0.83077 prc_auc 0.92443[0m
[92maverage training of epoch 21: loss -11.26842 acc 0.33333 roc_auc 0.45330 prc_auc 0.61791[0m
[93maverage test of epoch 21: loss -11.48078 acc 0.34211 roc_auc 0.80154 prc_auc 0.91576[0m
[92maverage training of epoch 22: loss -11.66007 acc 0.33333 roc_auc 0.43780 prc_auc 0.61076[0m
[93maverage test of epoch 22: loss -11.86888 acc 0.34211 roc_auc 0.80769 prc_auc 0.92093[0m
[92maverage training of epoch 23: loss -12.04488 acc 0.33333 roc_auc 0.42500 prc_auc 0.60408[0m
[93maverage test of epoch 23: loss -12.25106 acc 0.34211 roc_auc 0.66462 prc_auc 0.84016[0m
[92maverage training of epoch 24: loss -12.42450 acc 0.33333 roc_auc 0.41580 prc_auc 0.60030[0m
[93maverage test of epoch 24: loss -12.62873 acc 0.34211 roc_auc 0.40154 prc_auc 0.69309[0m
[92maverage training of epoch 25: loss -12.80015 acc 0.33333 roc_auc 0.40920 prc_auc 0.59726[0m
[93maverage test of epoch 25: loss -13.00293 acc 0.34211 roc_auc 0.29538 prc_auc 0.58665[0m
[92maverage training of epoch 26: loss -13.17275 acc 0.33333 roc_auc 0.40490 prc_auc 0.59640[0m
[93maverage test of epoch 26: loss -13.37450 acc 0.34211 roc_auc 0.18154 prc_auc 0.53283[0m
[92maverage training of epoch 27: loss -13.54310 acc 0.33333 roc_auc 0.39860 prc_auc 0.59499[0m
[93maverage test of epoch 27: loss -13.74418 acc 0.34211 roc_auc 0.14615 prc_auc 0.49667[0m
[92maverage training of epoch 28: loss -13.91189 acc 0.33333 roc_auc 0.39340 prc_auc 0.59212[0m
[93maverage test of epoch 28: loss -14.11262 acc 0.34211 roc_auc 0.07385 prc_auc 0.48015[0m
[92maverage training of epoch 29: loss -14.27972 acc 0.33333 roc_auc 0.37910 prc_auc 0.58692[0m
[93maverage test of epoch 29: loss -14.48035 acc 0.34211 roc_auc 0.07692 prc_auc 0.47133[0m
[92maverage training of epoch 30: loss -14.64707 acc 0.33333 roc_auc 0.37320 prc_auc 0.58526[0m
[93maverage test of epoch 30: loss -14.84781 acc 0.34211 roc_auc 0.08923 prc_auc 0.47823[0m
[92maverage training of epoch 31: loss -15.01437 acc 0.33333 roc_auc 0.37320 prc_auc 0.58526[0m
[93maverage test of epoch 31: loss -15.21540 acc 0.34211 roc_auc 0.08154 prc_auc 0.47195[0m
[92maverage training of epoch 32: loss -15.38198 acc 0.33333 roc_auc 0.37620 prc_auc 0.58611[0m
[93maverage test of epoch 32: loss -15.58344 acc 0.34211 roc_auc 0.10154 prc_auc 0.47925[0m
[92maverage training of epoch 33: loss -15.75020 acc 0.33333 roc_auc 0.37520 prc_auc 0.58511[0m
[93maverage test of epoch 33: loss -15.95224 acc 0.34211 roc_auc 0.09692 prc_auc 0.48243[0m
[92maverage training of epoch 34: loss -16.11933 acc 0.33333 roc_auc 0.37080 prc_auc 0.58362[0m
[93maverage test of epoch 34: loss -16.32206 acc 0.34211 roc_auc 0.12769 prc_auc 0.48867[0m
[92maverage training of epoch 35: loss -16.48961 acc 0.33333 roc_auc 0.36900 prc_auc 0.58244[0m
[93maverage test of epoch 35: loss -16.69311 acc 0.34211 roc_auc 0.12462 prc_auc 0.49211[0m
[92maverage training of epoch 36: loss -16.86125 acc 0.33333 roc_auc 0.36600 prc_auc 0.58171[0m
[93maverage test of epoch 36: loss -17.06560 acc 0.34211 roc_auc 0.10615 prc_auc 0.48099[0m
[92maverage training of epoch 37: loss -17.23444 acc 0.33333 roc_auc 0.36470 prc_auc 0.58116[0m
[93maverage test of epoch 37: loss -17.43973 acc 0.34211 roc_auc 0.12000 prc_auc 0.49633[0m
[92maverage training of epoch 38: loss -17.60936 acc 0.33333 roc_auc 0.36660 prc_auc 0.58123[0m
[93maverage test of epoch 38: loss -17.81564 acc 0.34211 roc_auc 0.12462 prc_auc 0.50223[0m
[92maverage training of epoch 39: loss -17.98615 acc 0.33333 roc_auc 0.36760 prc_auc 0.58066[0m
[93maverage test of epoch 39: loss -18.19348 acc 0.34211 roc_auc 0.09385 prc_auc 0.48468[0m
[92maverage training of epoch 40: loss -18.36498 acc 0.33333 roc_auc 0.36770 prc_auc 0.58006[0m
[93maverage test of epoch 40: loss -18.57340 acc 0.34211 roc_auc 0.15231 prc_auc 0.51483[0m
[92maverage training of epoch 41: loss -18.74595 acc 0.33333 roc_auc 0.36840 prc_auc 0.57945[0m
[93maverage test of epoch 41: loss -18.95551 acc 0.34211 roc_auc 0.15692 prc_auc 0.50727[0m
[92maverage training of epoch 42: loss -19.12917 acc 0.33333 roc_auc 0.36650 prc_auc 0.57853[0m
[93maverage test of epoch 42: loss -19.33990 acc 0.34211 roc_auc 0.14308 prc_auc 0.50353[0m
[92maverage training of epoch 43: loss -19.51476 acc 0.33333 roc_auc 0.36600 prc_auc 0.57798[0m
[93maverage test of epoch 43: loss -19.72666 acc 0.34211 roc_auc 0.14000 prc_auc 0.49653[0m
[92maverage training of epoch 44: loss -19.90276 acc 0.33333 roc_auc 0.36560 prc_auc 0.57783[0m
[93maverage test of epoch 44: loss -20.11590 acc 0.34211 roc_auc 0.17538 prc_auc 0.52864[0m
[92maverage training of epoch 45: loss -20.29332 acc 0.33333 roc_auc 0.36560 prc_auc 0.57750[0m
[93maverage test of epoch 45: loss -20.50769 acc 0.34211 roc_auc 0.11538 prc_auc 0.48472[0m
[92maverage training of epoch 46: loss -20.68647 acc 0.33333 roc_auc 0.36380 prc_auc 0.57647[0m
[93maverage test of epoch 46: loss -20.90210 acc 0.34211 roc_auc 0.04615 prc_auc 0.49807[0m
[92maverage training of epoch 47: loss -21.08229 acc 0.33333 roc_auc 0.36480 prc_auc 0.57656[0m
[93maverage test of epoch 47: loss -21.29919 acc 0.34211 roc_auc 0.13077 prc_auc 0.49097[0m
[92maverage training of epoch 48: loss -21.48082 acc 0.33333 roc_auc 0.36400 prc_auc 0.57576[0m
[93maverage test of epoch 48: loss -21.69899 acc 0.34211 roc_auc 0.10615 prc_auc 0.50713[0m
[92maverage training of epoch 49: loss -21.88211 acc 0.33333 roc_auc 0.36160 prc_auc 0.57444[0m
[93maverage test of epoch 49: loss -22.10156 acc 0.34211 roc_auc 0.05231 prc_auc 0.49526[0m
Training model with dataset, testing using fold 1
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.12442 acc 0.66667 roc_auc 0.32220 prc_auc 0.55290[0m
[93maverage test of epoch 0: loss 0.07425 acc 0.65789 roc_auc 0.19385 prc_auc 0.52436[0m
[92maverage training of epoch 1: loss 0.01368 acc 0.66667 roc_auc 0.28180 prc_auc 0.54537[0m
[93maverage test of epoch 1: loss -0.03147 acc 0.65789 roc_auc 0.22154 prc_auc 0.60785[0m
[92maverage training of epoch 2: loss -0.08556 acc 0.66667 roc_auc 0.26460 prc_auc 0.53679[0m
[93maverage test of epoch 2: loss -0.12503 acc 0.65789 roc_auc 0.34462 prc_auc 0.70390[0m
[92maverage training of epoch 3: loss -0.17406 acc 0.66667 roc_auc 0.26920 prc_auc 0.54097[0m
[93maverage test of epoch 3: loss -0.20928 acc 0.65789 roc_auc 0.37231 prc_auc 0.72390[0m
[92maverage training of epoch 4: loss -0.25553 acc 0.66667 roc_auc 0.30200 prc_auc 0.54887[0m
[93maverage test of epoch 4: loss -0.28800 acc 0.65789 roc_auc 0.36000 prc_auc 0.70936[0m
[92maverage training of epoch 5: loss -0.33363 acc 0.66667 roc_auc 0.35680 prc_auc 0.58203[0m
[93maverage test of epoch 5: loss -0.36511 acc 0.65789 roc_auc 0.39692 prc_auc 0.62476[0m
[92maverage training of epoch 6: loss -0.41123 acc 0.66667 roc_auc 0.42820 prc_auc 0.63826[0m
[93maverage test of epoch 6: loss -0.44313 acc 0.65789 roc_auc 0.48000 prc_auc 0.65535[0m
[92maverage training of epoch 7: loss -0.49053 acc 0.66667 roc_auc 0.46480 prc_auc 0.67170[0m
[93maverage test of epoch 7: loss -0.52421 acc 0.65789 roc_auc 0.54462 prc_auc 0.70858[0m
[92maverage training of epoch 8: loss -0.57393 acc 0.66667 roc_auc 0.51700 prc_auc 0.71523[0m
[93maverage test of epoch 8: loss -0.61099 acc 0.65789 roc_auc 0.71692 prc_auc 0.76393[0m
[92maverage training of epoch 9: loss -0.66469 acc 0.66667 roc_auc 0.57120 prc_auc 0.74864[0m
[93maverage test of epoch 9: loss -0.70699 acc 0.65789 roc_auc 0.76308 prc_auc 0.81169[0m
[92maverage training of epoch 10: loss -0.76877 acc 0.66667 roc_auc 0.62860 prc_auc 0.78277[0m
[93maverage test of epoch 10: loss -0.82073 acc 0.65789 roc_auc 0.84000 prc_auc 0.90247[0m
[92maverage training of epoch 11: loss -0.89889 acc 0.66667 roc_auc 0.67960 prc_auc 0.81144[0m
[93maverage test of epoch 11: loss -0.96921 acc 0.65789 roc_auc 0.84308 prc_auc 0.90723[0m
[92maverage training of epoch 12: loss -1.16367 acc 0.66667 roc_auc 0.56520 prc_auc 0.72076[0m
[93maverage test of epoch 12: loss -1.44443 acc 0.65789 roc_auc 0.76615 prc_auc 0.86905[0m
[92maverage training of epoch 13: loss -1.71960 acc 0.66667 roc_auc 0.62260 prc_auc 0.77333[0m
[93maverage test of epoch 13: loss -1.93281 acc 0.65789 roc_auc 0.80308 prc_auc 0.88761[0m
[92maverage training of epoch 14: loss -2.14252 acc 0.66667 roc_auc 0.64640 prc_auc 0.77897[0m
[93maverage test of epoch 14: loss -2.31349 acc 0.65789 roc_auc 0.83385 prc_auc 0.89707[0m
[92maverage training of epoch 15: loss -2.49907 acc 0.66667 roc_auc 0.62760 prc_auc 0.76083[0m
[93maverage test of epoch 15: loss -2.64944 acc 0.65789 roc_auc 0.83077 prc_auc 0.89478[0m
[92maverage training of epoch 16: loss -2.82421 acc 0.66667 roc_auc 0.61020 prc_auc 0.75357[0m
[93maverage test of epoch 16: loss -2.96214 acc 0.65789 roc_auc 0.85538 prc_auc 0.91504[0m
[92maverage training of epoch 17: loss -3.13104 acc 0.66667 roc_auc 0.58570 prc_auc 0.73763[0m
[93maverage test of epoch 17: loss -3.26013 acc 0.65789 roc_auc 0.85846 prc_auc 0.91736[0m
[92maverage training of epoch 18: loss -3.42483 acc 0.66667 roc_auc 0.55500 prc_auc 0.71937[0m
[93maverage test of epoch 18: loss -3.54710 acc 0.65789 roc_auc 0.85846 prc_auc 0.91710[0m
[92maverage training of epoch 19: loss -3.70910 acc 0.66667 roc_auc 0.53280 prc_auc 0.70654[0m
[93maverage test of epoch 19: loss -3.82619 acc 0.65789 roc_auc 0.85846 prc_auc 0.91710[0m
[92maverage training of epoch 20: loss -3.98629 acc 0.66667 roc_auc 0.51740 prc_auc 0.69527[0m
[93maverage test of epoch 20: loss -4.09892 acc 0.65789 roc_auc 0.85538 prc_auc 0.91404[0m
[92maverage training of epoch 21: loss -4.25791 acc 0.66667 roc_auc 0.50500 prc_auc 0.68421[0m
[93maverage test of epoch 21: loss -4.36704 acc 0.65789 roc_auc 0.85692 prc_auc 0.91809[0m
[92maverage training of epoch 22: loss -4.52543 acc 0.66667 roc_auc 0.49560 prc_auc 0.67522[0m
[93maverage test of epoch 22: loss -4.63149 acc 0.65789 roc_auc 0.85385 prc_auc 0.91500[0m
[92maverage training of epoch 23: loss -4.79012 acc 0.66667 roc_auc 0.48340 prc_auc 0.66307[0m
[93maverage test of epoch 23: loss -4.89399 acc 0.65789 roc_auc 0.85692 prc_auc 0.91862[0m
[92maverage training of epoch 24: loss -5.05313 acc 0.66667 roc_auc 0.47880 prc_auc 0.66116[0m
[93maverage test of epoch 24: loss -5.15488 acc 0.65789 roc_auc 0.85846 prc_auc 0.91777[0m
[92maverage training of epoch 25: loss -5.31503 acc 0.66667 roc_auc 0.47540 prc_auc 0.65900[0m
[93maverage test of epoch 25: loss -5.41515 acc 0.65789 roc_auc 0.85692 prc_auc 0.91753[0m
[92maverage training of epoch 26: loss -5.57716 acc 0.66667 roc_auc 0.47340 prc_auc 0.65769[0m
[93maverage test of epoch 26: loss -5.67672 acc 0.65789 roc_auc 0.85692 prc_auc 0.91408[0m
[92maverage training of epoch 27: loss -5.84221 acc 0.66667 roc_auc 0.47220 prc_auc 0.65758[0m
[93maverage test of epoch 27: loss -5.94344 acc 0.65789 roc_auc 0.85692 prc_auc 0.91155[0m
[92maverage training of epoch 28: loss -6.11610 acc 0.66667 roc_auc 0.47100 prc_auc 0.65637[0m
[93maverage test of epoch 28: loss -6.22456 acc 0.65789 roc_auc 0.86154 prc_auc 0.91581[0m
[92maverage training of epoch 29: loss -6.41380 acc 0.66667 roc_auc 0.47100 prc_auc 0.65629[0m
[93maverage test of epoch 29: loss -6.54506 acc 0.65789 roc_auc 0.85692 prc_auc 0.91511[0m
[92maverage training of epoch 30: loss -6.77602 acc 0.66667 roc_auc 0.47420 prc_auc 0.66098[0m
[93maverage test of epoch 30: loss -6.97243 acc 0.65789 roc_auc 0.86923 prc_auc 0.92350[0m
[92maverage training of epoch 31: loss -7.28664 acc 0.66667 roc_auc 0.50010 prc_auc 0.68771[0m
[93maverage test of epoch 31: loss -7.57305 acc 0.65789 roc_auc 0.87692 prc_auc 0.92890[0m
[92maverage training of epoch 32: loss -7.88906 acc 0.66667 roc_auc 0.54980 prc_auc 0.72324[0m
[93maverage test of epoch 32: loss -8.12414 acc 0.65789 roc_auc 0.87692 prc_auc 0.92890[0m
[92maverage training of epoch 33: loss -8.38541 acc 0.66667 roc_auc 0.54850 prc_auc 0.71810[0m
[93maverage test of epoch 33: loss -8.56243 acc 0.65789 roc_auc 0.87692 prc_auc 0.92915[0m
[92maverage training of epoch 34: loss -8.80291 acc 0.66667 roc_auc 0.53040 prc_auc 0.70353[0m
[93maverage test of epoch 34: loss -8.95491 acc 0.65789 roc_auc 0.87846 prc_auc 0.92937[0m
[92maverage training of epoch 35: loss -9.18713 acc 0.66667 roc_auc 0.51630 prc_auc 0.69146[0m
[93maverage test of epoch 35: loss -9.32583 acc 0.65789 roc_auc 0.87692 prc_auc 0.92737[0m
[92maverage training of epoch 36: loss -9.55442 acc 0.66667 roc_auc 0.50380 prc_auc 0.68257[0m
[93maverage test of epoch 36: loss -9.68474 acc 0.65789 roc_auc 0.87538 prc_auc 0.92381[0m
[92maverage training of epoch 37: loss -9.91186 acc 0.66667 roc_auc 0.49470 prc_auc 0.67713[0m
[93maverage test of epoch 37: loss -10.03628 acc 0.65789 roc_auc 0.87846 prc_auc 0.92810[0m
[92maverage training of epoch 38: loss -10.26320 acc 0.66667 roc_auc 0.48990 prc_auc 0.67460[0m
[93maverage test of epoch 38: loss -10.38320 acc 0.65789 roc_auc 0.88154 prc_auc 0.92732[0m
[92maverage training of epoch 39: loss -10.61075 acc 0.66667 roc_auc 0.48630 prc_auc 0.67084[0m
[93maverage test of epoch 39: loss -10.72736 acc 0.65789 roc_auc 0.86615 prc_auc 0.91316[0m
[92maverage training of epoch 40: loss -10.95620 acc 0.66667 roc_auc 0.48220 prc_auc 0.66638[0m
[93maverage test of epoch 40: loss -11.07018 acc 0.65789 roc_auc 0.85692 prc_auc 0.89185[0m
[92maverage training of epoch 41: loss -11.29992 acc 0.66667 roc_auc 0.47900 prc_auc 0.66040[0m
[93maverage test of epoch 41: loss -11.40685 acc 0.65789 roc_auc 0.87077 prc_auc 0.89689[0m
[92maverage training of epoch 42: loss -11.63000 acc 0.66667 roc_auc 0.47590 prc_auc 0.65899[0m
[93maverage test of epoch 42: loss -11.73039 acc 0.65789 roc_auc 0.86462 prc_auc 0.89079[0m
[92maverage training of epoch 43: loss -11.95573 acc 0.66667 roc_auc 0.47410 prc_auc 0.65752[0m
[93maverage test of epoch 43: loss -12.05436 acc 0.65789 roc_auc 0.85077 prc_auc 0.87024[0m
[92maverage training of epoch 44: loss -12.28212 acc 0.66667 roc_auc 0.47270 prc_auc 0.65691[0m
[93maverage test of epoch 44: loss -12.37930 acc 0.65789 roc_auc 0.84308 prc_auc 0.86222[0m
[92maverage training of epoch 45: loss -12.60963 acc 0.66667 roc_auc 0.47210 prc_auc 0.65693[0m
[93maverage test of epoch 45: loss -12.70559 acc 0.65789 roc_auc 0.82000 prc_auc 0.84667[0m
[92maverage training of epoch 46: loss -12.93864 acc 0.66667 roc_auc 0.47100 prc_auc 0.65672[0m
[93maverage test of epoch 46: loss -13.03356 acc 0.65789 roc_auc 0.81846 prc_auc 0.84819[0m
[92maverage training of epoch 47: loss -13.26941 acc 0.66667 roc_auc 0.47120 prc_auc 0.65626[0m
[93maverage test of epoch 47: loss -13.36343 acc 0.65789 roc_auc 0.82308 prc_auc 0.84667[0m
[92maverage training of epoch 48: loss -13.60219 acc 0.66667 roc_auc 0.47050 prc_auc 0.65617[0m
[93maverage test of epoch 48: loss -13.69542 acc 0.65789 roc_auc 0.71692 prc_auc 0.77697[0m
[92maverage training of epoch 49: loss -13.93714 acc 0.66667 roc_auc 0.47070 prc_auc 0.65657[0m
[93maverage test of epoch 49: loss -14.02969 acc 0.65789 roc_auc 0.82308 prc_auc 0.84667[0m
Training model with dataset, testing using fold 2
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.36970 acc 0.34000 roc_auc 0.52260 prc_auc 0.73570[0m
[93maverage test of epoch 0: loss -0.38470 acc 0.34211 roc_auc 0.80308 prc_auc 0.92598[0m
[92maverage training of epoch 1: loss -0.39754 acc 0.34667 roc_auc 0.54060 prc_auc 0.74900[0m
[93maverage test of epoch 1: loss -0.41001 acc 0.34211 roc_auc 0.72000 prc_auc 0.89070[0m
[92maverage training of epoch 2: loss -0.42234 acc 0.37333 roc_auc 0.53750 prc_auc 0.74697[0m
[93maverage test of epoch 2: loss -0.43363 acc 0.36842 roc_auc 0.54769 prc_auc 0.80699[0m
[92maverage training of epoch 3: loss -0.44544 acc 0.52667 roc_auc 0.53620 prc_auc 0.74969[0m
[93maverage test of epoch 3: loss -0.45435 acc 0.65789 roc_auc 0.52923 prc_auc 0.80185[0m
[92maverage training of epoch 4: loss -0.46512 acc 0.66667 roc_auc 0.47940 prc_auc 0.67129[0m
[93maverage test of epoch 4: loss -0.46877 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 5: loss -0.47881 acc 0.66667 roc_auc 0.40300 prc_auc 0.61950[0m
[93maverage test of epoch 5: loss -0.47978 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 6: loss -0.49062 acc 0.66667 roc_auc 0.40250 prc_auc 0.61864[0m
[93maverage test of epoch 6: loss -0.49079 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 7: loss -0.50240 acc 0.66667 roc_auc 0.40250 prc_auc 0.61864[0m
[93maverage test of epoch 7: loss -0.50181 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 8: loss -0.51413 acc 0.66667 roc_auc 0.40250 prc_auc 0.61879[0m
[93maverage test of epoch 8: loss -0.51282 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 9: loss -0.52588 acc 0.66667 roc_auc 0.40210 prc_auc 0.61846[0m
[93maverage test of epoch 9: loss -0.52384 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 10: loss -0.53787 acc 0.66667 roc_auc 0.40200 prc_auc 0.61836[0m
[93maverage test of epoch 10: loss -0.53485 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 11: loss -0.54999 acc 0.66667 roc_auc 0.40170 prc_auc 0.61805[0m
[93maverage test of epoch 11: loss -0.54586 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 12: loss -0.56214 acc 0.66667 roc_auc 0.40150 prc_auc 0.61805[0m
[93maverage test of epoch 12: loss -0.55688 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 13: loss -0.57488 acc 0.66667 roc_auc 0.42290 prc_auc 0.64564[0m
[93maverage test of epoch 13: loss -0.56856 acc 0.65789 roc_auc 0.52000 prc_auc 0.67158[0m
[92maverage training of epoch 14: loss -0.58864 acc 0.66667 roc_auc 0.42290 prc_auc 0.64564[0m
[93maverage test of epoch 14: loss -0.58094 acc 0.65789 roc_auc 0.52000 prc_auc 0.67158[0m
[92maverage training of epoch 15: loss -0.60327 acc 0.66667 roc_auc 0.44360 prc_auc 0.67397[0m
[93maverage test of epoch 15: loss -0.59571 acc 0.65789 roc_auc 0.62000 prc_auc 0.74000[0m
[92maverage training of epoch 16: loss -0.71881 acc 0.66667 roc_auc 0.48000 prc_auc 0.68429[0m
[93maverage test of epoch 16: loss -0.82160 acc 0.65789 roc_auc 0.28615 prc_auc 0.66116[0m
[92maverage training of epoch 17: loss -0.94067 acc 0.66667 roc_auc 0.47420 prc_auc 0.68016[0m
[93maverage test of epoch 17: loss -1.02800 acc 0.65789 roc_auc 0.46923 prc_auc 0.76910[0m
[92maverage training of epoch 18: loss -1.16108 acc 0.66667 roc_auc 0.48480 prc_auc 0.69002[0m
[93maverage test of epoch 18: loss -1.26421 acc 0.65789 roc_auc 0.93538 prc_auc 0.97317[0m
[92maverage training of epoch 19: loss -1.41531 acc 0.66667 roc_auc 0.49760 prc_auc 0.69883[0m
[93maverage test of epoch 19: loss -1.53061 acc 0.65789 roc_auc 0.94462 prc_auc 0.97555[0m
[92maverage training of epoch 20: loss -1.68637 acc 0.66667 roc_auc 0.52040 prc_auc 0.71476[0m
[93maverage test of epoch 20: loss -1.79909 acc 0.65789 roc_auc 0.94769 prc_auc 0.97702[0m
[92maverage training of epoch 21: loss -1.94517 acc 0.66667 roc_auc 0.54540 prc_auc 0.73991[0m
[93maverage test of epoch 21: loss -2.04549 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 22: loss -2.17763 acc 0.66667 roc_auc 0.58060 prc_auc 0.77508[0m
[93maverage test of epoch 22: loss -2.26486 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 23: loss -2.38975 acc 0.66667 roc_auc 0.60060 prc_auc 0.79022[0m
[93maverage test of epoch 23: loss -2.47200 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 24: loss -2.59925 acc 0.66667 roc_auc 0.61140 prc_auc 0.79860[0m
[93maverage test of epoch 24: loss -2.68428 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 25: loss -2.81787 acc 0.66667 roc_auc 0.61140 prc_auc 0.79464[0m
[93maverage test of epoch 25: loss -2.90631 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 26: loss -3.04133 acc 0.66667 roc_auc 0.60800 prc_auc 0.78792[0m
[93maverage test of epoch 26: loss -3.12582 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 27: loss -3.25537 acc 0.66667 roc_auc 0.60000 prc_auc 0.78107[0m
[93maverage test of epoch 27: loss -3.33006 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 28: loss -3.45209 acc 0.66667 roc_auc 0.59120 prc_auc 0.77457[0m
[93maverage test of epoch 28: loss -3.51676 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 29: loss -3.63330 acc 0.66667 roc_auc 0.58620 prc_auc 0.77204[0m
[93maverage test of epoch 29: loss -3.69056 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 30: loss -3.80387 acc 0.66667 roc_auc 0.57960 prc_auc 0.76701[0m
[93maverage test of epoch 30: loss -3.85610 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 31: loss -3.96779 acc 0.66667 roc_auc 0.57560 prc_auc 0.76256[0m
[93maverage test of epoch 31: loss -4.01659 acc 0.65789 roc_auc 0.95077 prc_auc 0.97791[0m
[92maverage training of epoch 32: loss -4.12761 acc 0.66667 roc_auc 0.56810 prc_auc 0.75645[0m
[93maverage test of epoch 32: loss -4.17387 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 33: loss -4.28474 acc 0.66667 roc_auc 0.55830 prc_auc 0.74710[0m
[93maverage test of epoch 33: loss -4.32904 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 34: loss -4.44020 acc 0.66667 roc_auc 0.54510 prc_auc 0.73092[0m
[93maverage test of epoch 34: loss -4.48296 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 35: loss -4.59474 acc 0.66667 roc_auc 0.53280 prc_auc 0.72203[0m
[93maverage test of epoch 35: loss -4.63630 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 36: loss -4.74896 acc 0.66667 roc_auc 0.51700 prc_auc 0.70653[0m
[93maverage test of epoch 36: loss -4.78958 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 37: loss -4.90332 acc 0.66667 roc_auc 0.50920 prc_auc 0.70195[0m
[93maverage test of epoch 37: loss -4.94324 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 38: loss -5.05821 acc 0.66667 roc_auc 0.50060 prc_auc 0.69465[0m
[93maverage test of epoch 38: loss -5.09762 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 39: loss -5.21394 acc 0.66667 roc_auc 0.49300 prc_auc 0.68905[0m
[93maverage test of epoch 39: loss -5.25299 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 40: loss -5.37073 acc 0.66667 roc_auc 0.48500 prc_auc 0.68292[0m
[93maverage test of epoch 40: loss -5.40955 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 41: loss -5.52879 acc 0.66667 roc_auc 0.47580 prc_auc 0.67619[0m
[93maverage test of epoch 41: loss -5.56751 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 42: loss -5.68833 acc 0.66667 roc_auc 0.46770 prc_auc 0.66676[0m
[93maverage test of epoch 42: loss -5.72704 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 43: loss -5.84951 acc 0.66667 roc_auc 0.46000 prc_auc 0.66085[0m
[93maverage test of epoch 43: loss -5.88831 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 44: loss -6.01248 acc 0.66667 roc_auc 0.45120 prc_auc 0.65367[0m
[93maverage test of epoch 44: loss -6.05144 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 45: loss -6.17737 acc 0.66667 roc_auc 0.44460 prc_auc 0.64789[0m
[93maverage test of epoch 45: loss -6.21657 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 46: loss -6.34430 acc 0.66667 roc_auc 0.43770 prc_auc 0.64310[0m
[93maverage test of epoch 46: loss -6.38380 acc 0.65789 roc_auc 0.95231 prc_auc 0.97791[0m
[92maverage training of epoch 47: loss -6.51339 acc 0.66667 roc_auc 0.43280 prc_auc 0.63843[0m
[93maverage test of epoch 47: loss -6.55325 acc 0.65789 roc_auc 0.95385 prc_auc 0.97808[0m
[92maverage training of epoch 48: loss -6.68472 acc 0.66667 roc_auc 0.42910 prc_auc 0.63639[0m
[93maverage test of epoch 48: loss -6.72499 acc 0.65789 roc_auc 0.95538 prc_auc 0.97946[0m
[92maverage training of epoch 49: loss -6.85840 acc 0.66667 roc_auc 0.42430 prc_auc 0.63187[0m
[93maverage test of epoch 49: loss -6.89913 acc 0.65789 roc_auc 0.95231 prc_auc 0.97697[0m
Training model with dataset, testing using fold 3
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.04946 acc 0.33775 roc_auc 0.50098 prc_auc 0.71982[0m
[93maverage test of epoch 0: loss -0.08274 acc 0.32432 roc_auc 0.79667 prc_auc 0.88906[0m
[92maverage training of epoch 1: loss -0.12435 acc 0.34437 roc_auc 0.51216 prc_auc 0.73228[0m
[93maverage test of epoch 1: loss -0.15988 acc 0.35135 roc_auc 0.83000 prc_auc 0.89963[0m
[92maverage training of epoch 2: loss -0.20319 acc 0.35099 roc_auc 0.52784 prc_auc 0.74461[0m
[93maverage test of epoch 2: loss -0.24137 acc 0.35135 roc_auc 0.84000 prc_auc 0.90374[0m
[92maverage training of epoch 3: loss -0.28659 acc 0.35099 roc_auc 0.54549 prc_auc 0.75928[0m
[93maverage test of epoch 3: loss -0.32762 acc 0.35135 roc_auc 0.84333 prc_auc 0.90873[0m
[92maverage training of epoch 4: loss -0.39533 acc 0.35099 roc_auc 0.51039 prc_auc 0.71710[0m
[93maverage test of epoch 4: loss -0.46951 acc 0.35135 roc_auc 0.67667 prc_auc 0.87432[0m
[92maverage training of epoch 5: loss -0.54809 acc 0.35099 roc_auc 0.51490 prc_auc 0.73089[0m
[93maverage test of epoch 5: loss -0.61835 acc 0.35135 roc_auc 0.79000 prc_auc 0.90470[0m
[92maverage training of epoch 6: loss -0.69276 acc 0.35099 roc_auc 0.52039 prc_auc 0.73762[0m
[93maverage test of epoch 6: loss -0.75906 acc 0.35135 roc_auc 0.85667 prc_auc 0.92909[0m
[92maverage training of epoch 7: loss -0.85542 acc 0.35099 roc_auc 0.55039 prc_auc 0.76840[0m
[93maverage test of epoch 7: loss -0.94726 acc 0.35135 roc_auc 0.82167 prc_auc 0.91317[0m
[92maverage training of epoch 8: loss -1.03923 acc 0.35099 roc_auc 0.50784 prc_auc 0.74418[0m
[93maverage test of epoch 8: loss -1.12528 acc 0.35135 roc_auc 0.70000 prc_auc 0.86883[0m
[92maverage training of epoch 9: loss -1.21627 acc 0.35762 roc_auc 0.51882 prc_auc 0.74868[0m
[93maverage test of epoch 9: loss -1.30571 acc 0.35135 roc_auc 0.67500 prc_auc 0.86283[0m
[92maverage training of epoch 10: loss -1.40157 acc 0.37086 roc_auc 0.52961 prc_auc 0.75357[0m
[93maverage test of epoch 10: loss -1.50087 acc 0.37838 roc_auc 0.65667 prc_auc 0.85645[0m
[92maverage training of epoch 11: loss -1.60844 acc 0.38411 roc_auc 0.53667 prc_auc 0.75822[0m
[93maverage test of epoch 11: loss -1.72585 acc 0.37838 roc_auc 0.50000 prc_auc 0.78077[0m
[92maverage training of epoch 12: loss -1.85276 acc 0.45695 roc_auc 0.53373 prc_auc 0.75662[0m
[93maverage test of epoch 12: loss -1.99644 acc 0.37838 roc_auc 0.41333 prc_auc 0.74012[0m
[92maverage training of epoch 13: loss -2.14434 acc 0.56954 roc_auc 0.51667 prc_auc 0.74360[0m
[93maverage test of epoch 13: loss -2.31249 acc 0.67568 roc_auc 0.34667 prc_auc 0.69795[0m
[92maverage training of epoch 14: loss -2.46960 acc 0.66225 roc_auc 0.51059 prc_auc 0.74529[0m
[93maverage test of epoch 14: loss -2.64529 acc 0.67568 roc_auc 0.37000 prc_auc 0.70983[0m
[92maverage training of epoch 15: loss -2.79937 acc 0.66225 roc_auc 0.57333 prc_auc 0.79209[0m
[93maverage test of epoch 15: loss -2.96868 acc 0.67568 roc_auc 0.53000 prc_auc 0.79830[0m
[92maverage training of epoch 16: loss -3.11883 acc 0.66225 roc_auc 0.63059 prc_auc 0.82619[0m
[93maverage test of epoch 16: loss -3.27854 acc 0.67568 roc_auc 0.84333 prc_auc 0.91568[0m
[92maverage training of epoch 17: loss -3.42460 acc 0.66225 roc_auc 0.66353 prc_auc 0.83708[0m
[93maverage test of epoch 17: loss -3.57669 acc 0.67568 roc_auc 0.86333 prc_auc 0.92385[0m
[92maverage training of epoch 18: loss -3.71666 acc 0.66225 roc_auc 0.66196 prc_auc 0.83428[0m
[93maverage test of epoch 18: loss -3.86396 acc 0.67568 roc_auc 0.86333 prc_auc 0.92385[0m
[92maverage training of epoch 19: loss -3.99958 acc 0.66225 roc_auc 0.65471 prc_auc 0.82843[0m
[93maverage test of epoch 19: loss -4.14658 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 20: loss -4.28096 acc 0.66225 roc_auc 0.64490 prc_auc 0.82228[0m
[93maverage test of epoch 20: loss -4.43160 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 21: loss -4.56755 acc 0.66225 roc_auc 0.63627 prc_auc 0.81821[0m
[93maverage test of epoch 21: loss -4.72537 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 22: loss -4.86538 acc 0.66225 roc_auc 0.62745 prc_auc 0.81235[0m
[93maverage test of epoch 22: loss -5.03374 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 23: loss -5.17940 acc 0.66225 roc_auc 0.61784 prc_auc 0.80729[0m
[93maverage test of epoch 23: loss -5.36028 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 24: loss -5.51129 acc 0.66225 roc_auc 0.60471 prc_auc 0.79602[0m
[93maverage test of epoch 24: loss -5.70420 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 25: loss -5.85873 acc 0.66225 roc_auc 0.58941 prc_auc 0.77959[0m
[93maverage test of epoch 25: loss -6.06131 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 26: loss -6.21601 acc 0.66225 roc_auc 0.57549 prc_auc 0.76289[0m
[93maverage test of epoch 26: loss -6.42383 acc 0.67568 roc_auc 0.87000 prc_auc 0.92661[0m
[92maverage training of epoch 27: loss -6.57602 acc 0.66225 roc_auc 0.56020 prc_auc 0.75124[0m
[93maverage test of epoch 27: loss -6.78588 acc 0.67568 roc_auc 0.87333 prc_auc 0.92668[0m
[92maverage training of epoch 28: loss -6.93465 acc 0.66225 roc_auc 0.55000 prc_auc 0.74418[0m
[93maverage test of epoch 28: loss -7.14532 acc 0.67568 roc_auc 0.86667 prc_auc 0.92514[0m
[92maverage training of epoch 29: loss -7.29083 acc 0.66225 roc_auc 0.54618 prc_auc 0.74290[0m
[93maverage test of epoch 29: loss -7.50206 acc 0.67568 roc_auc 0.87000 prc_auc 0.92661[0m
[92maverage training of epoch 30: loss -7.64480 acc 0.66225 roc_auc 0.53922 prc_auc 0.73700[0m
[93maverage test of epoch 30: loss -7.85669 acc 0.67568 roc_auc 0.86667 prc_auc 0.92586[0m
[92maverage training of epoch 31: loss -7.99723 acc 0.66225 roc_auc 0.53059 prc_auc 0.72899[0m
[93maverage test of epoch 31: loss -8.21015 acc 0.67568 roc_auc 0.87000 prc_auc 0.92614[0m
[92maverage training of epoch 32: loss -8.34909 acc 0.66225 roc_auc 0.52294 prc_auc 0.72141[0m
[93maverage test of epoch 32: loss -8.56345 acc 0.67568 roc_auc 0.87000 prc_auc 0.92614[0m
[92maverage training of epoch 33: loss -8.70130 acc 0.66225 roc_auc 0.51255 prc_auc 0.71254[0m
[93maverage test of epoch 33: loss -8.91746 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 34: loss -9.05461 acc 0.66225 roc_auc 0.50196 prc_auc 0.70130[0m
[93maverage test of epoch 34: loss -9.27286 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 35: loss -9.40964 acc 0.66225 roc_auc 0.49078 prc_auc 0.68659[0m
[93maverage test of epoch 35: loss -9.63019 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 36: loss -9.76683 acc 0.66225 roc_auc 0.47824 prc_auc 0.67521[0m
[93maverage test of epoch 36: loss -9.98982 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 37: loss -10.12657 acc 0.66225 roc_auc 0.46716 prc_auc 0.66334[0m
[93maverage test of epoch 37: loss -10.35217 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 38: loss -10.48921 acc 0.66225 roc_auc 0.45647 prc_auc 0.65390[0m
[93maverage test of epoch 38: loss -10.71755 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 39: loss -10.85504 acc 0.66225 roc_auc 0.44569 prc_auc 0.63927[0m
[93maverage test of epoch 39: loss -11.08620 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 40: loss -11.22428 acc 0.66225 roc_auc 0.43784 prc_auc 0.63201[0m
[93maverage test of epoch 40: loss -11.45837 acc 0.67568 roc_auc 0.87000 prc_auc 0.92752[0m
[92maverage training of epoch 41: loss -11.59713 acc 0.66225 roc_auc 0.42941 prc_auc 0.62401[0m
[93maverage test of epoch 41: loss -11.83420 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 42: loss -11.97375 acc 0.66225 roc_auc 0.42549 prc_auc 0.62140[0m
[93maverage test of epoch 42: loss -12.21387 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 43: loss -12.35426 acc 0.66225 roc_auc 0.42127 prc_auc 0.61617[0m
[93maverage test of epoch 43: loss -12.59749 acc 0.67568 roc_auc 0.87000 prc_auc 0.92729[0m
[92maverage training of epoch 44: loss -12.73879 acc 0.66225 roc_auc 0.41745 prc_auc 0.61396[0m
[93maverage test of epoch 44: loss -12.98519 acc 0.67568 roc_auc 0.86833 prc_auc 0.92618[0m
[92maverage training of epoch 45: loss -13.12744 acc 0.66225 roc_auc 0.41451 prc_auc 0.60727[0m
[93maverage test of epoch 45: loss -13.37707 acc 0.67568 roc_auc 0.86833 prc_auc 0.92495[0m
[92maverage training of epoch 46: loss -13.52030 acc 0.66225 roc_auc 0.41088 prc_auc 0.60380[0m
[93maverage test of epoch 46: loss -13.77321 acc 0.67568 roc_auc 0.86667 prc_auc 0.92385[0m
[92maverage training of epoch 47: loss -13.91746 acc 0.66225 roc_auc 0.40784 prc_auc 0.60044[0m
[93maverage test of epoch 47: loss -14.17369 acc 0.67568 roc_auc 0.86667 prc_auc 0.92401[0m
[92maverage training of epoch 48: loss -14.31899 acc 0.66225 roc_auc 0.40569 prc_auc 0.59889[0m
[93maverage test of epoch 48: loss -14.57860 acc 0.67568 roc_auc 0.86667 prc_auc 0.92401[0m
[92maverage training of epoch 49: loss -14.72495 acc 0.66225 roc_auc 0.40373 prc_auc 0.59758[0m
[93maverage test of epoch 49: loss -14.98799 acc 0.67568 roc_auc 0.86667 prc_auc 0.92401[0m
Training model with dataset, testing using fold 4
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.38857 acc 0.33775 roc_auc 0.20373 prc_auc 0.51893[0m
[93maverage test of epoch 0: loss 0.35255 acc 0.32432 roc_auc 0.06667 prc_auc 0.48823[0m
[92maverage training of epoch 1: loss 0.25508 acc 0.33775 roc_auc 0.20275 prc_auc 0.51815[0m
[93maverage test of epoch 1: loss 0.21947 acc 0.32432 roc_auc 0.06667 prc_auc 0.48823[0m
[92maverage training of epoch 2: loss 0.12157 acc 0.33775 roc_auc 0.20098 prc_auc 0.51754[0m
[93maverage test of epoch 2: loss 0.08594 acc 0.32432 roc_auc 0.06667 prc_auc 0.48823[0m
[92maverage training of epoch 3: loss -0.01318 acc 0.33775 roc_auc 0.19725 prc_auc 0.51745[0m
[93maverage test of epoch 3: loss -0.04923 acc 0.32432 roc_auc 0.06667 prc_auc 0.48823[0m
[92maverage training of epoch 4: loss -0.14909 acc 0.33775 roc_auc 0.19686 prc_auc 0.52631[0mUsing backend: pytorch
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

[93maverage test of epoch 4: loss -0.18566 acc 0.32432 roc_auc 0.06667 prc_auc 0.48823[0m
[92maverage training of epoch 5: loss -0.28492 acc 0.33775 roc_auc 0.19412 prc_auc 0.52650[0m
[93maverage test of epoch 5: loss -0.32159 acc 0.32432 roc_auc 0.06667 prc_auc 0.48823[0m
[92maverage training of epoch 6: loss -0.42011 acc 0.33775 roc_auc 0.18843 prc_auc 0.52565[0m
[93maverage test of epoch 6: loss -0.45697 acc 0.32432 roc_auc 0.06667 prc_auc 0.48823[0m
[92maverage training of epoch 7: loss -0.55563 acc 0.33775 roc_auc 0.17961 prc_auc 0.52258[0m
[93maverage test of epoch 7: loss -0.59363 acc 0.32432 roc_auc 0.06667 prc_auc 0.48823[0m
[92maverage training of epoch 8: loss -0.69574 acc 0.33775 roc_auc 0.16392 prc_auc 0.51673[0m
[93maverage test of epoch 8: loss -0.73899 acc 0.32432 roc_auc 0.07000 prc_auc 0.48890[0m
[92maverage training of epoch 9: loss -0.85105 acc 0.33775 roc_auc 0.22294 prc_auc 0.56226[0m
[93maverage test of epoch 9: loss -0.90506 acc 0.32432 roc_auc 0.07000 prc_auc 0.48890[0m
[92maverage training of epoch 10: loss -1.06259 acc 0.33775 roc_auc 0.48510 prc_auc 0.75224[0m
[93maverage test of epoch 10: loss -1.17777 acc 0.32432 roc_auc 0.93000 prc_auc 0.97264[0m
[92maverage training of epoch 11: loss -1.45237 acc 0.33775 roc_auc 0.65941 prc_auc 0.81336[0m
[93maverage test of epoch 11: loss -1.73757 acc 0.32432 roc_auc 0.93000 prc_auc 0.97264[0m
[92maverage training of epoch 12: loss -1.93520 acc 0.33775 roc_auc 0.84039 prc_auc 0.91660[0m
[93maverage test of epoch 12: loss -2.07493 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 13: loss -2.21303 acc 0.33775 roc_auc 0.87765 prc_auc 0.93799[0m
[93maverage test of epoch 13: loss -2.31795 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 14: loss -2.43146 acc 0.33775 roc_auc 0.87451 prc_auc 0.93371[0m
[93maverage test of epoch 14: loss -2.51482 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 15: loss -2.61438 acc 0.33775 roc_auc 0.87392 prc_auc 0.93033[0m
[93maverage test of epoch 15: loss -2.68569 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 16: loss -2.77779 acc 0.33775 roc_auc 0.87353 prc_auc 0.92797[0m
[93maverage test of epoch 16: loss -2.84214 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 17: loss -2.93038 acc 0.33775 roc_auc 0.87098 prc_auc 0.92390[0m
[93maverage test of epoch 17: loss -2.99075 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 18: loss -3.07709 acc 0.33775 roc_auc 0.87098 prc_auc 0.92316[0m
[93maverage test of epoch 18: loss -3.13485 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 19: loss -3.22067 acc 0.33775 roc_auc 0.87196 prc_auc 0.92403[0m
[93maverage test of epoch 19: loss -3.27723 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 20: loss -3.36320 acc 0.33775 roc_auc 0.86961 prc_auc 0.92126[0m
[93maverage test of epoch 20: loss -3.41959 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 21: loss -3.50631 acc 0.33775 roc_auc 0.87118 prc_auc 0.92131[0m
[93maverage test of epoch 21: loss -3.56241 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 22: loss -3.65082 acc 0.33775 roc_auc 0.87392 prc_auc 0.92293[0m
[93maverage test of epoch 22: loss -3.70640 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 23: loss -3.79679 acc 0.33775 roc_auc 0.87412 prc_auc 0.92159[0m
[93maverage test of epoch 23: loss -3.85317 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 24: loss -3.94577 acc 0.33775 roc_auc 0.87608 prc_auc 0.92200[0m
[93maverage test of epoch 24: loss -4.00201 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 25: loss -4.09734 acc 0.33775 roc_auc 0.87765 prc_auc 0.92270[0m
[93maverage test of epoch 25: loss -4.15392 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 26: loss -4.25184 acc 0.33775 roc_auc 0.87745 prc_auc 0.92190[0m
[93maverage test of epoch 26: loss -4.30983 acc 0.32432 roc_auc 0.92333 prc_auc 0.96958[0m
[92maverage training of epoch 27: loss -4.40964 acc 0.33775 roc_auc 0.87725 prc_auc 0.92132[0m
[93maverage test of epoch 27: loss -4.46821 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 28: loss -4.57067 acc 0.33775 roc_auc 0.87647 prc_auc 0.91923[0m
[93maverage test of epoch 28: loss -4.62932 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 29: loss -4.73441 acc 0.33775 roc_auc 0.87647 prc_auc 0.91466[0m
[93maverage test of epoch 29: loss -4.79448 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 30: loss -4.90204 acc 0.33775 roc_auc 0.86647 prc_auc 0.89039[0m
[93maverage test of epoch 30: loss -4.96382 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 31: loss -5.07266 acc 0.33775 roc_auc 0.86333 prc_auc 0.87291[0m
[93maverage test of epoch 31: loss -5.13342 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 32: loss -5.24495 acc 0.33775 roc_auc 0.86196 prc_auc 0.87074[0m
[93maverage test of epoch 32: loss -5.30630 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 33: loss -5.42000 acc 0.33775 roc_auc 0.86039 prc_auc 0.86425[0m
[93maverage test of epoch 33: loss -5.48042 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 34: loss -5.59634 acc 0.33775 roc_auc 0.85667 prc_auc 0.86184[0m
[93maverage test of epoch 34: loss -5.65586 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 35: loss -5.77365 acc 0.33775 roc_auc 0.84765 prc_auc 0.85086[0m
[93maverage test of epoch 35: loss -5.83205 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 36: loss -5.95142 acc 0.33775 roc_auc 0.84059 prc_auc 0.83542[0m
[93maverage test of epoch 36: loss -6.00762 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 37: loss -6.12866 acc 0.33775 roc_auc 0.83902 prc_auc 0.83297[0m
[93maverage test of epoch 37: loss -6.18197 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 38: loss -6.30422 acc 0.33775 roc_auc 0.83804 prc_auc 0.82995[0m
[93maverage test of epoch 38: loss -6.35450 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 39: loss -6.47750 acc 0.33775 roc_auc 0.83686 prc_auc 0.82806[0m
[93maverage test of epoch 39: loss -6.52507 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 40: loss -6.64860 acc 0.33775 roc_auc 0.83608 prc_auc 0.82590[0m
[93maverage test of epoch 40: loss -6.69431 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 41: loss -6.81856 acc 0.33775 roc_auc 0.83431 prc_auc 0.82351[0m
[93maverage test of epoch 41: loss -6.86246 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 42: loss -6.98760 acc 0.33775 roc_auc 0.83451 prc_auc 0.82333[0m
[93maverage test of epoch 42: loss -7.03004 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 43: loss -7.15616 acc 0.33775 roc_auc 0.83392 prc_auc 0.82301[0m
[93maverage test of epoch 43: loss -7.19742 acc 0.32432 roc_auc 0.93000 prc_auc 0.97235[0m
[92maverage training of epoch 44: loss -7.32440 acc 0.33775 roc_auc 0.83471 prc_auc 0.82353[0m
[93maverage test of epoch 44: loss -7.36491 acc 0.32432 roc_auc 0.93000 prc_auc 0.97235[0m
[92maverage training of epoch 45: loss -7.49304 acc 0.33775 roc_auc 0.83412 prc_auc 0.82290[0m
[93maverage test of epoch 45: loss -7.53260 acc 0.32432 roc_auc 0.93000 prc_auc 0.97235[0m
[92maverage training of epoch 46: loss -7.66206 acc 0.33775 roc_auc 0.83412 prc_auc 0.82282[0m
[93maverage test of epoch 46: loss -7.70072 acc 0.32432 roc_auc 0.93000 prc_auc 0.97235[0m
[92maverage training of epoch 47: loss -7.83147 acc 0.33775 roc_auc 0.83373 prc_auc 0.82223[0m
[93maverage test of epoch 47: loss -7.86962 acc 0.32432 roc_auc 0.93000 prc_auc 0.97235[0m
[92maverage training of epoch 48: loss -8.00169 acc 0.33775 roc_auc 0.83392 prc_auc 0.82337[0m
[93maverage test of epoch 48: loss -8.03923 acc 0.32432 roc_auc 0.93000 prc_auc 0.97235[0m
[92maverage training of epoch 49: loss -8.17255 acc 0.33775 roc_auc 0.83314 prc_auc 0.82218[0m
[93maverage test of epoch 49: loss -8.20988 acc 0.32432 roc_auc 0.93000 prc_auc 0.97235[0m
Run statistics: 
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 50)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 1)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))


Accuracy (avg): 0.53158 ROC_AUC (avg): 0.72487 PRC_AUC (avg): 0.84305 

Average forward propagation time taken(ms): 2.7765959182084754
Average backward propagation time taken(ms): 0.9343628504460317

