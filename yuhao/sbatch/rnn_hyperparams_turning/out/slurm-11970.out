# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======






Params Turning Set
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 100)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 16)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 1)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))






args.base_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/
args.graph_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/
args.min_dfscode_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/
args.min_dfscode_tensor_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/
*** 1 train_index:  [  0   2   3   6   9  10  11  12  13  14  15  16  17  19  20  21  22  23
  24  26  27  30  31  32  33  35  36  38  39  41  42  43  44  45  46  47
  48  49  50  52  53  54  56  57  59  60  61  62  63  64  65  66  67  69
  70  71  72  73  74  76  77  78  79  80  81  82  83  84  87  88  89  91
  92  94  95  96  97  98 100 101 102 103 104 106 107 108 109 110 112 113
 115 116 117 118 119 120 121 122 124 125 126 127 129 130 131 132 134 136
 138 139 140 141 142 144 145 146 147 148 149 151 152 153 154 155 156 157
 158 159 160 161 164 165 166 167 169 170 171 172 173 175 176 177 178 179
 180 182 184 185 186 187]
*** 2 test_index:  [  1   4   5   7   8  18  25  28  29  34  37  40  51  55  58  68  75  85
  86  90  93  99 105 111 114 123 128 133 135 137 143 150 162 163 168 174
 181 183]
*** 1 train_index:  [  0   1   2   4   5   6   7   8  10  11  12  13  14  15  16  17  18  19
  20  21  22  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39
  40  41  43  44  45  46  49  50  51  52  53  54  55  56  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  74  75  76  77  79  80  81  82
  83  84  85  86  88  90  91  92  93  94  96  97  98  99 101 102 103 104
 105 106 107 111 112 113 114 115 120 121 122 123 124 125 126 128 129 130
 131 133 134 135 136 137 138 139 140 143 144 145 147 148 150 153 154 155
 156 159 162 163 164 165 167 168 169 170 171 172 173 174 175 177 178 179
 180 181 182 183 186 187]
*** 2 test_index:  [  3   9  23  31  42  47  48  57  72  73  78  87  89  95 100 108 109 110
 116 117 118 119 127 132 141 142 146 149 151 152 157 158 160 161 166 176
 184 185]
*** 1 train_index:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19
  20  23  24  25  26  28  29  31  32  34  35  36  37  38  40  41  42  43
  44  47  48  51  53  54  55  56  57  58  59  60  64  66  68  70  71  72
  73  74  75  76  77  78  79  81  82  84  85  86  87  88  89  90  92  93
  95  96  97  98  99 100 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134
 135 137 139 140 141 142 143 144 146 148 149 150 151 152 153 156 157 158
 159 160 161 162 163 165 166 167 168 169 171 172 173 174 175 176 177 178
 179 181 183 184 185 186]
*** 2 test_index:  [  0  16  21  22  27  30  33  39  45  46  49  50  52  61  62  63  65  67
  69  80  83  91  94 101 102 103 122 136 138 145 147 154 155 164 170 180
 182 187]
*** 1 train_index:  [  0   1   3   4   5   6   7   8   9  13  14  16  18  19  21  22  23  25
  26  27  28  29  30  31  33  34  36  37  38  39  40  42  43  45  46  47
  48  49  50  51  52  53  55  56  57  58  60  61  62  63  65  66  67  68
  69  71  72  73  75  76  77  78  79  80  81  83  84  85  86  87  88  89
  90  91  92  93  94  95  97  98  99 100 101 102 103 105 106 108 109 110
 111 113 114 115 116 117 118 119 120 121 122 123 125 127 128 131 132 133
 134 135 136 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152
 154 155 156 157 158 160 161 162 163 164 165 166 168 170 171 174 175 176
 180 181 182 183 184 185 187]
*** 2 test_index:  [  2  10  11  12  15  17  20  24  32  35  41  44  54  59  64  70  74  82
  96 104 107 112 124 126 129 130 139 153 159 167 169 172 173 177 178 179
 186]
*** 1 train_index:  [  0   1   2   3   4   5   7   8   9  10  11  12  15  16  17  18  20  21
  22  23  24  25  27  28  29  30  31  32  33  34  35  37  39  40  41  42
  44  45  46  47  48  49  50  51  52  54  55  57  58  59  61  62  63  64
  65  67  68  69  70  72  73  74  75  78  80  82  83  85  86  87  89  90
  91  93  94  95  96  99 100 101 102 103 104 105 107 108 109 110 111 112
 114 116 117 118 119 122 123 124 126 127 128 129 130 132 133 135 136 137
 138 139 141 142 143 145 146 147 149 150 151 152 153 154 155 157 158 159
 160 161 162 163 164 166 167 168 169 170 172 173 174 176 177 178 179 180
 181 182 183 184 185 186 187]
*** 2 test_index:  [  6  13  14  19  26  36  38  43  53  56  60  66  71  76  77  79  81  84
  88  92  97  98 106 113 115 120 121 125 131 134 140 144 148 156 165 171
 175]
{'base_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/',
 'batch_size': 1,
 'clean_temp': False,
 'clean_tensorboard': False,
 'current_dataset_path': None,
 'current_min_dfscode_path': None,
 'current_model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/DFScodeRNN_cls_LSTM_MUTAG_2021-01-18-02-55-37/',
 'current_processed_dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'current_temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/DFScodeRNN_cls_LSTM_MUTAG_2021-01-18-02-55-37/',
 'dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/datasets/',
 'device': device(type='cuda', index=0),
 'dfscode_rnn_dropout': 0.2,
 'embedding_size_dfscode_rnn': 16,
 'epochs': 100,
 'epochs_end': 10000,
 'epochs_save': 20,
 'epochs_validate': 1,
 'fname': 'DFScodeRNN_cls_LSTM_MUTAG',
 'gradient_clipping': True,
 'graph_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/',
 'graph_type': 'MUTAG',
 'graphgen_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/',
 'hidden_size_dfscode_rnn': 4,
 'load_device': device(type='cuda', index=0),
 'load_model': False,
 'load_model_path': '',
 'log_tensorboard': False,
 'lr': 0.0001,
 'max_prev_node': None,
 'min_dfscode_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/',
 'min_dfscode_tensor_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/',
 'note': 'DFScodeRNN_cls_LSTM',
 'num_graphs': None,
 'num_layers': 1,
 'num_workers': 8,
 'number_of_mlp_layer': 1,
 'rnn_type': 'LSTM',
 'save_model': False,
 'temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/',
 'tensorboard_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tensorboard/',
 'time': '2021-01-18-02-55-37',
 'weights': False}


graphgen args.__dict__: None




dataset_features: {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}




config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 100, 'learning_rate': 0.0001, 'seed': 1800, 'k_fold': 5, 'model': 'DFScodeRNN_cls_LSTM', 'dataset': 'MUTAG'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}}


Training a new model: DFScodeRNN_cls_LSTM
Training model with dataset, testing using fold 0
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.34835 acc 0.66667 roc_auc 0.43760 prc_auc 0.64968[0m
[93maverage test of epoch 0: loss -0.45235 acc 0.65789 roc_auc 0.74462 prc_auc 0.87061[0m
[92maverage training of epoch 1: loss -0.57489 acc 0.66667 roc_auc 0.44860 prc_auc 0.66055[0m
[93maverage test of epoch 1: loss -0.68678 acc 0.65789 roc_auc 0.87077 prc_auc 0.92518[0m
[92maverage training of epoch 2: loss -0.81973 acc 0.66667 roc_auc 0.45820 prc_auc 0.67200[0m
[93maverage test of epoch 2: loss -0.93527 acc 0.65789 roc_auc 0.77846 prc_auc 0.90128[0m
[92maverage training of epoch 3: loss -1.07256 acc 0.66667 roc_auc 0.46820 prc_auc 0.68022[0m
[93maverage test of epoch 3: loss -1.18682 acc 0.65789 roc_auc 0.84000 prc_auc 0.92129[0m
[92maverage training of epoch 4: loss -1.31359 acc 0.66667 roc_auc 0.51380 prc_auc 0.73119[0m
[93maverage test of epoch 4: loss -1.40477 acc 0.65789 roc_auc 0.89538 prc_auc 0.94416[0m
[92maverage training of epoch 5: loss -1.52795 acc 0.66667 roc_auc 0.53960 prc_auc 0.74871[0m
[93maverage test of epoch 5: loss -1.61928 acc 0.65789 roc_auc 0.88923 prc_auc 0.94137[0m
[92maverage training of epoch 6: loss -1.75210 acc 0.66667 roc_auc 0.55920 prc_auc 0.76458[0m
[93maverage test of epoch 6: loss -1.85050 acc 0.65789 roc_auc 0.87692 prc_auc 0.93740[0m
[92maverage training of epoch 7: loss -1.99348 acc 0.66667 roc_auc 0.57800 prc_auc 0.77992[0m
[93maverage test of epoch 7: loss -2.09840 acc 0.65789 roc_auc 0.86462 prc_auc 0.93326[0m
[92maverage training of epoch 8: loss -2.24809 acc 0.66667 roc_auc 0.59860 prc_auc 0.79534[0m
[93maverage test of epoch 8: loss -2.35338 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 9: loss -2.50297 acc 0.66667 roc_auc 0.61900 prc_auc 0.80328[0m
[93maverage test of epoch 9: loss -2.59890 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 10: loss -2.74319 acc 0.66667 roc_auc 0.60420 prc_auc 0.78516[0m
[93maverage test of epoch 10: loss -2.82586 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 11: loss -2.96364 acc 0.66667 roc_auc 0.56440 prc_auc 0.75301[0m
[93maverage test of epoch 11: loss -3.03563 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 12: loss -3.17020 acc 0.66667 roc_auc 0.52800 prc_auc 0.72486[0m
[93maverage test of epoch 12: loss -3.23562 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 13: loss -3.37033 acc 0.66667 roc_auc 0.50160 prc_auc 0.70706[0m
[93maverage test of epoch 13: loss -3.43166 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 14: loss -3.63987 acc 0.66667 roc_auc 0.48480 prc_auc 0.69222[0m
[93maverage test of epoch 14: loss -3.77864 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 15: loss -3.98259 acc 0.66667 roc_auc 0.47240 prc_auc 0.68276[0m
[93maverage test of epoch 15: loss -4.10445 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 16: loss -4.30173 acc 0.66667 roc_auc 0.46260 prc_auc 0.67794[0m
[93maverage test of epoch 16: loss -4.41590 acc 0.65789 roc_auc 0.85846 prc_auc 0.93142[0m
[92maverage training of epoch 17: loss -4.60894 acc 0.66667 roc_auc 0.45560 prc_auc 0.67189[0m
[93maverage test of epoch 17: loss -4.71705 acc 0.65789 roc_auc 0.85846 prc_auc 0.93153[0m
[92maverage training of epoch 18: loss -4.90666 acc 0.66667 roc_auc 0.45120 prc_auc 0.66959[0m
[93maverage test of epoch 18: loss -5.00952 acc 0.65789 roc_auc 0.85846 prc_auc 0.93153[0m
[92maverage training of epoch 19: loss -5.19657 acc 0.66667 roc_auc 0.44780 prc_auc 0.66772[0m
[93maverage test of epoch 19: loss -5.29512 acc 0.65789 roc_auc 0.86000 prc_auc 0.93153[0m
[92maverage training of epoch 20: loss -5.47978 acc 0.66667 roc_auc 0.44520 prc_auc 0.66603[0m
[93maverage test of epoch 20: loss -5.57401 acc 0.65789 roc_auc 0.85846 prc_auc 0.93111[0m
[92maverage training of epoch 21: loss -5.75678 acc 0.66667 roc_auc 0.44290 prc_auc 0.66376[0m
[93maverage test of epoch 21: loss -5.84753 acc 0.65789 roc_auc 0.86000 prc_auc 0.93148[0m
[92maverage training of epoch 22: loss -6.02914 acc 0.66667 roc_auc 0.43990 prc_auc 0.65959[0m
[93maverage test of epoch 22: loss -6.11713 acc 0.65789 roc_auc 0.86000 prc_auc 0.93165[0m
[92maverage training of epoch 23: loss -6.29816 acc 0.66667 roc_auc 0.43840 prc_auc 0.65885[0m
[93maverage test of epoch 23: loss -6.38396 acc 0.65789 roc_auc 0.85846 prc_auc 0.93184[0m
[92maverage training of epoch 24: loss -6.56489 acc 0.66667 roc_auc 0.43720 prc_auc 0.65276[0m
[93maverage test of epoch 24: loss -6.64894 acc 0.65789 roc_auc 0.86000 prc_auc 0.93187[0m
[92maverage training of epoch 25: loss -6.83015 acc 0.66667 roc_auc 0.43570 prc_auc 0.65069[0m
[93maverage test of epoch 25: loss -6.91282 acc 0.65789 roc_auc 0.86615 prc_auc 0.93320[0m
[92maverage training of epoch 26: loss -7.09465 acc 0.66667 roc_auc 0.43460 prc_auc 0.64931[0m
[93maverage test of epoch 26: loss -7.17623 acc 0.65789 roc_auc 0.86615 prc_auc 0.93303[0m
[92maverage training of epoch 27: loss -7.35896 acc 0.66667 roc_auc 0.43210 prc_auc 0.64729[0m
[93maverage test of epoch 27: loss -7.43971 acc 0.65789 roc_auc 0.86308 prc_auc 0.90724[0m
[92maverage training of epoch 28: loss -7.62352 acc 0.66667 roc_auc 0.43070 prc_auc 0.64662[0m
[93maverage test of epoch 28: loss -7.70370 acc 0.65789 roc_auc 0.84000 prc_auc 0.89564[0m
[92maverage training of epoch 29: loss -7.88877 acc 0.66667 roc_auc 0.43070 prc_auc 0.64662[0m
[93maverage test of epoch 29: loss -7.96857 acc 0.65789 roc_auc 0.88462 prc_auc 0.92019[0m
[92maverage training of epoch 30: loss -8.15510 acc 0.66667 roc_auc 0.43040 prc_auc 0.64640[0m
[93maverage test of epoch 30: loss -8.23467 acc 0.65789 roc_auc 0.87538 prc_auc 0.89850[0m
[92maverage training of epoch 31: loss -8.42279 acc 0.66667 roc_auc 0.43020 prc_auc 0.64623[0m
[93maverage test of epoch 31: loss -8.50225 acc 0.65789 roc_auc 0.85231 prc_auc 0.88546[0m
[92maverage training of epoch 32: loss -8.69211 acc 0.66667 roc_auc 0.42950 prc_auc 0.64600[0m
[93maverage test of epoch 32: loss -8.77156 acc 0.65789 roc_auc 0.89231 prc_auc 0.90896[0m
[92maverage training of epoch 33: loss -8.96328 acc 0.66667 roc_auc 0.42890 prc_auc 0.64570[0m
[93maverage test of epoch 33: loss -9.04280 acc 0.65789 roc_auc 0.82308 prc_auc 0.85662[0m
[92maverage training of epoch 34: loss -9.23648 acc 0.66667 roc_auc 0.42840 prc_auc 0.64214[0m
[93maverage test of epoch 34: loss -9.31615 acc 0.65789 roc_auc 0.65385 prc_auc 0.73529[0m
[92maverage training of epoch 35: loss -9.51188 acc 0.66667 roc_auc 0.42760 prc_auc 0.63951[0m
[93maverage test of epoch 35: loss -9.59176 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 36: loss -9.78963 acc 0.66667 roc_auc 0.42700 prc_auc 0.63818[0m
[93maverage test of epoch 36: loss -9.86977 acc 0.65789 roc_auc 0.84154 prc_auc 0.87989[0m
[92maverage training of epoch 37: loss -10.06984 acc 0.66667 roc_auc 0.42690 prc_auc 0.63812[0m
[93maverage test of epoch 37: loss -10.15029 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 38: loss -10.35264 acc 0.66667 roc_auc 0.42650 prc_auc 0.63782[0m
[93maverage test of epoch 38: loss -10.43343 acc 0.65789 roc_auc 0.76615 prc_auc 0.81086[0m
[92maverage training of epoch 39: loss -10.63811 acc 0.66667 roc_auc 0.42650 prc_auc 0.63806[0m
[93maverage test of epoch 39: loss -10.71927 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 40: loss -10.92634 acc 0.66667 roc_auc 0.42600 prc_auc 0.63723[0m
[93maverage test of epoch 40: loss -11.00790 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 41: loss -11.21742 acc 0.66667 roc_auc 0.42590 prc_auc 0.63687[0m
[93maverage test of epoch 41: loss -11.29940 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -11.51139 acc 0.66667 roc_auc 0.42590 prc_auc 0.63693[0m
[93maverage test of epoch 42: loss -11.59381 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -11.80828 acc 0.66667 roc_auc 0.42550 prc_auc 0.63607[0m
[93maverage test of epoch 43: loss -11.89100 acc 0.65789 roc_auc 0.80462 prc_auc 0.84026[0m
[92maverage training of epoch 44: loss -12.10774 acc 0.66667 roc_auc 0.42420 prc_auc 0.63516[0m
[93maverage test of epoch 44: loss -12.19065 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -12.40972 acc 0.66667 roc_auc 0.42420 prc_auc 0.63528[0m
[93maverage test of epoch 45: loss -12.49287 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -12.71435 acc 0.66667 roc_auc 0.42160 prc_auc 0.63279[0m
[93maverage test of epoch 46: loss -12.79778 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -13.02173 acc 0.66667 roc_auc 0.42210 prc_auc 0.63291[0m
[93maverage test of epoch 47: loss -13.10548 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -13.33195 acc 0.66667 roc_auc 0.42130 prc_auc 0.63252[0m
[93maverage test of epoch 48: loss -13.41606 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -13.64510 acc 0.66667 roc_auc 0.42180 prc_auc 0.63297[0m
[93maverage test of epoch 49: loss -13.72957 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 50: loss -13.96122 acc 0.66667 roc_auc 0.42140 prc_auc 0.63231[0m
[93maverage test of epoch 50: loss -14.04609 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 51: loss -14.28039 acc 0.66667 roc_auc 0.42060 prc_auc 0.63170[0m
[93maverage test of epoch 51: loss -14.36567 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 52: loss -14.60266 acc 0.66667 roc_auc 0.41890 prc_auc 0.63003[0m
[93maverage test of epoch 52: loss -14.68835 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 53: loss -14.92805 acc 0.66667 roc_auc 0.41970 prc_auc 0.62365[0m
[93maverage test of epoch 53: loss -15.01417 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 54: loss -15.25660 acc 0.66667 roc_auc 0.41720 prc_auc 0.62165[0m
[93maverage test of epoch 54: loss -15.34308 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 55: loss -15.58818 acc 0.66667 roc_auc 0.41870 prc_auc 0.62282[0m
[93maverage test of epoch 55: loss -15.67498 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 56: loss -15.92278 acc 0.66667 roc_auc 0.41590 prc_auc 0.62128[0m
[93maverage test of epoch 56: loss -16.00991 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 57: loss -16.26046 acc 0.66667 roc_auc 0.41690 prc_auc 0.62067[0m
[93maverage test of epoch 57: loss -16.34793 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 58: loss -16.60124 acc 0.66667 roc_auc 0.41760 prc_auc 0.62199[0m
[93maverage test of epoch 58: loss -16.68906 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 59: loss -16.94518 acc 0.66667 roc_auc 0.41710 prc_auc 0.61942[0m
[93maverage test of epoch 59: loss -17.03335 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 60: loss -17.29229 acc 0.66667 roc_auc 0.41860 prc_auc 0.62084[0m
[93maverage test of epoch 60: loss -17.38083 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 61: loss -17.64261 acc 0.66667 roc_auc 0.41920 prc_auc 0.62419[0m
[93maverage test of epoch 61: loss -17.73151 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 62: loss -17.99616 acc 0.66667 roc_auc 0.41580 prc_auc 0.62229[0m
[93maverage test of epoch 62: loss -18.08542 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 63: loss -18.35295 acc 0.66667 roc_auc 0.41470 prc_auc 0.62169[0m
[93maverage test of epoch 63: loss -18.44257 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 64: loss -18.71301 acc 0.66667 roc_auc 0.41550 prc_auc 0.62048[0m
[93maverage test of epoch 64: loss -18.80298 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 65: loss -19.07635 acc 0.66667 roc_auc 0.41370 prc_auc 0.61727[0m
[93maverage test of epoch 65: loss -19.16667 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 66: loss -19.44297 acc 0.66667 roc_auc 0.40910 prc_auc 0.61547[0m
[93maverage test of epoch 66: loss -19.53365 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 67: loss -19.81290 acc 0.66667 roc_auc 0.41300 prc_auc 0.61786[0m
[93maverage test of epoch 67: loss -19.90391 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 68: loss -20.18614 acc 0.66667 roc_auc 0.40110 prc_auc 0.61102[0m
[93maverage test of epoch 68: loss -20.27749 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 69: loss -20.56270 acc 0.66667 roc_auc 0.39940 prc_auc 0.60881[0m
[93maverage test of epoch 69: loss -20.65437 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 70: loss -20.94258 acc 0.66667 roc_auc 0.40550 prc_auc 0.61590[0m
[93maverage test of epoch 70: loss -21.03459 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 71: loss -21.32580 acc 0.66667 roc_auc 0.41970 prc_auc 0.62477[0m
[93maverage test of epoch 71: loss -21.41812 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 72: loss -21.71234 acc 0.66667 roc_auc 0.41750 prc_auc 0.62733[0m
[93maverage test of epoch 72: loss -21.80496 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 73: loss -22.10221 acc 0.66667 roc_auc 0.39630 prc_auc 0.60786[0m
[93maverage test of epoch 73: loss -22.19513 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 74: loss -22.49543 acc 0.66667 roc_auc 0.40860 prc_auc 0.62365[0m
[93maverage test of epoch 74: loss -22.58865 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 75: loss -22.89202 acc 0.66667 roc_auc 0.40480 prc_auc 0.61667[0m
[93maverage test of epoch 75: loss -22.98553 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 76: loss -23.29197 acc 0.66667 roc_auc 0.41800 prc_auc 0.63320[0m
[93maverage test of epoch 76: loss -23.38576 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 77: loss -23.69527 acc 0.66667 roc_auc 0.41990 prc_auc 0.62558[0m
[93maverage test of epoch 77: loss -23.78933 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 78: loss -24.10193 acc 0.66667 roc_auc 0.39040 prc_auc 0.61461[0m
[93maverage test of epoch 78: loss -24.19622 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 79: loss -24.51189 acc 0.66667 roc_auc 0.39690 prc_auc 0.61917[0m
[93maverage test of epoch 79: loss -24.60642 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 80: loss -24.92521 acc 0.66667 roc_auc 0.44170 prc_auc 0.64679[0m
[93maverage test of epoch 80: loss -25.01998 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 81: loss -25.34188 acc 0.66667 roc_auc 0.41070 prc_auc 0.62592[0m
[93maverage test of epoch 81: loss -25.43687 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 82: loss -25.76458 acc 0.66667 roc_auc 0.43500 prc_auc 0.63948[0m
[93maverage test of epoch 82: loss -25.86892 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 83: loss -26.21184 acc 0.66667 roc_auc 0.49570 prc_auc 0.66507[0m
[93maverage test of epoch 83: loss -26.32388 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 84: loss -26.67262 acc 0.66667 roc_auc 0.40000 prc_auc 0.62886[0m
[93maverage test of epoch 84: loss -26.78773 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 85: loss -27.14138 acc 0.66667 roc_auc 0.42500 prc_auc 0.63681[0m
[93maverage test of epoch 85: loss -27.25875 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 86: loss -27.61697 acc 0.66667 roc_auc 0.39500 prc_auc 0.62742[0m
[93maverage test of epoch 86: loss -27.73623 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 87: loss -28.09879 acc 0.66667 roc_auc 0.47500 prc_auc 0.65952[0m
[93maverage test of epoch 87: loss -28.21960 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 88: loss -28.58629 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 88: loss -28.70843 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 89: loss -29.07920 acc 0.66667 roc_auc 0.44000 prc_auc 0.64167[0m
[93maverage test of epoch 89: loss -29.20253 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 90: loss -29.57681 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 90: loss -29.70059 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 91: loss -30.07822 acc 0.66667 roc_auc 0.43500 prc_auc 0.63969[0m
[93maverage test of epoch 91: loss -30.20243 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 92: loss -30.58356 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 92: loss -30.70837 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 93: loss -31.09304 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 93: loss -31.21843 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 94: loss -31.60679 acc 0.66667 roc_auc 0.42500 prc_auc 0.63571[0m
[93maverage test of epoch 94: loss -31.73290 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 95: loss -32.12501 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 95: loss -32.25179 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 96: loss -32.64764 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 96: loss -32.77513 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 97: loss -33.17485 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 97: loss -33.30312 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 98: loss -33.70676 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 98: loss -33.83581 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 99: loss -34.24339 acc 0.66667 roc_auc 0.50000 prc_auc 0.66667[0m
[93maverage test of epoch 99: loss -34.37323 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 1
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.13990 acc 0.66667 roc_auc 0.42500 prc_auc 0.61837[0m
[93maverage test of epoch 0: loss -0.17275 acc 0.65789 roc_auc 0.16308 prc_auc 0.49746[0m
[92maverage training of epoch 1: loss -0.22550 acc 0.66667 roc_auc 0.42620 prc_auc 0.61869[0m
[93maverage test of epoch 1: loss -0.25862 acc 0.65789 roc_auc 0.14462 prc_auc 0.49565[0m
[92maverage training of epoch 2: loss -0.31182 acc 0.66667 roc_auc 0.43110 prc_auc 0.62036[0m
[93maverage test of epoch 2: loss -0.34708 acc 0.65789 roc_auc 0.14769 prc_auc 0.49538[0m
[92maverage training of epoch 3: loss -0.39988 acc 0.66667 roc_auc 0.43580 prc_auc 0.62152[0m
[93maverage test of epoch 3: loss -0.43741 acc 0.65789 roc_auc 0.15077 prc_auc 0.49557[0m
[92maverage training of epoch 4: loss -0.48947 acc 0.66667 roc_auc 0.44500 prc_auc 0.62690[0m
[93maverage test of epoch 4: loss -0.52976 acc 0.65789 roc_auc 0.16308 prc_auc 0.49842[0m
[92maverage training of epoch 5: loss -0.58092 acc 0.66667 roc_auc 0.45920 prc_auc 0.63779[0m
[93maverage test of epoch 5: loss -0.62425 acc 0.65789 roc_auc 0.63077 prc_auc 0.84105[0m
[92maverage training of epoch 6: loss -0.67581 acc 0.66667 roc_auc 0.46780 prc_auc 0.65721[0m
[93maverage test of epoch 6: loss -0.72340 acc 0.65789 roc_auc 0.87231 prc_auc 0.95188[0m
[92maverage training of epoch 7: loss -0.77883 acc 0.66667 roc_auc 0.47510 prc_auc 0.66195[0m
[93maverage test of epoch 7: loss -0.83475 acc 0.65789 roc_auc 0.91385 prc_auc 0.95365[0m
[92maverage training of epoch 8: loss -0.90248 acc 0.66667 roc_auc 0.47760 prc_auc 0.66518[0m
[93maverage test of epoch 8: loss -0.97898 acc 0.65789 roc_auc 0.89846 prc_auc 0.94428[0m
[92maverage training of epoch 9: loss -1.08054 acc 0.66667 roc_auc 0.48420 prc_auc 0.67127[0m
[93maverage test of epoch 9: loss -1.20409 acc 0.65789 roc_auc 0.89231 prc_auc 0.93725[0m
[92maverage training of epoch 10: loss -1.33669 acc 0.66667 roc_auc 0.50760 prc_auc 0.68860[0m
[93maverage test of epoch 10: loss -1.45937 acc 0.65789 roc_auc 0.88923 prc_auc 0.93317[0m
[92maverage training of epoch 11: loss -1.55282 acc 0.66667 roc_auc 0.50620 prc_auc 0.68633[0m
[93maverage test of epoch 11: loss -1.63478 acc 0.65789 roc_auc 0.89846 prc_auc 0.94003[0m
[92maverage training of epoch 12: loss -1.71304 acc 0.66667 roc_auc 0.49480 prc_auc 0.67936[0m
[93maverage test of epoch 12: loss -1.78212 acc 0.65789 roc_auc 0.90154 prc_auc 0.94336[0m
[92maverage training of epoch 13: loss -1.85562 acc 0.66667 roc_auc 0.48680 prc_auc 0.67288[0m
[93maverage test of epoch 13: loss -1.91977 acc 0.65789 roc_auc 0.90154 prc_auc 0.94367[0m
[92maverage training of epoch 14: loss -1.99118 acc 0.66667 roc_auc 0.48020 prc_auc 0.66569[0m
[93maverage test of epoch 14: loss -2.05284 acc 0.65789 roc_auc 0.90769 prc_auc 0.94930[0m
[92maverage training of epoch 15: loss -2.12324 acc 0.66667 roc_auc 0.47510 prc_auc 0.65875[0m
[93maverage test of epoch 15: loss -2.18347 acc 0.65789 roc_auc 0.90769 prc_auc 0.94930[0m
[92maverage training of epoch 16: loss -2.25343 acc 0.66667 roc_auc 0.47300 prc_auc 0.65813[0m
[93maverage test of epoch 16: loss -2.31279 acc 0.65789 roc_auc 0.90462 prc_auc 0.94797[0m
[92maverage training of epoch 17: loss -2.38265 acc 0.66667 roc_auc 0.47120 prc_auc 0.65694[0m
[93maverage test of epoch 17: loss -2.44146 acc 0.65789 roc_auc 0.90769 prc_auc 0.94943[0m
[92maverage training of epoch 18: loss -2.51143 acc 0.66667 roc_auc 0.46940 prc_auc 0.65529[0m
[93maverage test of epoch 18: loss -2.56988 acc 0.65789 roc_auc 0.89692 prc_auc 0.93753[0m
[92maverage training of epoch 19: loss -2.64012 acc 0.66667 roc_auc 0.46820 prc_auc 0.65445[0m
[93maverage test of epoch 19: loss -2.69834 acc 0.65789 roc_auc 0.89231 prc_auc 0.93772[0m
[92maverage training of epoch 20: loss -2.76894 acc 0.66667 roc_auc 0.46680 prc_auc 0.65320[0m
[93maverage test of epoch 20: loss -2.82701 acc 0.65789 roc_auc 0.89385 prc_auc 0.93800[0m
[92maverage training of epoch 21: loss -2.89807 acc 0.66667 roc_auc 0.46580 prc_auc 0.65230[0m
[93maverage test of epoch 21: loss -2.95606 acc 0.65789 roc_auc 0.89385 prc_auc 0.93800[0m
[92maverage training of epoch 22: loss -3.02764 acc 0.66667 roc_auc 0.46480 prc_auc 0.65135[0m
[93maverage test of epoch 22: loss -3.08561 acc 0.65789 roc_auc 0.89077 prc_auc 0.93595[0m
[92maverage training of epoch 23: loss -3.15777 acc 0.66667 roc_auc 0.46360 prc_auc 0.65046[0m
[93maverage test of epoch 23: loss -3.21576 acc 0.65789 roc_auc 0.89077 prc_auc 0.93664[0m
[92maverage training of epoch 24: loss -3.28857 acc 0.66667 roc_auc 0.46220 prc_auc 0.64954[0m
[93maverage test of epoch 24: loss -3.34663 acc 0.65789 roc_auc 0.89077 prc_auc 0.93382[0m
[92maverage training of epoch 25: loss -3.42013 acc 0.66667 roc_auc 0.46050 prc_auc 0.64776[0m
[93maverage test of epoch 25: loss -3.47830 acc 0.65789 roc_auc 0.89385 prc_auc 0.93715[0m
[92maverage training of epoch 26: loss -3.55254 acc 0.66667 roc_auc 0.45960 prc_auc 0.64723[0m
[93maverage test of epoch 26: loss -3.61085 acc 0.65789 roc_auc 0.88769 prc_auc 0.93177[0m
[92maverage training of epoch 27: loss -3.68589 acc 0.66667 roc_auc 0.45900 prc_auc 0.64686[0m
[93maverage test of epoch 27: loss -3.74438 acc 0.65789 roc_auc 0.89846 prc_auc 0.93675[0m
[92maverage training of epoch 28: loss -3.82024 acc 0.66667 roc_auc 0.45820 prc_auc 0.64618[0m
[93maverage test of epoch 28: loss -3.87893 acc 0.65789 roc_auc 0.88769 prc_auc 0.93103[0m
[92maverage training of epoch 29: loss -3.95560 acc 0.66667 roc_auc 0.45790 prc_auc 0.64578[0m
[93maverage test of epoch 29: loss -4.01441 acc 0.65789 roc_auc 0.90308 prc_auc 0.94406[0m
[92maverage training of epoch 30: loss -4.09175 acc 0.66667 roc_auc 0.45760 prc_auc 0.64563[0m
[93maverage test of epoch 30: loss -4.15056 acc 0.65789 roc_auc 0.89538 prc_auc 0.93192[0m
[92maverage training of epoch 31: loss -4.22855 acc 0.66667 roc_auc 0.45680 prc_auc 0.64512[0m
[93maverage test of epoch 31: loss -4.28738 acc 0.65789 roc_auc 0.89231 prc_auc 0.92875[0m
[92maverage training of epoch 32: loss -4.36609 acc 0.66667 roc_auc 0.45660 prc_auc 0.64507[0m
[93maverage test of epoch 32: loss -4.42501 acc 0.65789 roc_auc 0.89077 prc_auc 0.92635[0m
[92maverage training of epoch 33: loss -4.50452 acc 0.66667 roc_auc 0.45640 prc_auc 0.64518[0m
[93maverage test of epoch 33: loss -4.56358 acc 0.65789 roc_auc 0.88923 prc_auc 0.92499[0m
[92maverage training of epoch 34: loss -4.64393 acc 0.66667 roc_auc 0.45630 prc_auc 0.64483[0m
[93maverage test of epoch 34: loss -4.70318 acc 0.65789 roc_auc 0.88923 prc_auc 0.92349[0m
[92maverage training of epoch 35: loss -4.78442 acc 0.66667 roc_auc 0.45620 prc_auc 0.64483[0m
[93maverage test of epoch 35: loss -4.84390 acc 0.65789 roc_auc 0.89077 prc_auc 0.92425[0m
[92maverage training of epoch 36: loss -4.92608 acc 0.66667 roc_auc 0.45600 prc_auc 0.64477[0m
[93maverage test of epoch 36: loss -4.98583 acc 0.65789 roc_auc 0.87538 prc_auc 0.90048[0m
[92maverage training of epoch 37: loss -5.06897 acc 0.66667 roc_auc 0.45580 prc_auc 0.64474[0m
[93maverage test of epoch 37: loss -5.12900 acc 0.65789 roc_auc 0.88923 prc_auc 0.92349[0m
[92maverage training of epoch 38: loss -5.21315 acc 0.66667 roc_auc 0.45580 prc_auc 0.64474[0m
[93maverage test of epoch 38: loss -5.27350 acc 0.65789 roc_auc 0.87538 prc_auc 0.90094[0m
[92maverage training of epoch 39: loss -5.35867 acc 0.66667 roc_auc 0.45580 prc_auc 0.64474[0m
[93maverage test of epoch 39: loss -5.41936 acc 0.65789 roc_auc 0.86769 prc_auc 0.89651[0m
[92maverage training of epoch 40: loss -5.50558 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 40: loss -5.56662 acc 0.65789 roc_auc 0.88923 prc_auc 0.91671[0m
[92maverage training of epoch 41: loss -5.65391 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 41: loss -5.71532 acc 0.65789 roc_auc 0.88923 prc_auc 0.91150[0m
[92maverage training of epoch 42: loss -5.80370 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 42: loss -5.86549 acc 0.65789 roc_auc 0.87231 prc_auc 0.90062[0m
[92maverage training of epoch 43: loss -5.95498 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 43: loss -6.01716 acc 0.65789 roc_auc 0.86769 prc_auc 0.89556[0m
[92maverage training of epoch 44: loss -6.10777 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 44: loss -6.17034 acc 0.65789 roc_auc 0.87077 prc_auc 0.89689[0m
[92maverage training of epoch 45: loss -6.26209 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 45: loss -6.32506 acc 0.65789 roc_auc 0.88615 prc_auc 0.90595[0m
[92maverage training of epoch 46: loss -6.41795 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 46: loss -6.48133 acc 0.65789 roc_auc 0.89231 prc_auc 0.91449[0m
[92maverage training of epoch 47: loss -6.57538 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 47: loss -6.63918 acc 0.65789 roc_auc 0.86000 prc_auc 0.88857[0m
[92maverage training of epoch 48: loss -6.73440 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 48: loss -6.79860 acc 0.65789 roc_auc 0.85538 prc_auc 0.87407[0m
[92maverage training of epoch 49: loss -6.89500 acc 0.66667 roc_auc 0.45560 prc_auc 0.63977[0m
[93maverage test of epoch 49: loss -6.95962 acc 0.65789 roc_auc 0.89077 prc_auc 0.90676[0m
[92maverage training of epoch 50: loss -7.05720 acc 0.66667 roc_auc 0.45560 prc_auc 0.63977[0m
[93maverage test of epoch 50: loss -7.12224 acc 0.65789 roc_auc 0.83231 prc_auc 0.86094[0m
[92maverage training of epoch 51: loss -7.22097 acc 0.66667 roc_auc 0.45560 prc_auc 0.63974[0m
[93maverage test of epoch 51: loss -7.28633 acc 0.65789 roc_auc 0.84000 prc_auc 0.86453[0m
[92maverage training of epoch 52: loss -7.38615 acc 0.66667 roc_auc 0.45540 prc_auc 0.63843[0m
[93maverage test of epoch 52: loss -7.45181 acc 0.65789 roc_auc 0.78615 prc_auc 0.82583[0m
[92maverage training of epoch 53: loss -7.55275 acc 0.66667 roc_auc 0.45540 prc_auc 0.63841[0m
[93maverage test of epoch 53: loss -7.61872 acc 0.65789 roc_auc 0.82308 prc_auc 0.84667[0m
[92maverage training of epoch 54: loss -7.72078 acc 0.66667 roc_auc 0.45540 prc_auc 0.63841[0m
[93maverage test of epoch 54: loss -7.78709 acc 0.65789 roc_auc 0.86769 prc_auc 0.88351[0m
[92maverage training of epoch 55: loss -7.89029 acc 0.66667 roc_auc 0.45540 prc_auc 0.63841[0m
[93maverage test of epoch 55: loss -7.95693 acc 0.65789 roc_auc 0.72462 prc_auc 0.80058[0m
[92maverage training of epoch 56: loss -8.06130 acc 0.66667 roc_auc 0.45540 prc_auc 0.63841[0m
[93maverage test of epoch 56: loss -8.12827 acc 0.65789 roc_auc 0.83077 prc_auc 0.86004[0m
[92maverage training of epoch 57: loss -8.23382 acc 0.66667 roc_auc 0.45540 prc_auc 0.63841[0m
[93maverage test of epoch 57: loss -8.30114 acc 0.65789 roc_auc 0.71231 prc_auc 0.79222[0m
[92maverage training of epoch 58: loss -8.40787 acc 0.66667 roc_auc 0.45520 prc_auc 0.63746[0m
[93maverage test of epoch 58: loss -8.47554 acc 0.65789 roc_auc 0.78308 prc_auc 0.82263[0m
[92maverage training of epoch 59: loss -8.58347 acc 0.66667 roc_auc 0.45520 prc_auc 0.63746[0m
[93maverage test of epoch 59: loss -8.65150 acc 0.65789 roc_auc 0.74923 prc_auc 0.81462[0m
[92maverage training of epoch 60: loss -8.76063 acc 0.66667 roc_auc 0.45520 prc_auc 0.63746[0m
[93maverage test of epoch 60: loss -8.82902 acc 0.65789 roc_auc 0.54615 prc_auc 0.69158[0m
[92maverage training of epoch 61: loss -8.93938 acc 0.66667 roc_auc 0.45520 prc_auc 0.63746[0m
[93maverage test of epoch 61: loss -9.00811 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 62: loss -9.11969 acc 0.66667 roc_auc 0.45520 prc_auc 0.63746[0m
[93maverage test of epoch 62: loss -9.18879 acc 0.65789 roc_auc 0.72769 prc_auc 0.78752[0m
[92maverage training of epoch 63: loss -9.30162 acc 0.66667 roc_auc 0.45520 prc_auc 0.63746[0m
[93maverage test of epoch 63: loss -9.37106 acc 0.65789 roc_auc 0.60000 prc_auc 0.72632[0m
[92maverage training of epoch 64: loss -9.48512 acc 0.66667 roc_auc 0.45520 prc_auc 0.63746[0m
[93maverage test of epoch 64: loss -9.55493 acc 0.65789 roc_auc 0.80308 prc_auc 0.83459[0m
[92maverage training of epoch 65: loss -9.67025 acc 0.66667 roc_auc 0.45520 prc_auc 0.63748[0m
[93maverage test of epoch 65: loss -9.74043 acc 0.65789 roc_auc 0.57538 prc_auc 0.70162[0m
[92maverage training of epoch 66: loss -9.85701 acc 0.66667 roc_auc 0.45520 prc_auc 0.63746[0m
[93maverage test of epoch 66: loss -9.92753 acc 0.65789 roc_auc 0.70923 prc_auc 0.78947[0m
[92maverage training of epoch 67: loss -10.04535 acc 0.66667 roc_auc 0.45520 prc_auc 0.63748[0m
[93maverage test of epoch 67: loss -10.11621 acc 0.65789 roc_auc 0.75846 prc_auc 0.81158[0m
[92maverage training of epoch 68: loss -10.23531 acc 0.66667 roc_auc 0.45520 prc_auc 0.63748[0m
[93maverage test of epoch 68: loss -10.30653 acc 0.65789 roc_auc 0.82308 prc_auc 0.84667[0m
[92maverage training of epoch 69: loss -10.42692 acc 0.66667 roc_auc 0.45520 prc_auc 0.63748[0m
[93maverage test of epoch 69: loss -10.49849 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 70: loss -10.62018 acc 0.66667 roc_auc 0.45520 prc_auc 0.63746[0m
[93maverage test of epoch 70: loss -10.69210 acc 0.65789 roc_auc 0.75231 prc_auc 0.79875[0m
[92maverage training of epoch 71: loss -10.81508 acc 0.66667 roc_auc 0.45520 prc_auc 0.63746[0m
[93maverage test of epoch 71: loss -10.88735 acc 0.65789 roc_auc 0.78308 prc_auc 0.82263[0m
[92maverage training of epoch 72: loss -11.01163 acc 0.66667 roc_auc 0.45520 prc_auc 0.63746[0m
[93maverage test of epoch 72: loss -11.08423 acc 0.65789 roc_auc 0.58000 prc_auc 0.71263[0m
[92maverage training of epoch 73: loss -11.20979 acc 0.66667 roc_auc 0.45520 prc_auc 0.63746[0m
[93maverage test of epoch 73: loss -11.28270 acc 0.65789 roc_auc 0.79846 prc_auc 0.83587[0m
[92maverage training of epoch 74: loss -11.40956 acc 0.66667 roc_auc 0.45520 prc_auc 0.63746[0m
[93maverage test of epoch 74: loss -11.48280 acc 0.65789 roc_auc 0.68462 prc_auc 0.77842[0m
[92maverage training of epoch 75: loss -11.61098 acc 0.66667 roc_auc 0.45500 prc_auc 0.63713[0m
[93maverage test of epoch 75: loss -11.68455 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 76: loss -11.81405 acc 0.66667 roc_auc 0.45520 prc_auc 0.63746[0m
[93maverage test of epoch 76: loss -11.88795 acc 0.65789 roc_auc 0.75846 prc_auc 0.81158[0m
[92maverage training of epoch 77: loss -12.01878 acc 0.66667 roc_auc 0.45510 prc_auc 0.63728[0m
[93maverage test of epoch 77: loss -12.09300 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 78: loss -12.22516 acc 0.66667 roc_auc 0.45510 prc_auc 0.63725[0m
[93maverage test of epoch 78: loss -12.29970 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 79: loss -12.43320 acc 0.66667 roc_auc 0.45510 prc_auc 0.63710[0m
[93maverage test of epoch 79: loss -12.50805 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 80: loss -12.64289 acc 0.66667 roc_auc 0.45530 prc_auc 0.63747[0m
[93maverage test of epoch 80: loss -12.71805 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 81: loss -12.85423 acc 0.66667 roc_auc 0.45500 prc_auc 0.63725[0m
[93maverage test of epoch 81: loss -12.92970 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 82: loss -13.06722 acc 0.66667 roc_auc 0.45530 prc_auc 0.63747[0m
[93maverage test of epoch 82: loss -13.14297 acc 0.65789 roc_auc 0.57385 prc_auc 0.71105[0m
[92maverage training of epoch 83: loss -13.28183 acc 0.66667 roc_auc 0.45500 prc_auc 0.63715[0m
[93maverage test of epoch 83: loss -13.35787 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 84: loss -13.49808 acc 0.66667 roc_auc 0.45520 prc_auc 0.63747[0m
[93maverage test of epoch 84: loss -13.57441 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 85: loss -13.71599 acc 0.66667 roc_auc 0.45520 prc_auc 0.63732[0m
[93maverage test of epoch 85: loss -13.79261 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 86: loss -13.93556 acc 0.66667 roc_auc 0.45500 prc_auc 0.63700[0m
[93maverage test of epoch 86: loss -14.01246 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 87: loss -14.15678 acc 0.66667 roc_auc 0.45500 prc_auc 0.63715[0m
[93maverage test of epoch 87: loss -14.23396 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 88: loss -14.37966 acc 0.66667 roc_auc 0.45500 prc_auc 0.63699[0m
[93maverage test of epoch 88: loss -14.45711 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 89: loss -14.60419 acc 0.66667 roc_auc 0.45510 prc_auc 0.63715[0m
[93maverage test of epoch 89: loss -14.68191 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 90: loss -14.83038 acc 0.66667 roc_auc 0.45520 prc_auc 0.63732[0m
[93maverage test of epoch 90: loss -14.90836 acc 0.65789 roc_auc 0.81846 prc_auc 0.84819[0m
[92maverage training of epoch 91: loss -15.05822 acc 0.66667 roc_auc 0.45530 prc_auc 0.63747[0m
[93maverage test of epoch 91: loss -15.13646 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 92: loss -15.28772 acc 0.66667 roc_auc 0.45490 prc_auc 0.63699[0m
[93maverage test of epoch 92: loss -15.36622 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 93: loss -15.51888 acc 0.66667 roc_auc 0.45530 prc_auc 0.63717[0m
[93maverage test of epoch 93: loss -15.59762 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 94: loss -15.75169 acc 0.66667 roc_auc 0.45500 prc_auc 0.63684[0m
[93maverage test of epoch 94: loss -15.83068 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 95: loss -15.98616 acc 0.66667 roc_auc 0.45500 prc_auc 0.63699[0m
[93maverage test of epoch 95: loss -16.06539 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 96: loss -16.22228 acc 0.66667 roc_auc 0.45490 prc_auc 0.63684[0m
[93maverage test of epoch 96: loss -16.30175 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 97: loss -16.46006 acc 0.66667 roc_auc 0.45500 prc_auc 0.63684[0m
[93maverage test of epoch 97: loss -16.53976 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 98: loss -16.69950 acc 0.66667 roc_auc 0.45500 prc_auc 0.63684[0m
[93maverage test of epoch 98: loss -16.77942 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 99: loss -16.94058 acc 0.66667 roc_auc 0.45490 prc_auc 0.63681[0m
[93maverage test of epoch 99: loss -17.02071 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
Training model with dataset, testing using fold 2
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss -0.24475 acc 0.33333 roc_auc 0.41300 prc_auc 0.60643[0m
[93maverage test of epoch 0: loss -0.28349 acc 0.34211 roc_auc 0.55231 prc_auc 0.78687[0m
[92maverage training of epoch 1: loss -0.28284 acc 0.33333 roc_auc 0.37240 prc_auc 0.58699[0m
[93maverage test of epoch 1: loss -0.30083 acc 0.34211 roc_auc 0.54000 prc_auc 0.78012[0m
[92maverage training of epoch 2: loss -0.30069 acc 0.33333 roc_auc 0.36560 prc_auc 0.58017[0m
[93maverage test of epoch 2: loss -0.31740 acc 0.34211 roc_auc 0.53231 prc_auc 0.77764[0m
[92maverage training of epoch 3: loss -0.31802 acc 0.33333 roc_auc 0.36600 prc_auc 0.57981[0m
[93maverage test of epoch 3: loss -0.33368 acc 0.34211 roc_auc 0.52308 prc_auc 0.77712[0m
[92maverage training of epoch 4: loss -0.33508 acc 0.33333 roc_auc 0.36580 prc_auc 0.57799[0m
[93maverage test of epoch 4: loss -0.34963 acc 0.34211 roc_auc 0.53385 prc_auc 0.78882[0m
[92maverage training of epoch 5: loss -0.35184 acc 0.33333 roc_auc 0.36640 prc_auc 0.57852[0m
[93maverage test of epoch 5: loss -0.36526 acc 0.34211 roc_auc 0.53385 prc_auc 0.78882[0m
[92maverage training of epoch 6: loss -0.36833 acc 0.33333 roc_auc 0.36500 prc_auc 0.57781[0m
[93maverage test of epoch 6: loss -0.38059 acc 0.34211 roc_auc 0.53692 prc_auc 0.79789[0m
[92maverage training of epoch 7: loss -0.38431 acc 0.33333 roc_auc 0.36540 prc_auc 0.57794[0m
[93maverage test of epoch 7: loss -0.39579 acc 0.34211 roc_auc 0.56462 prc_auc 0.81008[0m
[92maverage training of epoch 8: loss -0.40002 acc 0.33333 roc_auc 0.36740 prc_auc 0.57725[0m
[93maverage test of epoch 8: loss -0.41074 acc 0.34211 roc_auc 0.59231 prc_auc 0.82029[0m
[92maverage training of epoch 9: loss -0.41547 acc 0.33333 roc_auc 0.37160 prc_auc 0.57937[0m
[93maverage test of epoch 9: loss -0.42557 acc 0.34211 roc_auc 0.65385 prc_auc 0.85453[0m
[92maverage training of epoch 10: loss -0.43082 acc 0.33333 roc_auc 0.37720 prc_auc 0.58212[0m
[93maverage test of epoch 10: loss -0.44033 acc 0.34211 roc_auc 0.69231 prc_auc 0.87184[0m
[92maverage training of epoch 11: loss -0.44552 acc 0.33333 roc_auc 0.37350 prc_auc 0.57381[0m
[93maverage test of epoch 11: loss -0.45239 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 12: loss -0.45695 acc 0.33333 roc_auc 0.37180 prc_auc 0.57359[0m
[93maverage test of epoch 12: loss -0.46341 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 13: loss -0.46806 acc 0.33333 roc_auc 0.37340 prc_auc 0.57373[0m
[93maverage test of epoch 13: loss -0.47442 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 14: loss -0.47915 acc 0.33333 roc_auc 0.37550 prc_auc 0.57429[0m
[93maverage test of epoch 14: loss -0.48544 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 15: loss -0.49023 acc 0.33333 roc_auc 0.37500 prc_auc 0.57406[0m
[93maverage test of epoch 15: loss -0.49645 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 16: loss -0.50128 acc 0.33333 roc_auc 0.37550 prc_auc 0.57451[0m
[93maverage test of epoch 16: loss -0.50747 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 17: loss -0.51236 acc 0.33333 roc_auc 0.37540 prc_auc 0.57426[0m
[93maverage test of epoch 17: loss -0.51848 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 18: loss -0.52344 acc 0.33333 roc_auc 0.37540 prc_auc 0.57426[0m
[93maverage test of epoch 18: loss -0.52950 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 19: loss -0.53451 acc 0.33333 roc_auc 0.37670 prc_auc 0.57458[0m
[93maverage test of epoch 19: loss -0.54052 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 20: loss -0.54556 acc 0.33333 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 20: loss -0.55153 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 21: loss -0.55661 acc 0.33333 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 21: loss -0.56255 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 22: loss -0.56766 acc 0.33333 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 22: loss -0.57357 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 23: loss -0.57871 acc 0.33333 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 23: loss -0.58458 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 24: loss -0.58975 acc 0.33333 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 24: loss -0.59560 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 25: loss -0.60080 acc 0.33333 roc_auc 0.37640 prc_auc 0.57439[0m
[93maverage test of epoch 25: loss -0.60661 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 26: loss -0.61185 acc 0.33333 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 26: loss -0.61763 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 27: loss -0.62290 acc 0.33333 roc_auc 0.37640 prc_auc 0.57448[0m
[93maverage test of epoch 27: loss -0.62865 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 28: loss -0.63394 acc 0.33333 roc_auc 0.37640 prc_auc 0.57448[0m
[93maverage test of epoch 28: loss -0.63966 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 29: loss -0.64499 acc 0.33333 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 29: loss -0.65068 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 30: loss -0.65604 acc 0.33333 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 30: loss -0.66170 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 31: loss -0.66709 acc 0.33333 roc_auc 0.37640 prc_auc 0.57448[0m
[93maverage test of epoch 31: loss -0.67271 acc 0.34211 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 32: loss -0.67814 acc 0.58000 roc_auc 0.37640 prc_auc 0.57445[0m
[93maverage test of epoch 32: loss -0.68373 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 33: loss -0.68918 acc 0.66667 roc_auc 0.37660 prc_auc 0.57445[0m
[93maverage test of epoch 33: loss -0.69474 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 34: loss -0.70023 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 34: loss -0.70576 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 35: loss -0.71128 acc 0.66667 roc_auc 0.37650 prc_auc 0.57445[0m
[93maverage test of epoch 35: loss -0.71678 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 36: loss -0.72233 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 36: loss -0.72779 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 37: loss -0.73337 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 37: loss -0.73881 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 38: loss -0.74442 acc 0.66667 roc_auc 0.37650 prc_auc 0.57448[0m
[93maverage test of epoch 38: loss -0.74983 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 39: loss -0.75547 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 39: loss -0.76084 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 40: loss -0.76652 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 40: loss -0.77186 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 41: loss -0.77756 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 41: loss -0.78287 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -0.78861 acc 0.66667 roc_auc 0.37640 prc_auc 0.57439[0m
[93maverage test of epoch 42: loss -0.79389 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -0.79966 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 43: loss -0.80491 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 44: loss -0.81071 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 44: loss -0.81592 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 45: loss -0.82175 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 45: loss -0.82694 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -0.83280 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 46: loss -0.83795 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 47: loss -0.84385 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 47: loss -0.84897 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -0.85490 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 48: loss -0.85999 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -0.86589 acc 0.66667 roc_auc 0.36890 prc_auc 0.57174[0m
[93maverage test of epoch 49: loss -0.87100 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 50: loss -0.87699 acc 0.66667 roc_auc 0.37640 prc_auc 0.57439[0m
[93maverage test of epoch 50: loss -0.88202 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 51: loss -0.88804 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 51: loss -0.89303 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 52: loss -0.89909 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 52: loss -0.90405 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 53: loss -0.91013 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 53: loss -0.91507 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 54: loss -0.92118 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 54: loss -0.92608 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 55: loss -0.93223 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 55: loss -0.93710 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 56: loss -0.94328 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 56: loss -0.94811 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 57: loss -0.95432 acc 0.66667 roc_auc 0.37640 prc_auc 0.57442[0m
[93maverage test of epoch 57: loss -0.95913 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 58: loss -0.96537 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 58: loss -0.97015 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 59: loss -0.97642 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 59: loss -0.98116 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 60: loss -0.98747 acc 0.66667 roc_auc 0.37650 prc_auc 0.57448[0m
[93maverage test of epoch 60: loss -0.99218 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 61: loss -0.99851 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 61: loss -1.00319 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 62: loss -1.00956 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 62: loss -1.01421 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 63: loss -1.02061 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 63: loss -1.02523 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 64: loss -1.03166 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 64: loss -1.03624 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 65: loss -1.04270 acc 0.66667 roc_auc 0.37650 prc_auc 0.57448[0m
[93maverage test of epoch 65: loss -1.04726 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 66: loss -1.05375 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 66: loss -1.05827 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 67: loss -1.06480 acc 0.66667 roc_auc 0.37640 prc_auc 0.57439[0m
[93maverage test of epoch 67: loss -1.06929 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 68: loss -1.07585 acc 0.66667 roc_auc 0.37640 prc_auc 0.57448[0m
[93maverage test of epoch 68: loss -1.08031 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 69: loss -1.08689 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 69: loss -1.09132 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 70: loss -1.09794 acc 0.66667 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 70: loss -1.10234 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 71: loss -1.10899 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 71: loss -1.11335 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 72: loss -1.12004 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 72: loss -1.12437 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 73: loss -1.13109 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 73: loss -1.13539 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 74: loss -1.14213 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 74: loss -1.14640 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 75: loss -1.15318 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 75: loss -1.15742 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 76: loss -1.16423 acc 0.66667 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 76: loss -1.16844 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 77: loss -1.17528 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 77: loss -1.17945 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 78: loss -1.18632 acc 0.66667 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 78: loss -1.19047 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 79: loss -1.19737 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 79: loss -1.20148 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 80: loss -1.20842 acc 0.66667 roc_auc 0.37460 prc_auc 0.57388[0m
[93maverage test of epoch 80: loss -1.21250 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 81: loss -1.21947 acc 0.66667 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 81: loss -1.22352 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 82: loss -1.23052 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 82: loss -1.23453 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 83: loss -1.24156 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 83: loss -1.24555 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 84: loss -1.25261 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 84: loss -1.25657 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 85: loss -1.26366 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 85: loss -1.26758 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 86: loss -1.27471 acc 0.66667 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 86: loss -1.27860 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 87: loss -1.28576 acc 0.66667 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 87: loss -1.28962 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 88: loss -1.29680 acc 0.66667 roc_auc 0.37670 prc_auc 0.57460[0m
[93maverage test of epoch 88: loss -1.30063 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 89: loss -1.30785 acc 0.66667 roc_auc 0.37670 prc_auc 0.57439[0m
[93maverage test of epoch 89: loss -1.31165 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 90: loss -1.31890 acc 0.66667 roc_auc 0.37670 prc_auc 0.57442[0m
[93maverage test of epoch 90: loss -1.32266 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 91: loss -1.32995 acc 0.66667 roc_auc 0.37680 prc_auc 0.57460[0m
[93maverage test of epoch 91: loss -1.33368 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 92: loss -1.34099 acc 0.66667 roc_auc 0.37680 prc_auc 0.57458[0m
[93maverage test of epoch 92: loss -1.34470 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 93: loss -1.35204 acc 0.66667 roc_auc 0.37670 prc_auc 0.57458[0m
[93maverage test of epoch 93: loss -1.35571 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 94: loss -1.36309 acc 0.66667 roc_auc 0.37670 prc_auc 0.57458[0m
[93maverage test of epoch 94: loss -1.36673 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 95: loss -1.37414 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 95: loss -1.37774 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 96: loss -1.38518 acc 0.66667 roc_auc 0.37670 prc_auc 0.57439[0m
[93maverage test of epoch 96: loss -1.38876 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 97: loss -1.39623 acc 0.66667 roc_auc 0.37670 prc_auc 0.57460[0m
[93maverage test of epoch 97: loss -1.39978 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 98: loss -1.40728 acc 0.66667 roc_auc 0.37670 prc_auc 0.57439[0m
[93maverage test of epoch 98: loss -1.41079 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 99: loss -1.41833 acc 0.66667 roc_auc 0.37680 prc_auc 0.57458[0m
[93maverage test of epoch 99: loss -1.42181 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 3
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.74547 acc 0.33775 roc_auc 0.46529 prc_auc 0.62572[0m
[93maverage test of epoch 0: loss 0.61892 acc 0.32432 roc_auc 0.32833 prc_auc 0.67325[0m
[92maverage training of epoch 1: loss 0.47883 acc 0.33775 roc_auc 0.49392 prc_auc 0.64874[0m
[93maverage test of epoch 1: loss 0.33289 acc 0.32432 roc_auc 0.15500 prc_auc 0.51313[0m
[92maverage training of epoch 2: loss 0.15326 acc 0.33775 roc_auc 0.49353 prc_auc 0.65700[0m
[93maverage test of epoch 2: loss -0.03397 acc 0.32432 roc_auc 0.15000 prc_auc 0.51156[0m
[92maverage training of epoch 3: loss -0.16248 acc 0.33775 roc_auc 0.37431 prc_auc 0.62332[0m
[93maverage test of epoch 3: loss -0.18835 acc 0.32432 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 4: loss -0.19674 acc 0.34437 roc_auc 0.41549 prc_auc 0.62725[0m
[93maverage test of epoch 4: loss -0.20035 acc 0.32432 roc_auc 0.52000 prc_auc 0.68865[0m
[92maverage training of epoch 5: loss -0.20857 acc 0.35099 roc_auc 0.42735 prc_auc 0.63176[0m
[93maverage test of epoch 5: loss -0.21244 acc 0.35135 roc_auc 0.54000 prc_auc 0.70162[0m
[92maverage training of epoch 6: loss -0.22033 acc 0.35099 roc_auc 0.46010 prc_auc 0.64872[0m
[93maverage test of epoch 6: loss -0.22476 acc 0.35135 roc_auc 0.54000 prc_auc 0.70162[0m
[92maverage training of epoch 7: loss -0.23236 acc 0.35099 roc_auc 0.42108 prc_auc 0.63651[0m
[93maverage test of epoch 7: loss -0.23733 acc 0.35135 roc_auc 0.54000 prc_auc 0.70162[0m
[92maverage training of epoch 8: loss -0.24478 acc 0.35762 roc_auc 0.44039 prc_auc 0.66462[0m
[93maverage test of epoch 8: loss -0.25055 acc 0.35135 roc_auc 0.54000 prc_auc 0.70162[0m
[92maverage training of epoch 9: loss -0.26262 acc 0.36424 roc_auc 0.67980 prc_auc 0.82174[0m
[93maverage test of epoch 9: loss -0.27943 acc 0.37838 roc_auc 0.87000 prc_auc 0.92071[0m
[92maverage training of epoch 10: loss -0.32664 acc 0.56954 roc_auc 0.64196 prc_auc 0.82665[0m
[93maverage test of epoch 10: loss -0.35771 acc 0.67568 roc_auc 0.85000 prc_auc 0.91538[0m
[92maverage training of epoch 11: loss -0.41457 acc 0.66225 roc_auc 0.72706 prc_auc 0.86907[0m
[93maverage test of epoch 11: loss -0.44771 acc 0.67568 roc_auc 0.85667 prc_auc 0.91693[0m
[92maverage training of epoch 12: loss -0.50379 acc 0.66225 roc_auc 0.78059 prc_auc 0.89112[0m
[93maverage test of epoch 12: loss -0.54325 acc 0.67568 roc_auc 0.85667 prc_auc 0.91693[0m
[92maverage training of epoch 13: loss -0.60036 acc 0.66225 roc_auc 0.81196 prc_auc 0.89870[0m
[93maverage test of epoch 13: loss -0.65247 acc 0.67568 roc_auc 0.85667 prc_auc 0.91693[0m
[92maverage training of epoch 14: loss -0.71933 acc 0.66225 roc_auc 0.82118 prc_auc 0.89691[0m
[93maverage test of epoch 14: loss -0.79005 acc 0.67568 roc_auc 0.86000 prc_auc 0.92137[0m
[92maverage training of epoch 15: loss -0.86862 acc 0.66225 roc_auc 0.83843 prc_auc 0.89859[0m
[93maverage test of epoch 15: loss -0.95239 acc 0.67568 roc_auc 0.86000 prc_auc 0.92137[0m
[92maverage training of epoch 16: loss -1.02185 acc 0.66225 roc_auc 0.86784 prc_auc 0.90774[0m
[93maverage test of epoch 16: loss -1.10010 acc 0.67568 roc_auc 0.85667 prc_auc 0.92034[0m
[92maverage training of epoch 17: loss -1.14881 acc 0.66225 roc_auc 0.87902 prc_auc 0.91453[0m
[93maverage test of epoch 17: loss -1.21921 acc 0.67568 roc_auc 0.85667 prc_auc 0.92034[0m
[92maverage training of epoch 18: loss -1.25336 acc 0.66225 roc_auc 0.87784 prc_auc 0.91198[0m
[93maverage test of epoch 18: loss -1.32101 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 19: loss -1.34660 acc 0.66225 roc_auc 0.87686 prc_auc 0.91024[0m
[93maverage test of epoch 19: loss -1.41424 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 20: loss -1.43448 acc 0.66225 roc_auc 0.87431 prc_auc 0.90657[0m
[93maverage test of epoch 20: loss -1.50323 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 21: loss -1.51988 acc 0.66225 roc_auc 0.87431 prc_auc 0.90560[0m
[93maverage test of epoch 21: loss -1.59003 acc 0.67568 roc_auc 0.85667 prc_auc 0.92337[0m
[92maverage training of epoch 22: loss -1.60413 acc 0.66225 roc_auc 0.87255 prc_auc 0.90364[0m
[93maverage test of epoch 22: loss -1.67594 acc 0.67568 roc_auc 0.85667 prc_auc 0.92337[0m
[92maverage training of epoch 23: loss -1.68809 acc 0.66225 roc_auc 0.87157 prc_auc 0.90319[0m
[93maverage test of epoch 23: loss -1.76161 acc 0.67568 roc_auc 0.85667 prc_auc 0.92337[0m
[92maverage training of epoch 24: loss -1.77221 acc 0.66225 roc_auc 0.87078 prc_auc 0.90269[0m
[93maverage test of epoch 24: loss -1.84751 acc 0.67568 roc_auc 0.86000 prc_auc 0.92585[0m
[92maverage training of epoch 25: loss -1.85688 acc 0.66225 roc_auc 0.87059 prc_auc 0.90303[0m
[93maverage test of epoch 25: loss -1.93389 acc 0.67568 roc_auc 0.86000 prc_auc 0.92585[0m
[92maverage training of epoch 26: loss -1.94227 acc 0.66225 roc_auc 0.86922 prc_auc 0.90248[0m
[93maverage test of epoch 26: loss -2.02098 acc 0.67568 roc_auc 0.86000 prc_auc 0.92585[0m
[92maverage training of epoch 27: loss -2.02857 acc 0.66225 roc_auc 0.86941 prc_auc 0.90378[0m
[93maverage test of epoch 27: loss -2.10892 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 28: loss -2.11589 acc 0.66225 roc_auc 0.87039 prc_auc 0.90542[0m
[93maverage test of epoch 28: loss -2.19791 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 29: loss -2.20433 acc 0.66225 roc_auc 0.87118 prc_auc 0.90745[0m
[93maverage test of epoch 29: loss -2.28792 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 30: loss -2.29390 acc 0.66225 roc_auc 0.87235 prc_auc 0.90839[0m
[93maverage test of epoch 30: loss -2.37901 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 31: loss -2.38467 acc 0.66225 roc_auc 0.87275 prc_auc 0.90884[0m
[93maverage test of epoch 31: loss -2.47120 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 32: loss -2.47681 acc 0.66225 roc_auc 0.87373 prc_auc 0.91032[0m
[93maverage test of epoch 32: loss -2.56444 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 33: loss -2.57034 acc 0.66225 roc_auc 0.87451 prc_auc 0.91185[0m
[93maverage test of epoch 33: loss -2.65882 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 34: loss -2.66555 acc 0.66225 roc_auc 0.87627 prc_auc 0.91318[0m
[93maverage test of epoch 34: loss -2.75443 acc 0.67568 roc_auc 0.85667 prc_auc 0.92185[0m
[92maverage training of epoch 35: loss -2.76256 acc 0.66225 roc_auc 0.87569 prc_auc 0.91405[0m
[93maverage test of epoch 35: loss -2.85148 acc 0.67568 roc_auc 0.86000 prc_auc 0.92282[0m
[92maverage training of epoch 36: loss -2.86154 acc 0.66225 roc_auc 0.87745 prc_auc 0.91784[0m
[93maverage test of epoch 36: loss -2.95027 acc 0.67568 roc_auc 0.86000 prc_auc 0.92282[0m
[92maverage training of epoch 37: loss -2.96253 acc 0.66225 roc_auc 0.87902 prc_auc 0.92013[0mUsing backend: pytorch
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

[93maverage test of epoch 37: loss -3.05100 acc 0.67568 roc_auc 0.86000 prc_auc 0.92282[0m
[92maverage training of epoch 38: loss -3.06541 acc 0.66225 roc_auc 0.88157 prc_auc 0.92156[0m
[93maverage test of epoch 38: loss -3.15380 acc 0.67568 roc_auc 0.86000 prc_auc 0.92282[0m
[92maverage training of epoch 39: loss -3.17008 acc 0.66225 roc_auc 0.88216 prc_auc 0.92100[0m
[93maverage test of epoch 39: loss -3.25867 acc 0.67568 roc_auc 0.85667 prc_auc 0.92034[0m
[92maverage training of epoch 40: loss -3.27645 acc 0.66225 roc_auc 0.88157 prc_auc 0.91862[0m
[93maverage test of epoch 40: loss -3.36546 acc 0.67568 roc_auc 0.85667 prc_auc 0.92034[0m
[92maverage training of epoch 41: loss -3.38440 acc 0.66225 roc_auc 0.88176 prc_auc 0.91647[0m
[93maverage test of epoch 41: loss -3.47404 acc 0.67568 roc_auc 0.85667 prc_auc 0.92034[0m
[92maverage training of epoch 42: loss -3.49389 acc 0.66225 roc_auc 0.88118 prc_auc 0.91422[0m
[93maverage test of epoch 42: loss -3.58437 acc 0.67568 roc_auc 0.85667 prc_auc 0.92034[0m
[92maverage training of epoch 43: loss -3.60496 acc 0.66225 roc_auc 0.88078 prc_auc 0.91318[0m
[93maverage test of epoch 43: loss -3.69644 acc 0.67568 roc_auc 0.85667 prc_auc 0.92034[0m
[92maverage training of epoch 44: loss -3.71773 acc 0.66225 roc_auc 0.88010 prc_auc 0.90998[0m
[93maverage test of epoch 44: loss -3.81028 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 45: loss -3.83228 acc 0.66225 roc_auc 0.88000 prc_auc 0.91004[0m
[93maverage test of epoch 45: loss -3.92598 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 46: loss -3.94874 acc 0.66225 roc_auc 0.87980 prc_auc 0.90790[0m
[93maverage test of epoch 46: loss -4.04366 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 47: loss -4.06721 acc 0.66225 roc_auc 0.87922 prc_auc 0.90713[0m
[93maverage test of epoch 47: loss -4.16349 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 48: loss -4.18787 acc 0.66225 roc_auc 0.87961 prc_auc 0.90764[0m
[93maverage test of epoch 48: loss -4.28564 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 49: loss -4.31099 acc 0.66225 roc_auc 0.87863 prc_auc 0.90696[0m
[93maverage test of epoch 49: loss -4.41039 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 50: loss -4.43686 acc 0.66225 roc_auc 0.87804 prc_auc 0.90684[0m
[93maverage test of epoch 50: loss -4.53791 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 51: loss -4.56566 acc 0.66225 roc_auc 0.87784 prc_auc 0.90635[0m
[93maverage test of epoch 51: loss -4.66847 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 52: loss -4.69761 acc 0.66225 roc_auc 0.87725 prc_auc 0.90612[0m
[93maverage test of epoch 52: loss -4.80243 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 53: loss -4.83278 acc 0.66225 roc_auc 0.87686 prc_auc 0.90578[0m
[93maverage test of epoch 53: loss -4.94004 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 54: loss -4.97097 acc 0.66225 roc_auc 0.87647 prc_auc 0.90508[0m
[93maverage test of epoch 54: loss -5.08101 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 55: loss -5.11163 acc 0.66225 roc_auc 0.87588 prc_auc 0.90431[0m
[93maverage test of epoch 55: loss -5.22455 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 56: loss -5.25417 acc 0.66225 roc_auc 0.87627 prc_auc 0.90391[0m
[93maverage test of epoch 56: loss -5.36992 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 57: loss -5.39817 acc 0.66225 roc_auc 0.87549 prc_auc 0.90332[0m
[93maverage test of epoch 57: loss -5.51672 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 58: loss -5.54363 acc 0.66225 roc_auc 0.87490 prc_auc 0.90223[0m
[93maverage test of epoch 58: loss -5.66497 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 59: loss -5.69077 acc 0.66225 roc_auc 0.87431 prc_auc 0.90117[0m
[93maverage test of epoch 59: loss -5.81493 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 60: loss -5.83985 acc 0.66225 roc_auc 0.87402 prc_auc 0.89985[0m
[93maverage test of epoch 60: loss -5.96684 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 61: loss -5.99108 acc 0.66225 roc_auc 0.87353 prc_auc 0.89751[0m
[93maverage test of epoch 61: loss -6.12090 acc 0.67568 roc_auc 0.85333 prc_auc 0.91974[0m
[92maverage training of epoch 62: loss -6.14460 acc 0.66225 roc_auc 0.87265 prc_auc 0.89610[0m
[93maverage test of epoch 62: loss -6.27725 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 63: loss -6.30054 acc 0.66225 roc_auc 0.87216 prc_auc 0.89531[0m
[93maverage test of epoch 63: loss -6.43603 acc 0.67568 roc_auc 0.85333 prc_auc 0.91974[0m
[92maverage training of epoch 64: loss -6.45902 acc 0.66225 roc_auc 0.87186 prc_auc 0.89415[0m
[93maverage test of epoch 64: loss -6.59733 acc 0.67568 roc_auc 0.85333 prc_auc 0.91974[0m
[92maverage training of epoch 65: loss -6.62011 acc 0.66225 roc_auc 0.87108 prc_auc 0.89179[0m
[93maverage test of epoch 65: loss -6.76124 acc 0.67568 roc_auc 0.85333 prc_auc 0.91974[0m
[92maverage training of epoch 66: loss -6.78389 acc 0.66225 roc_auc 0.87059 prc_auc 0.89123[0m
[93maverage test of epoch 66: loss -6.92783 acc 0.67568 roc_auc 0.85333 prc_auc 0.91974[0m
[92maverage training of epoch 67: loss -6.95039 acc 0.66225 roc_auc 0.87059 prc_auc 0.89126[0m
[93maverage test of epoch 67: loss -7.09713 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 68: loss -7.11968 acc 0.66225 roc_auc 0.87039 prc_auc 0.89011[0m
[93maverage test of epoch 68: loss -7.26923 acc 0.67568 roc_auc 0.85333 prc_auc 0.91974[0m
[92maverage training of epoch 69: loss -7.29183 acc 0.66225 roc_auc 0.87000 prc_auc 0.88949[0m
[93maverage test of epoch 69: loss -7.44420 acc 0.67568 roc_auc 0.85333 prc_auc 0.91974[0m
[92maverage training of epoch 70: loss -7.46688 acc 0.66225 roc_auc 0.86990 prc_auc 0.88930[0m
[93maverage test of epoch 70: loss -7.62207 acc 0.67568 roc_auc 0.85333 prc_auc 0.91999[0m
[92maverage training of epoch 71: loss -7.64488 acc 0.66225 roc_auc 0.87029 prc_auc 0.89041[0m
[93maverage test of epoch 71: loss -7.80291 acc 0.67568 roc_auc 0.85333 prc_auc 0.91937[0m
[92maverage training of epoch 72: loss -7.82587 acc 0.66225 roc_auc 0.87010 prc_auc 0.88952[0m
[93maverage test of epoch 72: loss -7.98674 acc 0.67568 roc_auc 0.85333 prc_auc 0.91999[0m
[92maverage training of epoch 73: loss -8.00989 acc 0.66225 roc_auc 0.87010 prc_auc 0.88919[0m
[93maverage test of epoch 73: loss -8.17359 acc 0.67568 roc_auc 0.85333 prc_auc 0.91974[0m
[92maverage training of epoch 74: loss -8.19691 acc 0.66225 roc_auc 0.86980 prc_auc 0.88774[0m
[93maverage test of epoch 74: loss -8.36337 acc 0.67568 roc_auc 0.85333 prc_auc 0.91974[0m
[92maverage training of epoch 75: loss -8.38672 acc 0.66225 roc_auc 0.86971 prc_auc 0.88819[0m
[93maverage test of epoch 75: loss -8.55582 acc 0.67568 roc_auc 0.85333 prc_auc 0.91999[0m
[92maverage training of epoch 76: loss -8.57925 acc 0.66225 roc_auc 0.86931 prc_auc 0.88732[0m
[93maverage test of epoch 76: loss -8.75099 acc 0.67568 roc_auc 0.85333 prc_auc 0.91999[0m
[92maverage training of epoch 77: loss -8.77454 acc 0.66225 roc_auc 0.86931 prc_auc 0.88715[0m
[93maverage test of epoch 77: loss -8.94892 acc 0.67568 roc_auc 0.85167 prc_auc 0.91529[0m
[92maverage training of epoch 78: loss -8.97265 acc 0.66225 roc_auc 0.86912 prc_auc 0.88642[0m
[93maverage test of epoch 78: loss -9.14967 acc 0.67568 roc_auc 0.85167 prc_auc 0.91555[0m
[92maverage training of epoch 79: loss -9.17360 acc 0.66225 roc_auc 0.86931 prc_auc 0.88640[0m
[93maverage test of epoch 79: loss -9.35327 acc 0.67568 roc_auc 0.84167 prc_auc 0.88888[0m
[92maverage training of epoch 80: loss -9.37743 acc 0.66225 roc_auc 0.86882 prc_auc 0.88615[0m
[93maverage test of epoch 80: loss -9.55975 acc 0.67568 roc_auc 0.85500 prc_auc 0.91999[0m
[92maverage training of epoch 81: loss -9.58418 acc 0.66225 roc_auc 0.86941 prc_auc 0.88630[0m
[93maverage test of epoch 81: loss -9.76913 acc 0.67568 roc_auc 0.84333 prc_auc 0.88888[0m
[92maverage training of epoch 82: loss -9.79387 acc 0.66225 roc_auc 0.86931 prc_auc 0.88529[0m
[93maverage test of epoch 82: loss -9.98146 acc 0.67568 roc_auc 0.85167 prc_auc 0.91110[0m
[92maverage training of epoch 83: loss -10.00653 acc 0.66225 roc_auc 0.86892 prc_auc 0.88529[0m
[93maverage test of epoch 83: loss -10.19674 acc 0.67568 roc_auc 0.85667 prc_auc 0.91655[0m
[92maverage training of epoch 84: loss -10.22218 acc 0.66225 roc_auc 0.86892 prc_auc 0.88488[0m
[93maverage test of epoch 84: loss -10.41500 acc 0.67568 roc_auc 0.84500 prc_auc 0.89136[0m
[92maverage training of epoch 85: loss -10.44085 acc 0.66225 roc_auc 0.86873 prc_auc 0.88449[0m
[93maverage test of epoch 85: loss -10.63627 acc 0.67568 roc_auc 0.84667 prc_auc 0.89473[0m
[92maverage training of epoch 86: loss -10.66255 acc 0.66225 roc_auc 0.86882 prc_auc 0.88448[0m
[93maverage test of epoch 86: loss -10.86055 acc 0.67568 roc_auc 0.84333 prc_auc 0.88888[0m
[92maverage training of epoch 87: loss -10.88730 acc 0.66225 roc_auc 0.86892 prc_auc 0.88410[0m
[93maverage test of epoch 87: loss -11.08785 acc 0.67568 roc_auc 0.84667 prc_auc 0.89473[0m
[92maverage training of epoch 88: loss -11.11510 acc 0.66225 roc_auc 0.86853 prc_auc 0.88398[0m
[93maverage test of epoch 88: loss -11.31819 acc 0.67568 roc_auc 0.85000 prc_auc 0.89577[0m
[92maverage training of epoch 89: loss -11.34597 acc 0.66225 roc_auc 0.86863 prc_auc 0.88346[0m
[93maverage test of epoch 89: loss -11.55159 acc 0.67568 roc_auc 0.85167 prc_auc 0.89824[0m
[92maverage training of epoch 90: loss -11.57993 acc 0.66225 roc_auc 0.86882 prc_auc 0.88388[0m
[93maverage test of epoch 90: loss -11.78805 acc 0.67568 roc_auc 0.85000 prc_auc 0.89596[0m
[92maverage training of epoch 91: loss -11.81698 acc 0.66225 roc_auc 0.86863 prc_auc 0.88261[0m
[93maverage test of epoch 91: loss -12.02760 acc 0.67568 roc_auc 0.85000 prc_auc 0.89596[0m
[92maverage training of epoch 92: loss -12.05715 acc 0.66225 roc_auc 0.86892 prc_auc 0.88256[0m
[93maverage test of epoch 92: loss -12.27024 acc 0.67568 roc_auc 0.86500 prc_auc 0.92227[0m
[92maverage training of epoch 93: loss -12.30046 acc 0.66225 roc_auc 0.86814 prc_auc 0.88172[0m
[93maverage test of epoch 93: loss -12.51600 acc 0.67568 roc_auc 0.85167 prc_auc 0.89958[0m
[92maverage training of epoch 94: loss -12.54695 acc 0.66225 roc_auc 0.86873 prc_auc 0.88314[0m
[93maverage test of epoch 94: loss -12.76491 acc 0.67568 roc_auc 0.85000 prc_auc 0.89683[0m
[92maverage training of epoch 95: loss -12.79662 acc 0.66225 roc_auc 0.86863 prc_auc 0.88307[0m
[93maverage test of epoch 95: loss -13.01699 acc 0.67568 roc_auc 0.84833 prc_auc 0.89701[0m
[92maverage training of epoch 96: loss -13.04946 acc 0.66225 roc_auc 0.86922 prc_auc 0.88371[0m
[93maverage test of epoch 96: loss -13.27223 acc 0.67568 roc_auc 0.84667 prc_auc 0.89435[0m
[92maverage training of epoch 97: loss -13.30548 acc 0.66225 roc_auc 0.86873 prc_auc 0.88441[0m
[93maverage test of epoch 97: loss -13.53064 acc 0.67568 roc_auc 0.84667 prc_auc 0.89454[0m
[92maverage training of epoch 98: loss -13.56470 acc 0.66225 roc_auc 0.86824 prc_auc 0.88242[0m
[93maverage test of epoch 98: loss -13.79225 acc 0.67568 roc_auc 0.85000 prc_auc 0.89958[0m
[92maverage training of epoch 99: loss -13.82715 acc 0.66225 roc_auc 0.86824 prc_auc 0.88349[0m
[93maverage test of epoch 99: loss -14.05705 acc 0.67568 roc_auc 0.85000 prc_auc 0.89955[0m
Training model with dataset, testing using fold 4
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=16, bias=True)
  (rnn): LSTM(16, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=4, bias=True)
    (ReLU0): ReLU()
    (Linear1): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0001
[92maverage training of epoch 0: loss 0.62180 acc 0.33775 roc_auc 0.43569 prc_auc 0.64988[0m
[93maverage test of epoch 0: loss 0.54548 acc 0.32432 roc_auc 0.89333 prc_auc 0.95487[0m
[92maverage training of epoch 1: loss 0.44922 acc 0.33775 roc_auc 0.44196 prc_auc 0.65268[0m
[93maverage test of epoch 1: loss 0.39215 acc 0.32432 roc_auc 0.92333 prc_auc 0.96923[0m
[92maverage training of epoch 2: loss 0.29521 acc 0.33775 roc_auc 0.44549 prc_auc 0.65746[0m
[93maverage test of epoch 2: loss 0.23224 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 3: loss 0.13515 acc 0.33775 roc_auc 0.45020 prc_auc 0.66167[0m
[93maverage test of epoch 3: loss 0.06055 acc 0.32432 roc_auc 0.92667 prc_auc 0.97104[0m
[92maverage training of epoch 4: loss 0.01679 acc 0.33775 roc_auc 0.37706 prc_auc 0.57020[0m
[93maverage test of epoch 4: loss 0.01828 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 5: loss 0.00265 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 5: loss 0.00715 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 6: loss -0.00844 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 6: loss -0.00399 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 7: loss -0.01954 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 7: loss -0.01514 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 8: loss -0.03063 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 8: loss -0.02628 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 9: loss -0.04173 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 9: loss -0.03742 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 10: loss -0.05282 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 10: loss -0.04856 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 11: loss -0.06392 acc 0.33775 roc_auc 0.36980 prc_auc 0.56840[0m
[93maverage test of epoch 11: loss -0.05971 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 12: loss -0.07502 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 12: loss -0.07085 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 13: loss -0.08611 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 13: loss -0.08199 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 14: loss -0.09721 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 14: loss -0.09314 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 15: loss -0.10831 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 15: loss -0.10428 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 16: loss -0.11940 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 16: loss -0.11543 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 17: loss -0.13050 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 17: loss -0.12657 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 18: loss -0.14160 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 18: loss -0.13772 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 19: loss -0.15270 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 19: loss -0.14886 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 20: loss -0.16379 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 20: loss -0.16001 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 21: loss -0.17489 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 21: loss -0.17115 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 22: loss -0.18599 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 22: loss -0.18230 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 23: loss -0.19709 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 23: loss -0.19344 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 24: loss -0.20819 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 24: loss -0.20459 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 25: loss -0.21928 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 25: loss -0.21573 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 26: loss -0.23038 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 26: loss -0.22688 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 27: loss -0.24148 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 27: loss -0.23802 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 28: loss -0.25258 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 28: loss -0.24917 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 29: loss -0.26368 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 29: loss -0.26031 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 30: loss -0.27477 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 30: loss -0.27146 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 31: loss -0.28587 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 31: loss -0.28261 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 32: loss -0.29697 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 32: loss -0.29375 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 33: loss -0.30807 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 33: loss -0.30490 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 34: loss -0.31917 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 34: loss -0.31604 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 35: loss -0.33027 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 35: loss -0.32719 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 36: loss -0.34136 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 36: loss -0.33833 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 37: loss -0.35246 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 37: loss -0.34948 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 38: loss -0.36356 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 38: loss -0.36062 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 39: loss -0.37466 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 39: loss -0.37177 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 40: loss -0.38576 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 40: loss -0.38291 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 41: loss -0.39685 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 41: loss -0.39406 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 42: loss -0.40795 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 42: loss -0.40520 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 43: loss -0.41905 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 43: loss -0.41635 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 44: loss -0.43015 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 44: loss -0.42750 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 45: loss -0.44125 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 45: loss -0.43864 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 46: loss -0.45234 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 46: loss -0.44979 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 47: loss -0.46344 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 47: loss -0.46093 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 48: loss -0.47454 acc 0.33775 roc_auc 0.36971 prc_auc 0.56808[0m
[93maverage test of epoch 48: loss -0.47208 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 49: loss -0.48564 acc 0.33775 roc_auc 0.36971 prc_auc 0.56808[0m
[93maverage test of epoch 49: loss -0.48322 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 50: loss -0.49674 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 50: loss -0.49437 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 51: loss -0.50784 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 51: loss -0.50551 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 52: loss -0.51893 acc 0.33775 roc_auc 0.36971 prc_auc 0.56808[0m
[93maverage test of epoch 52: loss -0.51666 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 53: loss -0.53003 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 53: loss -0.52780 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 54: loss -0.54113 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 54: loss -0.53895 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 55: loss -0.55223 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 55: loss -0.55009 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 56: loss -0.56333 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 56: loss -0.56124 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 57: loss -0.57442 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 57: loss -0.57238 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 58: loss -0.58552 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 58: loss -0.58353 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 59: loss -0.59662 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 59: loss -0.59467 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 60: loss -0.60772 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 60: loss -0.60582 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 61: loss -0.61881 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 61: loss -0.61696 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 62: loss -0.62991 acc 0.33775 roc_auc 0.36971 prc_auc 0.56808[0m
[93maverage test of epoch 62: loss -0.62811 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 63: loss -0.64101 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 63: loss -0.63925 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 64: loss -0.65211 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 64: loss -0.65040 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 65: loss -0.66321 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 65: loss -0.66154 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 66: loss -0.67430 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 66: loss -0.67269 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 67: loss -0.68540 acc 0.33775 roc_auc 0.36971 prc_auc 0.56808[0m
[93maverage test of epoch 67: loss -0.68383 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 68: loss -0.69650 acc 0.33775 roc_auc 0.36971 prc_auc 0.56808[0m
[93maverage test of epoch 68: loss -0.69498 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 69: loss -0.70760 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 69: loss -0.70612 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 70: loss -0.71870 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 70: loss -0.71727 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 71: loss -0.72979 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 71: loss -0.72841 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 72: loss -0.74089 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 72: loss -0.73956 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 73: loss -0.75199 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 73: loss -0.75071 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 74: loss -0.76309 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 74: loss -0.76185 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 75: loss -0.77419 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 75: loss -0.77300 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 76: loss -0.78528 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 76: loss -0.78414 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 77: loss -0.79638 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 77: loss -0.79529 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 78: loss -0.80748 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 78: loss -0.80643 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 79: loss -0.81858 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 79: loss -0.81758 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 80: loss -0.82967 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 80: loss -0.82872 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 81: loss -0.84077 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 81: loss -0.83987 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 82: loss -0.85187 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 82: loss -0.85101 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 83: loss -0.86297 acc 0.33775 roc_auc 0.37000 prc_auc 0.56830[0m
[93maverage test of epoch 83: loss -0.86216 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 84: loss -0.87407 acc 0.33775 roc_auc 0.36971 prc_auc 0.56808[0m
[93maverage test of epoch 84: loss -0.87330 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 85: loss -0.88516 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 85: loss -0.88445 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 86: loss -0.89626 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 86: loss -0.89559 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 87: loss -0.90736 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 87: loss -0.90674 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 88: loss -0.91846 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 88: loss -0.91788 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 89: loss -0.92956 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 89: loss -0.92903 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 90: loss -0.94065 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 90: loss -0.94017 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 91: loss -0.95175 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 91: loss -0.95132 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 92: loss -0.96285 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 92: loss -0.96246 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 93: loss -0.97395 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 93: loss -0.97361 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 94: loss -0.98504 acc 0.33775 roc_auc 0.36971 prc_auc 0.56808[0m
[93maverage test of epoch 94: loss -0.98475 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 95: loss -0.99614 acc 0.33775 roc_auc 0.36971 prc_auc 0.56808[0m
[93maverage test of epoch 95: loss -0.99590 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 96: loss -1.00724 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 96: loss -1.00704 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 97: loss -1.01834 acc 0.33775 roc_auc 0.36971 prc_auc 0.56810[0m
[93maverage test of epoch 97: loss -1.01819 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 98: loss -1.02943 acc 0.33775 roc_auc 0.36980 prc_auc 0.56821[0m
[93maverage test of epoch 98: loss -1.02933 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 99: loss -1.04053 acc 0.33775 roc_auc 0.36980 prc_auc 0.56819[0m
[93maverage test of epoch 99: loss -1.04048 acc 0.32432 roc_auc 0.50000 prc_auc 0.67568[0m
Run statistics: 
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 100)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 16)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 1)
('lr', 0.0001)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))


Accuracy (avg): 0.59474 ROC_AUC (avg): 0.578 PRC_AUC (avg): 0.71526 

Average forward propagation time taken(ms): 2.801402527820076
Average backward propagation time taken(ms): 0.9372665657726397

