# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======






Params Turning Set
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 100)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 0)
('lr', 0.0003)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))






args.base_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/
args.graph_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/
args.min_dfscode_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/
args.min_dfscode_tensor_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/
*** 1 train_index:  [  0   2   3   6   9  10  11  12  13  14  15  16  17  19  20  21  22  23
  24  26  27  30  31  32  33  35  36  38  39  41  42  43  44  45  46  47
  48  49  50  52  53  54  56  57  59  60  61  62  63  64  65  66  67  69
  70  71  72  73  74  76  77  78  79  80  81  82  83  84  87  88  89  91
  92  94  95  96  97  98 100 101 102 103 104 106 107 108 109 110 112 113
 115 116 117 118 119 120 121 122 124 125 126 127 129 130 131 132 134 136
 138 139 140 141 142 144 145 146 147 148 149 151 152 153 154 155 156 157
 158 159 160 161 164 165 166 167 169 170 171 172 173 175 176 177 178 179
 180 182 184 185 186 187]
*** 2 test_index:  [  1   4   5   7   8  18  25  28  29  34  37  40  51  55  58  68  75  85
  86  90  93  99 105 111 114 123 128 133 135 137 143 150 162 163 168 174
 181 183]
*** 1 train_index:  [  0   1   2   4   5   6   7   8  10  11  12  13  14  15  16  17  18  19
  20  21  22  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39
  40  41  43  44  45  46  49  50  51  52  53  54  55  56  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  74  75  76  77  79  80  81  82
  83  84  85  86  88  90  91  92  93  94  96  97  98  99 101 102 103 104
 105 106 107 111 112 113 114 115 120 121 122 123 124 125 126 128 129 130
 131 133 134 135 136 137 138 139 140 143 144 145 147 148 150 153 154 155
 156 159 162 163 164 165 167 168 169 170 171 172 173 174 175 177 178 179
 180 181 182 183 186 187]
*** 2 test_index:  [  3   9  23  31  42  47  48  57  72  73  78  87  89  95 100 108 109 110
 116 117 118 119 127 132 141 142 146 149 151 152 157 158 160 161 166 176
 184 185]
*** 1 train_index:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19
  20  23  24  25  26  28  29  31  32  34  35  36  37  38  40  41  42  43
  44  47  48  51  53  54  55  56  57  58  59  60  64  66  68  70  71  72
  73  74  75  76  77  78  79  81  82  84  85  86  87  88  89  90  92  93
  95  96  97  98  99 100 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134
 135 137 139 140 141 142 143 144 146 148 149 150 151 152 153 156 157 158
 159 160 161 162 163 165 166 167 168 169 171 172 173 174 175 176 177 178
 179 181 183 184 185 186]
*** 2 test_index:  [  0  16  21  22  27  30  33  39  45  46  49  50  52  61  62  63  65  67
  69  80  83  91  94 101 102 103 122 136 138 145 147 154 155 164 170 180
 182 187]
*** 1 train_index:  [  0   1   3   4   5   6   7   8   9  13  14  16  18  19  21  22  23  25
  26  27  28  29  30  31  33  34  36  37  38  39  40  42  43  45  46  47
  48  49  50  51  52  53  55  56  57  58  60  61  62  63  65  66  67  68
  69  71  72  73  75  76  77  78  79  80  81  83  84  85  86  87  88  89
  90  91  92  93  94  95  97  98  99 100 101 102 103 105 106 108 109 110
 111 113 114 115 116 117 118 119 120 121 122 123 125 127 128 131 132 133
 134 135 136 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152
 154 155 156 157 158 160 161 162 163 164 165 166 168 170 171 174 175 176
 180 181 182 183 184 185 187]
*** 2 test_index:  [  2  10  11  12  15  17  20  24  32  35  41  44  54  59  64  70  74  82
  96 104 107 112 124 126 129 130 139 153 159 167 169 172 173 177 178 179
 186]
*** 1 train_index:  [  0   1   2   3   4   5   7   8   9  10  11  12  15  16  17  18  20  21
  22  23  24  25  27  28  29  30  31  32  33  34  35  37  39  40  41  42
  44  45  46  47  48  49  50  51  52  54  55  57  58  59  61  62  63  64
  65  67  68  69  70  72  73  74  75  78  80  82  83  85  86  87  89  90
  91  93  94  95  96  99 100 101 102 103 104 105 107 108 109 110 111 112
 114 116 117 118 119 122 123 124 126 127 128 129 130 132 133 135 136 137
 138 139 141 142 143 145 146 147 149 150 151 152 153 154 155 157 158 159
 160 161 162 163 164 166 167 168 169 170 172 173 174 176 177 178 179 180
 181 182 183 184 185 186 187]
*** 2 test_index:  [  6  13  14  19  26  36  38  43  53  56  60  66  71  76  77  79  81  84
  88  92  97  98 106 113 115 120 121 125 131 134 140 144 148 156 165 171
 175]
{'base_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/',
 'batch_size': 1,
 'clean_temp': False,
 'clean_tensorboard': False,
 'current_dataset_path': None,
 'current_min_dfscode_path': None,
 'current_model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-23-43-35/',
 'current_processed_dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'current_temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/DFScodeRNN_cls_LSTM_MUTAG_2021-01-17-23-43-35/',
 'dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/datasets/',
 'device': device(type='cuda', index=0),
 'dfscode_rnn_dropout': 0.2,
 'embedding_size_dfscode_rnn': 8,
 'epochs': 100,
 'epochs_end': 10000,
 'epochs_save': 20,
 'epochs_validate': 1,
 'fname': 'DFScodeRNN_cls_LSTM_MUTAG',
 'gradient_clipping': True,
 'graph_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/graphs/',
 'graph_type': 'MUTAG',
 'graphgen_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/',
 'hidden_size_dfscode_rnn': 4,
 'load_device': device(type='cuda', index=0),
 'load_model': False,
 'load_model_path': '',
 'log_tensorboard': False,
 'lr': 0.0003,
 'max_prev_node': None,
 'min_dfscode_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscodes/',
 'min_dfscode_tensor_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/min_dfscode_tensors/',
 'model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/model_save/',
 'note': 'DFScodeRNN_cls_LSTM',
 'num_graphs': None,
 'num_layers': 1,
 'num_workers': 8,
 'number_of_mlp_layer': 0,
 'rnn_type': 'LSTM',
 'save_model': False,
 'temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tmp/',
 'tensorboard_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/MUTAG/graphgen/tensorboard/',
 'time': '2021-01-17-23-43-35',
 'weights': False}


graphgen args.__dict__: None




dataset_features: {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}




config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 100, 'learning_rate': 0.0003, 'seed': 1800, 'k_fold': 5, 'model': 'DFScodeRNN_cls_LSTM', 'dataset': 'MUTAG'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4, 'label_size': 2}}


Training a new model: DFScodeRNN_cls_LSTM
Training model with dataset, testing using fold 0
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss -0.08108 acc 0.33333 roc_auc 0.43760 prc_auc 0.67590[0m
[93maverage test of epoch 0: loss -0.25412 acc 0.34211 roc_auc 0.85846 prc_auc 0.93026[0m
[92maverage training of epoch 1: loss -0.44363 acc 0.33333 roc_auc 0.49600 prc_auc 0.72621[0m
[93maverage test of epoch 1: loss -0.65060 acc 0.34211 roc_auc 0.81846 prc_auc 0.91518[0m
[92maverage training of epoch 2: loss -0.82425 acc 0.36000 roc_auc 0.53200 prc_auc 0.75518[0m
[93maverage test of epoch 2: loss -0.99915 acc 0.39474 roc_auc 0.83692 prc_auc 0.92684[0m
[92maverage training of epoch 3: loss -1.14703 acc 0.48000 roc_auc 0.55200 prc_auc 0.77541[0m
[93maverage test of epoch 3: loss -1.31171 acc 0.65789 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 4: loss -1.45696 acc 0.66667 roc_auc 0.60400 prc_auc 0.80818[0m
[93maverage test of epoch 4: loss -1.61483 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 5: loss -1.74829 acc 0.66667 roc_auc 0.70180 prc_auc 0.85680[0m
[93maverage test of epoch 5: loss -1.88522 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 6: loss -2.02260 acc 0.66667 roc_auc 0.73000 prc_auc 0.86186[0m
[93maverage test of epoch 6: loss -2.15116 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 7: loss -2.28815 acc 0.66667 roc_auc 0.73560 prc_auc 0.85546[0m
[93maverage test of epoch 7: loss -2.40958 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 8: loss -2.55655 acc 0.66667 roc_auc 0.71640 prc_auc 0.84105[0m
[93maverage test of epoch 8: loss -2.68283 acc 0.65789 roc_auc 0.86154 prc_auc 0.93308[0m
[92maverage training of epoch 9: loss -2.83782 acc 0.66667 roc_auc 0.68900 prc_auc 0.82316[0m
[93maverage test of epoch 9: loss -2.96306 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 10: loss -3.11578 acc 0.66667 roc_auc 0.66700 prc_auc 0.80782[0m
[93maverage test of epoch 10: loss -3.23168 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 11: loss -3.37718 acc 0.66667 roc_auc 0.64760 prc_auc 0.79390[0m
[93maverage test of epoch 11: loss -3.48212 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 12: loss -3.62042 acc 0.66667 roc_auc 0.62580 prc_auc 0.77700[0m
[93maverage test of epoch 12: loss -3.71609 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 13: loss -3.84851 acc 0.66667 roc_auc 0.59900 prc_auc 0.75751[0m
[93maverage test of epoch 13: loss -3.93693 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 14: loss -4.06480 acc 0.66667 roc_auc 0.57440 prc_auc 0.74046[0m
[93maverage test of epoch 14: loss -4.14758 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 15: loss -4.27197 acc 0.66667 roc_auc 0.54840 prc_auc 0.72401[0m
[93maverage test of epoch 15: loss -4.35033 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 16: loss -4.47206 acc 0.66667 roc_auc 0.52100 prc_auc 0.70442[0m
[93maverage test of epoch 16: loss -4.54686 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 17: loss -4.66655 acc 0.66667 roc_auc 0.49720 prc_auc 0.68623[0m
[93maverage test of epoch 17: loss -4.73844 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 18: loss -4.85655 acc 0.66667 roc_auc 0.47160 prc_auc 0.66832[0m
[93maverage test of epoch 18: loss -4.92600 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 19: loss -5.04291 acc 0.66667 roc_auc 0.45020 prc_auc 0.65487[0m
[93maverage test of epoch 19: loss -5.11029 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 20: loss -5.22627 acc 0.66667 roc_auc 0.43340 prc_auc 0.64433[0m
[93maverage test of epoch 20: loss -5.29187 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 21: loss -5.40715 acc 0.66667 roc_auc 0.42260 prc_auc 0.63607[0m
[93maverage test of epoch 21: loss -5.47118 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 22: loss -5.58594 acc 0.66667 roc_auc 0.41560 prc_auc 0.63196[0m
[93maverage test of epoch 22: loss -5.64858 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 23: loss -5.76298 acc 0.66667 roc_auc 0.40960 prc_auc 0.62672[0m
[93maverage test of epoch 23: loss -5.82437 acc 0.65789 roc_auc 0.86462 prc_auc 0.93401[0m
[92maverage training of epoch 24: loss -5.93852 acc 0.66667 roc_auc 0.40260 prc_auc 0.62102[0m
[93maverage test of epoch 24: loss -5.99879 acc 0.65789 roc_auc 0.86462 prc_auc 0.93440[0m
[92maverage training of epoch 25: loss -6.11279 acc 0.66667 roc_auc 0.39830 prc_auc 0.61851[0m
[93maverage test of epoch 25: loss -6.17202 acc 0.65789 roc_auc 0.86462 prc_auc 0.93440[0m
[92maverage training of epoch 26: loss -6.28597 acc 0.66667 roc_auc 0.39170 prc_auc 0.61346[0m
[93maverage test of epoch 26: loss -6.34425 acc 0.65789 roc_auc 0.86462 prc_auc 0.93472[0m
[92maverage training of epoch 27: loss -6.45821 acc 0.66667 roc_auc 0.38840 prc_auc 0.60769[0m
[93maverage test of epoch 27: loss -6.51560 acc 0.65789 roc_auc 0.86154 prc_auc 0.92805[0m
[92maverage training of epoch 28: loss -6.62964 acc 0.66667 roc_auc 0.38450 prc_auc 0.60378[0m
[93maverage test of epoch 28: loss -6.68619 acc 0.65789 roc_auc 0.86000 prc_auc 0.92022[0m
[92maverage training of epoch 29: loss -6.80036 acc 0.66667 roc_auc 0.37990 prc_auc 0.59827[0m
[93maverage test of epoch 29: loss -6.85613 acc 0.65789 roc_auc 0.85538 prc_auc 0.91380[0m
[92maverage training of epoch 30: loss -6.97047 acc 0.66667 roc_auc 0.37720 prc_auc 0.59552[0m
[93maverage test of epoch 30: loss -7.02550 acc 0.65789 roc_auc 0.85077 prc_auc 0.90343[0m
[92maverage training of epoch 31: loss -7.14005 acc 0.66667 roc_auc 0.37370 prc_auc 0.59324[0m
[93maverage test of epoch 31: loss -7.19437 acc 0.65789 roc_auc 0.86615 prc_auc 0.93259[0m
[92maverage training of epoch 32: loss -7.30916 acc 0.66667 roc_auc 0.37300 prc_auc 0.59341[0m
[93maverage test of epoch 32: loss -7.36281 acc 0.65789 roc_auc 0.84923 prc_auc 0.90247[0m
[92maverage training of epoch 33: loss -7.47787 acc 0.66667 roc_auc 0.37140 prc_auc 0.59205[0m
[93maverage test of epoch 33: loss -7.53087 acc 0.65789 roc_auc 0.86462 prc_auc 0.92295[0m
[92maverage training of epoch 34: loss -7.64622 acc 0.66667 roc_auc 0.37030 prc_auc 0.58929[0m
[93maverage test of epoch 34: loss -7.69860 acc 0.65789 roc_auc 0.85846 prc_auc 0.91081[0m
[92maverage training of epoch 35: loss -7.81426 acc 0.66667 roc_auc 0.36910 prc_auc 0.58809[0m
[93maverage test of epoch 35: loss -7.86603 acc 0.65789 roc_auc 0.86769 prc_auc 0.93318[0m
[92maverage training of epoch 36: loss -7.98203 acc 0.66667 roc_auc 0.36820 prc_auc 0.58690[0m
[93maverage test of epoch 36: loss -8.03321 acc 0.65789 roc_auc 0.84923 prc_auc 0.90509[0m
[92maverage training of epoch 37: loss -8.14957 acc 0.66667 roc_auc 0.36760 prc_auc 0.58605[0m
[93maverage test of epoch 37: loss -8.20017 acc 0.65789 roc_auc 0.85385 prc_auc 0.90633[0m
[92maverage training of epoch 38: loss -8.31689 acc 0.66667 roc_auc 0.36760 prc_auc 0.58605[0m
[93maverage test of epoch 38: loss -8.36692 acc 0.65789 roc_auc 0.85692 prc_auc 0.91494[0m
[92maverage training of epoch 39: loss -8.48402 acc 0.66667 roc_auc 0.36700 prc_auc 0.58515[0m
[93maverage test of epoch 39: loss -8.53351 acc 0.65789 roc_auc 0.84769 prc_auc 0.89843[0m
[92maverage training of epoch 40: loss -8.65099 acc 0.66667 roc_auc 0.36610 prc_auc 0.58399[0m
[93maverage test of epoch 40: loss -8.69994 acc 0.65789 roc_auc 0.86000 prc_auc 0.91322[0m
[92maverage training of epoch 41: loss -8.81782 acc 0.66667 roc_auc 0.36570 prc_auc 0.58351[0m
[93maverage test of epoch 41: loss -8.86623 acc 0.65789 roc_auc 0.85231 prc_auc 0.90738[0m
[92maverage training of epoch 42: loss -8.98453 acc 0.66667 roc_auc 0.36550 prc_auc 0.58398[0m
[93maverage test of epoch 42: loss -9.03241 acc 0.65789 roc_auc 0.84769 prc_auc 0.90261[0m
[92maverage training of epoch 43: loss -9.15112 acc 0.66667 roc_auc 0.36380 prc_auc 0.56868[0m
[93maverage test of epoch 43: loss -9.19848 acc 0.65789 roc_auc 0.83077 prc_auc 0.88460[0m
[92maverage training of epoch 44: loss -9.31761 acc 0.66667 roc_auc 0.36150 prc_auc 0.56538[0m
[93maverage test of epoch 44: loss -9.36446 acc 0.65789 roc_auc 0.85692 prc_auc 0.91315[0m
[92maverage training of epoch 45: loss -9.48402 acc 0.66667 roc_auc 0.36050 prc_auc 0.56456[0m
[93maverage test of epoch 45: loss -9.53035 acc 0.65789 roc_auc 0.79846 prc_auc 0.85265[0m
[92maverage training of epoch 46: loss -9.65034 acc 0.66667 roc_auc 0.35980 prc_auc 0.56383[0m
[93maverage test of epoch 46: loss -9.69618 acc 0.65789 roc_auc 0.80154 prc_auc 0.84655[0m
[92maverage training of epoch 47: loss -9.81660 acc 0.66667 roc_auc 0.35920 prc_auc 0.56326[0m
[93maverage test of epoch 47: loss -9.86193 acc 0.65789 roc_auc 0.86769 prc_auc 0.88460[0m
[92maverage training of epoch 48: loss -9.98280 acc 0.66667 roc_auc 0.35880 prc_auc 0.56334[0m
[93maverage test of epoch 48: loss -10.02764 acc 0.65789 roc_auc 0.65692 prc_auc 0.73811[0m
[92maverage training of epoch 49: loss -10.14895 acc 0.66667 roc_auc 0.35890 prc_auc 0.56381[0m
[93maverage test of epoch 49: loss -10.19329 acc 0.65789 roc_auc 0.81077 prc_auc 0.84311[0m
[92maverage training of epoch 50: loss -10.31505 acc 0.66667 roc_auc 0.35880 prc_auc 0.56384[0m
[93maverage test of epoch 50: loss -10.35890 acc 0.65789 roc_auc 0.65231 prc_auc 0.73521[0m
[92maverage training of epoch 51: loss -10.48110 acc 0.66667 roc_auc 0.35850 prc_auc 0.56293[0m
[93maverage test of epoch 51: loss -10.52446 acc 0.65789 roc_auc 0.65385 prc_auc 0.73529[0m
[92maverage training of epoch 52: loss -10.64712 acc 0.66667 roc_auc 0.35820 prc_auc 0.56271[0m
[93maverage test of epoch 52: loss -10.68999 acc 0.65789 roc_auc 0.61231 prc_auc 0.71354[0m
[92maverage training of epoch 53: loss -10.81311 acc 0.66667 roc_auc 0.35820 prc_auc 0.56271[0m
[93maverage test of epoch 53: loss -10.85549 acc 0.65789 roc_auc 0.80462 prc_auc 0.84026[0m
[92maverage training of epoch 54: loss -10.97906 acc 0.66667 roc_auc 0.35790 prc_auc 0.56245[0m
[93maverage test of epoch 54: loss -11.02096 acc 0.65789 roc_auc 0.65385 prc_auc 0.73529[0m
[92maverage training of epoch 55: loss -11.14499 acc 0.66667 roc_auc 0.35780 prc_auc 0.56251[0m
[93maverage test of epoch 55: loss -11.18641 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 56: loss -11.31090 acc 0.66667 roc_auc 0.35760 prc_auc 0.56247[0m
[93maverage test of epoch 56: loss -11.35183 acc 0.65789 roc_auc 0.65385 prc_auc 0.73529[0m
[92maverage training of epoch 57: loss -11.47678 acc 0.66667 roc_auc 0.35780 prc_auc 0.56257[0m
[93maverage test of epoch 57: loss -11.51724 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 58: loss -11.64265 acc 0.66667 roc_auc 0.35750 prc_auc 0.56226[0m
[93maverage test of epoch 58: loss -11.68262 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 59: loss -11.80850 acc 0.66667 roc_auc 0.35750 prc_auc 0.56224[0m
[93maverage test of epoch 59: loss -11.84800 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 60: loss -11.97434 acc 0.66667 roc_auc 0.35760 prc_auc 0.56215[0m
[93maverage test of epoch 60: loss -12.01335 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 61: loss -12.14016 acc 0.66667 roc_auc 0.35740 prc_auc 0.56200[0m
[93maverage test of epoch 61: loss -12.17870 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 62: loss -12.30597 acc 0.66667 roc_auc 0.35740 prc_auc 0.56205[0m
[93maverage test of epoch 62: loss -12.34403 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 63: loss -12.47177 acc 0.66667 roc_auc 0.35720 prc_auc 0.56190[0m
[93maverage test of epoch 63: loss -12.50935 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 64: loss -12.63756 acc 0.66667 roc_auc 0.35720 prc_auc 0.56184[0m
[93maverage test of epoch 64: loss -12.67467 acc 0.65789 roc_auc 0.67231 prc_auc 0.74632[0m
[92maverage training of epoch 65: loss -12.80334 acc 0.66667 roc_auc 0.35730 prc_auc 0.56162[0m
[93maverage test of epoch 65: loss -12.83997 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 66: loss -12.96911 acc 0.66667 roc_auc 0.35740 prc_auc 0.56199[0m
[93maverage test of epoch 66: loss -13.00527 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 67: loss -13.13488 acc 0.66667 roc_auc 0.35740 prc_auc 0.56190[0m
[93maverage test of epoch 67: loss -13.17056 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 68: loss -13.30064 acc 0.66667 roc_auc 0.35740 prc_auc 0.56170[0m
[93maverage test of epoch 68: loss -13.33585 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 69: loss -13.46640 acc 0.66667 roc_auc 0.35740 prc_auc 0.56180[0m
[93maverage test of epoch 69: loss -13.50113 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 70: loss -13.63215 acc 0.66667 roc_auc 0.35750 prc_auc 0.56168[0m
[93maverage test of epoch 70: loss -13.66641 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 71: loss -13.79789 acc 0.66667 roc_auc 0.35740 prc_auc 0.56177[0m
[93maverage test of epoch 71: loss -13.83168 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 72: loss -13.96364 acc 0.66667 roc_auc 0.35710 prc_auc 0.56152[0m
[93maverage test of epoch 72: loss -13.99695 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 73: loss -14.12938 acc 0.66667 roc_auc 0.35740 prc_auc 0.56174[0m
[93maverage test of epoch 73: loss -14.16221 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 74: loss -14.29511 acc 0.66667 roc_auc 0.35730 prc_auc 0.56189[0m
[93maverage test of epoch 74: loss -14.32747 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 75: loss -14.46084 acc 0.66667 roc_auc 0.35760 prc_auc 0.56188[0m
[93maverage test of epoch 75: loss -14.49273 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 76: loss -14.62657 acc 0.66667 roc_auc 0.35720 prc_auc 0.56175[0m
[93maverage test of epoch 76: loss -14.65798 acc 0.65789 roc_auc 0.74308 prc_auc 0.80573[0m
[92maverage training of epoch 77: loss -14.79230 acc 0.66667 roc_auc 0.35720 prc_auc 0.56183[0m
[93maverage test of epoch 77: loss -14.82324 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 78: loss -14.95802 acc 0.66667 roc_auc 0.35730 prc_auc 0.56212[0m
[93maverage test of epoch 78: loss -14.98849 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 79: loss -15.12375 acc 0.66667 roc_auc 0.35780 prc_auc 0.56232[0m
[93maverage test of epoch 79: loss -15.15374 acc 0.65789 roc_auc 0.61538 prc_auc 0.71429[0m
[92maverage training of epoch 80: loss -15.28947 acc 0.66667 roc_auc 0.35730 prc_auc 0.56179[0m
[93maverage test of epoch 80: loss -15.31898 acc 0.65789 roc_auc 0.76154 prc_auc 0.82566[0m
[92maverage training of epoch 81: loss -15.45518 acc 0.66667 roc_auc 0.35760 prc_auc 0.56269[0m
[93maverage test of epoch 81: loss -15.48423 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 82: loss -15.62090 acc 0.66667 roc_auc 0.35760 prc_auc 0.56194[0m
[93maverage test of epoch 82: loss -15.64947 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 83: loss -15.78661 acc 0.66667 roc_auc 0.35780 prc_auc 0.56235[0m
[93maverage test of epoch 83: loss -15.81471 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 84: loss -15.95233 acc 0.66667 roc_auc 0.35750 prc_auc 0.56181[0m
[93maverage test of epoch 84: loss -15.97995 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 85: loss -16.11804 acc 0.66667 roc_auc 0.35780 prc_auc 0.56253[0m
[93maverage test of epoch 85: loss -16.14519 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 86: loss -16.28375 acc 0.66667 roc_auc 0.35750 prc_auc 0.56249[0m
[93maverage test of epoch 86: loss -16.31043 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 87: loss -16.44946 acc 0.66667 roc_auc 0.35760 prc_auc 0.56260[0m
[93maverage test of epoch 87: loss -16.47567 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 88: loss -16.61517 acc 0.66667 roc_auc 0.35720 prc_auc 0.56272[0m
[93maverage test of epoch 88: loss -16.64090 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 89: loss -16.78088 acc 0.66667 roc_auc 0.35810 prc_auc 0.56278[0m
[93maverage test of epoch 89: loss -16.80614 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 90: loss -16.94659 acc 0.66667 roc_auc 0.35770 prc_auc 0.56230[0m
[93maverage test of epoch 90: loss -16.97138 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 91: loss -17.11230 acc 0.66667 roc_auc 0.35810 prc_auc 0.56251[0m
[93maverage test of epoch 91: loss -17.13661 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 92: loss -17.27801 acc 0.66667 roc_auc 0.35750 prc_auc 0.56328[0m
[93maverage test of epoch 92: loss -17.30185 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 93: loss -17.44371 acc 0.66667 roc_auc 0.35850 prc_auc 0.56269[0m
[93maverage test of epoch 93: loss -17.46708 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 94: loss -17.60942 acc 0.66667 roc_auc 0.35760 prc_auc 0.56301[0m
[93maverage test of epoch 94: loss -17.63232 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 95: loss -17.77512 acc 0.66667 roc_auc 0.35780 prc_auc 0.56277[0m
[93maverage test of epoch 95: loss -17.79755 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 96: loss -17.94083 acc 0.66667 roc_auc 0.35740 prc_auc 0.56226[0m
[93maverage test of epoch 96: loss -17.96278 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 97: loss -18.10654 acc 0.66667 roc_auc 0.35780 prc_auc 0.56257[0m
[93maverage test of epoch 97: loss -18.12801 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 98: loss -18.27224 acc 0.66667 roc_auc 0.35720 prc_auc 0.56248[0m
[93maverage test of epoch 98: loss -18.29325 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 99: loss -18.43794 acc 0.66667 roc_auc 0.35710 prc_auc 0.56259[0m
[93maverage test of epoch 99: loss -18.45848 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 1
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss 0.25330 acc 0.33333 roc_auc 0.57820 prc_auc 0.73535[0m
[93maverage test of epoch 0: loss 0.10208 acc 0.34211 roc_auc 0.72308 prc_auc 0.87159[0m
[92maverage training of epoch 1: loss -0.03479 acc 0.33333 roc_auc 0.54780 prc_auc 0.70513[0m
[93maverage test of epoch 1: loss -0.20896 acc 0.34211 roc_auc 0.24615 prc_auc 0.58395[0m
[92maverage training of epoch 2: loss -0.38432 acc 0.33333 roc_auc 0.47740 prc_auc 0.63825[0m
[93maverage test of epoch 2: loss -0.62911 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 3: loss -0.92459 acc 0.33333 roc_auc 0.44020 prc_auc 0.61732[0m
[93maverage test of epoch 3: loss -1.29161 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 4: loss -1.51519 acc 0.33333 roc_auc 0.35720 prc_auc 0.57404[0m
[93maverage test of epoch 4: loss -1.75346 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 5: loss -1.89108 acc 0.33333 roc_auc 0.18420 prc_auc 0.49693[0m
[93maverage test of epoch 5: loss -2.07375 acc 0.34211 roc_auc 0.12000 prc_auc 0.48608[0m
[92maverage training of epoch 6: loss -2.18484 acc 0.33333 roc_auc 0.16720 prc_auc 0.49416[0m
[93maverage test of epoch 6: loss -2.34768 acc 0.34211 roc_auc 0.11692 prc_auc 0.48512[0m
[92maverage training of epoch 7: loss -2.44368 acc 0.33333 roc_auc 0.26480 prc_auc 0.53204[0m
[93maverage test of epoch 7: loss -2.59462 acc 0.34211 roc_auc 0.11692 prc_auc 0.48512[0m
[92maverage training of epoch 8: loss -2.68000 acc 0.33333 roc_auc 0.32160 prc_auc 0.55384[0m
[93maverage test of epoch 8: loss -2.82208 acc 0.34211 roc_auc 0.11692 prc_auc 0.48512[0m
[92maverage training of epoch 9: loss -2.89933 acc 0.33333 roc_auc 0.35600 prc_auc 0.56652[0m
[93maverage test of epoch 9: loss -3.03407 acc 0.34211 roc_auc 0.11385 prc_auc 0.48477[0m
[92maverage training of epoch 10: loss -3.10490 acc 0.33333 roc_auc 0.37420 prc_auc 0.57403[0m
[93maverage test of epoch 10: loss -3.23325 acc 0.34211 roc_auc 0.12308 prc_auc 0.48679[0m
[92maverage training of epoch 11: loss -3.29895 acc 0.33333 roc_auc 0.38400 prc_auc 0.57783[0m
[93maverage test of epoch 11: loss -3.42164 acc 0.34211 roc_auc 0.12923 prc_auc 0.48859[0m
[92maverage training of epoch 12: loss -3.48328 acc 0.33333 roc_auc 0.39300 prc_auc 0.58090[0m
[93maverage test of epoch 12: loss -3.60100 acc 0.34211 roc_auc 0.13846 prc_auc 0.49031[0m
[92maverage training of epoch 13: loss -3.65950 acc 0.33333 roc_auc 0.39860 prc_auc 0.58270[0m
[93maverage test of epoch 13: loss -3.77289 acc 0.34211 roc_auc 0.14308 prc_auc 0.49170[0m
[92maverage training of epoch 14: loss -3.82904 acc 0.33333 roc_auc 0.40320 prc_auc 0.58452[0m
[93maverage test of epoch 14: loss -3.93874 acc 0.34211 roc_auc 0.16923 prc_auc 0.52970[0m
[92maverage training of epoch 15: loss -3.99327 acc 0.33333 roc_auc 0.41340 prc_auc 0.60262[0m
[93maverage test of epoch 15: loss -4.09997 acc 0.34211 roc_auc 0.21231 prc_auc 0.56552[0m
[92maverage training of epoch 16: loss -4.15358 acc 0.33333 roc_auc 0.42020 prc_auc 0.61275[0m
[93maverage test of epoch 16: loss -4.25809 acc 0.34211 roc_auc 0.26154 prc_auc 0.60112[0m
[92maverage training of epoch 17: loss -4.31160 acc 0.33333 roc_auc 0.42760 prc_auc 0.61857[0m
[93maverage test of epoch 17: loss -4.41512 acc 0.34211 roc_auc 0.27538 prc_auc 0.60339[0m
[92maverage training of epoch 18: loss -4.46979 acc 0.33333 roc_auc 0.43460 prc_auc 0.62570[0m
[93maverage test of epoch 18: loss -4.57447 acc 0.34211 roc_auc 0.31385 prc_auc 0.63487[0m
[92maverage training of epoch 19: loss -4.63276 acc 0.33333 roc_auc 0.44420 prc_auc 0.63460[0m
[93maverage test of epoch 19: loss -4.74316 acc 0.34211 roc_auc 0.34615 prc_auc 0.64814[0m
[92maverage training of epoch 20: loss -4.81106 acc 0.33333 roc_auc 0.45660 prc_auc 0.64543[0m
[93maverage test of epoch 20: loss -4.93848 acc 0.34211 roc_auc 0.49077 prc_auc 0.75768[0m
[92maverage training of epoch 21: loss -5.03294 acc 0.33333 roc_auc 0.47500 prc_auc 0.66400[0m
[93maverage test of epoch 21: loss -5.20558 acc 0.34211 roc_auc 0.75385 prc_auc 0.89532[0m
[92maverage training of epoch 22: loss -5.34667 acc 0.33333 roc_auc 0.55250 prc_auc 0.74674[0m
[93maverage test of epoch 22: loss -5.52378 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 23: loss -5.62578 acc 0.33333 roc_auc 0.59940 prc_auc 0.76339[0m
[93maverage test of epoch 23: loss -5.75334 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 24: loss -5.84501 acc 0.33333 roc_auc 0.56280 prc_auc 0.73486[0m
[93maverage test of epoch 24: loss -5.95972 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 25: loss -6.04719 acc 0.33333 roc_auc 0.53680 prc_auc 0.71404[0m
[93maverage test of epoch 25: loss -6.15527 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 26: loss -6.24031 acc 0.33333 roc_auc 0.51340 prc_auc 0.69500[0m
[93maverage test of epoch 26: loss -6.34399 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 27: loss -6.42752 acc 0.33333 roc_auc 0.49600 prc_auc 0.68003[0m
[93maverage test of epoch 27: loss -6.52796 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 28: loss -6.61053 acc 0.33333 roc_auc 0.48200 prc_auc 0.66656[0m
[93maverage test of epoch 28: loss -6.70845 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 29: loss -6.79041 acc 0.33333 roc_auc 0.46880 prc_auc 0.65131[0m
[93maverage test of epoch 29: loss -6.88628 acc 0.34211 roc_auc 0.88923 prc_auc 0.93807[0m
[92maverage training of epoch 30: loss -6.96787 acc 0.33333 roc_auc 0.45840 prc_auc 0.64372[0m
[93maverage test of epoch 30: loss -7.06203 acc 0.34211 roc_auc 0.88615 prc_auc 0.93236[0m
[92maverage training of epoch 31: loss -7.14342 acc 0.33333 roc_auc 0.45080 prc_auc 0.63541[0m
[93maverage test of epoch 31: loss -7.23611 acc 0.34211 roc_auc 0.88615 prc_auc 0.93236[0m
[92maverage training of epoch 32: loss -7.31743 acc 0.33333 roc_auc 0.44500 prc_auc 0.63092[0m
[93maverage test of epoch 32: loss -7.40883 acc 0.34211 roc_auc 0.88615 prc_auc 0.93403[0m
[92maverage training of epoch 33: loss -7.49019 acc 0.33333 roc_auc 0.44000 prc_auc 0.62636[0m
[93maverage test of epoch 33: loss -7.58045 acc 0.34211 roc_auc 0.88462 prc_auc 0.93091[0m
[92maverage training of epoch 34: loss -7.66192 acc 0.33333 roc_auc 0.43660 prc_auc 0.62290[0m
[93maverage test of epoch 34: loss -7.75114 acc 0.34211 roc_auc 0.88615 prc_auc 0.93288[0m
[92maverage training of epoch 35: loss -7.83278 acc 0.33333 roc_auc 0.43260 prc_auc 0.62067[0m
[93maverage test of epoch 35: loss -7.92106 acc 0.34211 roc_auc 0.88615 prc_auc 0.93262[0m
[92maverage training of epoch 36: loss -8.00293 acc 0.33333 roc_auc 0.43000 prc_auc 0.61897[0m
[93maverage test of epoch 36: loss -8.09034 acc 0.34211 roc_auc 0.87077 prc_auc 0.90572[0m
[92maverage training of epoch 37: loss -8.17247 acc 0.33333 roc_auc 0.42900 prc_auc 0.61871[0m
[93maverage test of epoch 37: loss -8.25906 acc 0.34211 roc_auc 0.87385 prc_auc 0.90596[0m
[92maverage training of epoch 38: loss -8.34149 acc 0.40000 roc_auc 0.42760 prc_auc 0.61806[0m
[93maverage test of epoch 38: loss -8.42732 acc 0.65789 roc_auc 0.86000 prc_auc 0.89709[0m
[92maverage training of epoch 39: loss -8.51007 acc 0.66667 roc_auc 0.42600 prc_auc 0.61747[0m
[93maverage test of epoch 39: loss -8.59518 acc 0.65789 roc_auc 0.86923 prc_auc 0.90021[0m
[92maverage training of epoch 40: loss -8.67827 acc 0.66667 roc_auc 0.42500 prc_auc 0.61682[0m
[93maverage test of epoch 40: loss -8.76270 acc 0.65789 roc_auc 0.85846 prc_auc 0.89199[0m
[92maverage training of epoch 41: loss -8.84615 acc 0.66667 roc_auc 0.42440 prc_auc 0.61662[0m
[93maverage test of epoch 41: loss -8.92992 acc 0.65789 roc_auc 0.84615 prc_auc 0.87049[0m
[92maverage training of epoch 42: loss -9.01375 acc 0.66667 roc_auc 0.42400 prc_auc 0.61653[0m
[93maverage test of epoch 42: loss -9.09690 acc 0.65789 roc_auc 0.79692 prc_auc 0.84841[0m
[92maverage training of epoch 43: loss -9.18112 acc 0.66667 roc_auc 0.42340 prc_auc 0.61137[0m
[93maverage test of epoch 43: loss -9.26365 acc 0.65789 roc_auc 0.87077 prc_auc 0.90063[0m
[92maverage training of epoch 44: loss -9.34828 acc 0.66667 roc_auc 0.42320 prc_auc 0.61131[0m
[93maverage test of epoch 44: loss -9.43021 acc 0.65789 roc_auc 0.88000 prc_auc 0.90513[0m
[92maverage training of epoch 45: loss -9.51525 acc 0.66667 roc_auc 0.42300 prc_auc 0.61127[0m
[93maverage test of epoch 45: loss -9.59661 acc 0.65789 roc_auc 0.87385 prc_auc 0.90318[0m
[92maverage training of epoch 46: loss -9.68207 acc 0.66667 roc_auc 0.42280 prc_auc 0.61121[0m
[93maverage test of epoch 46: loss -9.76287 acc 0.65789 roc_auc 0.79385 prc_auc 0.83556[0m
[92maverage training of epoch 47: loss -9.84876 acc 0.66667 roc_auc 0.42260 prc_auc 0.61116[0m
[93maverage test of epoch 47: loss -9.92900 acc 0.65789 roc_auc 0.82462 prc_auc 0.84843[0m
[92maverage training of epoch 48: loss -10.01533 acc 0.66667 roc_auc 0.42240 prc_auc 0.60983[0m
[93maverage test of epoch 48: loss -10.09503 acc 0.65789 roc_auc 0.83385 prc_auc 0.85841[0m
[92maverage training of epoch 49: loss -10.18179 acc 0.66667 roc_auc 0.42220 prc_auc 0.60979[0m
[93maverage test of epoch 49: loss -10.26096 acc 0.65789 roc_auc 0.82769 prc_auc 0.85119[0m
[92maverage training of epoch 50: loss -10.34817 acc 0.66667 roc_auc 0.42220 prc_auc 0.60979[0m
[93maverage test of epoch 50: loss -10.42680 acc 0.65789 roc_auc 0.87077 prc_auc 0.89641[0m
[92maverage training of epoch 51: loss -10.51447 acc 0.66667 roc_auc 0.42200 prc_auc 0.60829[0m
[93maverage test of epoch 51: loss -10.59258 acc 0.65789 roc_auc 0.82462 prc_auc 0.84889[0m
[92maverage training of epoch 52: loss -10.68070 acc 0.66667 roc_auc 0.42140 prc_auc 0.60718[0m
[93maverage test of epoch 52: loss -10.75830 acc 0.65789 roc_auc 0.78308 prc_auc 0.83232[0m
[92maverage training of epoch 53: loss -10.84687 acc 0.66667 roc_auc 0.42120 prc_auc 0.60647[0m
[93maverage test of epoch 53: loss -10.92396 acc 0.65789 roc_auc 0.83385 prc_auc 0.85889[0m
[92maverage training of epoch 54: loss -11.01299 acc 0.66667 roc_auc 0.42100 prc_auc 0.60588[0m
[93maverage test of epoch 54: loss -11.08957 acc 0.65789 roc_auc 0.87077 prc_auc 0.88795[0m
[92maverage training of epoch 55: loss -11.17906 acc 0.66667 roc_auc 0.42080 prc_auc 0.60544[0m
[93maverage test of epoch 55: loss -11.25514 acc 0.65789 roc_auc 0.82462 prc_auc 0.84889[0m
[92maverage training of epoch 56: loss -11.34509 acc 0.66667 roc_auc 0.42080 prc_auc 0.60544[0m
[93maverage test of epoch 56: loss -11.42068 acc 0.65789 roc_auc 0.82000 prc_auc 0.84667[0m
[92maverage training of epoch 57: loss -11.51108 acc 0.66667 roc_auc 0.42080 prc_auc 0.60544[0m
[93maverage test of epoch 57: loss -11.58618 acc 0.65789 roc_auc 0.80308 prc_auc 0.84593[0m
[92maverage training of epoch 58: loss -11.67705 acc 0.66667 roc_auc 0.42070 prc_auc 0.60536[0m
[93maverage test of epoch 58: loss -11.75165 acc 0.65789 roc_auc 0.84308 prc_auc 0.86222[0m
[92maverage training of epoch 59: loss -11.84298 acc 0.66667 roc_auc 0.42080 prc_auc 0.60544[0m
[93maverage test of epoch 59: loss -11.91709 acc 0.65789 roc_auc 0.82308 prc_auc 0.84667[0m
[92maverage training of epoch 60: loss -12.00889 acc 0.66667 roc_auc 0.42080 prc_auc 0.60544[0m
[93maverage test of epoch 60: loss -12.08251 acc 0.65789 roc_auc 0.83846 prc_auc 0.86000[0m
[92maverage training of epoch 61: loss -12.17478 acc 0.66667 roc_auc 0.42030 prc_auc 0.60466[0m
[93maverage test of epoch 61: loss -12.24792 acc 0.65789 roc_auc 0.83846 prc_auc 0.86000[0m
[92maverage training of epoch 62: loss -12.34065 acc 0.66667 roc_auc 0.42030 prc_auc 0.60485[0m
[93maverage test of epoch 62: loss -12.41330 acc 0.65789 roc_auc 0.81538 prc_auc 0.84161[0m
[92maverage training of epoch 63: loss -12.50651 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 63: loss -12.57867 acc 0.65789 roc_auc 0.84615 prc_auc 0.86667[0m
[92maverage training of epoch 64: loss -12.67234 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 64: loss -12.74403 acc 0.65789 roc_auc 0.81846 prc_auc 0.84661[0m
[92maverage training of epoch 65: loss -12.83817 acc 0.66667 roc_auc 0.42020 prc_auc 0.60463[0m
[93maverage test of epoch 65: loss -12.90937 acc 0.65789 roc_auc 0.76154 prc_auc 0.81739[0m
[92maverage training of epoch 66: loss -13.00398 acc 0.66667 roc_auc 0.42020 prc_auc 0.60463[0m
[93maverage test of epoch 66: loss -13.07470 acc 0.65789 roc_auc 0.77077 prc_auc 0.82158[0m
[92maverage training of epoch 67: loss -13.16979 acc 0.66667 roc_auc 0.42020 prc_auc 0.60463[0m
[93maverage test of epoch 67: loss -13.24002 acc 0.65789 roc_auc 0.77385 prc_auc 0.81506[0m
[92maverage training of epoch 68: loss -13.33558 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 68: loss -13.40534 acc 0.65789 roc_auc 0.77692 prc_auc 0.82009[0m
[92maverage training of epoch 69: loss -13.50136 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 69: loss -13.57065 acc 0.65789 roc_auc 0.74308 prc_auc 0.80036[0m
[92maverage training of epoch 70: loss -13.66714 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 70: loss -13.73595 acc 0.65789 roc_auc 0.71538 prc_auc 0.78605[0m
[92maverage training of epoch 71: loss -13.83291 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 71: loss -13.90124 acc 0.65789 roc_auc 0.76308 prc_auc 0.80681[0m
[92maverage training of epoch 72: loss -13.99868 acc 0.66667 roc_auc 0.42020 prc_auc 0.60463[0m
[93maverage test of epoch 72: loss -14.06653 acc 0.65789 roc_auc 0.75077 prc_auc 0.79948[0m
[92maverage training of epoch 73: loss -14.16444 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 73: loss -14.23181 acc 0.65789 roc_auc 0.75231 prc_auc 0.80752[0m
[92maverage training of epoch 74: loss -14.33019 acc 0.66667 roc_auc 0.42020 prc_auc 0.60463[0m
[93maverage test of epoch 74: loss -14.39709 acc 0.65789 roc_auc 0.44462 prc_auc 0.71137[0m
[92maverage training of epoch 75: loss -14.49594 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 75: loss -14.56236 acc 0.65789 roc_auc 0.59846 prc_auc 0.74013[0m
[92maverage training of epoch 76: loss -14.66169 acc 0.66667 roc_auc 0.42020 prc_auc 0.60463[0m
[93maverage test of epoch 76: loss -14.72763 acc 0.65789 roc_auc 0.50462 prc_auc 0.69303[0m
[92maverage training of epoch 77: loss -14.82743 acc 0.66667 roc_auc 0.42020 prc_auc 0.60516[0m
[93maverage test of epoch 77: loss -14.89290 acc 0.65789 roc_auc 0.62615 prc_auc 0.75002[0m
[92maverage training of epoch 78: loss -14.99317 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 78: loss -15.05816 acc 0.65789 roc_auc 0.55077 prc_auc 0.70532[0m
[92maverage training of epoch 79: loss -15.15891 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 79: loss -15.22343 acc 0.65789 roc_auc 0.63692 prc_auc 0.75780[0m
[92maverage training of epoch 80: loss -15.32464 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 80: loss -15.38868 acc 0.65789 roc_auc 0.69231 prc_auc 0.77124[0m
[92maverage training of epoch 81: loss -15.49037 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 81: loss -15.55394 acc 0.65789 roc_auc 0.61077 prc_auc 0.71889[0m
[92maverage training of epoch 82: loss -15.65610 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 82: loss -15.71920 acc 0.65789 roc_auc 0.64615 prc_auc 0.73714[0m
[92maverage training of epoch 83: loss -15.82183 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 83: loss -15.88445 acc 0.65789 roc_auc 0.49692 prc_auc 0.69012[0m
[92maverage training of epoch 84: loss -15.98756 acc 0.66667 roc_auc 0.42020 prc_auc 0.60463[0m
[93maverage test of epoch 84: loss -16.04971 acc 0.65789 roc_auc 0.61692 prc_auc 0.75264[0m
[92maverage training of epoch 85: loss -16.15328 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 85: loss -16.21496 acc 0.65789 roc_auc 0.57231 prc_auc 0.70641[0m
[92maverage training of epoch 86: loss -16.31901 acc 0.66667 roc_auc 0.42020 prc_auc 0.60463[0m
[93maverage test of epoch 86: loss -16.38021 acc 0.65789 roc_auc 0.60462 prc_auc 0.72868[0m
[92maverage training of epoch 87: loss -16.48473 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 87: loss -16.54546 acc 0.65789 roc_auc 0.55077 prc_auc 0.69228[0m
[92maverage training of epoch 88: loss -16.65045 acc 0.66667 roc_auc 0.42000 prc_auc 0.60455[0m
[93maverage test of epoch 88: loss -16.71070 acc 0.65789 roc_auc 0.76308 prc_auc 0.81427[0m
[92maverage training of epoch 89: loss -16.81617 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 89: loss -16.87595 acc 0.65789 roc_auc 0.62923 prc_auc 0.74140[0m
[92maverage training of epoch 90: loss -16.98189 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 90: loss -17.04120 acc 0.65789 roc_auc 0.77231 prc_auc 0.81519[0m
[92maverage training of epoch 91: loss -17.14761 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 91: loss -17.20644 acc 0.65789 roc_auc 0.53538 prc_auc 0.68463[0m
[92maverage training of epoch 92: loss -17.31333 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 92: loss -17.37169 acc 0.65789 roc_auc 0.78462 prc_auc 0.84390[0m
[92maverage training of epoch 93: loss -17.47904 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 93: loss -17.53693 acc 0.65789 roc_auc 0.61077 prc_auc 0.71889[0m
[92maverage training of epoch 94: loss -17.64476 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 94: loss -17.70217 acc 0.65789 roc_auc 0.64615 prc_auc 0.73714[0m
[92maverage training of epoch 95: loss -17.81048 acc 0.66667 roc_auc 0.42020 prc_auc 0.60463[0m
[93maverage test of epoch 95: loss -17.86742 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 96: loss -17.97619 acc 0.66667 roc_auc 0.42020 prc_auc 0.60463[0m
[93maverage test of epoch 96: loss -18.03266 acc 0.65789 roc_auc 0.68462 prc_auc 0.77842[0m
[92maverage training of epoch 97: loss -18.14191 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 97: loss -18.19790 acc 0.65789 roc_auc 0.54000 prc_auc 0.68526[0m
[92maverage training of epoch 98: loss -18.30762 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 98: loss -18.36315 acc 0.65789 roc_auc 0.68000 prc_auc 0.78105[0m
[92maverage training of epoch 99: loss -18.47334 acc 0.66667 roc_auc 0.42010 prc_auc 0.60455[0m
[93maverage test of epoch 99: loss -18.52839 acc 0.65789 roc_auc 0.78769 prc_auc 0.82194[0m
Training model with dataset, testing using fold 2
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss -0.78870 acc 0.33333 roc_auc 0.43600 prc_auc 0.63362[0m
[93maverage test of epoch 0: loss -1.00518 acc 0.34211 roc_auc 0.07385 prc_auc 0.47052[0m
[92maverage training of epoch 1: loss -1.19566 acc 0.37333 roc_auc 0.44440 prc_auc 0.64905[0m
[93maverage test of epoch 1: loss -1.39538 acc 0.65789 roc_auc 0.19077 prc_auc 0.54431[0m
[92maverage training of epoch 2: loss -1.58875 acc 0.66667 roc_auc 0.45240 prc_auc 0.65551[0m
[93maverage test of epoch 2: loss -1.79232 acc 0.65789 roc_auc 0.87077 prc_auc 0.94930[0m
[92maverage training of epoch 3: loss -2.06153 acc 0.66667 roc_auc 0.45620 prc_auc 0.66417[0m
[93maverage test of epoch 3: loss -2.38338 acc 0.65789 roc_auc 0.95538 prc_auc 0.98000[0m
[92maverage training of epoch 4: loss -2.70843 acc 0.66667 roc_auc 0.49800 prc_auc 0.69713[0m
[93maverage test of epoch 4: loss -2.93070 acc 0.65789 roc_auc 0.95692 prc_auc 0.98069[0m
[92maverage training of epoch 5: loss -3.11530 acc 0.66667 roc_auc 0.46320 prc_auc 0.66449[0m
[93maverage test of epoch 5: loss -3.25389 acc 0.65789 roc_auc 0.95692 prc_auc 0.98069[0m
[92maverage training of epoch 6: loss -3.40896 acc 0.66667 roc_auc 0.43280 prc_auc 0.64361[0m
[93maverage test of epoch 6: loss -3.52209 acc 0.65789 roc_auc 0.95385 prc_auc 0.97933[0m
[92maverage training of epoch 7: loss -3.66117 acc 0.66667 roc_auc 0.42380 prc_auc 0.64568[0m
[93maverage test of epoch 7: loss -3.75948 acc 0.65789 roc_auc 0.94462 prc_auc 0.97443[0m
[92maverage training of epoch 8: loss -3.88847 acc 0.66667 roc_auc 0.41280 prc_auc 0.63631[0m
[93maverage test of epoch 8: loss -3.97746 acc 0.65789 roc_auc 0.93846 prc_auc 0.97198[0m
[92maverage training of epoch 9: loss -4.09999 acc 0.66667 roc_auc 0.40420 prc_auc 0.62901[0m
[93maverage test of epoch 9: loss -4.18316 acc 0.65789 roc_auc 0.92154 prc_auc 0.96274[0m
[92maverage training of epoch 10: loss -4.30164 acc 0.66667 roc_auc 0.40360 prc_auc 0.62579[0m
[93maverage test of epoch 10: loss -4.38131 acc 0.65789 roc_auc 0.89231 prc_auc 0.95253[0m
[92maverage training of epoch 11: loss -4.49733 acc 0.66667 roc_auc 0.40420 prc_auc 0.62485[0m
[93maverage test of epoch 11: loss -4.57505 acc 0.65789 roc_auc 0.86462 prc_auc 0.94107[0m
[92maverage training of epoch 12: loss -4.68965 acc 0.66667 roc_auc 0.40620 prc_auc 0.62665[0m
[93maverage test of epoch 12: loss -4.76637 acc 0.65789 roc_auc 0.84923 prc_auc 0.93443[0m
[92maverage training of epoch 13: loss -4.88013 acc 0.66667 roc_auc 0.40580 prc_auc 0.62655[0m
[93maverage test of epoch 13: loss -4.95631 acc 0.65789 roc_auc 0.82154 prc_auc 0.92647[0m
[92maverage training of epoch 14: loss -5.06948 acc 0.66667 roc_auc 0.40240 prc_auc 0.62463[0m
[93maverage test of epoch 14: loss -5.14522 acc 0.65789 roc_auc 0.79846 prc_auc 0.91892[0m
[92maverage training of epoch 15: loss -5.25785 acc 0.66667 roc_auc 0.39880 prc_auc 0.62201[0m
[93maverage test of epoch 15: loss -5.33303 acc 0.65789 roc_auc 0.78154 prc_auc 0.90989[0m
[92maverage training of epoch 16: loss -5.44506 acc 0.66667 roc_auc 0.39530 prc_auc 0.61793[0m
[93maverage test of epoch 16: loss -5.51948 acc 0.65789 roc_auc 0.77077 prc_auc 0.90685[0m
[92maverage training of epoch 17: loss -5.63088 acc 0.66667 roc_auc 0.39360 prc_auc 0.61719[0m
[93maverage test of epoch 17: loss -5.70432 acc 0.65789 roc_auc 0.74923 prc_auc 0.89912[0m
[92maverage training of epoch 18: loss -5.81508 acc 0.66667 roc_auc 0.39460 prc_auc 0.61419[0m
[93maverage test of epoch 18: loss -5.88741 acc 0.65789 roc_auc 0.72000 prc_auc 0.88533[0m
[92maverage training of epoch 19: loss -5.99757 acc 0.66667 roc_auc 0.39600 prc_auc 0.61334[0m
[93maverage test of epoch 19: loss -6.06871 acc 0.65789 roc_auc 0.71385 prc_auc 0.88371[0m
[92maverage training of epoch 20: loss -6.17833 acc 0.66667 roc_auc 0.39240 prc_auc 0.60728[0m
[93maverage test of epoch 20: loss -6.24826 acc 0.65789 roc_auc 0.69231 prc_auc 0.86667[0m
[92maverage training of epoch 21: loss -6.35744 acc 0.66667 roc_auc 0.38700 prc_auc 0.59583[0m
[93maverage test of epoch 21: loss -6.42617 acc 0.65789 roc_auc 0.66769 prc_auc 0.84464[0m
[92maverage training of epoch 22: loss -6.53501 acc 0.66667 roc_auc 0.38360 prc_auc 0.59360[0m
[93maverage test of epoch 22: loss -6.60259 acc 0.65789 roc_auc 0.65077 prc_auc 0.82037[0m
[92maverage training of epoch 23: loss -6.71116 acc 0.66667 roc_auc 0.38340 prc_auc 0.59292[0m
[93maverage test of epoch 23: loss -6.77766 acc 0.65789 roc_auc 0.68308 prc_auc 0.84422[0m
[92maverage training of epoch 24: loss -6.88606 acc 0.66667 roc_auc 0.38480 prc_auc 0.59343[0m
[93maverage test of epoch 24: loss -6.95153 acc 0.65789 roc_auc 0.53692 prc_auc 0.73407[0m
[92maverage training of epoch 25: loss -7.05984 acc 0.66667 roc_auc 0.38300 prc_auc 0.59181[0m
[93maverage test of epoch 25: loss -7.12434 acc 0.65789 roc_auc 0.45231 prc_auc 0.64949[0m
[92maverage training of epoch 26: loss -7.23263 acc 0.66667 roc_auc 0.38330 prc_auc 0.59215[0m
[93maverage test of epoch 26: loss -7.29622 acc 0.65789 roc_auc 0.46462 prc_auc 0.67378[0m
[92maverage training of epoch 27: loss -7.40456 acc 0.66667 roc_auc 0.38260 prc_auc 0.59184[0m
[93maverage test of epoch 27: loss -7.46729 acc 0.65789 roc_auc 0.50000 prc_auc 0.69287[0m
[92maverage training of epoch 28: loss -7.57572 acc 0.66667 roc_auc 0.38180 prc_auc 0.58511[0m
[93maverage test of epoch 28: loss -7.63764 acc 0.65789 roc_auc 0.39077 prc_auc 0.61194[0m
[92maverage training of epoch 29: loss -7.74621 acc 0.66667 roc_auc 0.38000 prc_auc 0.57668[0m
[93maverage test of epoch 29: loss -7.80737 acc 0.65789 roc_auc 0.38000 prc_auc 0.61905[0m
[92maverage training of epoch 30: loss -7.91613 acc 0.66667 roc_auc 0.37960 prc_auc 0.57513[0m
[93maverage test of epoch 30: loss -7.97655 acc 0.65789 roc_auc 0.38000 prc_auc 0.60730[0m
[92maverage training of epoch 31: loss -8.08553 acc 0.66667 roc_auc 0.37810 prc_auc 0.57347[0m
[93maverage test of epoch 31: loss -8.14526 acc 0.65789 roc_auc 0.35846 prc_auc 0.61596[0m
[92maverage training of epoch 32: loss -8.25449 acc 0.66667 roc_auc 0.37750 prc_auc 0.57346[0m
[93maverage test of epoch 32: loss -8.31356 acc 0.65789 roc_auc 0.37385 prc_auc 0.60598[0m
[92maverage training of epoch 33: loss -8.42306 acc 0.66667 roc_auc 0.37740 prc_auc 0.57367[0m
[93maverage test of epoch 33: loss -8.48148 acc 0.65789 roc_auc 0.36000 prc_auc 0.60228[0m
[92maverage training of epoch 34: loss -8.59130 acc 0.66667 roc_auc 0.37780 prc_auc 0.57558[0m
[93maverage test of epoch 34: loss -8.64909 acc 0.65789 roc_auc 0.42000 prc_auc 0.62409[0m
[92maverage training of epoch 35: loss -8.75923 acc 0.66667 roc_auc 0.37750 prc_auc 0.57518[0m
[93maverage test of epoch 35: loss -8.81642 acc 0.65789 roc_auc 0.48000 prc_auc 0.64902[0m
[92maverage training of epoch 36: loss -8.92690 acc 0.66667 roc_auc 0.37760 prc_auc 0.57537[0m
[93maverage test of epoch 36: loss -8.98350 acc 0.65789 roc_auc 0.34000 prc_auc 0.59586[0m
[92maverage training of epoch 37: loss -9.09434 acc 0.66667 roc_auc 0.37760 prc_auc 0.57537[0m
[93maverage test of epoch 37: loss -9.15037 acc 0.65789 roc_auc 0.44462 prc_auc 0.63895[0m
[92maverage training of epoch 38: loss -9.26157 acc 0.66667 roc_auc 0.37730 prc_auc 0.57517[0m
[93maverage test of epoch 38: loss -9.31704 acc 0.65789 roc_auc 0.42000 prc_auc 0.62409[0m
[92maverage training of epoch 39: loss -9.42863 acc 0.66667 roc_auc 0.37690 prc_auc 0.57485[0m
[93maverage test of epoch 39: loss -9.48355 acc 0.65789 roc_auc 0.56000 prc_auc 0.69895[0m
[92maverage training of epoch 40: loss -9.59554 acc 0.66667 roc_auc 0.37690 prc_auc 0.57496[0m
[93maverage test of epoch 40: loss -9.64992 acc 0.65789 roc_auc 0.48769 prc_auc 0.65248[0m
[92maverage training of epoch 41: loss -9.76230 acc 0.66667 roc_auc 0.37690 prc_auc 0.57496[0m
[93maverage test of epoch 41: loss -9.81615 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 42: loss -9.92895 acc 0.66667 roc_auc 0.37680 prc_auc 0.57458[0m
[93maverage test of epoch 42: loss -9.98228 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 43: loss -10.09549 acc 0.66667 roc_auc 0.37670 prc_auc 0.57460[0m
[93maverage test of epoch 43: loss -10.14830 acc 0.65789 roc_auc 0.38000 prc_auc 0.60914[0m
[92maverage training of epoch 44: loss -10.26193 acc 0.66667 roc_auc 0.37670 prc_auc 0.57460[0m
[93maverage test of epoch 44: loss -10.31423 acc 0.65789 roc_auc 0.66000 prc_auc 0.76737[0m
[92maverage training of epoch 45: loss -10.42829 acc 0.66667 roc_auc 0.37670 prc_auc 0.57458[0m
[93maverage test of epoch 45: loss -10.48008 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 46: loss -10.59458 acc 0.66667 roc_auc 0.37660 prc_auc 0.57460[0m
[93maverage test of epoch 46: loss -10.64587 acc 0.65789 roc_auc 0.72000 prc_auc 0.80842[0m
[92maverage training of epoch 47: loss -10.76081 acc 0.66667 roc_auc 0.37660 prc_auc 0.57442[0m
[93maverage test of epoch 47: loss -10.81159 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 48: loss -10.92697 acc 0.66667 roc_auc 0.37670 prc_auc 0.57442[0m
[93maverage test of epoch 48: loss -10.97726 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 49: loss -11.09309 acc 0.66667 roc_auc 0.37650 prc_auc 0.57439[0m
[93maverage test of epoch 49: loss -11.14288 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 50: loss -11.25916 acc 0.66667 roc_auc 0.37650 prc_auc 0.57442[0m
[93maverage test of epoch 50: loss -11.30846 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 51: loss -11.42519 acc 0.66667 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 51: loss -11.47400 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 52: loss -11.59119 acc 0.66667 roc_auc 0.37660 prc_auc 0.57439[0m
[93maverage test of epoch 52: loss -11.63951 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 53: loss -11.75715 acc 0.66667 roc_auc 0.37660 prc_auc 0.57448[0m
[93maverage test of epoch 53: loss -11.80499 acc 0.65789 roc_auc 0.35692 prc_auc 0.60307[0m
[92maverage training of epoch 54: loss -11.92309 acc 0.66667 roc_auc 0.37660 prc_auc 0.57448[0m
[93maverage test of epoch 54: loss -11.97044 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 55: loss -12.08900 acc 0.66667 roc_auc 0.37660 prc_auc 0.57448[0m
[93maverage test of epoch 55: loss -12.13587 acc 0.65789 roc_auc 0.64000 prc_auc 0.75368[0m
[92maverage training of epoch 56: loss -12.25488 acc 0.66667 roc_auc 0.37670 prc_auc 0.57447[0m
[93maverage test of epoch 56: loss -12.30127 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 57: loss -12.42075 acc 0.66667 roc_auc 0.37660 prc_auc 0.57441[0m
[93maverage test of epoch 57: loss -12.46666 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 58: loss -12.58660 acc 0.66667 roc_auc 0.37670 prc_auc 0.57448[0m
[93maverage test of epoch 58: loss -12.63203 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 59: loss -12.75244 acc 0.66667 roc_auc 0.37660 prc_auc 0.57467[0m
[93maverage test of epoch 59: loss -12.79739 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 60: loss -12.91826 acc 0.66667 roc_auc 0.37670 prc_auc 0.57467[0m
[93maverage test of epoch 60: loss -12.96273 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 61: loss -13.08407 acc 0.66667 roc_auc 0.37670 prc_auc 0.57467[0m
[93maverage test of epoch 61: loss -13.12806 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 62: loss -13.24986 acc 0.66667 roc_auc 0.37660 prc_auc 0.57467[0m
[93maverage test of epoch 62: loss -13.29338 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 63: loss -13.41565 acc 0.66667 roc_auc 0.37670 prc_auc 0.57470[0m
[93maverage test of epoch 63: loss -13.45869 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 64: loss -13.58143 acc 0.66667 roc_auc 0.37670 prc_auc 0.57467[0m
[93maverage test of epoch 64: loss -13.62399 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 65: loss -13.74720 acc 0.66667 roc_auc 0.37660 prc_auc 0.57468[0m
[93maverage test of epoch 65: loss -13.78929 acc 0.65789 roc_auc 0.30000 prc_auc 0.58459[0m
[92maverage training of epoch 66: loss -13.91296 acc 0.66667 roc_auc 0.37660 prc_auc 0.57466[0m
[93maverage test of epoch 66: loss -13.95457 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 67: loss -14.07872 acc 0.66667 roc_auc 0.37680 prc_auc 0.57473[0m
[93maverage test of epoch 67: loss -14.11985 acc 0.65789 roc_auc 0.40000 prc_auc 0.61643[0m
[92maverage training of epoch 68: loss -14.24447 acc 0.66667 roc_auc 0.37660 prc_auc 0.57466[0m
[93maverage test of epoch 68: loss -14.28513 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 69: loss -14.41021 acc 0.66667 roc_auc 0.37650 prc_auc 0.57471[0m
[93maverage test of epoch 69: loss -14.45040 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 70: loss -14.57595 acc 0.66667 roc_auc 0.37660 prc_auc 0.57479[0m
[93maverage test of epoch 70: loss -14.61566 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 71: loss -14.74168 acc 0.66667 roc_auc 0.37680 prc_auc 0.57450[0m
[93maverage test of epoch 71: loss -14.78092 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 72: loss -14.90742 acc 0.66667 roc_auc 0.37680 prc_auc 0.57479[0m
[93maverage test of epoch 72: loss -14.94618 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 73: loss -15.07315 acc 0.66667 roc_auc 0.37690 prc_auc 0.57466[0m
[93maverage test of epoch 73: loss -15.11144 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 74: loss -15.23887 acc 0.66667 roc_auc 0.37690 prc_auc 0.57474[0m
[93maverage test of epoch 74: loss -15.27669 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 75: loss -15.40459 acc 0.66667 roc_auc 0.37690 prc_auc 0.57470[0m
[93maverage test of epoch 75: loss -15.44194 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 76: loss -15.57031 acc 0.66667 roc_auc 0.37680 prc_auc 0.57468[0m
[93maverage test of epoch 76: loss -15.60718 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 77: loss -15.73603 acc 0.66667 roc_auc 0.37660 prc_auc 0.57496[0m
[93maverage test of epoch 77: loss -15.77243 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 78: loss -15.90175 acc 0.66667 roc_auc 0.37670 prc_auc 0.57514[0m
[93maverage test of epoch 78: loss -15.93767 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 79: loss -16.06746 acc 0.66667 roc_auc 0.37700 prc_auc 0.57514[0m
[93maverage test of epoch 79: loss -16.10291 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 80: loss -16.23317 acc 0.66667 roc_auc 0.37650 prc_auc 0.57462[0m
[93maverage test of epoch 80: loss -16.26814 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 81: loss -16.39888 acc 0.66667 roc_auc 0.37680 prc_auc 0.57503[0m
[93maverage test of epoch 81: loss -16.43338 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 82: loss -16.56459 acc 0.66667 roc_auc 0.37690 prc_auc 0.57537[0m
[93maverage test of epoch 82: loss -16.59861 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 83: loss -16.73029 acc 0.66667 roc_auc 0.37680 prc_auc 0.57530[0m
[93maverage test of epoch 83: loss -16.76385 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 84: loss -16.89600 acc 0.66667 roc_auc 0.37660 prc_auc 0.57559[0m
[93maverage test of epoch 84: loss -16.92908 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 85: loss -17.06170 acc 0.66667 roc_auc 0.37700 prc_auc 0.57542[0m
[93maverage test of epoch 85: loss -17.09431 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 86: loss -17.22740 acc 0.66667 roc_auc 0.37690 prc_auc 0.57522[0m
[93maverage test of epoch 86: loss -17.25954 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 87: loss -17.39310 acc 0.66667 roc_auc 0.37680 prc_auc 0.57565[0m
[93maverage test of epoch 87: loss -17.42477 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 88: loss -17.55880 acc 0.66667 roc_auc 0.37660 prc_auc 0.57589[0m
[93maverage test of epoch 88: loss -17.58999 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 89: loss -17.72450 acc 0.66667 roc_auc 0.37680 prc_auc 0.57543[0m
[93maverage test of epoch 89: loss -17.75522 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 90: loss -17.89020 acc 0.66667 roc_auc 0.37680 prc_auc 0.57563[0m
[93maverage test of epoch 90: loss -17.92045 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 91: loss -18.05590 acc 0.66667 roc_auc 0.37630 prc_auc 0.57613[0m
[93maverage test of epoch 91: loss -18.08567 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 92: loss -18.22160 acc 0.66667 roc_auc 0.37650 prc_auc 0.57555[0m
[93maverage test of epoch 92: loss -18.25089 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 93: loss -18.38729 acc 0.66667 roc_auc 0.37690 prc_auc 0.57516[0m
[93maverage test of epoch 93: loss -18.41612 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 94: loss -18.55299 acc 0.66667 roc_auc 0.37660 prc_auc 0.57588[0m
[93maverage test of epoch 94: loss -18.58134 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 95: loss -18.71869 acc 0.66667 roc_auc 0.37660 prc_auc 0.57533[0m
[93maverage test of epoch 95: loss -18.74657 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 96: loss -18.88438 acc 0.66667 roc_auc 0.37700 prc_auc 0.57753[0m
[93maverage test of epoch 96: loss -18.91179 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 97: loss -19.05008 acc 0.66667 roc_auc 0.37720 prc_auc 0.57720[0m
[93maverage test of epoch 97: loss -19.07701 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 98: loss -19.21577 acc 0.66667 roc_auc 0.37630 prc_auc 0.57567[0m
[93maverage test of epoch 98: loss -19.24223 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
[92maverage training of epoch 99: loss -19.38146 acc 0.66667 roc_auc 0.37690 prc_auc 0.57637[0m
[93maverage test of epoch 99: loss -19.40746 acc 0.65789 roc_auc 0.50000 prc_auc 0.65789[0m
Training model with dataset, testing using fold 3
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss -0.14893 acc 0.33775 roc_auc 0.41392 prc_auc 0.59278[0m
[93maverage test of epoch 0: loss -0.23816 acc 0.32432 roc_auc 0.10167 prc_auc 0.49676[0m
[92maverage training of epoch 1: loss -0.36725 acc 0.33775 roc_auc 0.44686 prc_auc 0.62361[0m
[93maverage test of epoch 1: loss -0.46140 acc 0.32432 roc_auc 0.23667 prc_auc 0.59860[0m
[92maverage training of epoch 2: loss -0.59604 acc 0.33775 roc_auc 0.50059 prc_auc 0.66061[0m
[93maverage test of epoch 2: loss -0.69007 acc 0.32432 roc_auc 0.38333 prc_auc 0.69233[0m
[92maverage training of epoch 3: loss -0.82870 acc 0.33775 roc_auc 0.55431 prc_auc 0.70705[0m
[93maverage test of epoch 3: loss -0.92118 acc 0.32432 roc_auc 0.63667 prc_auc 0.82797[0m
[92maverage training of epoch 4: loss -1.06569 acc 0.33775 roc_auc 0.60000 prc_auc 0.75252[0m
[93maverage test of epoch 4: loss -1.15886 acc 0.32432 roc_auc 0.82333 prc_auc 0.90243[0m
[92maverage training of epoch 5: loss -1.31221 acc 0.33775 roc_auc 0.63647 prc_auc 0.80104[0m
[93maverage test of epoch 5: loss -1.41087 acc 0.32432 roc_auc 0.84833 prc_auc 0.91092[0m
[92maverage training of epoch 6: loss -1.58135 acc 0.33775 roc_auc 0.67569 prc_auc 0.83109[0m
[93maverage test of epoch 6: loss -1.69654 acc 0.32432 roc_auc 0.85000 prc_auc 0.91183[0m
[92maverage training of epoch 7: loss -1.87901 acc 0.33775 roc_auc 0.73412 prc_auc 0.86685[0m
[93maverage test of epoch 7: loss -1.99412 acc 0.32432 roc_auc 0.84667 prc_auc 0.91099[0m
[92maverage training of epoch 8: loss -2.16423 acc 0.33775 roc_auc 0.75039 prc_auc 0.88463[0m
[93maverage test of epoch 8: loss -2.26401 acc 0.32432 roc_auc 0.84667 prc_auc 0.91092[0m
[92maverage training of epoch 9: loss -2.42123 acc 0.33775 roc_auc 0.68216 prc_auc 0.84592[0m
[93maverage test of epoch 9: loss -2.50845 acc 0.32432 roc_auc 0.84333 prc_auc 0.90866[0m
[92maverage training of epoch 10: loss -2.65652 acc 0.33775 roc_auc 0.57902 prc_auc 0.78201[0m
[93maverage test of epoch 10: loss -2.73514 acc 0.32432 roc_auc 0.85000 prc_auc 0.91125[0m
[92maverage training of epoch 11: loss -2.87613 acc 0.33775 roc_auc 0.47824 prc_auc 0.69589[0m
[93maverage test of epoch 11: loss -2.94883 acc 0.32432 roc_auc 0.70667 prc_auc 0.84243[0m
[92maverage training of epoch 12: loss -3.08440 acc 0.33775 roc_auc 0.40098 prc_auc 0.61374[0m
[93maverage test of epoch 12: loss -3.15299 acc 0.32432 roc_auc 0.53667 prc_auc 0.77245[0m
[92maverage training of epoch 13: loss -3.28439 acc 0.33775 roc_auc 0.37863 prc_auc 0.58636[0m
[93maverage test of epoch 13: loss -3.35011 acc 0.32432 roc_auc 0.40667 prc_auc 0.69478[0m
[92maverage training of epoch 14: loss -3.47819 acc 0.33775 roc_auc 0.37275 prc_auc 0.58089[0m
[93maverage test of epoch 14: loss -3.54190 acc 0.32432 roc_auc 0.27667 prc_auc 0.61834[0m
[92maverage training of epoch 15: loss -3.66724 acc 0.33775 roc_auc 0.37137 prc_auc 0.57290[0m
[93maverage test of epoch 15: loss -3.72953 acc 0.32432 roc_auc 0.19667 prc_auc 0.56945[0m
[92maverage training of epoch 16: loss -3.85256 acc 0.33775 roc_auc 0.37000 prc_auc 0.56962[0m
[93maverage test of epoch 16: loss -3.91386 acc 0.32432 roc_auc 0.15667 prc_auc 0.52329[0m
[92maverage training of epoch 17: loss -4.03489 acc 0.33775 roc_auc 0.36706 prc_auc 0.56419[0m
[93maverage test of epoch 17: loss -4.09552 acc 0.32432 roc_auc 0.12000 prc_auc 0.50330[0m
[92maverage training of epoch 18: loss -4.21478 acc 0.33775 roc_auc 0.36784 prc_auc 0.56469[0m
[93maverage test of epoch 18: loss -4.27500 acc 0.32432 roc_auc 0.10333 prc_auc 0.49893[0m
[92maverage training of epoch 19: loss -4.39267 acc 0.33775 roc_auc 0.36863 prc_auc 0.56464[0m
[93maverage test of epoch 19: loss -4.45268 acc 0.32432 roc_auc 0.09167 prc_auc 0.49547[0m
[92maverage training of epoch 20: loss -4.56891 acc 0.33775 roc_auc 0.37029 prc_auc 0.56581[0m
[93maverage test of epoch 20: loss -4.62884 acc 0.32432 roc_auc 0.09667 prc_auc 0.49947[0m
[92maverage training of epoch 21: loss -4.74375 acc 0.33775 roc_auc 0.37118 prc_auc 0.56626[0m
[93maverage test of epoch 21: loss -4.80374 acc 0.32432 roc_auc 0.09000 prc_auc 0.49387[0m
[92maverage training of epoch 22: loss -4.91743 acc 0.33775 roc_auc 0.37176 prc_auc 0.56665[0m
[93maverage test of epoch 22: loss -4.97758 acc 0.32432 roc_auc 0.08333 prc_auc 0.49789[0m
[92maverage training of epoch 23: loss -5.09012 acc 0.33775 roc_auc 0.37176 prc_auc 0.56646[0m
[93maverage test of epoch 23: loss -5.15050 acc 0.32432 roc_auc 0.08167 prc_auc 0.50284[0m
[92maverage training of epoch 24: loss -5.26196 acc 0.33775 roc_auc 0.37255 prc_auc 0.56702[0m
[93maverage test of epoch 24: loss -5.32266 acc 0.32432 roc_auc 0.08667 prc_auc 0.50010[0m
[92maverage training of epoch 25: loss -5.43309 acc 0.33775 roc_auc 0.37333 prc_auc 0.56758[0m
[93maverage test of epoch 25: loss -5.49415 acc 0.32432 roc_auc 0.08333 prc_auc 0.50527[0m
[92maverage training of epoch 26: loss -5.60360 acc 0.33775 roc_auc 0.37412 prc_auc 0.56838[0m
[93maverage test of epoch 26: loss -5.66507 acc 0.32432 roc_auc 0.08500 prc_auc 0.49953[0m
[92maverage training of epoch 27: loss -5.77358 acc 0.33775 roc_auc 0.37422 prc_auc 0.56841[0m
[93maverage test of epoch 27: loss -5.83551 acc 0.32432 roc_auc 0.09833 prc_auc 0.52162[0m
[92maverage training of epoch 28: loss -5.94310 acc 0.33775 roc_auc 0.37412 prc_auc 0.56847[0m
[93maverage test of epoch 28: loss -6.00552 acc 0.32432 roc_auc 0.09333 prc_auc 0.51589[0m
[92maverage training of epoch 29: loss -6.11223 acc 0.33775 roc_auc 0.37422 prc_auc 0.56826[0m
[93maverage test of epoch 29: loss -6.17518 acc 0.32432 roc_auc 0.08333 prc_auc 0.50858[0m
[92maverage training of epoch 30: loss -6.28102 acc 0.33775 roc_auc 0.37471 prc_auc 0.56866[0m
[93maverage test of epoch 30: loss -6.34451 acc 0.32432 roc_auc 0.10167 prc_auc 0.51462[0m
[92maverage training of epoch 31: loss -6.44952 acc 0.33775 roc_auc 0.37471 prc_auc 0.56868[0m
[93maverage test of epoch 31: loss -6.51357 acc 0.32432 roc_auc 0.09167 prc_auc 0.50871[0m
[92maverage training of epoch 32: loss -6.61776 acc 0.33775 roc_auc 0.37490 prc_auc 0.56898[0m
[93maverage test of epoch 32: loss -6.68240 acc 0.32432 roc_auc 0.11167 prc_auc 0.52143[0m
[92maverage training of epoch 33: loss -6.78578 acc 0.33775 roc_auc 0.37529 prc_auc 0.56942[0m
[93maverage test of epoch 33: loss -6.85102 acc 0.32432 roc_auc 0.09667 prc_auc 0.51960[0m
[92maverage training of epoch 34: loss -6.95361 acc 0.33775 roc_auc 0.37529 prc_auc 0.56942[0m
[93maverage test of epoch 34: loss -7.01946 acc 0.32432 roc_auc 0.11000 prc_auc 0.52275[0m
[92maverage training of epoch 35: loss -7.12126 acc 0.33775 roc_auc 0.37529 prc_auc 0.56942[0m
[93maverage test of epoch 35: loss -7.18775 acc 0.32432 roc_auc 0.10667 prc_auc 0.53618[0m
[92maverage training of epoch 36: loss -7.28877 acc 0.33775 roc_auc 0.37549 prc_auc 0.56951[0m
[93maverage test of epoch 36: loss -7.35589 acc 0.32432 roc_auc 0.14667 prc_auc 0.53791[0m
[92maverage training of epoch 37: loss -7.45616 acc 0.33775 roc_auc 0.37549 prc_auc 0.56951[0m
[93maverage test of epoch 37: loss -7.52392 acc 0.32432 roc_auc 0.15167 prc_auc 0.52960[0m
[92maverage training of epoch 38: loss -7.62343 acc 0.33775 roc_auc 0.37549 prc_auc 0.56951[0mUsing backend: pytorch
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
//home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

[93maverage test of epoch 38: loss -7.69185 acc 0.32432 roc_auc 0.16333 prc_auc 0.53901[0m
[92maverage training of epoch 39: loss -7.79060 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 39: loss -7.85968 acc 0.32432 roc_auc 0.14833 prc_auc 0.54509[0m
[92maverage training of epoch 40: loss -7.95768 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 40: loss -8.02743 acc 0.32432 roc_auc 0.22333 prc_auc 0.56021[0m
[92maverage training of epoch 41: loss -8.12469 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 41: loss -8.19511 acc 0.32432 roc_auc 0.22833 prc_auc 0.56335[0m
[92maverage training of epoch 42: loss -8.29164 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 42: loss -8.36273 acc 0.32432 roc_auc 0.26167 prc_auc 0.58598[0m
[92maverage training of epoch 43: loss -8.45853 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 43: loss -8.53030 acc 0.32432 roc_auc 0.25500 prc_auc 0.59794[0m
[92maverage training of epoch 44: loss -8.62536 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 44: loss -8.69782 acc 0.32432 roc_auc 0.33500 prc_auc 0.61112[0m
[92maverage training of epoch 45: loss -8.79215 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 45: loss -8.86530 acc 0.32432 roc_auc 0.24000 prc_auc 0.61369[0m
[92maverage training of epoch 46: loss -8.95890 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 46: loss -9.03274 acc 0.32432 roc_auc 0.28667 prc_auc 0.59655[0m
[92maverage training of epoch 47: loss -9.12562 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 47: loss -9.20014 acc 0.32432 roc_auc 0.52167 prc_auc 0.68637[0m
[92maverage training of epoch 48: loss -9.29231 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 48: loss -9.36752 acc 0.32432 roc_auc 0.58667 prc_auc 0.72188[0m
[92maverage training of epoch 49: loss -9.45896 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 49: loss -9.53488 acc 0.32432 roc_auc 0.38000 prc_auc 0.62284[0m
[92maverage training of epoch 50: loss -9.62560 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 50: loss -9.70221 acc 0.32432 roc_auc 0.24000 prc_auc 0.58703[0m
[92maverage training of epoch 51: loss -9.79221 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 51: loss -9.86952 acc 0.32432 roc_auc 0.54000 prc_auc 0.73560[0m
[92maverage training of epoch 52: loss -9.95881 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 52: loss -10.03681 acc 0.32432 roc_auc 0.20500 prc_auc 0.59159[0m
[92maverage training of epoch 53: loss -10.12539 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 53: loss -10.20409 acc 0.32432 roc_auc 0.46667 prc_auc 0.65903[0m
[92maverage training of epoch 54: loss -10.29195 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 54: loss -10.37136 acc 0.32432 roc_auc 0.54500 prc_auc 0.70999[0m
[92maverage training of epoch 55: loss -10.45851 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 55: loss -10.53861 acc 0.32432 roc_auc 0.42000 prc_auc 0.64369[0m
[92maverage training of epoch 56: loss -10.62505 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 56: loss -10.70586 acc 0.32432 roc_auc 0.36167 prc_auc 0.63082[0m
[92maverage training of epoch 57: loss -10.79158 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 57: loss -10.87309 acc 0.32432 roc_auc 0.25333 prc_auc 0.58360[0m
[92maverage training of epoch 58: loss -10.95810 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 58: loss -11.04031 acc 0.32432 roc_auc 0.66000 prc_auc 0.76417[0m
[92maverage training of epoch 59: loss -11.12461 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 59: loss -11.20753 acc 0.32432 roc_auc 0.31500 prc_auc 0.64703[0m
[92maverage training of epoch 60: loss -11.29111 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 60: loss -11.37474 acc 0.32432 roc_auc 0.37500 prc_auc 0.63096[0m
[92maverage training of epoch 61: loss -11.45761 acc 0.33775 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 61: loss -11.54194 acc 0.32432 roc_auc 0.45000 prc_auc 0.65498[0m
[92maverage training of epoch 62: loss -11.62411 acc 0.31788 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 62: loss -11.70914 acc 0.67568 roc_auc 0.48000 prc_auc 0.66703[0m
[92maverage training of epoch 63: loss -11.79060 acc 0.62252 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 63: loss -11.87634 acc 0.67568 roc_auc 0.38000 prc_auc 0.63113[0m
[92maverage training of epoch 64: loss -11.95708 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 64: loss -12.04353 acc 0.67568 roc_auc 0.57000 prc_auc 0.72286[0m
[92maverage training of epoch 65: loss -12.12356 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 65: loss -12.21071 acc 0.67568 roc_auc 0.35333 prc_auc 0.63456[0m
[92maverage training of epoch 66: loss -12.29004 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 66: loss -12.37789 acc 0.67568 roc_auc 0.61333 prc_auc 0.74756[0m
[92maverage training of epoch 67: loss -12.45651 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 67: loss -12.54507 acc 0.67568 roc_auc 0.41833 prc_auc 0.69005[0m
[92maverage training of epoch 68: loss -12.62298 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 68: loss -12.71225 acc 0.67568 roc_auc 0.64000 prc_auc 0.75473[0m
[92maverage training of epoch 69: loss -12.78944 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 69: loss -12.87942 acc 0.67568 roc_auc 0.66667 prc_auc 0.76480[0m
[92maverage training of epoch 70: loss -12.95590 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 70: loss -13.04659 acc 0.67568 roc_auc 0.37667 prc_auc 0.63267[0m
[92maverage training of epoch 71: loss -13.12236 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 71: loss -13.21376 acc 0.67568 roc_auc 0.48000 prc_auc 0.66703[0m
[92maverage training of epoch 72: loss -13.28882 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 72: loss -13.38092 acc 0.67568 roc_auc 0.53000 prc_auc 0.70116[0m
[92maverage training of epoch 73: loss -13.45528 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 73: loss -13.54809 acc 0.67568 roc_auc 0.39667 prc_auc 0.65712[0m
[92maverage training of epoch 74: loss -13.62173 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 74: loss -13.71525 acc 0.67568 roc_auc 0.81833 prc_auc 0.85912[0m
[92maverage training of epoch 75: loss -13.78819 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 75: loss -13.88241 acc 0.67568 roc_auc 0.70500 prc_auc 0.80530[0m
[92maverage training of epoch 76: loss -13.95464 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 76: loss -14.04957 acc 0.67568 roc_auc 0.62000 prc_auc 0.75852[0m
[92maverage training of epoch 77: loss -14.12109 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 77: loss -14.21672 acc 0.67568 roc_auc 0.55833 prc_auc 0.70721[0m
[92maverage training of epoch 78: loss -14.28753 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 78: loss -14.38388 acc 0.67568 roc_auc 0.34333 prc_auc 0.61310[0m
[92maverage training of epoch 79: loss -14.45398 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 79: loss -14.55104 acc 0.67568 roc_auc 0.70833 prc_auc 0.80870[0m
[92maverage training of epoch 80: loss -14.62043 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 80: loss -14.71819 acc 0.67568 roc_auc 0.36000 prc_auc 0.63004[0m
[92maverage training of epoch 81: loss -14.78688 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 81: loss -14.88535 acc 0.67568 roc_auc 0.57000 prc_auc 0.75590[0m
[92maverage training of epoch 82: loss -14.95332 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 82: loss -15.05250 acc 0.67568 roc_auc 0.30000 prc_auc 0.63923[0m
[92maverage training of epoch 83: loss -15.11976 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 83: loss -15.21965 acc 0.67568 roc_auc 0.44667 prc_auc 0.68935[0m
[92maverage training of epoch 84: loss -15.28621 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 84: loss -15.38680 acc 0.67568 roc_auc 0.33167 prc_auc 0.62675[0m
[92maverage training of epoch 85: loss -15.45265 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 85: loss -15.55396 acc 0.67568 roc_auc 0.46000 prc_auc 0.67614[0m
[92maverage training of epoch 86: loss -15.61910 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 86: loss -15.72111 acc 0.67568 roc_auc 0.60000 prc_auc 0.72396[0m
[92maverage training of epoch 87: loss -15.78554 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 87: loss -15.88826 acc 0.67568 roc_auc 0.82667 prc_auc 0.88094[0m
[92maverage training of epoch 88: loss -15.95198 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 88: loss -16.05541 acc 0.67568 roc_auc 0.80333 prc_auc 0.85701[0m
[92maverage training of epoch 89: loss -16.11842 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 89: loss -16.22256 acc 0.67568 roc_auc 0.42833 prc_auc 0.68939[0m
[92maverage training of epoch 90: loss -16.28487 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 90: loss -16.38971 acc 0.67568 roc_auc 0.32333 prc_auc 0.62269[0m
[92maverage training of epoch 91: loss -16.45131 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 91: loss -16.55687 acc 0.67568 roc_auc 0.25167 prc_auc 0.66258[0m
[92maverage training of epoch 92: loss -16.61775 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 92: loss -16.72402 acc 0.67568 roc_auc 0.38333 prc_auc 0.66979[0m
[92maverage training of epoch 93: loss -16.78420 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 93: loss -16.89117 acc 0.67568 roc_auc 0.55000 prc_auc 0.71645[0m
[92maverage training of epoch 94: loss -16.95064 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 94: loss -17.05832 acc 0.67568 roc_auc 0.53000 prc_auc 0.74696[0m
[92maverage training of epoch 95: loss -17.11708 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 95: loss -17.22547 acc 0.67568 roc_auc 0.64000 prc_auc 0.75684[0m
[92maverage training of epoch 96: loss -17.28352 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 96: loss -17.39262 acc 0.67568 roc_auc 0.60000 prc_auc 0.72832[0m
[92maverage training of epoch 97: loss -17.44997 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 97: loss -17.55977 acc 0.67568 roc_auc 0.64500 prc_auc 0.76182[0m
[92maverage training of epoch 98: loss -17.61641 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 98: loss -17.72692 acc 0.67568 roc_auc 0.36000 prc_auc 0.63335[0m
[92maverage training of epoch 99: loss -17.78285 acc 0.66225 roc_auc 0.37569 prc_auc 0.56962[0m
[93maverage test of epoch 99: loss -17.89407 acc 0.67568 roc_auc 0.44000 prc_auc 0.65816[0m
Training model with dataset, testing using fold 4
use LSTM
{'dfs_code_rnn': RNN(
  (input): Linear(in_features=76, out_features=8, bias=True)
  (rnn): LSTM(8, 4, batch_first=True, dropout=0.2)
), 'output_layer': MLP_layers(
  (mlp): Sequential(
    (Linear0): Linear(in_features=4, out_features=2, bias=True)
  )
)}
DfscodeRnn_cls_LSTM args.lr: 0.0003
[92maverage training of epoch 0: loss -0.14750 acc 0.66225 roc_auc 0.38549 prc_auc 0.59222[0m
[93maverage test of epoch 0: loss -0.42306 acc 0.67568 roc_auc 0.18000 prc_auc 0.52480[0m
[92maverage training of epoch 1: loss -0.68322 acc 0.66225 roc_auc 0.39490 prc_auc 0.60005[0m
[93maverage test of epoch 1: loss -1.05760 acc 0.67568 roc_auc 0.57333 prc_auc 0.80280[0m
[92maverage training of epoch 2: loss -1.26656 acc 0.66225 roc_auc 0.40098 prc_auc 0.60713[0m
[93maverage test of epoch 2: loss -1.54508 acc 0.67568 roc_auc 0.66000 prc_auc 0.85408[0m
[92maverage training of epoch 3: loss -1.67854 acc 0.66225 roc_auc 0.41863 prc_auc 0.62638[0m
[93maverage test of epoch 3: loss -1.90736 acc 0.67568 roc_auc 0.76000 prc_auc 0.90500[0m
[92maverage training of epoch 4: loss -2.03222 acc 0.66225 roc_auc 0.43529 prc_auc 0.64446[0m
[93maverage test of epoch 4: loss -2.26128 acc 0.67568 roc_auc 0.84000 prc_auc 0.93466[0m
[92maverage training of epoch 5: loss -2.38704 acc 0.66225 roc_auc 0.43696 prc_auc 0.64189[0m
[93maverage test of epoch 5: loss -2.61674 acc 0.67568 roc_auc 0.86000 prc_auc 0.94180[0m
[92maverage training of epoch 6: loss -2.72921 acc 0.66225 roc_auc 0.43049 prc_auc 0.64021[0m
[93maverage test of epoch 6: loss -2.94426 acc 0.67568 roc_auc 0.93333 prc_auc 0.97438[0m
[92maverage training of epoch 7: loss -3.03337 acc 0.66225 roc_auc 0.41520 prc_auc 0.62773[0m
[93maverage test of epoch 7: loss -3.22904 acc 0.67568 roc_auc 0.93000 prc_auc 0.97054[0m
[92maverage training of epoch 8: loss -3.29868 acc 0.66225 roc_auc 0.40725 prc_auc 0.62038[0m
[93maverage test of epoch 8: loss -3.48073 acc 0.67568 roc_auc 0.91500 prc_auc 0.96114[0m
[92maverage training of epoch 9: loss -3.53715 acc 0.66225 roc_auc 0.39951 prc_auc 0.61378[0m
[93maverage test of epoch 9: loss -3.71098 acc 0.67568 roc_auc 0.90333 prc_auc 0.95302[0m
[92maverage training of epoch 10: loss -3.75825 acc 0.66225 roc_auc 0.39284 prc_auc 0.60904[0m
[93maverage test of epoch 10: loss -3.92702 acc 0.67568 roc_auc 0.89333 prc_auc 0.94427[0m
[92maverage training of epoch 11: loss -3.96758 acc 0.66225 roc_auc 0.38725 prc_auc 0.60417[0m
[93maverage test of epoch 11: loss -4.13311 acc 0.67568 roc_auc 0.87667 prc_auc 0.93099[0m
[92maverage training of epoch 12: loss -4.16852 acc 0.66225 roc_auc 0.38451 prc_auc 0.60117[0m
[93maverage test of epoch 12: loss -4.33191 acc 0.67568 roc_auc 0.85333 prc_auc 0.91497[0m
[92maverage training of epoch 13: loss -4.36319 acc 0.66225 roc_auc 0.38304 prc_auc 0.60084[0m
[93maverage test of epoch 13: loss -4.52516 acc 0.67568 roc_auc 0.82333 prc_auc 0.89579[0m
[92maverage training of epoch 14: loss -4.55305 acc 0.66225 roc_auc 0.38314 prc_auc 0.60148[0m
[93maverage test of epoch 14: loss -4.71409 acc 0.67568 roc_auc 0.80000 prc_auc 0.88655[0m
[92maverage training of epoch 15: loss -4.73912 acc 0.66225 roc_auc 0.37765 prc_auc 0.58632[0m
[93maverage test of epoch 15: loss -4.89959 acc 0.67568 roc_auc 0.81333 prc_auc 0.88121[0m
[92maverage training of epoch 16: loss -4.92215 acc 0.66225 roc_auc 0.37627 prc_auc 0.58506[0m
[93maverage test of epoch 16: loss -5.08229 acc 0.67568 roc_auc 0.76667 prc_auc 0.85828[0m
[92maverage training of epoch 17: loss -5.10271 acc 0.66225 roc_auc 0.37627 prc_auc 0.58602[0m
[93maverage test of epoch 17: loss -5.26272 acc 0.67568 roc_auc 0.73000 prc_auc 0.83576[0m
[92maverage training of epoch 18: loss -5.28122 acc 0.66225 roc_auc 0.37696 prc_auc 0.58685[0m
[93maverage test of epoch 18: loss -5.44125 acc 0.67568 roc_auc 0.76000 prc_auc 0.83986[0m
[92maverage training of epoch 19: loss -5.45803 acc 0.66225 roc_auc 0.37765 prc_auc 0.58863[0m
[93maverage test of epoch 19: loss -5.61820 acc 0.67568 roc_auc 0.74000 prc_auc 0.83135[0m
[92maverage training of epoch 20: loss -5.63342 acc 0.66225 roc_auc 0.37716 prc_auc 0.58836[0m
[93maverage test of epoch 20: loss -5.79381 acc 0.67568 roc_auc 0.58667 prc_auc 0.71638[0m
[92maverage training of epoch 21: loss -5.80760 acc 0.66225 roc_auc 0.37686 prc_auc 0.58817[0m
[93maverage test of epoch 21: loss -5.96830 acc 0.67568 roc_auc 0.60667 prc_auc 0.72641[0m
[92maverage training of epoch 22: loss -5.98076 acc 0.66225 roc_auc 0.37755 prc_auc 0.58974[0m
[93maverage test of epoch 22: loss -6.14182 acc 0.67568 roc_auc 0.69333 prc_auc 0.77828[0m
[92maverage training of epoch 23: loss -6.15305 acc 0.66225 roc_auc 0.37794 prc_auc 0.58999[0m
[93maverage test of epoch 23: loss -6.31452 acc 0.67568 roc_auc 0.56500 prc_auc 0.70560[0m
[92maverage training of epoch 24: loss -6.32458 acc 0.66225 roc_auc 0.37843 prc_auc 0.59030[0m
[93maverage test of epoch 24: loss -6.48652 acc 0.67568 roc_auc 0.48000 prc_auc 0.66703[0m
[92maverage training of epoch 25: loss -6.49548 acc 0.66225 roc_auc 0.37804 prc_auc 0.58961[0m
[93maverage test of epoch 25: loss -6.65790 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 26: loss -6.66581 acc 0.66225 roc_auc 0.37853 prc_auc 0.59060[0m
[93maverage test of epoch 26: loss -6.82876 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 27: loss -6.83567 acc 0.66225 roc_auc 0.37833 prc_auc 0.59107[0m
[93maverage test of epoch 27: loss -6.99916 acc 0.67568 roc_auc 0.58667 prc_auc 0.71638[0m
[92maverage training of epoch 28: loss -7.00511 acc 0.66225 roc_auc 0.37804 prc_auc 0.59110[0m
[93maverage test of epoch 28: loss -7.16917 acc 0.67568 roc_auc 0.48000 prc_auc 0.66703[0m
[92maverage training of epoch 29: loss -7.17418 acc 0.66225 roc_auc 0.37784 prc_auc 0.59052[0m
[93maverage test of epoch 29: loss -7.33884 acc 0.67568 roc_auc 0.60667 prc_auc 0.72641[0m
[92maverage training of epoch 30: loss -7.34294 acc 0.66225 roc_auc 0.37775 prc_auc 0.59022[0m
[93maverage test of epoch 30: loss -7.50820 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 31: loss -7.51143 acc 0.66225 roc_auc 0.37735 prc_auc 0.58967[0m
[93maverage test of epoch 31: loss -7.67730 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 32: loss -7.67967 acc 0.66225 roc_auc 0.37647 prc_auc 0.58393[0m
[93maverage test of epoch 32: loss -7.84617 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 33: loss -7.84770 acc 0.66225 roc_auc 0.37618 prc_auc 0.58247[0m
[93maverage test of epoch 33: loss -8.01485 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 34: loss -8.01555 acc 0.66225 roc_auc 0.37627 prc_auc 0.58234[0m
[93maverage test of epoch 34: loss -8.18334 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 35: loss -8.18324 acc 0.66225 roc_auc 0.37539 prc_auc 0.57465[0m
[93maverage test of epoch 35: loss -8.35168 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 36: loss -8.35078 acc 0.66225 roc_auc 0.37451 prc_auc 0.57288[0m
[93maverage test of epoch 36: loss -8.51989 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 37: loss -8.51820 acc 0.66225 roc_auc 0.37333 prc_auc 0.57150[0m
[93maverage test of epoch 37: loss -8.68798 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 38: loss -8.68551 acc 0.66225 roc_auc 0.37343 prc_auc 0.57108[0m
[93maverage test of epoch 38: loss -8.85596 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 39: loss -8.85273 acc 0.66225 roc_auc 0.37304 prc_auc 0.57055[0m
[93maverage test of epoch 39: loss -9.02385 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 40: loss -9.01985 acc 0.66225 roc_auc 0.37304 prc_auc 0.57120[0m
[93maverage test of epoch 40: loss -9.19167 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 41: loss -9.18691 acc 0.66225 roc_auc 0.37265 prc_auc 0.57137[0m
[93maverage test of epoch 41: loss -9.35940 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 42: loss -9.35390 acc 0.66225 roc_auc 0.37216 prc_auc 0.57085[0m
[93maverage test of epoch 42: loss -9.52708 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 43: loss -9.52082 acc 0.66225 roc_auc 0.37216 prc_auc 0.57042[0m
[93maverage test of epoch 43: loss -9.69470 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 44: loss -9.68770 acc 0.66225 roc_auc 0.37196 prc_auc 0.57059[0m
[93maverage test of epoch 44: loss -9.86227 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 45: loss -9.85453 acc 0.66225 roc_auc 0.37118 prc_auc 0.56998[0m
[93maverage test of epoch 45: loss -10.02979 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 46: loss -10.02132 acc 0.66225 roc_auc 0.37098 prc_auc 0.56994[0m
[93maverage test of epoch 46: loss -10.19728 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 47: loss -10.18808 acc 0.66225 roc_auc 0.37078 prc_auc 0.56919[0m
[93maverage test of epoch 47: loss -10.36473 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 48: loss -10.35480 acc 0.66225 roc_auc 0.37088 prc_auc 0.57076[0m
[93maverage test of epoch 48: loss -10.53215 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 49: loss -10.52149 acc 0.66225 roc_auc 0.37098 prc_auc 0.57062[0m
[93maverage test of epoch 49: loss -10.69954 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 50: loss -10.68816 acc 0.66225 roc_auc 0.37039 prc_auc 0.57012[0m
[93maverage test of epoch 50: loss -10.86691 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 51: loss -10.85480 acc 0.66225 roc_auc 0.37029 prc_auc 0.56913[0m
[93maverage test of epoch 51: loss -11.03426 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 52: loss -11.02143 acc 0.66225 roc_auc 0.37020 prc_auc 0.56966[0m
[93maverage test of epoch 52: loss -11.20159 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 53: loss -11.18804 acc 0.66225 roc_auc 0.37029 prc_auc 0.56956[0m
[93maverage test of epoch 53: loss -11.36890 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 54: loss -11.35464 acc 0.66225 roc_auc 0.37039 prc_auc 0.56973[0m
[93maverage test of epoch 54: loss -11.53620 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 55: loss -11.52122 acc 0.66225 roc_auc 0.36980 prc_auc 0.56880[0m
[93maverage test of epoch 55: loss -11.70349 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 56: loss -11.68779 acc 0.66225 roc_auc 0.36941 prc_auc 0.56983[0m
[93maverage test of epoch 56: loss -11.87076 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 57: loss -11.85435 acc 0.66225 roc_auc 0.36931 prc_auc 0.56889[0m
[93maverage test of epoch 57: loss -12.03802 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 58: loss -12.02089 acc 0.66225 roc_auc 0.36922 prc_auc 0.56889[0m
[93maverage test of epoch 58: loss -12.20527 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 59: loss -12.18743 acc 0.66225 roc_auc 0.37020 prc_auc 0.57041[0m
[93maverage test of epoch 59: loss -12.37251 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 60: loss -12.35395 acc 0.66225 roc_auc 0.37029 prc_auc 0.56961[0m
[93maverage test of epoch 60: loss -12.53974 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 61: loss -12.52047 acc 0.66225 roc_auc 0.36922 prc_auc 0.56923[0m
[93maverage test of epoch 61: loss -12.70697 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 62: loss -12.68698 acc 0.66225 roc_auc 0.36882 prc_auc 0.57085[0m
[93maverage test of epoch 62: loss -12.87419 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 63: loss -12.85349 acc 0.66225 roc_auc 0.36980 prc_auc 0.56892[0m
[93maverage test of epoch 63: loss -13.04140 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 64: loss -13.01999 acc 0.66225 roc_auc 0.36922 prc_auc 0.57097[0m
[93maverage test of epoch 64: loss -13.20860 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 65: loss -13.18648 acc 0.66225 roc_auc 0.36892 prc_auc 0.56812[0m
[93maverage test of epoch 65: loss -13.37580 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 66: loss -13.35297 acc 0.66225 roc_auc 0.36922 prc_auc 0.57071[0m
[93maverage test of epoch 66: loss -13.54300 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 67: loss -13.51946 acc 0.66225 roc_auc 0.36843 prc_auc 0.56995[0m
[93maverage test of epoch 67: loss -13.71019 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 68: loss -13.68594 acc 0.66225 roc_auc 0.36961 prc_auc 0.57114[0m
[93maverage test of epoch 68: loss -13.87738 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 69: loss -13.85242 acc 0.66225 roc_auc 0.36912 prc_auc 0.57045[0m
[93maverage test of epoch 69: loss -14.04456 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 70: loss -14.01889 acc 0.66225 roc_auc 0.36922 prc_auc 0.56899[0m
[93maverage test of epoch 70: loss -14.21175 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 71: loss -14.18536 acc 0.66225 roc_auc 0.36892 prc_auc 0.56932[0m
[93maverage test of epoch 71: loss -14.37893 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 72: loss -14.35183 acc 0.66225 roc_auc 0.36873 prc_auc 0.57052[0m
[93maverage test of epoch 72: loss -14.54610 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 73: loss -14.51830 acc 0.66225 roc_auc 0.36765 prc_auc 0.56997[0m
[93maverage test of epoch 73: loss -14.71328 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 74: loss -14.68477 acc 0.66225 roc_auc 0.36775 prc_auc 0.56966[0m
[93maverage test of epoch 74: loss -14.88045 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 75: loss -14.85123 acc 0.66225 roc_auc 0.36873 prc_auc 0.56916[0m
[93maverage test of epoch 75: loss -15.04763 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 76: loss -15.01770 acc 0.66225 roc_auc 0.37000 prc_auc 0.56964[0m
[93maverage test of epoch 76: loss -15.21480 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 77: loss -15.18416 acc 0.66225 roc_auc 0.36725 prc_auc 0.56904[0m
[93maverage test of epoch 77: loss -15.38197 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 78: loss -15.35062 acc 0.66225 roc_auc 0.36745 prc_auc 0.57220[0m
[93maverage test of epoch 78: loss -15.54914 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 79: loss -15.51708 acc 0.66225 roc_auc 0.36951 prc_auc 0.57114[0m
[93maverage test of epoch 79: loss -15.71630 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 80: loss -15.68354 acc 0.66225 roc_auc 0.36990 prc_auc 0.57074[0m
[93maverage test of epoch 80: loss -15.88347 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 81: loss -15.85000 acc 0.66225 roc_auc 0.36824 prc_auc 0.57038[0m
[93maverage test of epoch 81: loss -16.05064 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 82: loss -16.01646 acc 0.66225 roc_auc 0.36892 prc_auc 0.57075[0m
[93maverage test of epoch 82: loss -16.21780 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 83: loss -16.18291 acc 0.66225 roc_auc 0.36784 prc_auc 0.57087[0m
[93maverage test of epoch 83: loss -16.38496 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 84: loss -16.34937 acc 0.66225 roc_auc 0.36902 prc_auc 0.56970[0m
[93maverage test of epoch 84: loss -16.55213 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 85: loss -16.51582 acc 0.66225 roc_auc 0.37010 prc_auc 0.57055[0m
[93maverage test of epoch 85: loss -16.71929 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 86: loss -16.68227 acc 0.66225 roc_auc 0.37010 prc_auc 0.57240[0m
[93maverage test of epoch 86: loss -16.88645 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 87: loss -16.84873 acc 0.66225 roc_auc 0.36706 prc_auc 0.56706[0m
[93maverage test of epoch 87: loss -17.05361 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 88: loss -17.01518 acc 0.66225 roc_auc 0.36892 prc_auc 0.57133[0m
[93maverage test of epoch 88: loss -17.22078 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 89: loss -17.18163 acc 0.66225 roc_auc 0.36784 prc_auc 0.56931[0m
[93maverage test of epoch 89: loss -17.38794 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 90: loss -17.34809 acc 0.66225 roc_auc 0.37216 prc_auc 0.57381[0m
[93maverage test of epoch 90: loss -17.55510 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 91: loss -17.51454 acc 0.66225 roc_auc 0.36833 prc_auc 0.57146[0m
[93maverage test of epoch 91: loss -17.72226 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 92: loss -17.68100 acc 0.66225 roc_auc 0.37088 prc_auc 0.57517[0m
[93maverage test of epoch 92: loss -17.88942 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 93: loss -17.84745 acc 0.66225 roc_auc 0.36902 prc_auc 0.57122[0m
[93maverage test of epoch 93: loss -18.05659 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 94: loss -18.01390 acc 0.66225 roc_auc 0.36510 prc_auc 0.57003[0m
[93maverage test of epoch 94: loss -18.22375 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 95: loss -18.18036 acc 0.66225 roc_auc 0.36529 prc_auc 0.57047[0m
[93maverage test of epoch 95: loss -18.39091 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 96: loss -18.34681 acc 0.66225 roc_auc 0.37098 prc_auc 0.57243[0m
[93maverage test of epoch 96: loss -18.55807 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 97: loss -18.51326 acc 0.66225 roc_auc 0.37010 prc_auc 0.57139[0m
[93maverage test of epoch 97: loss -18.72523 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 98: loss -18.67972 acc 0.66225 roc_auc 0.36627 prc_auc 0.57225[0m
[93maverage test of epoch 98: loss -18.89240 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
[92maverage training of epoch 99: loss -18.84617 acc 0.66225 roc_auc 0.37118 prc_auc 0.57488[0m
[93maverage test of epoch 99: loss -19.05956 acc 0.67568 roc_auc 0.50000 prc_auc 0.67568[0m
Run statistics: 
('note', 'DFScodeRNN_cls_LSTM')
('graph_type', 'MUTAG')
('epochs', 100)
('batch_size', 1)
('num_layers', 1)
('embedding_size_dfscode_rnn', 8)
('hidden_size_dfscode_rnn', 4)
('dfscode_rnn_dropout', 0.2)
('number_of_mlp_layer', 0)
('lr', 0.0003)
('rnn_type', 'LSTM')
('gradient_clipping', True)
('device', device(type='cuda', index=0))


Accuracy (avg): 0.66501 ROC_AUC (avg): 0.54554 PRC_AUC (avg): 0.69431 

Average forward propagation time taken(ms): 2.4605944505672315
Average backward propagation time taken(ms): 0.8648265641318356

