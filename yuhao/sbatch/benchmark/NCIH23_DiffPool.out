# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======


torch.cuda.is_available():  True 


load_data.py load_model_data(): Unserialising pickled dataset into Graph objects
==== Dataset Information ====
== General Information == 
Number of graphs: 2500
Number of classes: 2
Class distribution: 
0:2000 1:500 

== Node information== 
Average number of nodes: 27
Average number of edges (undirected): 29
Max number of nodes: 93
Number of distinct node labels: 18
Average number of distinct node labels: 3
Node labels distribution: 
0:9454 1:6143 13:139 16:2 18:1 2:49955 20:15 21:29 25:5 26:3 3:921 4:570 44:1 49:1 5:70 6:485 8:14 9:4 

*** 3 dataset_features:  {'name': 'NCI-H23', 'num_class': 2, 'label_dict': {'0': 0, '1': 1}, 'have_node_labels': True, 'have_node_attributions': False, 'node_dict': {'0': 0, '1': 1, '13': 2, '16': 3, '18': 4, '2': 5, '20': 6, '21': 7, '25': 8, '26': 9, '3': 10, '4': 11, '44': 12, '49': 13, '5': 14, '6': 15, '8': 16, '9': 17, 'UNKNOWN': 18}, 'feat_dim': 19, 'edge_feat_dim': 0, 'max_num_nodes': 93, 'avg_num_nodes': 27, 'graph_sizes_list': [34, 30, 34, 19, 20, 20, 13, 45, 22, 20, 26, 20, 15, 25, 56, 34, 19, 56, 24, 22, 25, 23, 24, 27, 25, 29, 27, 24, 22, 38, 22, 26, 23, 9, 7, 25, 35, 22, 18, 15, 24, 24, 27, 20, 18, 31, 25, 28, 46, 13, 29, 31, 21, 29, 20, 25, 48, 42, 28, 23, 34, 17, 28, 28, 15, 41, 35, 30, 20, 27, 25, 19, 31, 17, 29, 30, 75, 34, 22, 24, 33, 14, 22, 13, 24, 16, 17, 22, 13, 31, 26, 19, 15, 34, 26, 28, 24, 29, 18, 21, 12, 23, 23, 25, 22, 18, 16, 12, 17, 20, 27, 22, 19, 48, 23, 25, 33, 24, 17, 19, 24, 24, 10, 23, 14, 30, 22, 18, 26, 32, 31, 27, 30, 15, 29, 20, 19, 20, 34, 14, 15, 31, 18, 18, 24, 41, 44, 20, 38, 30, 28, 16, 29, 23, 31, 15, 11, 35, 29, 34, 19, 40, 18, 29, 27, 32, 17, 20, 20, 21, 15, 21, 31, 20, 57, 23, 13, 49, 32, 34, 31, 9, 20, 12, 16, 21, 28, 27, 17, 37, 32, 27, 16, 40, 21, 7, 30, 24, 16, 31, 18, 30, 20, 13, 20, 22, 23, 31, 20, 26, 32, 43, 14, 20, 14, 16, 20, 62, 19, 8, 31, 23, 35, 37, 20, 42, 26, 17, 46, 23, 19, 15, 23, 24, 23, 29, 33, 21, 20, 12, 58, 25, 15, 14, 26, 37, 15, 28, 48, 24, 21, 23, 22, 22, 51, 14, 17, 24, 23, 11, 18, 26, 35, 10, 19, 18, 14, 28, 23, 28, 18, 38, 42, 29, 24, 21, 28, 18, 44, 27, 27, 19, 32, 32, 20, 41, 24, 34, 32, 25, 18, 16, 28, 10, 22, 29, 21, 29, 17, 21, 24, 12, 41, 20, 24, 21, 26, 29, 16, 31, 30, 41, 35, 29, 27, 25, 31, 32, 29, 53, 13, 42, 27, 17, 23, 19, 40, 21, 21, 23, 18, 48, 33, 25, 29, 21, 18, 24, 22, 34, 27, 25, 15, 24, 23, 19, 13, 24, 19, 30, 27, 31, 41, 24, 60, 36, 40, 13, 16, 23, 28, 15, 28, 13, 13, 35, 21, 23, 24, 14, 25, 52, 25, 19, 16, 24, 26, 16, 13, 33, 12, 24, 26, 21, 11, 20, 23, 15, 21, 18, 47, 24, 23, 24, 24, 24, 22, 18, 27, 19, 18, 32, 24, 23, 30, 19, 21, 24, 21, 20, 50, 32, 26, 37, 17, 22, 31, 22, 19, 31, 14, 14, 36, 17, 22, 36, 17, 22, 21, 34, 24, 27, 16, 15, 16, 16, 29, 15, 21, 20, 32, 42, 35, 17, 27, 17, 28, 25, 28, 13, 20, 29, 20, 23, 16, 10, 16, 24, 24, 15, 26, 14, 27, 30, 8, 21, 27, 45, 34, 26, 17, 35, 31, 26, 31, 24, 22, 18, 29, 18, 21, 16, 21, 28, 18, 51, 41, 32, 19, 26, 24, 16, 20, 14, 41, 29, 43, 17, 29, 27, 32, 22, 20, 16, 15, 30, 26, 25, 25, 20, 41, 17, 14, 24, 21, 15, 24, 29, 24, 39, 22, 33, 15, 33, 28, 17, 19, 22, 9, 30, 25, 28, 17, 12, 38, 35, 15, 31, 24, 64, 18, 26, 22, 34, 11, 26, 19, 15, 36, 33, 38, 19, 20, 68, 18, 18, 19, 26, 12, 24, 30, 17, 26, 22, 22, 20, 59, 9, 32, 30, 16, 33, 33, 18, 40, 22, 30, 36, 20, 39, 40, 16, 19, 19, 30, 23, 33, 43, 22, 35, 26, 34, 30, 36, 50, 22, 17, 19, 24, 30, 26, 14, 26, 22, 22, 22, 34, 32, 24, 18, 43, 15, 35, 26, 14, 20, 24, 33, 25, 19, 16, 12, 25, 30, 15, 22, 20, 52, 16, 31, 21, 14, 18, 17, 18, 38, 37, 15, 27, 16, 19, 32, 19, 36, 21, 24, 23, 22, 55, 25, 15, 21, 25, 28, 19, 16, 29, 15, 30, 27, 55, 23, 15, 30, 21, 21, 35, 24, 27, 20, 14, 31, 31, 27, 20, 19, 33, 18, 12, 24, 44, 19, 23, 26, 22, 16, 12, 28, 32, 26, 14, 25, 18, 22, 20, 22, 30, 34, 16, 30, 13, 14, 37, 17, 54, 29, 22, 23, 29, 20, 15, 76, 27, 42, 20, 35, 23, 20, 21, 30, 39, 27, 18, 27, 28, 31, 30, 24, 11, 31, 35, 45, 20, 17, 31, 28, 19, 17, 19, 19, 29, 28, 18, 25, 21, 12, 37, 21, 19, 32, 22, 29, 19, 24, 22, 22, 21, 26, 29, 31, 25, 10, 19, 35, 18, 17, 22, 29, 30, 17, 37, 17, 29, 18, 20, 37, 31, 28, 9, 18, 19, 18, 27, 26, 27, 16, 15, 20, 10, 20, 21, 32, 41, 14, 19, 24, 36, 33, 29, 25, 38, 27, 23, 7, 25, 20, 34, 38, 24, 22, 19, 27, 23, 20, 15, 23, 21, 16, 23, 41, 17, 26, 24, 14, 18, 28, 27, 34, 29, 58, 28, 27, 29, 58, 17, 35, 23, 24, 17, 27, 28, 29, 17, 19, 17, 30, 17, 41, 16, 45, 18, 22, 24, 21, 30, 20, 17, 32, 16, 25, 19, 36, 23, 39, 23, 32, 24, 25, 36, 26, 20, 70, 16, 24, 17, 23, 17, 24, 25, 31, 19, 25, 32, 35, 16, 25, 18, 22, 20, 22, 36, 24, 20, 22, 34, 38, 22, 15, 13, 43, 24, 17, 48, 38, 18, 36, 27, 25, 23, 14, 20, 25, 38, 21, 80, 15, 20, 34, 22, 22, 20, 38, 26, 31, 23, 22, 52, 40, 22, 15, 32, 25, 24, 21, 36, 40, 38, 24, 47, 19, 25, 23, 10, 28, 35, 24, 37, 13, 37, 19, 46, 28, 18, 18, 10, 21, 40, 40, 14, 25, 39, 32, 20, 32, 18, 22, 22, 23, 14, 37, 19, 20, 30, 34, 20, 30, 15, 27, 22, 23, 45, 34, 25, 23, 25, 25, 23, 16, 25, 22, 18, 32, 46, 39, 20, 20, 37, 16, 26, 30, 27, 38, 27, 30, 22, 21, 22, 20, 26, 19, 13, 28, 15, 37, 26, 18, 28, 62, 23, 33, 16, 22, 13, 21, 27, 60, 58, 27, 30, 35, 18, 27, 27, 24, 23, 20, 23, 24, 28, 15, 62, 30, 24, 22, 46, 25, 20, 27, 24, 36, 22, 23, 12, 21, 33, 27, 11, 23, 23, 12, 20, 31, 17, 20, 23, 8, 20, 22, 21, 27, 15, 32, 17, 35, 30, 42, 22, 17, 28, 21, 17, 44, 12, 15, 29, 24, 14, 16, 18, 63, 33, 30, 30, 37, 21, 20, 59, 13, 15, 18, 30, 28, 31, 22, 19, 42, 32, 35, 24, 26, 26, 23, 23, 27, 21, 19, 23, 17, 24, 32, 21, 16, 32, 13, 19, 20, 41, 17, 27, 26, 7, 18, 22, 21, 15, 23, 42, 30, 32, 19, 17, 54, 20, 22, 24, 31, 31, 14, 18, 19, 28, 15, 20, 10, 26, 21, 16, 19, 14, 29, 27, 22, 14, 23, 34, 37, 34, 27, 22, 20, 17, 24, 31, 57, 20, 45, 36, 23, 40, 22, 16, 24, 26, 23, 3, 19, 17, 32, 17, 38, 32, 20, 23, 25, 45, 19, 39, 21, 35, 29, 35, 24, 39, 26, 21, 15, 29, 38, 16, 43, 26, 31, 24, 27, 30, 25, 21, 28, 16, 24, 22, 16, 25, 14, 28, 32, 35, 23, 18, 18, 20, 12, 23, 22, 16, 22, 18, 30, 31, 24, 29, 26, 42, 7, 14, 44, 15, 23, 21, 22, 25, 35, 21, 15, 14, 21, 29, 23, 29, 23, 19, 28, 37, 30, 40, 30, 8, 20, 20, 28, 20, 15, 14, 33, 22, 26, 46, 30, 28, 26, 15, 26, 9, 20, 22, 26, 17, 15, 15, 31, 21, 21, 27, 22, 19, 29, 31, 22, 17, 12, 23, 26, 17, 24, 20, 31, 35, 21, 16, 17, 48, 40, 10, 27, 12, 19, 20, 19, 51, 30, 19, 15, 29, 22, 38, 34, 23, 63, 20, 22, 15, 27, 36, 18, 51, 17, 2, 26, 28, 30, 22, 20, 50, 15, 27, 21, 26, 21, 22, 18, 33, 20, 21, 39, 40, 18, 16, 16, 21, 17, 30, 17, 26, 26, 31, 44, 27, 31, 44, 19, 29, 34, 24, 16, 20, 18, 26, 29, 39, 35, 58, 20, 10, 27, 41, 21, 19, 19, 13, 28, 47, 20, 28, 14, 20, 17, 15, 28, 25, 24, 15, 27, 21, 27, 17, 47, 37, 5, 33, 31, 17, 45, 34, 35, 18, 22, 29, 25, 22, 21, 29, 24, 13, 26, 28, 23, 28, 28, 13, 22, 11, 29, 51, 24, 14, 25, 17, 41, 20, 16, 24, 25, 10, 31, 26, 25, 19, 18, 11, 14, 27, 18, 11, 17, 26, 23, 32, 28, 27, 27, 18, 15, 16, 31, 33, 33, 18, 22, 21, 22, 18, 31, 65, 31, 22, 25, 18, 22, 32, 27, 15, 22, 32, 41, 26, 79, 21, 21, 25, 10, 29, 35, 26, 13, 28, 29, 39, 31, 12, 39, 14, 27, 14, 23, 9, 29, 38, 21, 24, 31, 19, 33, 12, 32, 46, 17, 20, 21, 17, 23, 25, 43, 27, 24, 25, 17, 19, 20, 27, 14, 21, 14, 24, 29, 22, 29, 28, 20, 27, 25, 31, 26, 22, 25, 22, 29, 29, 26, 19, 23, 27, 22, 20, 17, 31, 17, 18, 22, 22, 21, 28, 22, 28, 45, 19, 34, 23, 28, 26, 19, 28, 29, 22, 24, 19, 19, 25, 14, 25, 19, 31, 29, 36, 22, 28, 14, 29, 20, 32, 31, 18, 34, 14, 27, 25, 13, 32, 39, 17, 35, 16, 22, 22, 26, 60, 30, 29, 27, 29, 30, 53, 20, 18, 26, 22, 33, 15, 19, 21, 19, 31, 33, 19, 58, 21, 12, 24, 31, 16, 28, 40, 30, 18, 62, 40, 33, 21, 11, 34, 21, 32, 22, 20, 26, 31, 61, 25, 17, 21, 34, 28, 23, 28, 26, 43, 31, 69, 17, 14, 18, 14, 33, 38, 27, 27, 27, 42, 20, 33, 20, 17, 44, 27, 33, 27, 27, 30, 22, 24, 25, 31, 20, 14, 14, 44, 32, 36, 40, 25, 40, 25, 26, 25, 20, 47, 26, 17, 16, 18, 20, 12, 19, 8, 20, 26, 29, 9, 25, 15, 21, 25, 40, 25, 25, 21, 26, 22, 26, 15, 12, 25, 32, 19, 27, 28, 54, 21, 22, 14, 36, 27, 29, 22, 23, 22, 32, 34, 18, 30, 33, 17, 35, 27, 23, 30, 31, 26, 32, 25, 34, 32, 29, 23, 30, 21, 34, 27, 25, 26, 38, 25, 18, 23, 24, 48, 25, 33, 23, 43, 26, 32, 19, 22, 37, 27, 28, 31, 16, 22, 30, 19, 29, 32, 26, 24, 39, 22, 17, 35, 38, 45, 19, 24, 23, 18, 21, 22, 23, 42, 12, 21, 25, 15, 28, 18, 24, 21, 34, 32, 18, 43, 41, 22, 46, 14, 27, 11, 22, 15, 28, 34, 22, 22, 31, 28, 30, 17, 80, 15, 26, 23, 16, 62, 18, 23, 37, 23, 23, 12, 27, 26, 21, 19, 18, 29, 19, 61, 25, 27, 43, 37, 20, 29, 19, 26, 21, 24, 22, 39, 37, 10, 41, 12, 31, 20, 17, 16, 21, 29, 20, 27, 12, 16, 21, 21, 27, 16, 38, 28, 20, 25, 35, 15, 14, 38, 26, 30, 33, 27, 9, 21, 26, 26, 18, 28, 31, 12, 36, 41, 23, 14, 24, 32, 13, 23, 57, 32, 32, 16, 39, 18, 29, 24, 35, 29, 9, 34, 20, 19, 36, 30, 34, 40, 35, 17, 18, 7, 31, 40, 21, 35, 13, 29, 18, 35, 19, 33, 32, 28, 22, 18, 22, 18, 32, 18, 32, 20, 27, 16, 21, 14, 20, 20, 18, 20, 24, 38, 17, 16, 26, 12, 20, 34, 20, 18, 27, 39, 59, 12, 28, 27, 24, 27, 15, 17, 17, 36, 15, 26, 18, 19, 24, 14, 21, 20, 30, 21, 36, 31, 32, 36, 29, 27, 25, 21, 47, 32, 26, 33, 26, 63, 57, 31, 39, 13, 40, 21, 34, 33, 62, 23, 38, 23, 23, 18, 34, 63, 43, 34, 17, 34, 70, 28, 53, 29, 26, 29, 17, 42, 18, 17, 62, 22, 24, 27, 16, 45, 41, 36, 42, 45, 28, 20, 32, 70, 67, 17, 46, 47, 40, 24, 22, 30, 37, 37, 29, 14, 29, 47, 53, 20, 23, 41, 44, 19, 61, 42, 28, 22, 34, 34, 51, 29, 11, 39, 33, 36, 82, 61, 32, 44, 69, 23, 27, 26, 39, 37, 58, 27, 23, 28, 21, 35, 39, 29, 60, 28, 30, 31, 21, 37, 19, 21, 27, 11, 44, 60, 19, 35, 57, 35, 78, 27, 62, 31, 19, 32, 39, 25, 29, 65, 53, 47, 22, 35, 62, 66, 93, 65, 35, 46, 27, 35, 35, 41, 27, 40, 24, 32, 33, 26, 28, 32, 20, 25, 25, 17, 41, 57, 35, 27, 42, 25, 16, 19, 53, 38, 28, 34, 40, 22, 27, 65, 30, 22, 27, 12, 26, 44, 24, 20, 12, 21, 39, 37, 33, 28, 30, 33, 45, 19, 68, 25, 28, 47, 52, 21, 28, 39, 36, 25, 22, 39, 52, 61, 16, 45, 38, 3, 45, 40, 25, 52, 20, 19, 19, 26, 19, 27, 23, 29, 26, 29, 27, 35, 20, 23, 26, 46, 34, 35, 20, 25, 35, 54, 36, 31, 42, 33, 23, 28, 27, 19, 33, 25, 41, 21, 16, 41, 19, 24, 37, 39, 26, 31, 26, 22, 20, 18, 39, 26, 30, 20, 31, 26, 22, 24, 41, 30, 28, 33, 22, 40, 42, 30, 22, 28, 20, 30, 36, 42, 52, 24, 32, 38, 26, 34, 24, 40, 19, 26, 28, 27, 27, 16, 21, 31, 20, 58, 58, 23, 24, 38, 39, 34, 29, 34, 26, 25, 17, 30, 31, 25, 21, 28, 30, 13, 42, 26, 40, 23, 40, 38, 55, 25, 19, 39, 20, 32, 32, 66, 35, 61, 28, 53, 46, 39, 37, 28, 90, 35, 45, 38, 22, 21, 20, 62, 40, 46, 41, 24, 28, 32, 38, 56, 29, 30, 21, 47, 33, 67, 27, 20, 13, 47, 12, 25, 22, 45, 38, 25, 36, 25, 26, 33, 48, 34, 19, 25, 21, 32, 35, 33, 36, 67, 38, 35, 27, 47, 28, 26, 49, 29, 38, 32, 28, 26, 21, 30, 30, 22, 25, 35, 45, 25, 32, 34, 57, 23, 66, 27, 25, 46, 31, 35, 29, 26, 33, 39, 28, 23, 38, 14, 55, 32, 30, 29, 24, 79, 46, 11, 30, 32, 44, 30, 39, 37, 25, 76, 30, 84, 31, 24, 39, 28, 37, 28, 59, 21, 34, 23, 42, 45, 18, 30, 25, 34, 34, 20, 24, 46, 31, 28, 46, 30, 37, 35, 32, 37, 19, 18, 52, 18, 31, 34, 32, 18, 22, 37, 26, 50, 40, 28, 20, 30, 19, 15, 16, 22, 30, 38, 19, 35, 17, 22, 20, 29, 59, 59, 43, 27, 28, 25, 26, 62, 28, 26, 30, 34, 35], 'attr_dim': 0, 'dataset_info': '==== Dataset Information ====\n== General Information == \nNumber of graphs: 2500\nNumber of classes: 2\nClass distribution: \n0:2000 1:500 \n\n== Node information== \nAverage number of nodes: 27\nAverage number of edges (undirected): 29\nMax number of nodes: 93\nNumber of distinct node labels: 18\nAverage number of distinct node labels: 3\nNode labels distribution: \n0:9454 1:6143 13:139 16:2 18:1 2:49955 20:15 21:29 25:5 26:3 3:921 4:570 44:1 49:1 5:70 6:485 8:14 9:4 \n'}
*** 1 train_index:  [   1    2    3 ... 2497 2498 2499]
*** 2 test_index:  [   0   20   26   28   33   37   38   43   44   50   52   63   66   67
   70   71   75   85   86   87   97  102  103  106  107  111  114  117
  118  124  132  146  150  157  163  165  169  175  181  182  184  194
  198  200  203  207  208  215  216  217  219  221  229  230  231  239
  249  253  265  271  278  285  287  297  302  309  312  313  317  322
  323  329  333  334  335  336  337  342  348  353  354  364  365  366
  367  368  369  377  378  383  384  390  391  400  406  408  415  416
  423  426  435  437  438  443  448  454  463  479  483  485  486  491
  499  510  511  515  521  535  537  539  541  545  548  552  559  561
  568  575  582  591  592  600  606  619  637  644  645  647  650  673
  675  681  686  690  695  701  712  713  714  721  726  727  732  736
  739  741  743  750  752  754  757  761  764  775  778  788  789  797
  802  806  810  811  812  827  829  836  847  866  867  868  871  872
  882  884  885  890  891  892  894  903  914  915  918  922  928  929
  931  936  947  950  952  953  968  972  980  983  986  988 1000 1010
 1011 1014 1016 1022 1030 1031 1032 1039 1040 1050 1056 1057 1059 1064
 1066 1068 1073 1091 1092 1113 1116 1125 1126 1134 1135 1137 1145 1158
 1162 1167 1169 1178 1180 1194 1199 1209 1211 1212 1215 1219 1223 1225
 1233 1234 1238 1243 1252 1256 1257 1259 1260 1267 1273 1278 1279 1285
 1289 1290 1292 1294 1315 1316 1333 1336 1346 1347 1353 1356 1378 1386
 1387 1391 1396 1403 1406 1407 1410 1420 1427 1428 1429 1437 1441 1446
 1453 1455 1456 1458 1463 1465 1475 1476 1477 1482 1488 1496 1503 1505
 1506 1507 1509 1516 1522 1523 1524 1526 1528 1532 1535 1538 1541 1544
 1549 1553 1571 1574 1583 1595 1598 1611 1624 1629 1637 1642 1647 1653
 1655 1663 1667 1668 1673 1674 1676 1678 1689 1696 1698 1702 1707 1721
 1722 1740 1747 1762 1766 1769 1773 1775 1780 1783 1788 1797 1798 1801
 1806 1816 1831 1833 1835 1844 1845 1846 1847 1859 1865 1867 1868 1875
 1879 1883 1885 1898 1906 1907 1914 1915 1929 1932 1946 1947 1948 1955
 1958 1967 1975 1984 1988 1989 1995 1996 2007 2009 2012 2022 2025 2026
 2030 2048 2049 2065 2071 2073 2077 2080 2084 2090 2094 2100 2101 2109
 2111 2126 2142 2143 2150 2152 2153 2159 2169 2172 2175 2176 2178 2179
 2184 2185 2196 2201 2202 2206 2208 2210 2214 2216 2219 2224 2227 2231
 2233 2240 2244 2250 2251 2254 2266 2267 2274 2277 2278 2281 2284 2287
 2293 2297 2298 2303 2305 2315 2319 2331 2334 2336 2337 2338 2349 2357
 2358 2362 2373 2377 2384 2397 2413 2418 2425 2429 2431 2436 2439 2441
 2452 2453 2458 2459 2468 2473 2476 2481 2484 2491]
*** 1 train_index:  [   0    1    2 ... 2497 2498 2499]
*** 2 test_index:  [   3    4    6   11   12   21   22   23   27   29   34   42   47   48
   54   58   62   64   65   76   77   81   82   91   96   98   99  105
  112  113  120  126  128  131  141  142  149  151  153  160  171  176
  178  183  188  190  197  206  212  213  214  223  224  234  238  242
  244  250  251  254  256  261  270  273  276  277  279  281  283  288
  293  300  308  311  328  331  344  349  356  361  362  374  375  380
  382  385  387  389  399  404  405  411  417  418  427  430  445  449
  452  455  456  460  462  465  469  473  475  481  504  505  513  518
  528  532  538  543  544  547  553  562  567  569  581  583  584  599
  601  609  612  616  627  631  632  638  639  641  648  652  656  671
  679  685  687  688  693  696  709  724  734  744  749  753  762  768
  769  770  771  772  785  787  791  792  799  803  807  815  818  821
  828  833  838  842  855  858  880  886  897  900  906  907  908  912
  913  923  924  930  934  938  939  948  949  951  956  958  960  963
  964  966  974  977  981  982  984  991  992  997 1001 1002 1006 1007
 1008 1019 1028 1044 1047 1049 1058 1061 1067 1071 1076 1081 1086 1087
 1088 1090 1097 1108 1110 1112 1119 1140 1148 1152 1155 1156 1157 1163
 1172 1185 1187 1191 1197 1198 1207 1213 1216 1221 1222 1224 1226 1228
 1229 1235 1239 1242 1245 1246 1261 1265 1268 1269 1272 1298 1299 1300
 1309 1310 1312 1313 1319 1326 1327 1331 1339 1341 1345 1352 1354 1362
 1370 1372 1375 1390 1393 1397 1400 1401 1404 1411 1417 1418 1424 1425
 1432 1435 1436 1439 1442 1454 1461 1479 1490 1498 1500 1510 1518 1519
 1529 1530 1531 1537 1550 1551 1559 1565 1577 1578 1586 1587 1597 1599
 1602 1606 1610 1615 1619 1620 1627 1628 1630 1641 1646 1648 1650 1651
 1658 1661 1670 1672 1682 1684 1690 1691 1708 1716 1717 1725 1727 1728
 1732 1733 1739 1743 1745 1754 1770 1771 1779 1781 1785 1790 1791 1793
 1795 1805 1813 1815 1817 1827 1834 1838 1841 1843 1850 1853 1856 1872
 1873 1882 1886 1894 1899 1904 1913 1916 1918 1935 1941 1950 1951 1952
 1957 1969 1972 1973 1979 1983 1994 1999 2005 2010 2014 2028 2031 2038
 2041 2045 2046 2050 2056 2059 2061 2062 2063 2064 2072 2074 2078 2082
 2083 2085 2091 2093 2098 2106 2114 2115 2116 2119 2125 2136 2138 2139
 2141 2144 2160 2174 2182 2190 2195 2197 2205 2211 2212 2218 2225 2230
 2234 2243 2246 2255 2258 2270 2275 2276 2289 2290 2294 2299 2307 2309
 2310 2314 2316 2318 2322 2324 2335 2344 2363 2364 2369 2372 2379 2380
 2381 2385 2386 2393 2395 2400 2405 2406 2409 2410 2412 2415 2420 2422
 2424 2426 2427 2428 2444 2457 2470 2471 2475 2494]
*** 1 train_index:  [   0    1    2 ... 2494 2495 2498]
*** 2 test_index:  [  15   17   25   32   39   40   53   56   59   61   69   73   79   80
   84   88   92   95  100  104  108  116  119  121  122  123  134  135
  137  155  156  159  167  168  173  179  186  187  191  195  196  210
  220  233  243  252  257  264  267  268  269  280  282  286  290  291
  298  304  310  315  324  338  339  345  347  350  370  386  388  392
  394  396  397  403  412  421  422  431  442  457  459  461  466  468
  474  476  488  496  498  503  508  514  529  536  546  550  555  563
  571  573  574  576  579  589  595  598  602  605  610  613  614  615
  624  626  629  630  634  635  655  657  660  664  669  672  680  691
  694  699  700  702  705  706  725  728  733  735  737  738  747  751
  759  773  774  776  777  781  783  784  793  794  796  798  800  805
  809  816  825  830  831  832  834  839  844  846  848  849  851  852
  853  857  860  863  889  899  902  909  917  920  927  933  937  941
  944  957  962  965  969  973  990  999 1003 1005 1013 1026 1033 1035
 1036 1038 1041 1043 1045 1051 1055 1060 1063 1065 1070 1077 1078 1085
 1094 1098 1100 1102 1105 1111 1114 1118 1120 1123 1128 1132 1136 1139
 1142 1149 1154 1165 1170 1174 1183 1186 1188 1192 1196 1201 1202 1203
 1206 1214 1218 1220 1244 1249 1254 1255 1270 1283 1284 1288 1291 1296
 1306 1308 1320 1324 1335 1343 1349 1350 1351 1355 1357 1364 1368 1373
 1374 1376 1377 1379 1382 1395 1402 1412 1426 1430 1440 1443 1444 1445
 1450 1451 1452 1464 1466 1467 1468 1470 1472 1473 1474 1480 1495 1504
 1512 1539 1552 1555 1557 1558 1560 1562 1566 1567 1568 1570 1573 1575
 1581 1585 1589 1593 1601 1605 1612 1621 1625 1638 1640 1645 1649 1652
 1654 1656 1657 1659 1662 1665 1681 1686 1693 1695 1697 1700 1718 1720
 1724 1729 1731 1734 1737 1742 1746 1748 1750 1752 1755 1757 1760 1761
 1765 1777 1778 1782 1784 1792 1799 1803 1808 1809 1810 1812 1814 1819
 1825 1828 1830 1836 1842 1848 1854 1855 1862 1870 1876 1877 1881 1889
 1897 1901 1908 1911 1912 1919 1922 1924 1927 1936 1940 1942 1943 1944
 1953 1968 1974 1976 1977 1982 1990 1998 2000 2002 2004 2015 2024 2036
 2044 2052 2053 2057 2058 2070 2075 2076 2087 2097 2102 2108 2110 2113
 2117 2118 2120 2128 2129 2154 2155 2156 2158 2163 2164 2165 2167 2168
 2170 2180 2181 2186 2188 2189 2192 2193 2199 2207 2209 2217 2220 2228
 2235 2241 2245 2247 2248 2253 2256 2268 2282 2283 2285 2296 2300 2311
 2317 2321 2346 2348 2351 2354 2359 2365 2366 2367 2371 2374 2375 2382
 2389 2390 2391 2408 2417 2421 2423 2435 2443 2446 2449 2454 2456 2465
 2467 2482 2483 2486 2487 2488 2490 2496 2497 2499]
*** 1 train_index:  [   0    2    3 ... 2497 2498 2499]
*** 2 test_index:  [   1    5   10   14   19   24   30   35   36   45   49   51   55   57
   74   78   83  109  125  129  133  140  143  144  145  148  152  154
  158  161  164  166  170  174  180  189  192  199  202  205  211  222
  225  228  232  241  245  246  248  259  272  274  275  284  289  294
  301  306  318  320  325  326  332  340  341  343  351  352  358  363
  379  393  398  401  407  410  414  420  424  428  434  440  441  446
  450  451  453  464  478  480  482  484  487  492  506  509  516  517
  524  525  527  530  534  540  549  551  554  556  560  564  565  566
  577  580  585  586  590  593  611  617  618  620  622  625  628  633
  640  642  651  653  658  661  662  665  668  674  676  678  684  692
  697  698  707  708  710  711  717  720  729  730  731  742  746  755
  760  766  767  779  780  801  808  814  817  819  820  822  824  835
  843  845  861  862  864  870  876  878  881  893  895  896  901  904
  905  911  919  921  932  935  954  959  967  971  978  979  985  987
  993  995  996  998 1009 1015 1021 1029 1037 1042 1046 1054 1062 1074
 1080 1083 1084 1095 1096 1099 1101 1103 1106 1109 1117 1121 1122 1124
 1130 1141 1143 1146 1147 1159 1160 1161 1164 1166 1168 1179 1182 1184
 1190 1200 1204 1208 1217 1230 1236 1237 1247 1248 1250 1251 1253 1258
 1262 1266 1271 1275 1277 1282 1286 1287 1295 1302 1303 1304 1305 1314
 1321 1329 1342 1344 1358 1359 1371 1381 1383 1384 1398 1399 1405 1408
 1414 1416 1419 1421 1422 1423 1434 1448 1449 1457 1459 1478 1484 1487
 1489 1492 1494 1497 1501 1502 1513 1514 1517 1521 1525 1546 1556 1561
 1564 1576 1580 1584 1588 1594 1600 1608 1609 1613 1617 1622 1623 1626
 1632 1633 1636 1643 1660 1664 1666 1669 1675 1687 1692 1699 1701 1705
 1709 1710 1712 1713 1714 1723 1726 1735 1738 1741 1749 1758 1759 1767
 1768 1774 1786 1789 1794 1796 1800 1804 1811 1820 1822 1823 1837 1839
 1849 1852 1860 1863 1864 1866 1871 1874 1878 1884 1890 1896 1902 1903
 1905 1909 1910 1920 1921 1923 1925 1930 1931 1933 1934 1949 1959 1961
 1963 1964 1966 1971 1986 1991 1992 1997 2003 2013 2017 2021 2032 2033
 2035 2039 2047 2054 2055 2066 2067 2069 2081 2088 2092 2096 2099 2107
 2112 2122 2123 2124 2130 2133 2135 2145 2146 2147 2161 2166 2173 2177
 2187 2191 2194 2198 2200 2203 2215 2223 2232 2237 2238 2239 2249 2252
 2257 2261 2263 2264 2265 2271 2272 2273 2286 2291 2295 2301 2306 2308
 2312 2326 2329 2330 2340 2342 2345 2347 2350 2355 2356 2360 2368 2370
 2378 2383 2394 2399 2401 2404 2414 2419 2430 2432 2433 2445 2447 2448
 2451 2455 2464 2472 2477 2479 2480 2485 2489 2495]
*** 1 train_index:  [   0    1    3 ... 2496 2497 2499]
*** 2 test_index:  [   2    7    8    9   13   16   18   31   41   46   60   68   72   89
   90   93   94  101  110  115  127  130  136  138  139  147  162  172
  177  185  193  201  204  209  218  226  227  235  236  237  240  247
  255  258  260  262  263  266  292  295  296  299  303  305  307  314
  316  319  321  327  330  346  355  357  359  360  371  372  373  376
  381  395  402  409  413  419  425  429  432  433  436  439  444  447
  458  467  470  471  472  477  489  490  493  494  495  497  500  501
  502  507  512  519  520  522  523  526  531  533  542  557  558  570
  572  578  587  588  594  596  597  603  604  607  608  621  623  636
  643  646  649  654  659  663  666  667  670  677  682  683  689  703
  704  715  716  718  719  722  723  740  745  748  756  758  763  765
  782  786  790  795  804  813  823  826  837  840  841  850  854  856
  859  865  869  873  874  875  877  879  883  887  888  898  910  916
  925  926  940  942  943  945  946  955  961  970  975  976  989  994
 1004 1012 1017 1018 1020 1023 1024 1025 1027 1034 1048 1052 1053 1069
 1072 1075 1079 1082 1089 1093 1104 1107 1115 1127 1129 1131 1133 1138
 1144 1150 1151 1153 1171 1173 1175 1176 1177 1181 1189 1193 1195 1205
 1210 1227 1231 1232 1240 1241 1263 1264 1274 1276 1280 1281 1293 1297
 1301 1307 1311 1317 1318 1322 1323 1325 1328 1330 1332 1334 1337 1338
 1340 1348 1360 1361 1363 1365 1366 1367 1369 1380 1385 1388 1389 1392
 1394 1409 1413 1415 1431 1433 1438 1447 1460 1462 1469 1471 1481 1483
 1485 1486 1491 1493 1499 1508 1511 1515 1520 1527 1533 1534 1536 1540
 1542 1543 1545 1547 1548 1554 1563 1569 1572 1579 1582 1590 1591 1592
 1596 1603 1604 1607 1614 1616 1618 1631 1634 1635 1639 1644 1671 1677
 1679 1680 1683 1685 1688 1694 1703 1704 1706 1711 1715 1719 1730 1736
 1744 1751 1753 1756 1763 1764 1772 1776 1787 1802 1807 1818 1821 1824
 1826 1829 1832 1840 1851 1857 1858 1861 1869 1880 1887 1888 1891 1892
 1893 1895 1900 1917 1926 1928 1937 1938 1939 1945 1954 1956 1960 1962
 1965 1970 1978 1980 1981 1985 1987 1993 2001 2006 2008 2011 2016 2018
 2019 2020 2023 2027 2029 2034 2037 2040 2042 2043 2051 2060 2068 2079
 2086 2089 2095 2103 2104 2105 2121 2127 2131 2132 2134 2137 2140 2148
 2149 2151 2157 2162 2171 2183 2204 2213 2221 2222 2226 2229 2236 2242
 2259 2260 2262 2269 2279 2280 2288 2292 2302 2304 2313 2320 2323 2325
 2327 2328 2332 2333 2339 2341 2343 2352 2353 2361 2376 2387 2388 2392
 2396 2398 2402 2403 2407 2411 2416 2434 2437 2438 2440 2442 2450 2460
 2461 2462 2463 2466 2469 2474 2478 2492 2493 2498]


config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 50, 'learning_rate': 0.0001, 'seed': 1800, 'k_fold': 5, 'model': 'DiffPool', 'dataset': 'NCI-H23'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'name': 'NCI-H23', 'num_class': 2, 'label_dict': {'0': 0, '1': 1}, 'have_node_labels': True, 'have_node_attributions': False, 'node_dict': {'0': 0, '1': 1, '13': 2, '16': 3, '18': 4, '2': 5, '20': 6, '21': 7, '25': 8, '26': 9, '3': 10, '4': 11, '44': 12, '49': 13, '5': 14, '6': 15, '8': 16, '9': 17, 'UNKNOWN': 18}, 'feat_dim': 19, 'edge_feat_dim': 0, 'max_num_nodes': 93, 'avg_num_nodes': 27, 'graph_sizes_list': [34, 30, 34, 19, 20, 20, 13, 45, 22, 20, 26, 20, 15, 25, 56, 34, 19, 56, 24, 22, 25, 23, 24, 27, 25, 29, 27, 24, 22, 38, 22, 26, 23, 9, 7, 25, 35, 22, 18, 15, 24, 24, 27, 20, 18, 31, 25, 28, 46, 13, 29, 31, 21, 29, 20, 25, 48, 42, 28, 23, 34, 17, 28, 28, 15, 41, 35, 30, 20, 27, 25, 19, 31, 17, 29, 30, 75, 34, 22, 24, 33, 14, 22, 13, 24, 16, 17, 22, 13, 31, 26, 19, 15, 34, 26, 28, 24, 29, 18, 21, 12, 23, 23, 25, 22, 18, 16, 12, 17, 20, 27, 22, 19, 48, 23, 25, 33, 24, 17, 19, 24, 24, 10, 23, 14, 30, 22, 18, 26, 32, 31, 27, 30, 15, 29, 20, 19, 20, 34, 14, 15, 31, 18, 18, 24, 41, 44, 20, 38, 30, 28, 16, 29, 23, 31, 15, 11, 35, 29, 34, 19, 40, 18, 29, 27, 32, 17, 20, 20, 21, 15, 21, 31, 20, 57, 23, 13, 49, 32, 34, 31, 9, 20, 12, 16, 21, 28, 27, 17, 37, 32, 27, 16, 40, 21, 7, 30, 24, 16, 31, 18, 30, 20, 13, 20, 22, 23, 31, 20, 26, 32, 43, 14, 20, 14, 16, 20, 62, 19, 8, 31, 23, 35, 37, 20, 42, 26, 17, 46, 23, 19, 15, 23, 24, 23, 29, 33, 21, 20, 12, 58, 25, 15, 14, 26, 37, 15, 28, 48, 24, 21, 23, 22, 22, 51, 14, 17, 24, 23, 11, 18, 26, 35, 10, 19, 18, 14, 28, 23, 28, 18, 38, 42, 29, 24, 21, 28, 18, 44, 27, 27, 19, 32, 32, 20, 41, 24, 34, 32, 25, 18, 16, 28, 10, 22, 29, 21, 29, 17, 21, 24, 12, 41, 20, 24, 21, 26, 29, 16, 31, 30, 41, 35, 29, 27, 25, 31, 32, 29, 53, 13, 42, 27, 17, 23, 19, 40, 21, 21, 23, 18, 48, 33, 25, 29, 21, 18, 24, 22, 34, 27, 25, 15, 24, 23, 19, 13, 24, 19, 30, 27, 31, 41, 24, 60, 36, 40, 13, 16, 23, 28, 15, 28, 13, 13, 35, 21, 23, 24, 14, 25, 52, 25, 19, 16, 24, 26, 16, 13, 33, 12, 24, 26, 21, 11, 20, 23, 15, 21, 18, 47, 24, 23, 24, 24, 24, 22, 18, 27, 19, 18, 32, 24, 23, 30, 19, 21, 24, 21, 20, 50, 32, 26, 37, 17, 22, 31, 22, 19, 31, 14, 14, 36, 17, 22, 36, 17, 22, 21, 34, 24, 27, 16, 15, 16, 16, 29, 15, 21, 20, 32, 42, 35, 17, 27, 17, 28, 25, 28, 13, 20, 29, 20, 23, 16, 10, 16, 24, 24, 15, 26, 14, 27, 30, 8, 21, 27, 45, 34, 26, 17, 35, 31, 26, 31, 24, 22, 18, 29, 18, 21, 16, 21, 28, 18, 51, 41, 32, 19, 26, 24, 16, 20, 14, 41, 29, 43, 17, 29, 27, 32, 22, 20, 16, 15, 30, 26, 25, 25, 20, 41, 17, 14, 24, 21, 15, 24, 29, 24, 39, 22, 33, 15, 33, 28, 17, 19, 22, 9, 30, 25, 28, 17, 12, 38, 35, 15, 31, 24, 64, 18, 26, 22, 34, 11, 26, 19, 15, 36, 33, 38, 19, 20, 68, 18, 18, 19, 26, 12, 24, 30, 17, 26, 22, 22, 20, 59, 9, 32, 30, 16, 33, 33, 18, 40, 22, 30, 36, 20, 39, 40, 16, 19, 19, 30, 23, 33, 43, 22, 35, 26, 34, 30, 36, 50, 22, 17, 19, 24, 30, 26, 14, 26, 22, 22, 22, 34, 32, 24, 18, 43, 15, 35, 26, 14, 20, 24, 33, 25, 19, 16, 12, 25, 30, 15, 22, 20, 52, 16, 31, 21, 14, 18, 17, 18, 38, 37, 15, 27, 16, 19, 32, 19, 36, 21, 24, 23, 22, 55, 25, 15, 21, 25, 28, 19, 16, 29, 15, 30, 27, 55, 23, 15, 30, 21, 21, 35, 24, 27, 20, 14, 31, 31, 27, 20, 19, 33, 18, 12, 24, 44, 19, 23, 26, 22, 16, 12, 28, 32, 26, 14, 25, 18, 22, 20, 22, 30, 34, 16, 30, 13, 14, 37, 17, 54, 29, 22, 23, 29, 20, 15, 76, 27, 42, 20, 35, 23, 20, 21, 30, 39, 27, 18, 27, 28, 31, 30, 24, 11, 31, 35, 45, 20, 17, 31, 28, 19, 17, 19, 19, 29, 28, 18, 25, 21, 12, 37, 21, 19, 32, 22, 29, 19, 24, 22, 22, 21, 26, 29, 31, 25, 10, 19, 35, 18, 17, 22, 29, 30, 17, 37, 17, 29, 18, 20, 37, 31, 28, 9, 18, 19, 18, 27, 26, 27, 16, 15, 20, 10, 20, 21, 32, 41, 14, 19, 24, 36, 33, 29, 25, 38, 27, 23, 7, 25, 20, 34, 38, 24, 22, 19, 27, 23, 20, 15, 23, 21, 16, 23, 41, 17, 26, 24, 14, 18, 28, 27, 34, 29, 58, 28, 27, 29, 58, 17, 35, 23, 24, 17, 27, 28, 29, 17, 19, 17, 30, 17, 41, 16, 45, 18, 22, 24, 21, 30, 20, 17, 32, 16, 25, 19, 36, 23, 39, 23, 32, 24, 25, 36, 26, 20, 70, 16, 24, 17, 23, 17, 24, 25, 31, 19, 25, 32, 35, 16, 25, 18, 22, 20, 22, 36, 24, 20, 22, 34, 38, 22, 15, 13, 43, 24, 17, 48, 38, 18, 36, 27, 25, 23, 14, 20, 25, 38, 21, 80, 15, 20, 34, 22, 22, 20, 38, 26, 31, 23, 22, 52, 40, 22, 15, 32, 25, 24, 21, 36, 40, 38, 24, 47, 19, 25, 23, 10, 28, 35, 24, 37, 13, 37, 19, 46, 28, 18, 18, 10, 21, 40, 40, 14, 25, 39, 32, 20, 32, 18, 22, 22, 23, 14, 37, 19, 20, 30, 34, 20, 30, 15, 27, 22, 23, 45, 34, 25, 23, 25, 25, 23, 16, 25, 22, 18, 32, 46, 39, 20, 20, 37, 16, 26, 30, 27, 38, 27, 30, 22, 21, 22, 20, 26, 19, 13, 28, 15, 37, 26, 18, 28, 62, 23, 33, 16, 22, 13, 21, 27, 60, 58, 27, 30, 35, 18, 27, 27, 24, 23, 20, 23, 24, 28, 15, 62, 30, 24, 22, 46, 25, 20, 27, 24, 36, 22, 23, 12, 21, 33, 27, 11, 23, 23, 12, 20, 31, 17, 20, 23, 8, 20, 22, 21, 27, 15, 32, 17, 35, 30, 42, 22, 17, 28, 21, 17, 44, 12, 15, 29, 24, 14, 16, 18, 63, 33, 30, 30, 37, 21, 20, 59, 13, 15, 18, 30, 28, 31, 22, 19, 42, 32, 35, 24, 26, 26, 23, 23, 27, 21, 19, 23, 17, 24, 32, 21, 16, 32, 13, 19, 20, 41, 17, 27, 26, 7, 18, 22, 21, 15, 23, 42, 30, 32, 19, 17, 54, 20, 22, 24, 31, 31, 14, 18, 19, 28, 15, 20, 10, 26, 21, 16, 19, 14, 29, 27, 22, 14, 23, 34, 37, 34, 27, 22, 20, 17, 24, 31, 57, 20, 45, 36, 23, 40, 22, 16, 24, 26, 23, 3, 19, 17, 32, 17, 38, 32, 20, 23, 25, 45, 19, 39, 21, 35, 29, 35, 24, 39, 26, 21, 15, 29, 38, 16, 43, 26, 31, 24, 27, 30, 25, 21, 28, 16, 24, 22, 16, 25, 14, 28, 32, 35, 23, 18, 18, 20, 12, 23, 22, 16, 22, 18, 30, 31, 24, 29, 26, 42, 7, 14, 44, 15, 23, 21, 22, 25, 35, 21, 15, 14, 21, 29, 23, 29, 23, 19, 28, 37, 30, 40, 30, 8, 20, 20, 28, 20, 15, 14, 33, 22, 26, 46, 30, 28, 26, 15, 26, 9, 20, 22, 26, 17, 15, 15, 31, 21, 21, 27, 22, 19, 29, 31, 22, 17, 12, 23, 26, 17, 24, 20, 31, 35, 21, 16, 17, 48, 40, 10, 27, 12, 19, 20, 19, 51, 30, 19, 15, 29, 22, 38, 34, 23, 63, 20, 22, 15, 27, 36, 18, 51, 17, 2, 26, 28, 30, 22, 20, 50, 15, 27, 21, 26, 21, 22, 18, 33, 20, 21, 39, 40, 18, 16, 16, 21, 17, 30, 17, 26, 26, 31, 44, 27, 31, 44, 19, 29, 34, 24, 16, 20, 18, 26, 29, 39, 35, 58, 20, 10, 27, 41, 21, 19, 19, 13, 28, 47, 20, 28, 14, 20, 17, 15, 28, 25, 24, 15, 27, 21, 27, 17, 47, 37, 5, 33, 31, 17, 45, 34, 35, 18, 22, 29, 25, 22, 21, 29, 24, 13, 26, 28, 23, 28, 28, 13, 22, 11, 29, 51, 24, 14, 25, 17, 41, 20, 16, 24, 25, 10, 31, 26, 25, 19, 18, 11, 14, 27, 18, 11, 17, 26, 23, 32, 28, 27, 27, 18, 15, 16, 31, 33, 33, 18, 22, 21, 22, 18, 31, 65, 31, 22, 25, 18, 22, 32, 27, 15, 22, 32, 41, 26, 79, 21, 21, 25, 10, 29, 35, 26, 13, 28, 29, 39, 31, 12, 39, 14, 27, 14, 23, 9, 29, 38, 21, 24, 31, 19, 33, 12, 32, 46, 17, 20, 21, 17, 23, 25, 43, 27, 24, 25, 17, 19, 20, 27, 14, 21, 14, 24, 29, 22, 29, 28, 20, 27, 25, 31, 26, 22, 25, 22, 29, 29, 26, 19, 23, 27, 22, 20, 17, 31, 17, 18, 22, 22, 21, 28, 22, 28, 45, 19, 34, 23, 28, 26, 19, 28, 29, 22, 24, 19, 19, 25, 14, 25, 19, 31, 29, 36, 22, 28, 14, 29, 20, 32, 31, 18, 34, 14, 27, 25, 13, 32, 39, 17, 35, 16, 22, 22, 26, 60, 30, 29, 27, 29, 30, 53, 20, 18, 26, 22, 33, 15, 19, 21, 19, 31, 33, 19, 58, 21, 12, 24, 31, 16, 28, 40, 30, 18, 62, 40, 33, 21, 11, 34, 21, 32, 22, 20, 26, 31, 61, 25, 17, 21, 34, 28, 23, 28, 26, 43, 31, 69, 17, 14, 18, 14, 33, 38, 27, 27, 27, 42, 20, 33, 20, 17, 44, 27, 33, 27, 27, 30, 22, 24, 25, 31, 20, 14, 14, 44, 32, 36, 40, 25, 40, 25, 26, 25, 20, 47, 26, 17, 16, 18, 20, 12, 19, 8, 20, 26, 29, 9, 25, 15, 21, 25, 40, 25, 25, 21, 26, 22, 26, 15, 12, 25, 32, 19, 27, 28, 54, 21, 22, 14, 36, 27, 29, 22, 23, 22, 32, 34, 18, 30, 33, 17, 35, 27, 23, 30, 31, 26, 32, 25, 34, 32, 29, 23, 30, 21, 34, 27, 25, 26, 38, 25, 18, 23, 24, 48, 25, 33, 23, 43, 26, 32, 19, 22, 37, 27, 28, 31, 16, 22, 30, 19, 29, 32, 26, 24, 39, 22, 17, 35, 38, 45, 19, 24, 23, 18, 21, 22, 23, 42, 12, 21, 25, 15, 28, 18, 24, 21, 34, 32, 18, 43, 41, 22, 46, 14, 27, 11, 22, 15, 28, 34, 22, 22, 31, 28, 30, 17, 80, 15, 26, 23, 16, 62, 18, 23, 37, 23, 23, 12, 27, 26, 21, 19, 18, 29, 19, 61, 25, 27, 43, 37, 20, 29, 19, 26, 21, 24, 22, 39, 37, 10, 41, 12, 31, 20, 17, 16, 21, 29, 20, 27, 12, 16, 21, 21, 27, 16, 38, 28, 20, 25, 35, 15, 14, 38, 26, 30, 33, 27, 9, 21, 26, 26, 18, 28, 31, 12, 36, 41, 23, 14, 24, 32, 13, 23, 57, 32, 32, 16, 39, 18, 29, 24, 35, 29, 9, 34, 20, 19, 36, 30, 34, 40, 35, 17, 18, 7, 31, 40, 21, 35, 13, 29, 18, 35, 19, 33, 32, 28, 22, 18, 22, 18, 32, 18, 32, 20, 27, 16, 21, 14, 20, 20, 18, 20, 24, 38, 17, 16, 26, 12, 20, 34, 20, 18, 27, 39, 59, 12, 28, 27, 24, 27, 15, 17, 17, 36, 15, 26, 18, 19, 24, 14, 21, 20, 30, 21, 36, 31, 32, 36, 29, 27, 25, 21, 47, 32, 26, 33, 26, 63, 57, 31, 39, 13, 40, 21, 34, 33, 62, 23, 38, 23, 23, 18, 34, 63, 43, 34, 17, 34, 70, 28, 53, 29, 26, 29, 17, 42, 18, 17, 62, 22, 24, 27, 16, 45, 41, 36, 42, 45, 28, 20, 32, 70, 67, 17, 46, 47, 40, 24, 22, 30, 37, 37, 29, 14, 29, 47, 53, 20, 23, 41, 44, 19, 61, 42, 28, 22, 34, 34, 51, 29, 11, 39, 33, 36, 82, 61, 32, 44, 69, 23, 27, 26, 39, 37, 58, 27, 23, 28, 21, 35, 39, 29, 60, 28, 30, 31, 21, 37, 19, 21, 27, 11, 44, 60, 19, 35, 57, 35, 78, 27, 62, 31, 19, 32, 39, 25, 29, 65, 53, 47, 22, 35, 62, 66, 93, 65, 35, 46, 27, 35, 35, 41, 27, 40, 24, 32, 33, 26, 28, 32, 20, 25, 25, 17, 41, 57, 35, 27, 42, 25, 16, 19, 53, 38, 28, 34, 40, 22, 27, 65, 30, 22, 27, 12, 26, 44, 24, 20, 12, 21, 39, 37, 33, 28, 30, 33, 45, 19, 68, 25, 28, 47, 52, 21, 28, 39, 36, 25, 22, 39, 52, 61, 16, 45, 38, 3, 45, 40, 25, 52, 20, 19, 19, 26, 19, 27, 23, 29, 26, 29, 27, 35, 20, 23, 26, 46, 34, 35, 20, 25, 35, 54, 36, 31, 42, 33, 23, 28, 27, 19, 33, 25, 41, 21, 16, 41, 19, 24, 37, 39, 26, 31, 26, 22, 20, 18, 39, 26, 30, 20, 31, 26, 22, 24, 41, 30, 28, 33, 22, 40, 42, 30, 22, 28, 20, 30, 36, 42, 52, 24, 32, 38, 26, 34, 24, 40, 19, 26, 28, 27, 27, 16, 21, 31, 20, 58, 58, 23, 24, 38, 39, 34, 29, 34, 26, 25, 17, 30, 31, 25, 21, 28, 30, 13, 42, 26, 40, 23, 40, 38, 55, 25, 19, 39, 20, 32, 32, 66, 35, 61, 28, 53, 46, 39, 37, 28, 90, 35, 45, 38, 22, 21, 20, 62, 40, 46, 41, 24, 28, 32, 38, 56, 29, 30, 21, 47, 33, 67, 27, 20, 13, 47, 12, 25, 22, 45, 38, 25, 36, 25, 26, 33, 48, 34, 19, 25, 21, 32, 35, 33, 36, 67, 38, 35, 27, 47, 28, 26, 49, 29, 38, 32, 28, 26, 21, 30, 30, 22, 25, 35, 45, 25, 32, 34, 57, 23, 66, 27, 25, 46, 31, 35, 29, 26, 33, 39, 28, 23, 38, 14, 55, 32, 30, 29, 24, 79, 46, 11, 30, 32, 44, 30, 39, 37, 25, 76, 30, 84, 31, 24, 39, 28, 37, 28, 59, 21, 34, 23, 42, 45, 18, 30, 25, 34, 34, 20, 24, 46, 31, 28, 46, 30, 37, 35, 32, 37, 19, 18, 52, 18, 31, 34, 32, 18, 22, 37, 26, 50, 40, 28, 20, 30, 19, 15, 16, 22, 30, 38, 19, 35, 17, 22, 20, 29, 59, 59, 43, 27, 28, 25, 26, 62, 28, 26, 30, 34, 35], 'attr_dim': 0, 'dataset_info': '==== Dataset Information ====\n== General Information == \nNumber of graphs: 2500\nNumber of classes: 2\nClass distribution: \n0:2000 1:500 \n\n== Node information== \nAverage number of nodes: 27\nAverage number of edges (undirected): 29\nMax number of nodes: 93\nNumber of distinct node labels: 18\nAverage number of distinct node labels: 3\nNode labels distribution: \n0:9454 1:6143 13:139 16:2 18:1 2:49955 20:15 21:29 25:5 26:3 3:921 4:570 44:1 49:1 5:70 6:485 8:14 9:4 \n'}}


Training a new model: DiffPool
Training model with dataset, testing using fold 0
[92maverage training of epoch 0: loss 0.22586 acc 0.93450 roc_auc 0.95807 prc_auc 0.90169[0m
[93maverage test of epoch 0: loss 3.31832 acc 0.20000 roc_auc 0.30981 prc_auc 0.14026[0m
[92maverage training of epoch 1: loss 0.33710 acc 0.83900 roc_auc 0.89494 prc_auc 0.61388[0m
[93maverage test of epoch 1: loss 2.05383 acc 0.20000 roc_auc 0.35268 prc_auc 0.14822[0m
[92maverage training of epoch 2: loss 0.27692 acc 0.87100 roc_auc 0.92144 prc_auc 0.75401[0m
[93maverage test of epoch 2: loss 2.44338 acc 0.20000 roc_auc 0.42186 prc_auc 0.17252[0m
[92maverage training of epoch 3: loss 0.19487 acc 0.92250 roc_auc 0.96364 prc_auc 0.93537[0m
[93maverage test of epoch 3: loss 3.73669 acc 0.20000 roc_auc 0.29894 prc_auc 0.13809[0m
[92maverage training of epoch 4: loss 0.19434 acc 0.93950 roc_auc 0.95437 prc_auc 0.81146[0m
[93maverage test of epoch 4: loss 3.15240 acc 0.20000 roc_auc 0.30789 prc_auc 0.13942[0m
[92maverage training of epoch 5: loss 0.18858 acc 0.94000 roc_auc 0.95363 prc_auc 0.85880[0m
[93maverage test of epoch 5: loss 3.17880 acc 0.20000 roc_auc 0.33369 prc_auc 0.14994[0m
[92maverage training of epoch 6: loss 0.18074 acc 0.94400 roc_auc 0.95512 prc_auc 0.86625[0m
[93maverage test of epoch 6: loss 3.20811 acc 0.20000 roc_auc 0.50521 prc_auc 0.18907[0m
[92maverage training of epoch 7: loss 0.17709 acc 0.94350 roc_auc 0.95852 prc_auc 0.86731[0m
[93maverage test of epoch 7: loss 3.23832 acc 0.20000 roc_auc 0.29607 prc_auc 0.13739[0m
[92maverage training of epoch 8: loss 0.22839 acc 0.92250 roc_auc 0.94294 prc_auc 0.77691[0m
[93maverage test of epoch 8: loss 2.85325 acc 0.20000 roc_auc 0.49340 prc_auc 0.18835[0m
[92maverage training of epoch 9: loss 0.25273 acc 0.90800 roc_auc 0.92957 prc_auc 0.78364[0m
[93maverage test of epoch 9: loss 2.86584 acc 0.20000 roc_auc 0.48204 prc_auc 0.19037[0m
[92maverage training of epoch 10: loss 0.23351 acc 0.92000 roc_auc 0.93753 prc_auc 0.86895[0m
[93maverage test of epoch 10: loss 3.86327 acc 0.20000 roc_auc 0.70791 prc_auc 0.41675[0m
[92maverage training of epoch 11: loss 0.21968 acc 0.93250 roc_auc 0.94155 prc_auc 0.78864[0m
[93maverage test of epoch 11: loss 3.15153 acc 0.20000 roc_auc 0.71405 prc_auc 0.43219[0m
[92maverage training of epoch 12: loss 0.21576 acc 0.92750 roc_auc 0.94355 prc_auc 0.83375[0m
[93maverage test of epoch 12: loss 3.15808 acc 0.20000 roc_auc 0.71255 prc_auc 0.42956[0m
[92maverage training of epoch 13: loss 0.22547 acc 0.92300 roc_auc 0.93977 prc_auc 0.80943[0m
[93maverage test of epoch 13: loss 3.07778 acc 0.20000 roc_auc 0.71708 prc_auc 0.43405[0m
[92maverage training of epoch 14: loss 0.22556 acc 0.92800 roc_auc 0.93975 prc_auc 0.86901[0m
[93maverage test of epoch 14: loss 3.48043 acc 0.20000 roc_auc 0.70916 prc_auc 0.42044[0m
[92maverage training of epoch 15: loss 0.21845 acc 0.92750 roc_auc 0.94348 prc_auc 0.80555[0m
[93maverage test of epoch 15: loss 3.17113 acc 0.20000 roc_auc 0.71041 prc_auc 0.42220[0m
[92maverage training of epoch 16: loss 0.23064 acc 0.93000 roc_auc 0.93687 prc_auc 0.82185[0m
[93maverage test of epoch 16: loss 3.11414 acc 0.20000 roc_auc 0.70846 prc_auc 0.42734[0m
[92maverage training of epoch 17: loss 0.20452 acc 0.93300 roc_auc 0.94842 prc_auc 0.84084[0m
[93maverage test of epoch 17: loss 3.09261 acc 0.20000 roc_auc 0.70790 prc_auc 0.42702[0m
[92maverage training of epoch 18: loss 0.20285 acc 0.93350 roc_auc 0.94903 prc_auc 0.85729[0m
[93maverage test of epoch 18: loss 3.18345 acc 0.20000 roc_auc 0.70734 prc_auc 0.42274[0m
[92maverage training of epoch 19: loss 0.20408 acc 0.93600 roc_auc 0.94736 prc_auc 0.84528[0m
[93maverage test of epoch 19: loss 3.14275 acc 0.20000 roc_auc 0.70686 prc_auc 0.42278[0m
[92maverage training of epoch 20: loss 0.20257 acc 0.93700 roc_auc 0.94702 prc_auc 0.87382[0m
[93maverage test of epoch 20: loss 3.41330 acc 0.20200 roc_auc 0.70384 prc_auc 0.41970[0m
[92maverage training of epoch 21: loss 0.19341 acc 0.94050 roc_auc 0.95107 prc_auc 0.84593[0m
[93maverage test of epoch 21: loss 3.25992 acc 0.20200 roc_auc 0.70237 prc_auc 0.41872[0m
[92maverage training of epoch 22: loss 0.18804 acc 0.94200 roc_auc 0.95293 prc_auc 0.87308[0m
[93maverage test of epoch 22: loss 3.29896 acc 0.20200 roc_auc 0.70203 prc_auc 0.41862[0m
[92maverage training of epoch 23: loss 0.19429 acc 0.94250 roc_auc 0.94774 prc_auc 0.86434[0m
[93maverage test of epoch 23: loss 3.26100 acc 0.20200 roc_auc 0.69942 prc_auc 0.41531[0m
[92maverage training of epoch 24: loss 0.17822 acc 0.94500 roc_auc 0.95629 prc_auc 0.88079[0m
[93maverage test of epoch 24: loss 3.28301 acc 0.20200 roc_auc 0.69857 prc_auc 0.41439[0m
[92maverage training of epoch 25: loss 0.17549 acc 0.94550 roc_auc 0.95813 prc_auc 0.88659[0m
[93maverage test of epoch 25: loss 3.31458 acc 0.20200 roc_auc 0.69727 prc_auc 0.41201[0m
[92maverage training of epoch 26: loss 0.17743 acc 0.94500 roc_auc 0.95722 prc_auc 0.88367[0m
[93maverage test of epoch 26: loss 3.31615 acc 0.20200 roc_auc 0.69863 prc_auc 0.41547[0m
[92maverage training of epoch 27: loss 0.17423 acc 0.94700 roc_auc 0.95816 prc_auc 0.89490[0m
[93maverage test of epoch 27: loss 3.52333 acc 0.20200 roc_auc 0.69919 prc_auc 0.41567[0m
[92maverage training of epoch 28: loss 0.17535 acc 0.94700 roc_auc 0.95735 prc_auc 0.87483[0m
[93maverage test of epoch 28: loss 3.36464 acc 0.20200 roc_auc 0.70022 prc_auc 0.41655[0m
[92maverage training of epoch 29: loss 0.17025 acc 0.94800 roc_auc 0.95874 prc_auc 0.89544[0m
[93maverage test of epoch 29: loss 3.41848 acc 0.20200 roc_auc 0.69872 prc_auc 0.41423[0m
[92maverage training of epoch 30: loss 0.16590 acc 0.95000 roc_auc 0.96018 prc_auc 0.89695[0m
[93maverage test of epoch 30: loss 3.43436 acc 0.20200 roc_auc 0.69875 prc_auc 0.41256[0m
[92maverage training of epoch 31: loss 0.16344 acc 0.95050 roc_auc 0.96100 prc_auc 0.89893[0m
[93maverage test of epoch 31: loss 3.43789 acc 0.20200 roc_auc 0.69835 prc_auc 0.41174[0m
[92maverage training of epoch 32: loss 0.16157 acc 0.95150 roc_auc 0.96165 prc_auc 0.90124[0m
[93maverage test of epoch 32: loss 3.44320 acc 0.20200 roc_auc 0.69866 prc_auc 0.41221[0m
[92maverage training of epoch 33: loss 0.15936 acc 0.95350 roc_auc 0.96227 prc_auc 0.90352[0m
[93maverage test of epoch 33: loss 3.45178 acc 0.20200 roc_auc 0.69877 prc_auc 0.41215[0m
[92maverage training of epoch 34: loss 0.15788 acc 0.95400 roc_auc 0.96279 prc_auc 0.90492[0m
[93maverage test of epoch 34: loss 3.45385 acc 0.20200 roc_auc 0.69949 prc_auc 0.41373[0m
[92maverage training of epoch 35: loss 0.15586 acc 0.95400 roc_auc 0.96334 prc_auc 0.90670[0m
[93maverage test of epoch 35: loss 3.48240 acc 0.20200 roc_auc 0.70005 prc_auc 0.41315[0m
[92maverage training of epoch 36: loss 0.15433 acc 0.95450 roc_auc 0.96375 prc_auc 0.90776[0m
[93maverage test of epoch 36: loss 3.47118 acc 0.20200 roc_auc 0.70067 prc_auc 0.41465[0m
[92maverage training of epoch 37: loss 0.15265 acc 0.95450 roc_auc 0.96417 prc_auc 0.90936[0m
[93maverage test of epoch 37: loss 3.47016 acc 0.20200 roc_auc 0.70128 prc_auc 0.41462[0m
[92maverage training of epoch 38: loss 0.15109 acc 0.95550 roc_auc 0.96464 prc_auc 0.91083[0m
[93maverage test of epoch 38: loss 3.47813 acc 0.20200 roc_auc 0.70210 prc_auc 0.41505[0m
[92maverage training of epoch 39: loss 0.15067 acc 0.95550 roc_auc 0.96493 prc_auc 0.91139[0m
[93maverage test of epoch 39: loss 3.47862 acc 0.20200 roc_auc 0.70271 prc_auc 0.41489[0m
[92maverage training of epoch 40: loss 0.14969 acc 0.95600 roc_auc 0.96530 prc_auc 0.91226[0m
[93maverage test of epoch 40: loss 3.47984 acc 0.20200 roc_auc 0.70359 prc_auc 0.41543[0m
[92maverage training of epoch 41: loss 0.14873 acc 0.95750 roc_auc 0.96559 prc_auc 0.91296[0m
[93maverage test of epoch 41: loss 3.47812 acc 0.20200 roc_auc 0.70480 prc_auc 0.41756[0m
[92maverage training of epoch 42: loss 0.14778 acc 0.95750 roc_auc 0.96593 prc_auc 0.91414[0m
[93maverage test of epoch 42: loss 3.48505 acc 0.20200 roc_auc 0.70515 prc_auc 0.41732[0m
[92maverage training of epoch 43: loss 0.14777 acc 0.95750 roc_auc 0.96552 prc_auc 0.91430[0m
[93maverage test of epoch 43: loss 3.48124 acc 0.20200 roc_auc 0.70585 prc_auc 0.41824[0m
[92maverage training of epoch 44: loss 0.14642 acc 0.95750 roc_auc 0.96672 prc_auc 0.91601[0m
[93maverage test of epoch 44: loss 3.48869 acc 0.20200 roc_auc 0.70665 prc_auc 0.41956[0m
[92maverage training of epoch 45: loss 0.14589 acc 0.95750 roc_auc 0.96671 prc_auc 0.91729[0m
[93maverage test of epoch 45: loss 3.54491 acc 0.20200 roc_auc 0.70732 prc_auc 0.41936[0m
[92maverage training of epoch 46: loss 0.14522 acc 0.95750 roc_auc 0.96683 prc_auc 0.91687[0m
[93maverage test of epoch 46: loss 3.52153 acc 0.20200 roc_auc 0.70836 prc_auc 0.42068[0m
[92maverage training of epoch 47: loss 0.14431 acc 0.95750 roc_auc 0.96715 prc_auc 0.92133[0m
[93maverage test of epoch 47: loss 3.63531 acc 0.20200 roc_auc 0.70958 prc_auc 0.42288[0m
[92maverage training of epoch 48: loss 0.14460 acc 0.95750 roc_auc 0.96692 prc_auc 0.91601[0m
[93maverage test of epoch 48: loss 3.56495 acc 0.20200 roc_auc 0.71058 prc_auc 0.42424[0m
[92maverage training of epoch 49: loss 0.14298 acc 0.95800 roc_auc 0.96723 prc_auc 0.92015[0m
[93maverage test of epoch 49: loss 3.55926 acc 0.20200 roc_auc 0.71142 prc_auc 0.42524[0m
Training model with dataset, testing using fold 1
[92maverage training of epoch 0: loss 0.23597 acc 0.92450 roc_auc 0.95907 prc_auc 0.91075[0m
[93maverage test of epoch 0: loss 2.62138 acc 0.20000 roc_auc 0.30134 prc_auc 0.13996[0m
[92maverage training of epoch 1: loss 0.24104 acc 0.90000 roc_auc 0.94138 prc_auc 0.80724[0m
[93maverage test of epoch 1: loss 2.78813 acc 0.20000 roc_auc 0.31507 prc_auc 0.14583[0m
[92maverage training of epoch 2: loss 0.21646 acc 0.92550 roc_auc 0.94663 prc_auc 0.80453[0m
[93maverage test of epoch 2: loss 2.77254 acc 0.20000 roc_auc 0.38482 prc_auc 0.15578[0m
[92maverage training of epoch 3: loss 0.20065 acc 0.93400 roc_auc 0.95377 prc_auc 0.82414[0m
[93maverage test of epoch 3: loss 2.71162 acc 0.20000 roc_auc 0.59496 prc_auc 0.24801[0m
[92maverage training of epoch 4: loss 0.22710 acc 0.91350 roc_auc 0.94941 prc_auc 0.88314[0m
[93maverage test of epoch 4: loss 3.38045 acc 0.20000 roc_auc 0.43734 prc_auc 0.18244[0m
[92maverage training of epoch 5: loss 0.17882 acc 0.94600 roc_auc 0.95796 prc_auc 0.88798[0m
[93maverage test of epoch 5: loss 3.51435 acc 0.20000 roc_auc 0.39606 prc_auc 0.16782[0m
[92maverage training of epoch 6: loss 0.20958 acc 0.93150 roc_auc 0.94903 prc_auc 0.80320[0m
[93maverage test of epoch 6: loss 3.04648 acc 0.20000 roc_auc 0.48329 prc_auc 0.20138[0m
[92maverage training of epoch 7: loss 0.21785 acc 0.92850 roc_auc 0.94426 prc_auc 0.84527[0m
[93maverage test of epoch 7: loss 3.13080 acc 0.20000 roc_auc 0.61128 prc_auc 0.24918[0m
[92maverage training of epoch 8: loss 0.19767 acc 0.93700 roc_auc 0.95116 prc_auc 0.86498[0m
[93maverage test of epoch 8: loss 3.23512 acc 0.20000 roc_auc 0.66630 prc_auc 0.27022[0m
[92maverage training of epoch 9: loss 0.20648 acc 0.93100 roc_auc 0.95090 prc_auc 0.87749[0m
[93maverage test of epoch 9: loss 3.80571 acc 0.20000 roc_auc 0.44024 prc_auc 0.19996[0m
[92maverage training of epoch 10: loss 0.23558 acc 0.92100 roc_auc 0.93865 prc_auc 0.78604[0m
[93maverage test of epoch 10: loss 3.23931 acc 0.20000 roc_auc 0.67936 prc_auc 0.31475[0m
[92maverage training of epoch 11: loss 0.23186 acc 0.92050 roc_auc 0.94088 prc_auc 0.85995[0m
[93maverage test of epoch 11: loss 3.71553 acc 0.20000 roc_auc 0.46021 prc_auc 0.19164[0m
[92maverage training of epoch 12: loss 0.21487 acc 0.93350 roc_auc 0.94526 prc_auc 0.83480[0m
[93maverage test of epoch 12: loss 3.61536 acc 0.20000 roc_auc 0.65610 prc_auc 0.28579[0m
[92maverage training of epoch 13: loss 0.22583 acc 0.92250 roc_auc 0.94257 prc_auc 0.79319[0m
[93maverage test of epoch 13: loss 3.16251 acc 0.20000 roc_auc 0.69359 prc_auc 0.40403[0m
[92maverage training of epoch 14: loss 0.22945 acc 0.92150 roc_auc 0.93823 prc_auc 0.83120[0m
[93maverage test of epoch 14: loss 3.21917 acc 0.20000 roc_auc 0.69065 prc_auc 0.39889[0m
[92maverage training of epoch 15: loss 0.21840 acc 0.92700 roc_auc 0.94151 prc_auc 0.85099[0m
[93maverage test of epoch 15: loss 3.32548 acc 0.20000 roc_auc 0.68604 prc_auc 0.39795[0m
[92maverage training of epoch 16: loss 0.21187 acc 0.93150 roc_auc 0.94368 prc_auc 0.83106[0m
[93maverage test of epoch 16: loss 3.26851 acc 0.20000 roc_auc 0.68856 prc_auc 0.40647[0m
[92maverage training of epoch 17: loss 0.20449 acc 0.93400 roc_auc 0.94690 prc_auc 0.83920[0m
[93maverage test of epoch 17: loss 3.22938 acc 0.20000 roc_auc 0.68783 prc_auc 0.40757[0m
[92maverage training of epoch 18: loss 0.20992 acc 0.93000 roc_auc 0.94571 prc_auc 0.84527[0m
[93maverage test of epoch 18: loss 3.26969 acc 0.20000 roc_auc 0.68950 prc_auc 0.41014[0m
[92maverage training of epoch 19: loss 0.20527 acc 0.93400 roc_auc 0.94650 prc_auc 0.84164[0m
[93maverage test of epoch 19: loss 3.24550 acc 0.20000 roc_auc 0.68981 prc_auc 0.40879[0m
[92maverage training of epoch 20: loss 0.21107 acc 0.93150 roc_auc 0.94532 prc_auc 0.84733[0m
[93maverage test of epoch 20: loss 3.31687 acc 0.20000 roc_auc 0.67979 prc_auc 0.40359[0m
[92maverage training of epoch 21: loss 0.20319 acc 0.93500 roc_auc 0.94743 prc_auc 0.83799[0m
[93maverage test of epoch 21: loss 3.24008 acc 0.20000 roc_auc 0.67910 prc_auc 0.40321[0m
[92maverage training of epoch 22: loss 0.18625 acc 0.94300 roc_auc 0.95570 prc_auc 0.86989[0m
[93maverage test of epoch 22: loss 3.25445 acc 0.20000 roc_auc 0.67648 prc_auc 0.40225[0m
[92maverage training of epoch 23: loss 0.17485 acc 0.94450 roc_auc 0.95960 prc_auc 0.87328[0m
[93maverage test of epoch 23: loss 3.25880 acc 0.20000 roc_auc 0.67429 prc_auc 0.40252[0m
[92maverage training of epoch 24: loss 0.17252 acc 0.94600 roc_auc 0.96049 prc_auc 0.88144[0m
[93maverage test of epoch 24: loss 3.29027 acc 0.20000 roc_auc 0.67075 prc_auc 0.39995[0m
[92maverage training of epoch 25: loss 0.17451 acc 0.94900 roc_auc 0.95621 prc_auc 0.88263[0m
[93maverage test of epoch 25: loss 3.32566 acc 0.20000 roc_auc 0.66886 prc_auc 0.39498[0m
[92maverage training of epoch 26: loss 0.16612 acc 0.95100 roc_auc 0.96252 prc_auc 0.87764[0m
[93maverage test of epoch 26: loss 3.30124 acc 0.20000 roc_auc 0.66460 prc_auc 0.39268[0m
[92maverage training of epoch 27: loss 0.17136 acc 0.94800 roc_auc 0.95930 prc_auc 0.88430[0m
[93maverage test of epoch 27: loss 3.32097 acc 0.20000 roc_auc 0.66641 prc_auc 0.39407[0m
[92maverage training of epoch 28: loss 0.16621 acc 0.95000 roc_auc 0.96144 prc_auc 0.88876[0m
[93maverage test of epoch 28: loss 3.34700 acc 0.20000 roc_auc 0.66355 prc_auc 0.39174[0m
[92maverage training of epoch 29: loss 0.16205 acc 0.95300 roc_auc 0.96257 prc_auc 0.89163[0m
[93maverage test of epoch 29: loss 3.36952 acc 0.20000 roc_auc 0.66137 prc_auc 0.38768[0m
[92maverage training of epoch 30: loss 0.16087 acc 0.95400 roc_auc 0.96270 prc_auc 0.89230[0m
[93maverage test of epoch 30: loss 3.38619 acc 0.20000 roc_auc 0.65947 prc_auc 0.38501[0m
[92maverage training of epoch 31: loss 0.15843 acc 0.95450 roc_auc 0.96297 prc_auc 0.89344[0m
[93maverage test of epoch 31: loss 3.38849 acc 0.20000 roc_auc 0.66040 prc_auc 0.38581[0m
[92maverage training of epoch 32: loss 0.15862 acc 0.95550 roc_auc 0.96297 prc_auc 0.89319[0m
[93maverage test of epoch 32: loss 3.39587 acc 0.20000 roc_auc 0.66060 prc_auc 0.38665[0m
[92maverage training of epoch 33: loss 0.15959 acc 0.95400 roc_auc 0.96252 prc_auc 0.89984[0m
[93maverage test of epoch 33: loss 3.48625 acc 0.20000 roc_auc 0.66023 prc_auc 0.38588[0m
[92maverage training of epoch 34: loss 0.15594 acc 0.95600 roc_auc 0.96339 prc_auc 0.88715[0m
[93maverage test of epoch 34: loss 3.38409 acc 0.20000 roc_auc 0.65999 prc_auc 0.38554[0m
[92maverage training of epoch 35: loss 0.15391 acc 0.95800 roc_auc 0.96139 prc_auc 0.90062[0m
[93maverage test of epoch 35: loss 3.38513 acc 0.20000 roc_auc 0.66063 prc_auc 0.38588[0m
[92maverage training of epoch 36: loss 0.14480 acc 0.95800 roc_auc 0.96848 prc_auc 0.90526[0m
[93maverage test of epoch 36: loss 3.37681 acc 0.20000 roc_auc 0.66065 prc_auc 0.38438[0m
[92maverage training of epoch 37: loss 0.14902 acc 0.95850 roc_auc 0.96421 prc_auc 0.90440[0m
[93maverage test of epoch 37: loss 3.38050 acc 0.20000 roc_auc 0.66063 prc_auc 0.38321[0m
[92maverage training of epoch 38: loss 0.14294 acc 0.95900 roc_auc 0.96880 prc_auc 0.90790[0m
[93maverage test of epoch 38: loss 3.38279 acc 0.20200 roc_auc 0.66130 prc_auc 0.38353[0m
[92maverage training of epoch 39: loss 0.14287 acc 0.95900 roc_auc 0.96865 prc_auc 0.90942[0m
[93maverage test of epoch 39: loss 3.39008 acc 0.20200 roc_auc 0.66114 prc_auc 0.38283[0m
[92maverage training of epoch 40: loss 0.14315 acc 0.95950 roc_auc 0.96836 prc_auc 0.90912[0m
[93maverage test of epoch 40: loss 3.39846 acc 0.20200 roc_auc 0.66075 prc_auc 0.38230[0m
[92maverage training of epoch 41: loss 0.15450 acc 0.95800 roc_auc 0.96113 prc_auc 0.90288[0m
[93maverage test of epoch 41: loss 3.41977 acc 0.20200 roc_auc 0.66192 prc_auc 0.38369[0m
[92maverage training of epoch 42: loss 0.14723 acc 0.95850 roc_auc 0.96630 prc_auc 0.90810[0m
[93maverage test of epoch 42: loss 3.43381 acc 0.20200 roc_auc 0.66263 prc_auc 0.38376[0m
[92maverage training of epoch 43: loss 0.14474 acc 0.95900 roc_auc 0.96700 prc_auc 0.90955[0m
[93maverage test of epoch 43: loss 3.42820 acc 0.20200 roc_auc 0.66223 prc_auc 0.38331[0m
[92maverage training of epoch 44: loss 0.14264 acc 0.95950 roc_auc 0.96772 prc_auc 0.91121[0m
[93maverage test of epoch 44: loss 3.43384 acc 0.20200 roc_auc 0.66225 prc_auc 0.38347[0m
[92maverage training of epoch 45: loss 0.13970 acc 0.96100 roc_auc 0.96857 prc_auc 0.91399[0m
[93maverage test of epoch 45: loss 3.43603 acc 0.20200 roc_auc 0.66275 prc_auc 0.38316[0m
[92maverage training of epoch 46: loss 0.14113 acc 0.96050 roc_auc 0.96656 prc_auc 0.91344[0m
[93maverage test of epoch 46: loss 3.43095 acc 0.20200 roc_auc 0.66385 prc_auc 0.38306[0m
[92maverage training of epoch 47: loss 0.13609 acc 0.96050 roc_auc 0.97082 prc_auc 0.91763[0m
[93maverage test of epoch 47: loss 3.44506 acc 0.20200 roc_auc 0.66397 prc_auc 0.38278[0m
[92maverage training of epoch 48: loss 0.13991 acc 0.96050 roc_auc 0.96717 prc_auc 0.91733[0m
[93maverage test of epoch 48: loss 3.51381 acc 0.20200 roc_auc 0.66470 prc_auc 0.38193[0m
[92maverage training of epoch 49: loss 0.13619 acc 0.96050 roc_auc 0.97061 prc_auc 0.91612[0m
[93maverage test of epoch 49: loss 3.47282 acc 0.20400 roc_auc 0.66440 prc_auc 0.38184[0m
Training model with dataset, testing using fold 2
[92maverage training of epoch 0: loss 0.21686 acc 0.95000 roc_auc 0.96260 prc_auc 0.92609[0m
[93maverage test of epoch 0: loss 2.75723 acc 0.20000 roc_auc 0.26993 prc_auc 0.13266[0m
[92maverage training of epoch 1: loss 0.27820 acc 0.87350 roc_auc 0.92695 prc_auc 0.75704[0m
[93maverage test of epoch 1: loss 2.93411 acc 0.20000 roc_auc 0.26246 prc_auc 0.13394[0m
[92maverage training of epoch 2: loss 0.25998 acc 0.89550 roc_auc 0.93258 prc_auc 0.74650[0m
[93maverage test of epoch 2: loss 2.73991 acc 0.20000 roc_auc 0.34790 prc_auc 0.14551[0m
[92maverage training of epoch 3: loss 0.25808 acc 0.90300 roc_auc 0.93153 prc_auc 0.81697[0m
[93maverage test of epoch 3: loss 3.09378 acc 0.20000 roc_auc 0.29005 prc_auc 0.13700[0m
[92maverage training of epoch 4: loss 0.19328 acc 0.93250 roc_auc 0.95929 prc_auc 0.90727[0m
[93maverage test of epoch 4: loss 3.55688 acc 0.20000 roc_auc 0.25429 prc_auc 0.13027[0m
[92maverage training of epoch 5: loss 0.21895 acc 0.92800 roc_auc 0.94616 prc_auc 0.77102[0m
[93maverage test of epoch 5: loss 2.76665 acc 0.20000 roc_auc 0.47346 prc_auc 0.19753[0m
[92maverage training of epoch 6: loss 0.23398 acc 0.92650 roc_auc 0.94244 prc_auc 0.85777[0m
[93maverage test of epoch 6: loss 3.12606 acc 0.20000 roc_auc 0.69184 prc_auc 0.35392[0m
[92maverage training of epoch 7: loss 0.19430 acc 0.93700 roc_auc 0.95230 prc_auc 0.87244[0m
[93maverage test of epoch 7: loss 3.41988 acc 0.20000 roc_auc 0.74872 prc_auc 0.47589[0m
[92maverage training of epoch 8: loss 0.18541 acc 0.94350 roc_auc 0.95560 prc_auc 0.88947[0m
[93maverage test of epoch 8: loss 3.83672 acc 0.20000 roc_auc 0.53369 prc_auc 0.20680[0m
[92maverage training of epoch 9: loss 0.18977 acc 0.94300 roc_auc 0.95344 prc_auc 0.82367[0m
[93maverage test of epoch 9: loss 3.41068 acc 0.20000 roc_auc 0.73535 prc_auc 0.39392[0m
[92maverage training of epoch 10: loss 0.20987 acc 0.93000 roc_auc 0.94816 prc_auc 0.79756[0m
[93maverage test of epoch 10: loss 3.03053 acc 0.20000 roc_auc 0.74751 prc_auc 0.47124[0m
[92maverage training of epoch 11: loss 0.19786 acc 0.93500 roc_auc 0.95324 prc_auc 0.89426[0m
[93maverage test of epoch 11: loss 3.57292 acc 0.20000 roc_auc 0.73689 prc_auc 0.40277[0m
[92maverage training of epoch 12: loss 0.21867 acc 0.92850 roc_auc 0.94577 prc_auc 0.77909[0m
[93maverage test of epoch 12: loss 3.02979 acc 0.20000 roc_auc 0.75219 prc_auc 0.49235[0m
[92maverage training of epoch 13: loss 0.21213 acc 0.92250 roc_auc 0.94847 prc_auc 0.86894[0m
[93maverage test of epoch 13: loss 3.31690 acc 0.20000 roc_auc 0.76164 prc_auc 0.49438[0m
[92maverage training of epoch 14: loss 0.22059 acc 0.92400 roc_auc 0.94419 prc_auc 0.79853[0m
[93maverage test of epoch 14: loss 3.08866 acc 0.20000 roc_auc 0.75292 prc_auc 0.50846[0m
[92maverage training of epoch 15: loss 0.22983 acc 0.91600 roc_auc 0.94001 prc_auc 0.81975[0m
[93maverage test of epoch 15: loss 3.10488 acc 0.20000 roc_auc 0.75346 prc_auc 0.51737[0m
[92maverage training of epoch 16: loss 0.20977 acc 0.92750 roc_auc 0.94696 prc_auc 0.86321[0m
[93maverage test of epoch 16: loss 3.33591 acc 0.20000 roc_auc 0.75651 prc_auc 0.52660[0m
[92maverage training of epoch 17: loss 0.21010 acc 0.93150 roc_auc 0.94647 prc_auc 0.84056[0m
[93maverage test of epoch 17: loss 3.30244 acc 0.20000 roc_auc 0.75920 prc_auc 0.53092[0m
[92maverage training of epoch 18: loss 0.19859 acc 0.93350 roc_auc 0.95166 prc_auc 0.83280[0m
[93maverage test of epoch 18: loss 3.16681 acc 0.20000 roc_auc 0.75849 prc_auc 0.52618[0m
[92maverage training of epoch 19: loss 0.19441 acc 0.93700 roc_auc 0.95314 prc_auc 0.86157[0m
[93maverage test of epoch 19: loss 3.19968 acc 0.20000 roc_auc 0.75895 prc_auc 0.52718[0m
[92maverage training of epoch 20: loss 0.19141 acc 0.93800 roc_auc 0.95395 prc_auc 0.86234[0m
[93maverage test of epoch 20: loss 3.22203 acc 0.20000 roc_auc 0.75755 prc_auc 0.52379[0m
[92maverage training of epoch 21: loss 0.19883 acc 0.93650 roc_auc 0.95068 prc_auc 0.85063[0m
[93maverage test of epoch 21: loss 3.21671 acc 0.20000 roc_auc 0.76211 prc_auc 0.52629[0m
[92maverage training of epoch 22: loss 0.20433 acc 0.93900 roc_auc 0.94363 prc_auc 0.86154[0m
[93maverage test of epoch 22: loss 3.24144 acc 0.20000 roc_auc 0.76218 prc_auc 0.52700[0m
[92maverage training of epoch 23: loss 0.18036 acc 0.94150 roc_auc 0.95767 prc_auc 0.86875[0m
[93maverage test of epoch 23: loss 3.24318 acc 0.20000 roc_auc 0.76316 prc_auc 0.52818[0m
[92maverage training of epoch 24: loss 0.17960 acc 0.94150 roc_auc 0.95819 prc_auc 0.87183[0m
[93maverage test of epoch 24: loss 3.23556 acc 0.20000 roc_auc 0.76317 prc_auc 0.52792[0m
[92maverage training of epoch 25: loss 0.17709 acc 0.94300 roc_auc 0.95894 prc_auc 0.88590[0m
[93maverage test of epoch 25: loss 3.31555 acc 0.20000 roc_auc 0.76275 prc_auc 0.52748[0m
[92maverage training of epoch 26: loss 0.18111 acc 0.94500 roc_auc 0.95289 prc_auc 0.88453[0m
[93maverage test of epoch 26: loss 3.34840 acc 0.20000 roc_auc 0.76341 prc_auc 0.52837[0m
[92maverage training of epoch 27: loss 0.16631 acc 0.94650 roc_auc 0.96228 prc_auc 0.88016[0m
[93maverage test of epoch 27: loss 3.30061 acc 0.20000 roc_auc 0.76316 prc_auc 0.52650[0m
[92maverage training of epoch 28: loss 0.16616 acc 0.94750 roc_auc 0.96032 prc_auc 0.89816[0m
[93maverage test of epoch 28: loss 3.38867 acc 0.20000 roc_auc 0.76428 prc_auc 0.52756[0m
[92maverage training of epoch 29: loss 0.16369 acc 0.94700 roc_auc 0.96348 prc_auc 0.88874[0m
[93maverage test of epoch 29: loss 3.34751 acc 0.20000 roc_auc 0.76449 prc_auc 0.52547[0m
[92maverage training of epoch 30: loss 0.16139 acc 0.94900 roc_auc 0.96407 prc_auc 0.90055[0m
[93maverage test of epoch 30: loss 3.39303 acc 0.20000 roc_auc 0.76427 prc_auc 0.52504[0m
[92maverage training of epoch 31: loss 0.15914 acc 0.95100 roc_auc 0.96426 prc_auc 0.90238[0m
[93maverage test of epoch 31: loss 3.45002 acc 0.20000 roc_auc 0.76460 prc_auc 0.52316[0m
[92maverage training of epoch 32: loss 0.15803 acc 0.95200 roc_auc 0.96488 prc_auc 0.90541[0m
[93maverage test of epoch 32: loss 3.49559 acc 0.20000 roc_auc 0.76434 prc_auc 0.52405[0m
[92maverage training of epoch 33: loss 0.15503 acc 0.95450 roc_auc 0.96569 prc_auc 0.90474[0m
[93maverage test of epoch 33: loss 3.50763 acc 0.20000 roc_auc 0.76442 prc_auc 0.52402[0m
[92maverage training of epoch 34: loss 0.15481 acc 0.95400 roc_auc 0.96577 prc_auc 0.90837[0m
[93maverage test of epoch 34: loss 3.55046 acc 0.20000 roc_auc 0.76560 prc_auc 0.52458[0m
[92maverage training of epoch 35: loss 0.15453 acc 0.95350 roc_auc 0.96567 prc_auc 0.90326[0m
[93maverage test of epoch 35: loss 3.49637 acc 0.20000 roc_auc 0.76610 prc_auc 0.52402[0m
[92maverage training of epoch 36: loss 0.15307 acc 0.95450 roc_auc 0.96609 prc_auc 0.90979[0m
[93maverage test of epoch 36: loss 3.50940 acc 0.20000 roc_auc 0.76597 prc_auc 0.52317[0m
[92maverage training of epoch 37: loss 0.15172 acc 0.95450 roc_auc 0.96659 prc_auc 0.91186[0m
[93maverage test of epoch 37: loss 3.53294 acc 0.20000 roc_auc 0.76723 prc_auc 0.52510[0m
[92maverage training of epoch 38: loss 0.15527 acc 0.95350 roc_auc 0.96424 prc_auc 0.91176[0m
[93maverage test of epoch 38: loss 3.59592 acc 0.20000 roc_auc 0.76896 prc_auc 0.52480[0m
[92maverage training of epoch 39: loss 0.15262 acc 0.95500 roc_auc 0.96528 prc_auc 0.90550[0m
[93maverage test of epoch 39: loss 3.50781 acc 0.20000 roc_auc 0.76860 prc_auc 0.52260[0m
[92maverage training of epoch 40: loss 0.15330 acc 0.95500 roc_auc 0.96513 prc_auc 0.91576[0m
[93maverage test of epoch 40: loss 3.65415 acc 0.20000 roc_auc 0.76890 prc_auc 0.52438[0m
[92maverage training of epoch 41: loss 0.15595 acc 0.95350 roc_auc 0.96397 prc_auc 0.90296[0m
[93maverage test of epoch 41: loss 3.55392 acc 0.20000 roc_auc 0.76841 prc_auc 0.52009[0m
[92maverage training of epoch 42: loss 0.16242 acc 0.94950 roc_auc 0.96200 prc_auc 0.90864[0m
[93maverage test of epoch 42: loss 3.65214 acc 0.20000 roc_auc 0.76935 prc_auc 0.52360[0m
[92maverage training of epoch 43: loss 0.15897 acc 0.94950 roc_auc 0.96249 prc_auc 0.90691[0m
[93maverage test of epoch 43: loss 3.64333 acc 0.20000 roc_auc 0.77031 prc_auc 0.52788[0m
[92maverage training of epoch 44: loss 0.15393 acc 0.95250 roc_auc 0.96447 prc_auc 0.91466[0m
[93maverage test of epoch 44: loss 3.72830 acc 0.20000 roc_auc 0.77040 prc_auc 0.52644[0m
[92maverage training of epoch 45: loss 0.15129 acc 0.95450 roc_auc 0.96544 prc_auc 0.91074[0m
[93maverage test of epoch 45: loss 3.61918 acc 0.20000 roc_auc 0.77062 prc_auc 0.52539[0m
[92maverage training of epoch 46: loss 0.14762 acc 0.95950 roc_auc 0.96636 prc_auc 0.92272[0m
[93maverage test of epoch 46: loss 3.67964 acc 0.20000 roc_auc 0.77022 prc_auc 0.52183[0m
[92maverage training of epoch 47: loss 0.14515 acc 0.96050 roc_auc 0.96923 prc_auc 0.92179[0m
[93maverage test of epoch 47: loss 3.67891 acc 0.20000 roc_auc 0.76965 prc_auc 0.51633[0m
[92maverage training of epoch 48: loss 0.13681 acc 0.96150 roc_auc 0.97109 prc_auc 0.92059[0m
[93maverage test of epoch 48: loss 3.65358 acc 0.20000 roc_auc 0.76985 prc_auc 0.51591[0m
[92maverage training of epoch 49: loss 0.14658 acc 0.96150 roc_auc 0.96461 prc_auc 0.91696[0m
[93maverage test of epoch 49: loss 3.61674 acc 0.20000 roc_auc 0.76895 prc_auc 0.51190[0m
Training model with dataset, testing using fold 3
[92maverage training of epoch 0: loss 0.24964 acc 0.91300 roc_auc 0.94659 prc_auc 0.86978[0m
[93maverage test of epoch 0: loss 1.78445 acc 0.20000 roc_auc 0.33735 prc_auc 0.15917[0m
[92maverage training of epoch 1: loss 0.30058 acc 0.85100 roc_auc 0.90692 prc_auc 0.70907[0m
[93maverage test of epoch 1: loss 2.59999 acc 0.20000 roc_auc 0.36921 prc_auc 0.15837[0m
[92maverage training of epoch 2: loss 0.27881 acc 0.88100 roc_auc 0.92165 prc_auc 0.66000[0m
[93maverage test of epoch 2: loss 1.81554 acc 0.20000 roc_auc 0.61910 prc_auc 0.27573[0m
[92maverage training of epoch 3: loss 0.24020 acc 0.89200 roc_auc 0.94153 prc_auc 0.85948[0m
[93maverage test of epoch 3: loss 3.36869 acc 0.20000 roc_auc 0.35445 prc_auc 0.15020[0m
[92maverage training of epoch 4: loss 0.21521 acc 0.93100 roc_auc 0.94522 prc_auc 0.79065[0m
[93maverage test of epoch 4: loss 2.87006 acc 0.20000 roc_auc 0.46074 prc_auc 0.18100[0m
[92maverage training of epoch 5: loss 0.18890 acc 0.93900 roc_auc 0.95588 prc_auc 0.90569[0m
[93maverage test of epoch 5: loss 3.49667 acc 0.20000 roc_auc 0.42297 prc_auc 0.17981[0m
[92maverage training of epoch 6: loss 0.20843 acc 0.93450 roc_auc 0.94686 prc_auc 0.80690[0m
[93maverage test of epoch 6: loss 3.09912 acc 0.20000 roc_auc 0.61291 prc_auc 0.28041[0m
[92maverage training of epoch 7: loss 0.21562 acc 0.93400 roc_auc 0.94249 prc_auc 0.83988[0m
[93maverage test of epoch 7: loss 3.12851 acc 0.20000 roc_auc 0.58880 prc_auc 0.23588[0m
[92maverage training of epoch 8: loss 0.20247 acc 0.93450 roc_auc 0.94909 prc_auc 0.82409[0m
[93maverage test of epoch 8: loss 3.02659 acc 0.20000 roc_auc 0.53664 prc_auc 0.20614[0m
[92maverage training of epoch 9: loss 0.21258 acc 0.92800 roc_auc 0.94540 prc_auc 0.80647[0m
[93maverage test of epoch 9: loss 2.89604 acc 0.20000 roc_auc 0.65654 prc_auc 0.35588[0m
[92maverage training of epoch 10: loss 0.21207 acc 0.93100 roc_auc 0.94666 prc_auc 0.85774[0m
[93maverage test of epoch 10: loss 3.12051 acc 0.20000 roc_auc 0.61955 prc_auc 0.26300[0m
[92maverage training of epoch 11: loss 0.20139 acc 0.93650 roc_auc 0.94898 prc_auc 0.82482[0m
[93maverage test of epoch 11: loss 3.00118 acc 0.20000 roc_auc 0.64395 prc_auc 0.33397[0m
[92maverage training of epoch 12: loss 0.21508 acc 0.92700 roc_auc 0.94537 prc_auc 0.84393[0m
[93maverage test of epoch 12: loss 3.09658 acc 0.20000 roc_auc 0.65099 prc_auc 0.36304[0m
[92maverage training of epoch 13: loss 0.20451 acc 0.93700 roc_auc 0.94709 prc_auc 0.85316[0m
[93maverage test of epoch 13: loss 3.14952 acc 0.20000 roc_auc 0.64699 prc_auc 0.34337[0m
[92maverage training of epoch 14: loss 0.19276 acc 0.94050 roc_auc 0.95146 prc_auc 0.84923[0m
[93maverage test of epoch 14: loss 3.09275 acc 0.20000 roc_auc 0.65171 prc_auc 0.36730[0m
[92maverage training of epoch 15: loss 0.17957 acc 0.94550 roc_auc 0.95625 prc_auc 0.88792[0m
[93maverage test of epoch 15: loss 3.25049 acc 0.20200 roc_auc 0.64771 prc_auc 0.35029[0m
[92maverage training of epoch 16: loss 0.16900 acc 0.94800 roc_auc 0.95963 prc_auc 0.89097[0m
[93maverage test of epoch 16: loss 3.30639 acc 0.20000 roc_auc 0.64790 prc_auc 0.34996[0m
[92maverage training of epoch 17: loss 0.16313 acc 0.95050 roc_auc 0.96263 prc_auc 0.90634[0m
[93maverage test of epoch 17: loss 3.40312 acc 0.20000 roc_auc 0.64676 prc_auc 0.34538[0m
[92maverage training of epoch 18: loss 0.14991 acc 0.95650 roc_auc 0.96629 prc_auc 0.90766[0m
[93maverage test of epoch 18: loss 3.44532 acc 0.20000 roc_auc 0.64776 prc_auc 0.34822[0m
[92maverage training of epoch 19: loss 0.14748 acc 0.95650 roc_auc 0.96734 prc_auc 0.90753[0m
[93maverage test of epoch 19: loss 3.42014 acc 0.20000 roc_auc 0.64805 prc_auc 0.34824[0m
[92maverage training of epoch 20: loss 0.14526 acc 0.95750 roc_auc 0.96852 prc_auc 0.91605[0m
[93maverage test of epoch 20: loss 3.48818 acc 0.20000 roc_auc 0.64803 prc_auc 0.34935[0m
[92maverage training of epoch 21: loss 0.14413 acc 0.95850 roc_auc 0.96882 prc_auc 0.91747[0m
[93maverage test of epoch 21: loss 3.51120 acc 0.20000 roc_auc 0.64828 prc_auc 0.35500[0m
[92maverage training of epoch 22: loss 0.14240 acc 0.95800 roc_auc 0.96951 prc_auc 0.91979[0m
[93maverage test of epoch 22: loss 3.56147 acc 0.20000 roc_auc 0.64795 prc_auc 0.34756[0m
[92maverage training of epoch 23: loss 0.14030 acc 0.95850 roc_auc 0.97028 prc_auc 0.92202[0m
[93maverage test of epoch 23: loss 3.57164 acc 0.20000 roc_auc 0.64872 prc_auc 0.35478[0m
[92maverage training of epoch 24: loss 0.13908 acc 0.95950 roc_auc 0.97065 prc_auc 0.92377[0m
[93maverage test of epoch 24: loss 3.58074 acc 0.20200 roc_auc 0.64959 prc_auc 0.35653[0m
[92maverage training of epoch 25: loss 0.13894 acc 0.95950 roc_auc 0.97087 prc_auc 0.92580[0m
[93maverage test of epoch 25: loss 3.66583 acc 0.20200 roc_auc 0.65045 prc_auc 0.35691[0m
[92maverage training of epoch 26: loss 0.14103 acc 0.95850 roc_auc 0.97037 prc_auc 0.91919[0m
[93maverage test of epoch 26: loss 3.56915 acc 0.20200 roc_auc 0.65103 prc_auc 0.36046[0m
[92maverage training of epoch 27: loss 0.13955 acc 0.95900 roc_auc 0.97104 prc_auc 0.92585[0m
[93maverage test of epoch 27: loss 3.62639 acc 0.20200 roc_auc 0.65159 prc_auc 0.36133[0m
[92maverage training of epoch 28: loss 0.13897 acc 0.95950 roc_auc 0.97115 prc_auc 0.92616[0m
[93maverage test of epoch 28: loss 3.62259 acc 0.20200 roc_auc 0.65272 prc_auc 0.36687[0m
[92maverage training of epoch 29: loss 0.13914 acc 0.96000 roc_auc 0.96967 prc_auc 0.92633[0m
[93maverage test of epoch 29: loss 3.63861 acc 0.20200 roc_auc 0.65373 prc_auc 0.36780[0m
[92maverage training of epoch 30: loss 0.13597 acc 0.96000 roc_auc 0.97263 prc_auc 0.92947[0m
[93maverage test of epoch 30: loss 3.69297 acc 0.20200 roc_auc 0.65423 prc_auc 0.36839[0m
[92maverage training of epoch 31: loss 0.13698 acc 0.96000 roc_auc 0.97229 prc_auc 0.92818[0m
[93maverage test of epoch 31: loss 3.65704 acc 0.20200 roc_auc 0.65490 prc_auc 0.37015[0m
[92maverage training of epoch 32: loss 0.13702 acc 0.96050 roc_auc 0.97233 prc_auc 0.92942[0m
[93maverage test of epoch 32: loss 3.68988 acc 0.20200 roc_auc 0.65578 prc_auc 0.37231[0m
[92maverage training of epoch 33: loss 0.13911 acc 0.96000 roc_auc 0.97136 prc_auc 0.92805[0m
[93maverage test of epoch 33: loss 3.68454 acc 0.20200 roc_auc 0.65670 prc_auc 0.37698[0m
[92maverage training of epoch 34: loss 0.13776 acc 0.96000 roc_auc 0.97191 prc_auc 0.92998[0m
[93maverage test of epoch 34: loss 3.71293 acc 0.20200 roc_auc 0.65618 prc_auc 0.37544[0m
[92maverage training of epoch 35: loss 0.13751 acc 0.96050 roc_auc 0.97063 prc_auc 0.93004[0m
[93maverage test of epoch 35: loss 3.70263 acc 0.20200 roc_auc 0.65678 prc_auc 0.37787[0m
[92maverage training of epoch 36: loss 0.13540 acc 0.96050 roc_auc 0.97292 prc_auc 0.93128[0m
[93maverage test of epoch 36: loss 3.72106 acc 0.20200 roc_auc 0.65712 prc_auc 0.38006[0m
[92maverage training of epoch 37: loss 0.13559 acc 0.96050 roc_auc 0.97283 prc_auc 0.93131[0m
[93maverage test of epoch 37: loss 3.70176 acc 0.20200 roc_auc 0.65736 prc_auc 0.38282[0m
[92maverage training of epoch 38: loss 0.13536 acc 0.96050 roc_auc 0.97278 prc_auc 0.93225[0m
[93maverage test of epoch 38: loss 3.77579 acc 0.20200 roc_auc 0.65742 prc_auc 0.38437[0m
[92maverage training of epoch 39: loss 0.13504 acc 0.96050 roc_auc 0.97274 prc_auc 0.93189[0m
[93maverage test of epoch 39: loss 3.72908 acc 0.20200 roc_auc 0.65798 prc_auc 0.38800[0m
[92maverage training of epoch 40: loss 0.13442 acc 0.96050 roc_auc 0.97287 prc_auc 0.93380[0m
[93maverage test of epoch 40: loss 3.79307 acc 0.20200 roc_auc 0.65931 prc_auc 0.38973[0m
[92maverage training of epoch 41: loss 0.13356 acc 0.96150 roc_auc 0.97299 prc_auc 0.93503[0m
[93maverage test of epoch 41: loss 3.78281 acc 0.20200 roc_auc 0.65995 prc_auc 0.39175[0m
[92maverage training of epoch 42: loss 0.13480 acc 0.96100 roc_auc 0.97262 prc_auc 0.93415[0m
[93maverage test of epoch 42: loss 3.78827 acc 0.20200 roc_auc 0.66082 prc_auc 0.39296[0m
[92maverage training of epoch 43: loss 0.13327 acc 0.96150 roc_auc 0.97290 prc_auc 0.93609[0m
[93maverage test of epoch 43: loss 3.83063 acc 0.20200 roc_auc 0.66106 prc_auc 0.39395[0m
[92maverage training of epoch 44: loss 0.13261 acc 0.96150 roc_auc 0.97303 prc_auc 0.93554[0m
[93maverage test of epoch 44: loss 3.77790 acc 0.20200 roc_auc 0.66163 prc_auc 0.39951[0m
[92maverage training of epoch 45: loss 0.13200 acc 0.96150 roc_auc 0.97308 prc_auc 0.93682[0m
[93maverage test of epoch 45: loss 3.80896 acc 0.20200 roc_auc 0.66192 prc_auc 0.39661[0m
[92maverage training of epoch 46: loss 0.13133 acc 0.96200 roc_auc 0.97317 prc_auc 0.93703[0m
[93maverage test of epoch 46: loss 3.77352 acc 0.20200 roc_auc 0.66175 prc_auc 0.39844[0m
[92maverage training of epoch 47: loss 0.13130 acc 0.96250 roc_auc 0.97308 prc_auc 0.93730[0m
[93maverage test of epoch 47: loss 3.78476 acc 0.20200 roc_auc 0.66136 prc_auc 0.40107[0m
[92maverage training of epoch 48: loss 0.13091 acc 0.96250 roc_auc 0.97337 prc_auc 0.93815[0m
[93maverage test of epoch 48: loss 3.80455 acc 0.20200 roc_auc 0.66120 prc_auc 0.40361[0m
[92maverage training of epoch 49: loss 0.13116 acc 0.96300 roc_auc 0.97319 prc_auc 0.93698[0m
[93maverage test of epoch 49: loss 3.76628 acc 0.20200 roc_auc 0.66119 prc_auc 0.40473[0m
Training model with dataset, testing using fold 4
[92maverage training of epoch 0: loss 0.26860 acc 0.91450 roc_auc 0.94962 prc_auc 0.88285[0m
[93maverage test of epoch 0: loss 1.15834 acc 0.20000 roc_auc 0.38182 prc_auc 0.16524[0m
[92maverage training of epoch 1: loss 0.40225 acc 0.75200 roc_auc 0.80030 prc_auc 0.36864[0m
[93maverage test of epoch 1: loss 0.92930 acc 0.20000 roc_auc 0.51743 prc_auc 0.20096[0m
[92maverage training of epoch 2: loss 0.19140 acc 0.94350 roc_auc 0.96827 prc_auc 0.93500[0m
[93maverage test of epoch 2: loss 1.70970 acc 0.20000 roc_auc 0.50083 prc_auc 0.19739[0m
[92maverage training of epoch 3: loss 0.19071 acc 0.93200 roc_auc 0.95900 prc_auc 0.89175[0m
[93maverage test of epoch 3: loss 2.51337 acc 0.20000 roc_auc 0.49609 prc_auc 0.18516[0m
[92maverage training of epoch 4: loss 0.19760 acc 0.93050 roc_auc 0.95407 prc_auc 0.86476[0m
[93maverage test of epoch 4: loss 2.90273 acc 0.20000 roc_auc 0.43798 prc_auc 0.17088[0m
[92maverage training of epoch 5: loss 0.22067 acc 0.92400 roc_auc 0.94436 prc_auc 0.75456[0m
[93maverage test of epoch 5: loss 2.36437 acc 0.20000 roc_auc 0.33644 prc_auc 0.14984[0m
[92maverage training of epoch 6: loss 0.23150 acc 0.92200 roc_auc 0.94268 prc_auc 0.83837[0m
[93maverage test of epoch 6: loss 2.56933 acc 0.20000 roc_auc 0.63139 prc_auc 0.27059[0m
[92maverage training of epoch 7: loss 0.22346 acc 0.92150 roc_auc 0.94237 prc_auc 0.78183[0m
[93maverage test of epoch 7: loss 2.50296 acc 0.20000 roc_auc 0.65258 prc_auc 0.31826[0m
[92maverage training of epoch 8: loss 0.22366 acc 0.91900 roc_auc 0.94404 prc_auc 0.84115[0m
[93maverage test of epoch 8: loss 3.07800 acc 0.20000 roc_auc 0.64105 prc_auc 0.27709[0m
[92maverage training of epoch 9: loss 0.22769 acc 0.92150 roc_auc 0.94202 prc_auc 0.77737[0m
[93maverage test of epoch 9: loss 2.74466 acc 0.20000 roc_auc 0.66781 prc_auc 0.37850[0m
[92maverage training of epoch 10: loss 0.21615 acc 0.92650 roc_auc 0.94485 prc_auc 0.81192[0m
[93maverage test of epoch 10: loss 2.74968 acc 0.20000 roc_auc 0.66976 prc_auc 0.38765[0m
[92maverage training of epoch 11: loss 0.21746 acc 0.92750 roc_auc 0.94512 prc_auc 0.83089[0m
[93maverage test of epoch 11: loss 2.85484 acc 0.20000 roc_auc 0.66803 prc_auc 0.39301[0m
[92maverage training of epoch 12: loss 0.20605 acc 0.93200 roc_auc 0.94885 prc_auc 0.83222[0m
[93maverage test of epoch 12: loss 2.85477 acc 0.20000 roc_auc 0.67374 prc_auc 0.41228[0m
[92maverage training of epoch 13: loss 0.20423 acc 0.93550 roc_auc 0.94869 prc_auc 0.84495[0m
[93maverage test of epoch 13: loss 2.91276 acc 0.20000 roc_auc 0.67553 prc_auc 0.41386[0m
[92maverage training of epoch 14: loss 0.20175 acc 0.93800 roc_auc 0.94710 prc_auc 0.83111[0m
[93maverage test of epoch 14: loss 2.84854 acc 0.20000 roc_auc 0.67936 prc_auc 0.42435[0m
[92maverage training of epoch 15: loss 0.17936 acc 0.94300 roc_auc 0.95817 prc_auc 0.87293[0m
[93maverage test of epoch 15: loss 2.93814 acc 0.20000 roc_auc 0.67954 prc_auc 0.42850[0m
[92maverage training of epoch 16: loss 0.17663 acc 0.94450 roc_auc 0.95902 prc_auc 0.86178[0m
[93maverage test of epoch 16: loss 2.91311 acc 0.20000 roc_auc 0.68071 prc_auc 0.42997[0m
[92maverage training of epoch 17: loss 0.17465 acc 0.94700 roc_auc 0.95952 prc_auc 0.87488[0m
[93maverage test of epoch 17: loss 2.93121 acc 0.20000 roc_auc 0.68151 prc_auc 0.42903[0m
[92maverage training of epoch 18: loss 0.16487 acc 0.94800 roc_auc 0.96322 prc_auc 0.85575[0m
[93maverage test of epoch 18: loss 2.83947 acc 0.20000 roc_auc 0.68248 prc_auc 0.43038[0m
[92maverage training of epoch 19: loss 0.16182 acc 0.94800 roc_auc 0.96461 prc_auc 0.89114[0m
[93maverage test of epoch 19: loss 2.93960 acc 0.20000 roc_auc 0.68300 prc_auc 0.43155[0m
[92maverage training of epoch 20: loss 0.16048 acc 0.94900 roc_auc 0.96480 prc_auc 0.88738[0m
[93maverage test of epoch 20: loss 2.98672 acc 0.20000 roc_auc 0.68242 prc_auc 0.42990[0m
[92maverage training of epoch 21: loss 0.15948 acc 0.94900 roc_auc 0.96478 prc_auc 0.87898[0m
[93maverage test of epoch 21: loss 2.95852 acc 0.20000 roc_auc 0.68306 prc_auc 0.43116[0m
[92maverage training of epoch 22: loss 0.16212 acc 0.94850 roc_auc 0.96387 prc_auc 0.87670[0m
[93maverage test of epoch 22: loss 2.92941 acc 0.20000 roc_auc 0.68415 prc_auc 0.43240[0m
[92maverage training of epoch 23: loss 0.15859 acc 0.94900 roc_auc 0.96445 prc_auc 0.89177[0m
[93maverage test of epoch 23: loss 2.97961 acc 0.20000 roc_auc 0.68433 prc_auc 0.43392[0m
[92maverage training of epoch 24: loss 0.16467 acc 0.95100 roc_auc 0.95823 prc_auc 0.88792[0m
[93maverage test of epoch 24: loss 3.00401 acc 0.20000 roc_auc 0.68421 prc_auc 0.43511[0m
[92maverage training of epoch 25: loss 0.15256 acc 0.95100 roc_auc 0.96747 prc_auc 0.88263[0m
[93maverage test of epoch 25: loss 2.97965 acc 0.20000 roc_auc 0.68407 prc_auc 0.43500[0m
[92maverage training of epoch 26: loss 0.15349 acc 0.95100 roc_auc 0.96718 prc_auc 0.88594[0m
[93maverage test of epoch 26: loss 2.95644 acc 0.20000 roc_auc 0.68397 prc_auc 0.43697[0m
[92maverage training of epoch 27: loss 0.15512 acc 0.95200 roc_auc 0.96517 prc_auc 0.89746[0m
[93maverage test of epoch 27: loss 3.01449 acc 0.20000 roc_auc 0.68405 prc_auc 0.43376[0m
[92maverage training of epoch 28: loss 0.15324 acc 0.95300 roc_auc 0.96562 prc_auc 0.88679[0m
[93maverage test of epoch 28: loss 2.96984 acc 0.20000 roc_auc 0.68377 prc_auc 0.43188[0m
[92maverage training of epoch 29: loss 0.14812 acc 0.95450 roc_auc 0.96870 prc_auc 0.89671[0m
[93maverage test of epoch 29: loss 2.99862 acc 0.20000 roc_auc 0.68415 prc_auc 0.43353[0m
[92maverage training of epoch 30: loss 0.14984 acc 0.95350 roc_auc 0.96811 prc_auc 0.89077[0m
[93maverage test of epoch 30: loss 2.97086 acc 0.20000 roc_auc 0.68390 prc_auc 0.43311[0m
[92maverage training of epoch 31: loss 0.14895 acc 0.95450 roc_auc 0.96848 prc_auc 0.89894[0m
[93maverage test of epoch 31: loss 2.99044 acc 0.20000 roc_auc 0.68407 prc_auc 0.43396[0m
[92maverage training of epoch 32: loss 0.14906 acc 0.95450 roc_auc 0.96819 prc_auc 0.89804[0m
[93maverage test of epoch 32: loss 3.00524 acc 0.20000 roc_auc 0.68385 prc_auc 0.43394[0m
[92maverage training of epoch 33: loss 0.14846 acc 0.95500 roc_auc 0.96832 prc_auc 0.89913[0m
[93maverage test of epoch 33: loss 3.00741 acc 0.20000 roc_auc 0.68405 prc_auc 0.43296[0m
[92maverage training of epoch 34: loss 0.14839 acc 0.95500 roc_auc 0.96833 prc_auc 0.90062[0m
[93maverage test of epoch 34: loss 3.01640 acc 0.20000 roc_auc 0.68424 prc_auc 0.43240[0m
[92maverage training of epoch 35: loss 0.15262 acc 0.95450 roc_auc 0.96497 prc_auc 0.89556[0m
[93maverage test of epoch 35: loss 3.01524 acc 0.20000 roc_auc 0.68375 prc_auc 0.43236[0m
[92maverage training of epoch 36: loss 0.14713 acc 0.95550 roc_auc 0.96840 prc_auc 0.90216[0m
[93maverage test of epoch 36: loss 3.01940 acc 0.20000 roc_auc 0.68373 prc_auc 0.42976[0m
[92maverage training of epoch 37: loss 0.14599 acc 0.95650 roc_auc 0.96893 prc_auc 0.90656[0m
[93maverage test of epoch 37: loss 3.06662 acc 0.20000 roc_auc 0.68585 prc_auc 0.43860[0m
[92maverage training of epoch 38: loss 0.14578 acc 0.95700 roc_auc 0.96871 prc_auc 0.90139[0m
[93maverage test of epoch 38: loss 3.04963 acc 0.20000 roc_auc 0.68388 prc_auc 0.43651[0m
[92maverage training of epoch 39: loss 0.14487 acc 0.95700 roc_auc 0.96908 prc_auc 0.90558[0m
[93maverage test of epoch 39: loss 3.05347 acc 0.20000 roc_auc 0.68442 prc_auc 0.43404[0m
[92maverage training of epoch 40: loss 0.15255 acc 0.95800 roc_auc 0.96260 prc_auc 0.90456[0m
[93maverage test of epoch 40: loss 3.07924 acc 0.20000 roc_auc 0.68355 prc_auc 0.43280[0m
[92maverage training of epoch 41: loss 0.14068 acc 0.95900 roc_auc 0.97108 prc_auc 0.90196[0m
[93maverage test of epoch 41: loss 3.03202 acc 0.20000 roc_auc 0.68458 prc_auc 0.43465[0m
[92maverage training of epoch 42: loss 0.14193 acc 0.95800 roc_auc 0.97097 prc_auc 0.91208[0m
[93maverage test of epoch 42: loss 3.11573 acc 0.20000 roc_auc 0.68425 prc_auc 0.43080[0m
[92maverage training of epoch 43: loss 0.14170 acc 0.95800 roc_auc 0.97079 prc_auc 0.90882[0m
[93maverage test of epoch 43: loss 3.10427 acc 0.20000 roc_auc 0.68484 prc_auc 0.43089[0m
[92maverage training of epoch 44: loss 0.14145 acc 0.95850 roc_auc 0.97073 prc_auc 0.91118[0m
[93maverage test of epoch 44: loss 3.12007 acc 0.20000 roc_auc 0.68480 prc_auc 0.43116[0m
[92maverage training of epoch 45: loss 0.14120 acc 0.95900 roc_auc 0.97068 prc_auc 0.91204[0m
[93maverage test of epoch 45: loss 3.12211 acc 0.20000 roc_auc 0.68459 prc_auc 0.43032[0m
[92maverage training of epoch 46: loss 0.14560 acc 0.95900 roc_auc 0.96651 prc_auc 0.91131[0m
[93maverage test of epoch 46: loss 3.15591 acc 0.20000 roc_auc 0.68490 prc_auc 0.42533[0m
[92maverage training of epoch 47: loss 0.14535 acc 0.95900 roc_auc 0.96742 prc_auc 0.90930[0m
[93maverage test of epoch 47: loss 3.14461 acc 0.20200 roc_auc 0.68390 prc_auc 0.42103[0m
[92maverage training of epoch 48: loss 0.13925 acc 0.96000 roc_auc 0.97169 prc_auc 0.91064[0m
[93maverage test of epoch 48: loss 3.11000 acc 0.20000 roc_auc 0.68453 prc_auc 0.42469[0m
[92maverage training of epoch 49: loss 0.14026 acc 0.95900 roc_auc 0.97144 prc_auc 0.91514[0m
[93maverage test of epoch 49: loss 3.14409 acc 0.20000 roc_auc 0.68390 prc_auc 0.41681[0m
Run statistics: 
==== Configuration Settings ====
== Run Settings ==
Model: DiffPool, Dataset: NCI-H23
num_epochs: 50
learning_rate: 0.0001
seed: 1800
k_fold: 5
model: DiffPool
dataset: NCI-H23

== Model Settings and results ==
convolution_layers_size: 64-64-64
pred_hidden_layers: 50-50-50
assign_ratio: 0.25
number_of_pooling: 1
concat_tensors: False

Accuracy (avg): 0.2016 ROC_AUC (avg): 0.69797 PRC_AUC (avg): 0.4281 

Average forward propagation time taken(ms): 2.4490246673868983
Average backward propagation time taken(ms): 3.51914497429505

