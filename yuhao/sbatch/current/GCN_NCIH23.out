# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======


torch.cuda.is_available():  True 


load_data.py load_model_data(): Unserialising pickled dataset into Graph objects
==== Dataset Information ====
== General Information == 
Number of graphs: 2500
Number of classes: 2
Class distribution: 
0:2000 1:500 

== Node information== 
Average number of nodes: 27
Average number of edges (undirected): 29
Max number of nodes: 93
Number of distinct node labels: 18
Average number of distinct node labels: 3
Node labels distribution: 
0:9454 1:6143 13:139 16:2 18:1 2:49955 20:15 21:29 25:5 26:3 3:921 4:570 44:1 49:1 5:70 6:485 8:14 9:4 

*** 3 dataset_features:  {'name': 'NCI-H23', 'num_class': 2, 'label_dict': {'0': 0, '1': 1}, 'have_node_labels': True, 'have_node_attributions': False, 'node_dict': {'0': 0, '1': 1, '13': 2, '16': 3, '18': 4, '2': 5, '20': 6, '21': 7, '25': 8, '26': 9, '3': 10, '4': 11, '44': 12, '49': 13, '5': 14, '6': 15, '8': 16, '9': 17, 'UNKNOWN': 18}, 'feat_dim': 19, 'edge_feat_dim': 0, 'max_num_nodes': 93, 'avg_num_nodes': 27, 'graph_sizes_list': [34, 30, 34, 19, 20, 20, 13, 45, 22, 20, 26, 20, 15, 25, 56, 34, 19, 56, 24, 22, 25, 23, 24, 27, 25, 29, 27, 24, 22, 38, 22, 26, 23, 9, 7, 25, 35, 22, 18, 15, 24, 24, 27, 20, 18, 31, 25, 28, 46, 13, 29, 31, 21, 29, 20, 25, 48, 42, 28, 23, 34, 17, 28, 28, 15, 41, 35, 30, 20, 27, 25, 19, 31, 17, 29, 30, 75, 34, 22, 24, 33, 14, 22, 13, 24, 16, 17, 22, 13, 31, 26, 19, 15, 34, 26, 28, 24, 29, 18, 21, 12, 23, 23, 25, 22, 18, 16, 12, 17, 20, 27, 22, 19, 48, 23, 25, 33, 24, 17, 19, 24, 24, 10, 23, 14, 30, 22, 18, 26, 32, 31, 27, 30, 15, 29, 20, 19, 20, 34, 14, 15, 31, 18, 18, 24, 41, 44, 20, 38, 30, 28, 16, 29, 23, 31, 15, 11, 35, 29, 34, 19, 40, 18, 29, 27, 32, 17, 20, 20, 21, 15, 21, 31, 20, 57, 23, 13, 49, 32, 34, 31, 9, 20, 12, 16, 21, 28, 27, 17, 37, 32, 27, 16, 40, 21, 7, 30, 24, 16, 31, 18, 30, 20, 13, 20, 22, 23, 31, 20, 26, 32, 43, 14, 20, 14, 16, 20, 62, 19, 8, 31, 23, 35, 37, 20, 42, 26, 17, 46, 23, 19, 15, 23, 24, 23, 29, 33, 21, 20, 12, 58, 25, 15, 14, 26, 37, 15, 28, 48, 24, 21, 23, 22, 22, 51, 14, 17, 24, 23, 11, 18, 26, 35, 10, 19, 18, 14, 28, 23, 28, 18, 38, 42, 29, 24, 21, 28, 18, 44, 27, 27, 19, 32, 32, 20, 41, 24, 34, 32, 25, 18, 16, 28, 10, 22, 29, 21, 29, 17, 21, 24, 12, 41, 20, 24, 21, 26, 29, 16, 31, 30, 41, 35, 29, 27, 25, 31, 32, 29, 53, 13, 42, 27, 17, 23, 19, 40, 21, 21, 23, 18, 48, 33, 25, 29, 21, 18, 24, 22, 34, 27, 25, 15, 24, 23, 19, 13, 24, 19, 30, 27, 31, 41, 24, 60, 36, 40, 13, 16, 23, 28, 15, 28, 13, 13, 35, 21, 23, 24, 14, 25, 52, 25, 19, 16, 24, 26, 16, 13, 33, 12, 24, 26, 21, 11, 20, 23, 15, 21, 18, 47, 24, 23, 24, 24, 24, 22, 18, 27, 19, 18, 32, 24, 23, 30, 19, 21, 24, 21, 20, 50, 32, 26, 37, 17, 22, 31, 22, 19, 31, 14, 14, 36, 17, 22, 36, 17, 22, 21, 34, 24, 27, 16, 15, 16, 16, 29, 15, 21, 20, 32, 42, 35, 17, 27, 17, 28, 25, 28, 13, 20, 29, 20, 23, 16, 10, 16, 24, 24, 15, 26, 14, 27, 30, 8, 21, 27, 45, 34, 26, 17, 35, 31, 26, 31, 24, 22, 18, 29, 18, 21, 16, 21, 28, 18, 51, 41, 32, 19, 26, 24, 16, 20, 14, 41, 29, 43, 17, 29, 27, 32, 22, 20, 16, 15, 30, 26, 25, 25, 20, 41, 17, 14, 24, 21, 15, 24, 29, 24, 39, 22, 33, 15, 33, 28, 17, 19, 22, 9, 30, 25, 28, 17, 12, 38, 35, 15, 31, 24, 64, 18, 26, 22, 34, 11, 26, 19, 15, 36, 33, 38, 19, 20, 68, 18, 18, 19, 26, 12, 24, 30, 17, 26, 22, 22, 20, 59, 9, 32, 30, 16, 33, 33, 18, 40, 22, 30, 36, 20, 39, 40, 16, 19, 19, 30, 23, 33, 43, 22, 35, 26, 34, 30, 36, 50, 22, 17, 19, 24, 30, 26, 14, 26, 22, 22, 22, 34, 32, 24, 18, 43, 15, 35, 26, 14, 20, 24, 33, 25, 19, 16, 12, 25, 30, 15, 22, 20, 52, 16, 31, 21, 14, 18, 17, 18, 38, 37, 15, 27, 16, 19, 32, 19, 36, 21, 24, 23, 22, 55, 25, 15, 21, 25, 28, 19, 16, 29, 15, 30, 27, 55, 23, 15, 30, 21, 21, 35, 24, 27, 20, 14, 31, 31, 27, 20, 19, 33, 18, 12, 24, 44, 19, 23, 26, 22, 16, 12, 28, 32, 26, 14, 25, 18, 22, 20, 22, 30, 34, 16, 30, 13, 14, 37, 17, 54, 29, 22, 23, 29, 20, 15, 76, 27, 42, 20, 35, 23, 20, 21, 30, 39, 27, 18, 27, 28, 31, 30, 24, 11, 31, 35, 45, 20, 17, 31, 28, 19, 17, 19, 19, 29, 28, 18, 25, 21, 12, 37, 21, 19, 32, 22, 29, 19, 24, 22, 22, 21, 26, 29, 31, 25, 10, 19, 35, 18, 17, 22, 29, 30, 17, 37, 17, 29, 18, 20, 37, 31, 28, 9, 18, 19, 18, 27, 26, 27, 16, 15, 20, 10, 20, 21, 32, 41, 14, 19, 24, 36, 33, 29, 25, 38, 27, 23, 7, 25, 20, 34, 38, 24, 22, 19, 27, 23, 20, 15, 23, 21, 16, 23, 41, 17, 26, 24, 14, 18, 28, 27, 34, 29, 58, 28, 27, 29, 58, 17, 35, 23, 24, 17, 27, 28, 29, 17, 19, 17, 30, 17, 41, 16, 45, 18, 22, 24, 21, 30, 20, 17, 32, 16, 25, 19, 36, 23, 39, 23, 32, 24, 25, 36, 26, 20, 70, 16, 24, 17, 23, 17, 24, 25, 31, 19, 25, 32, 35, 16, 25, 18, 22, 20, 22, 36, 24, 20, 22, 34, 38, 22, 15, 13, 43, 24, 17, 48, 38, 18, 36, 27, 25, 23, 14, 20, 25, 38, 21, 80, 15, 20, 34, 22, 22, 20, 38, 26, 31, 23, 22, 52, 40, 22, 15, 32, 25, 24, 21, 36, 40, 38, 24, 47, 19, 25, 23, 10, 28, 35, 24, 37, 13, 37, 19, 46, 28, 18, 18, 10, 21, 40, 40, 14, 25, 39, 32, 20, 32, 18, 22, 22, 23, 14, 37, 19, 20, 30, 34, 20, 30, 15, 27, 22, 23, 45, 34, 25, 23, 25, 25, 23, 16, 25, 22, 18, 32, 46, 39, 20, 20, 37, 16, 26, 30, 27, 38, 27, 30, 22, 21, 22, 20, 26, 19, 13, 28, 15, 37, 26, 18, 28, 62, 23, 33, 16, 22, 13, 21, 27, 60, 58, 27, 30, 35, 18, 27, 27, 24, 23, 20, 23, 24, 28, 15, 62, 30, 24, 22, 46, 25, 20, 27, 24, 36, 22, 23, 12, 21, 33, 27, 11, 23, 23, 12, 20, 31, 17, 20, 23, 8, 20, 22, 21, 27, 15, 32, 17, 35, 30, 42, 22, 17, 28, 21, 17, 44, 12, 15, 29, 24, 14, 16, 18, 63, 33, 30, 30, 37, 21, 20, 59, 13, 15, 18, 30, 28, 31, 22, 19, 42, 32, 35, 24, 26, 26, 23, 23, 27, 21, 19, 23, 17, 24, 32, 21, 16, 32, 13, 19, 20, 41, 17, 27, 26, 7, 18, 22, 21, 15, 23, 42, 30, 32, 19, 17, 54, 20, 22, 24, 31, 31, 14, 18, 19, 28, 15, 20, 10, 26, 21, 16, 19, 14, 29, 27, 22, 14, 23, 34, 37, 34, 27, 22, 20, 17, 24, 31, 57, 20, 45, 36, 23, 40, 22, 16, 24, 26, 23, 3, 19, 17, 32, 17, 38, 32, 20, 23, 25, 45, 19, 39, 21, 35, 29, 35, 24, 39, 26, 21, 15, 29, 38, 16, 43, 26, 31, 24, 27, 30, 25, 21, 28, 16, 24, 22, 16, 25, 14, 28, 32, 35, 23, 18, 18, 20, 12, 23, 22, 16, 22, 18, 30, 31, 24, 29, 26, 42, 7, 14, 44, 15, 23, 21, 22, 25, 35, 21, 15, 14, 21, 29, 23, 29, 23, 19, 28, 37, 30, 40, 30, 8, 20, 20, 28, 20, 15, 14, 33, 22, 26, 46, 30, 28, 26, 15, 26, 9, 20, 22, 26, 17, 15, 15, 31, 21, 21, 27, 22, 19, 29, 31, 22, 17, 12, 23, 26, 17, 24, 20, 31, 35, 21, 16, 17, 48, 40, 10, 27, 12, 19, 20, 19, 51, 30, 19, 15, 29, 22, 38, 34, 23, 63, 20, 22, 15, 27, 36, 18, 51, 17, 2, 26, 28, 30, 22, 20, 50, 15, 27, 21, 26, 21, 22, 18, 33, 20, 21, 39, 40, 18, 16, 16, 21, 17, 30, 17, 26, 26, 31, 44, 27, 31, 44, 19, 29, 34, 24, 16, 20, 18, 26, 29, 39, 35, 58, 20, 10, 27, 41, 21, 19, 19, 13, 28, 47, 20, 28, 14, 20, 17, 15, 28, 25, 24, 15, 27, 21, 27, 17, 47, 37, 5, 33, 31, 17, 45, 34, 35, 18, 22, 29, 25, 22, 21, 29, 24, 13, 26, 28, 23, 28, 28, 13, 22, 11, 29, 51, 24, 14, 25, 17, 41, 20, 16, 24, 25, 10, 31, 26, 25, 19, 18, 11, 14, 27, 18, 11, 17, 26, 23, 32, 28, 27, 27, 18, 15, 16, 31, 33, 33, 18, 22, 21, 22, 18, 31, 65, 31, 22, 25, 18, 22, 32, 27, 15, 22, 32, 41, 26, 79, 21, 21, 25, 10, 29, 35, 26, 13, 28, 29, 39, 31, 12, 39, 14, 27, 14, 23, 9, 29, 38, 21, 24, 31, 19, 33, 12, 32, 46, 17, 20, 21, 17, 23, 25, 43, 27, 24, 25, 17, 19, 20, 27, 14, 21, 14, 24, 29, 22, 29, 28, 20, 27, 25, 31, 26, 22, 25, 22, 29, 29, 26, 19, 23, 27, 22, 20, 17, 31, 17, 18, 22, 22, 21, 28, 22, 28, 45, 19, 34, 23, 28, 26, 19, 28, 29, 22, 24, 19, 19, 25, 14, 25, 19, 31, 29, 36, 22, 28, 14, 29, 20, 32, 31, 18, 34, 14, 27, 25, 13, 32, 39, 17, 35, 16, 22, 22, 26, 60, 30, 29, 27, 29, 30, 53, 20, 18, 26, 22, 33, 15, 19, 21, 19, 31, 33, 19, 58, 21, 12, 24, 31, 16, 28, 40, 30, 18, 62, 40, 33, 21, 11, 34, 21, 32, 22, 20, 26, 31, 61, 25, 17, 21, 34, 28, 23, 28, 26, 43, 31, 69, 17, 14, 18, 14, 33, 38, 27, 27, 27, 42, 20, 33, 20, 17, 44, 27, 33, 27, 27, 30, 22, 24, 25, 31, 20, 14, 14, 44, 32, 36, 40, 25, 40, 25, 26, 25, 20, 47, 26, 17, 16, 18, 20, 12, 19, 8, 20, 26, 29, 9, 25, 15, 21, 25, 40, 25, 25, 21, 26, 22, 26, 15, 12, 25, 32, 19, 27, 28, 54, 21, 22, 14, 36, 27, 29, 22, 23, 22, 32, 34, 18, 30, 33, 17, 35, 27, 23, 30, 31, 26, 32, 25, 34, 32, 29, 23, 30, 21, 34, 27, 25, 26, 38, 25, 18, 23, 24, 48, 25, 33, 23, 43, 26, 32, 19, 22, 37, 27, 28, 31, 16, 22, 30, 19, 29, 32, 26, 24, 39, 22, 17, 35, 38, 45, 19, 24, 23, 18, 21, 22, 23, 42, 12, 21, 25, 15, 28, 18, 24, 21, 34, 32, 18, 43, 41, 22, 46, 14, 27, 11, 22, 15, 28, 34, 22, 22, 31, 28, 30, 17, 80, 15, 26, 23, 16, 62, 18, 23, 37, 23, 23, 12, 27, 26, 21, 19, 18, 29, 19, 61, 25, 27, 43, 37, 20, 29, 19, 26, 21, 24, 22, 39, 37, 10, 41, 12, 31, 20, 17, 16, 21, 29, 20, 27, 12, 16, 21, 21, 27, 16, 38, 28, 20, 25, 35, 15, 14, 38, 26, 30, 33, 27, 9, 21, 26, 26, 18, 28, 31, 12, 36, 41, 23, 14, 24, 32, 13, 23, 57, 32, 32, 16, 39, 18, 29, 24, 35, 29, 9, 34, 20, 19, 36, 30, 34, 40, 35, 17, 18, 7, 31, 40, 21, 35, 13, 29, 18, 35, 19, 33, 32, 28, 22, 18, 22, 18, 32, 18, 32, 20, 27, 16, 21, 14, 20, 20, 18, 20, 24, 38, 17, 16, 26, 12, 20, 34, 20, 18, 27, 39, 59, 12, 28, 27, 24, 27, 15, 17, 17, 36, 15, 26, 18, 19, 24, 14, 21, 20, 30, 21, 36, 31, 32, 36, 29, 27, 25, 21, 47, 32, 26, 33, 26, 63, 57, 31, 39, 13, 40, 21, 34, 33, 62, 23, 38, 23, 23, 18, 34, 63, 43, 34, 17, 34, 70, 28, 53, 29, 26, 29, 17, 42, 18, 17, 62, 22, 24, 27, 16, 45, 41, 36, 42, 45, 28, 20, 32, 70, 67, 17, 46, 47, 40, 24, 22, 30, 37, 37, 29, 14, 29, 47, 53, 20, 23, 41, 44, 19, 61, 42, 28, 22, 34, 34, 51, 29, 11, 39, 33, 36, 82, 61, 32, 44, 69, 23, 27, 26, 39, 37, 58, 27, 23, 28, 21, 35, 39, 29, 60, 28, 30, 31, 21, 37, 19, 21, 27, 11, 44, 60, 19, 35, 57, 35, 78, 27, 62, 31, 19, 32, 39, 25, 29, 65, 53, 47, 22, 35, 62, 66, 93, 65, 35, 46, 27, 35, 35, 41, 27, 40, 24, 32, 33, 26, 28, 32, 20, 25, 25, 17, 41, 57, 35, 27, 42, 25, 16, 19, 53, 38, 28, 34, 40, 22, 27, 65, 30, 22, 27, 12, 26, 44, 24, 20, 12, 21, 39, 37, 33, 28, 30, 33, 45, 19, 68, 25, 28, 47, 52, 21, 28, 39, 36, 25, 22, 39, 52, 61, 16, 45, 38, 3, 45, 40, 25, 52, 20, 19, 19, 26, 19, 27, 23, 29, 26, 29, 27, 35, 20, 23, 26, 46, 34, 35, 20, 25, 35, 54, 36, 31, 42, 33, 23, 28, 27, 19, 33, 25, 41, 21, 16, 41, 19, 24, 37, 39, 26, 31, 26, 22, 20, 18, 39, 26, 30, 20, 31, 26, 22, 24, 41, 30, 28, 33, 22, 40, 42, 30, 22, 28, 20, 30, 36, 42, 52, 24, 32, 38, 26, 34, 24, 40, 19, 26, 28, 27, 27, 16, 21, 31, 20, 58, 58, 23, 24, 38, 39, 34, 29, 34, 26, 25, 17, 30, 31, 25, 21, 28, 30, 13, 42, 26, 40, 23, 40, 38, 55, 25, 19, 39, 20, 32, 32, 66, 35, 61, 28, 53, 46, 39, 37, 28, 90, 35, 45, 38, 22, 21, 20, 62, 40, 46, 41, 24, 28, 32, 38, 56, 29, 30, 21, 47, 33, 67, 27, 20, 13, 47, 12, 25, 22, 45, 38, 25, 36, 25, 26, 33, 48, 34, 19, 25, 21, 32, 35, 33, 36, 67, 38, 35, 27, 47, 28, 26, 49, 29, 38, 32, 28, 26, 21, 30, 30, 22, 25, 35, 45, 25, 32, 34, 57, 23, 66, 27, 25, 46, 31, 35, 29, 26, 33, 39, 28, 23, 38, 14, 55, 32, 30, 29, 24, 79, 46, 11, 30, 32, 44, 30, 39, 37, 25, 76, 30, 84, 31, 24, 39, 28, 37, 28, 59, 21, 34, 23, 42, 45, 18, 30, 25, 34, 34, 20, 24, 46, 31, 28, 46, 30, 37, 35, 32, 37, 19, 18, 52, 18, 31, 34, 32, 18, 22, 37, 26, 50, 40, 28, 20, 30, 19, 15, 16, 22, 30, 38, 19, 35, 17, 22, 20, 29, 59, 59, 43, 27, 28, 25, 26, 62, 28, 26, 30, 34, 35], 'attr_dim': 0, 'dataset_info': '==== Dataset Information ====\n== General Information == \nNumber of graphs: 2500\nNumber of classes: 2\nClass distribution: \n0:2000 1:500 \n\n== Node information== \nAverage number of nodes: 27\nAverage number of edges (undirected): 29\nMax number of nodes: 93\nNumber of distinct node labels: 18\nAverage number of distinct node labels: 3\nNode labels distribution: \n0:9454 1:6143 13:139 16:2 18:1 2:49955 20:15 21:29 25:5 26:3 3:921 4:570 44:1 49:1 5:70 6:485 8:14 9:4 \n'}
*** 1 train_index:  [   1    2    3 ... 2497 2498 2499]
*** 2 test_index:  [   0   20   26   28   33   37   38   43   44   50   52   63   66   67
   70   71   75   85   86   87   97  102  103  106  107  111  114  117
  118  124  132  146  150  157  163  165  169  175  181  182  184  194
  198  200  203  207  208  215  216  217  219  221  229  230  231  239
  249  253  265  271  278  285  287  297  302  309  312  313  317  322
  323  329  333  334  335  336  337  342  348  353  354  364  365  366
  367  368  369  377  378  383  384  390  391  400  406  408  415  416
  423  426  435  437  438  443  448  454  463  479  483  485  486  491
  499  510  511  515  521  535  537  539  541  545  548  552  559  561
  568  575  582  591  592  600  606  619  637  644  645  647  650  673
  675  681  686  690  695  701  712  713  714  721  726  727  732  736
  739  741  743  750  752  754  757  761  764  775  778  788  789  797
  802  806  810  811  812  827  829  836  847  866  867  868  871  872
  882  884  885  890  891  892  894  903  914  915  918  922  928  929
  931  936  947  950  952  953  968  972  980  983  986  988 1000 1010
 1011 1014 1016 1022 1030 1031 1032 1039 1040 1050 1056 1057 1059 1064
 1066 1068 1073 1091 1092 1113 1116 1125 1126 1134 1135 1137 1145 1158
 1162 1167 1169 1178 1180 1194 1199 1209 1211 1212 1215 1219 1223 1225
 1233 1234 1238 1243 1252 1256 1257 1259 1260 1267 1273 1278 1279 1285
 1289 1290 1292 1294 1315 1316 1333 1336 1346 1347 1353 1356 1378 1386
 1387 1391 1396 1403 1406 1407 1410 1420 1427 1428 1429 1437 1441 1446
 1453 1455 1456 1458 1463 1465 1475 1476 1477 1482 1488 1496 1503 1505
 1506 1507 1509 1516 1522 1523 1524 1526 1528 1532 1535 1538 1541 1544
 1549 1553 1571 1574 1583 1595 1598 1611 1624 1629 1637 1642 1647 1653
 1655 1663 1667 1668 1673 1674 1676 1678 1689 1696 1698 1702 1707 1721
 1722 1740 1747 1762 1766 1769 1773 1775 1780 1783 1788 1797 1798 1801
 1806 1816 1831 1833 1835 1844 1845 1846 1847 1859 1865 1867 1868 1875
 1879 1883 1885 1898 1906 1907 1914 1915 1929 1932 1946 1947 1948 1955
 1958 1967 1975 1984 1988 1989 1995 1996 2007 2009 2012 2022 2025 2026
 2030 2048 2049 2065 2071 2073 2077 2080 2084 2090 2094 2100 2101 2109
 2111 2126 2142 2143 2150 2152 2153 2159 2169 2172 2175 2176 2178 2179
 2184 2185 2196 2201 2202 2206 2208 2210 2214 2216 2219 2224 2227 2231
 2233 2240 2244 2250 2251 2254 2266 2267 2274 2277 2278 2281 2284 2287
 2293 2297 2298 2303 2305 2315 2319 2331 2334 2336 2337 2338 2349 2357
 2358 2362 2373 2377 2384 2397 2413 2418 2425 2429 2431 2436 2439 2441
 2452 2453 2458 2459 2468 2473 2476 2481 2484 2491]
*** 1 train_index:  [   0    1    2 ... 2497 2498 2499]
*** 2 test_index:  [   3    4    6   11   12   21   22   23   27   29   34   42   47   48
   54   58   62   64   65   76   77   81   82   91   96   98   99  105
  112  113  120  126  128  131  141  142  149  151  153  160  171  176
  178  183  188  190  197  206  212  213  214  223  224  234  238  242
  244  250  251  254  256  261  270  273  276  277  279  281  283  288
  293  300  308  311  328  331  344  349  356  361  362  374  375  380
  382  385  387  389  399  404  405  411  417  418  427  430  445  449
  452  455  456  460  462  465  469  473  475  481  504  505  513  518
  528  532  538  543  544  547  553  562  567  569  581  583  584  599
  601  609  612  616  627  631  632  638  639  641  648  652  656  671
  679  685  687  688  693  696  709  724  734  744  749  753  762  768
  769  770  771  772  785  787  791  792  799  803  807  815  818  821
  828  833  838  842  855  858  880  886  897  900  906  907  908  912
  913  923  924  930  934  938  939  948  949  951  956  958  960  963
  964  966  974  977  981  982  984  991  992  997 1001 1002 1006 1007
 1008 1019 1028 1044 1047 1049 1058 1061 1067 1071 1076 1081 1086 1087
 1088 1090 1097 1108 1110 1112 1119 1140 1148 1152 1155 1156 1157 1163
 1172 1185 1187 1191 1197 1198 1207 1213 1216 1221 1222 1224 1226 1228
 1229 1235 1239 1242 1245 1246 1261 1265 1268 1269 1272 1298 1299 1300
 1309 1310 1312 1313 1319 1326 1327 1331 1339 1341 1345 1352 1354 1362
 1370 1372 1375 1390 1393 1397 1400 1401 1404 1411 1417 1418 1424 1425
 1432 1435 1436 1439 1442 1454 1461 1479 1490 1498 1500 1510 1518 1519
 1529 1530 1531 1537 1550 1551 1559 1565 1577 1578 1586 1587 1597 1599
 1602 1606 1610 1615 1619 1620 1627 1628 1630 1641 1646 1648 1650 1651
 1658 1661 1670 1672 1682 1684 1690 1691 1708 1716 1717 1725 1727 1728
 1732 1733 1739 1743 1745 1754 1770 1771 1779 1781 1785 1790 1791 1793
 1795 1805 1813 1815 1817 1827 1834 1838 1841 1843 1850 1853 1856 1872
 1873 1882 1886 1894 1899 1904 1913 1916 1918 1935 1941 1950 1951 1952
 1957 1969 1972 1973 1979 1983 1994 1999 2005 2010 2014 2028 2031 2038
 2041 2045 2046 2050 2056 2059 2061 2062 2063 2064 2072 2074 2078 2082
 2083 2085 2091 2093 2098 2106 2114 2115 2116 2119 2125 2136 2138 2139
 2141 2144 2160 2174 2182 2190 2195 2197 2205 2211 2212 2218 2225 2230
 2234 2243 2246 2255 2258 2270 2275 2276 2289 2290 2294 2299 2307 2309
 2310 2314 2316 2318 2322 2324 2335 2344 2363 2364 2369 2372 2379 2380
 2381 2385 2386 2393 2395 2400 2405 2406 2409 2410 2412 2415 2420 2422
 2424 2426 2427 2428 2444 2457 2470 2471 2475 2494]
*** 1 train_index:  [   0    1    2 ... 2494 2495 2498]
*** 2 test_index:  [  15   17   25   32   39   40   53   56   59   61   69   73   79   80
   84   88   92   95  100  104  108  116  119  121  122  123  134  135
  137  155  156  159  167  168  173  179  186  187  191  195  196  210
  220  233  243  252  257  264  267  268  269  280  282  286  290  291
  298  304  310  315  324  338  339  345  347  350  370  386  388  392
  394  396  397  403  412  421  422  431  442  457  459  461  466  468
  474  476  488  496  498  503  508  514  529  536  546  550  555  563
  571  573  574  576  579  589  595  598  602  605  610  613  614  615
  624  626  629  630  634  635  655  657  660  664  669  672  680  691
  694  699  700  702  705  706  725  728  733  735  737  738  747  751
  759  773  774  776  777  781  783  784  793  794  796  798  800  805
  809  816  825  830  831  832  834  839  844  846  848  849  851  852
  853  857  860  863  889  899  902  909  917  920  927  933  937  941
  944  957  962  965  969  973  990  999 1003 1005 1013 1026 1033 1035
 1036 1038 1041 1043 1045 1051 1055 1060 1063 1065 1070 1077 1078 1085
 1094 1098 1100 1102 1105 1111 1114 1118 1120 1123 1128 1132 1136 1139
 1142 1149 1154 1165 1170 1174 1183 1186 1188 1192 1196 1201 1202 1203
 1206 1214 1218 1220 1244 1249 1254 1255 1270 1283 1284 1288 1291 1296
 1306 1308 1320 1324 1335 1343 1349 1350 1351 1355 1357 1364 1368 1373
 1374 1376 1377 1379 1382 1395 1402 1412 1426 1430 1440 1443 1444 1445
 1450 1451 1452 1464 1466 1467 1468 1470 1472 1473 1474 1480 1495 1504
 1512 1539 1552 1555 1557 1558 1560 1562 1566 1567 1568 1570 1573 1575
 1581 1585 1589 1593 1601 1605 1612 1621 1625 1638 1640 1645 1649 1652
 1654 1656 1657 1659 1662 1665 1681 1686 1693 1695 1697 1700 1718 1720
 1724 1729 1731 1734 1737 1742 1746 1748 1750 1752 1755 1757 1760 1761
 1765 1777 1778 1782 1784 1792 1799 1803 1808 1809 1810 1812 1814 1819
 1825 1828 1830 1836 1842 1848 1854 1855 1862 1870 1876 1877 1881 1889
 1897 1901 1908 1911 1912 1919 1922 1924 1927 1936 1940 1942 1943 1944
 1953 1968 1974 1976 1977 1982 1990 1998 2000 2002 2004 2015 2024 2036
 2044 2052 2053 2057 2058 2070 2075 2076 2087 2097 2102 2108 2110 2113
 2117 2118 2120 2128 2129 2154 2155 2156 2158 2163 2164 2165 2167 2168
 2170 2180 2181 2186 2188 2189 2192 2193 2199 2207 2209 2217 2220 2228
 2235 2241 2245 2247 2248 2253 2256 2268 2282 2283 2285 2296 2300 2311
 2317 2321 2346 2348 2351 2354 2359 2365 2366 2367 2371 2374 2375 2382
 2389 2390 2391 2408 2417 2421 2423 2435 2443 2446 2449 2454 2456 2465
 2467 2482 2483 2486 2487 2488 2490 2496 2497 2499]
*** 1 train_index:  [   0    2    3 ... 2497 2498 2499]
*** 2 test_index:  [   1    5   10   14   19   24   30   35   36   45   49   51   55   57
   74   78   83  109  125  129  133  140  143  144  145  148  152  154
  158  161  164  166  170  174  180  189  192  199  202  205  211  222
  225  228  232  241  245  246  248  259  272  274  275  284  289  294
  301  306  318  320  325  326  332  340  341  343  351  352  358  363
  379  393  398  401  407  410  414  420  424  428  434  440  441  446
  450  451  453  464  478  480  482  484  487  492  506  509  516  517
  524  525  527  530  534  540  549  551  554  556  560  564  565  566
  577  580  585  586  590  593  611  617  618  620  622  625  628  633
  640  642  651  653  658  661  662  665  668  674  676  678  684  692
  697  698  707  708  710  711  717  720  729  730  731  742  746  755
  760  766  767  779  780  801  808  814  817  819  820  822  824  835
  843  845  861  862  864  870  876  878  881  893  895  896  901  904
  905  911  919  921  932  935  954  959  967  971  978  979  985  987
  993  995  996  998 1009 1015 1021 1029 1037 1042 1046 1054 1062 1074
 1080 1083 1084 1095 1096 1099 1101 1103 1106 1109 1117 1121 1122 1124
 1130 1141 1143 1146 1147 1159 1160 1161 1164 1166 1168 1179 1182 1184
 1190 1200 1204 1208 1217 1230 1236 1237 1247 1248 1250 1251 1253 1258
 1262 1266 1271 1275 1277 1282 1286 1287 1295 1302 1303 1304 1305 1314
 1321 1329 1342 1344 1358 1359 1371 1381 1383 1384 1398 1399 1405 1408
 1414 1416 1419 1421 1422 1423 1434 1448 1449 1457 1459 1478 1484 1487
 1489 1492 1494 1497 1501 1502 1513 1514 1517 1521 1525 1546 1556 1561
 1564 1576 1580 1584 1588 1594 1600 1608 1609 1613 1617 1622 1623 1626
 1632 1633 1636 1643 1660 1664 1666 1669 1675 1687 1692 1699 1701 1705
 1709 1710 1712 1713 1714 1723 1726 1735 1738 1741 1749 1758 1759 1767
 1768 1774 1786 1789 1794 1796 1800 1804 1811 1820 1822 1823 1837 1839
 1849 1852 1860 1863 1864 1866 1871 1874 1878 1884 1890 1896 1902 1903
 1905 1909 1910 1920 1921 1923 1925 1930 1931 1933 1934 1949 1959 1961
 1963 1964 1966 1971 1986 1991 1992 1997 2003 2013 2017 2021 2032 2033
 2035 2039 2047 2054 2055 2066 2067 2069 2081 2088 2092 2096 2099 2107
 2112 2122 2123 2124 2130 2133 2135 2145 2146 2147 2161 2166 2173 2177
 2187 2191 2194 2198 2200 2203 2215 2223 2232 2237 2238 2239 2249 2252
 2257 2261 2263 2264 2265 2271 2272 2273 2286 2291 2295 2301 2306 2308
 2312 2326 2329 2330 2340 2342 2345 2347 2350 2355 2356 2360 2368 2370
 2378 2383 2394 2399 2401 2404 2414 2419 2430 2432 2433 2445 2447 2448
 2451 2455 2464 2472 2477 2479 2480 2485 2489 2495]
*** 1 train_index:  [   0    1    3 ... 2496 2497 2499]
*** 2 test_index:  [   2    7    8    9   13   16   18   31   41   46   60   68   72   89
   90   93   94  101  110  115  127  130  136  138  139  147  162  172
  177  185  193  201  204  209  218  226  227  235  236  237  240  247
  255  258  260  262  263  266  292  295  296  299  303  305  307  314
  316  319  321  327  330  346  355  357  359  360  371  372  373  376
  381  395  402  409  413  419  425  429  432  433  436  439  444  447
  458  467  470  471  472  477  489  490  493  494  495  497  500  501
  502  507  512  519  520  522  523  526  531  533  542  557  558  570
  572  578  587  588  594  596  597  603  604  607  608  621  623  636
  643  646  649  654  659  663  666  667  670  677  682  683  689  703
  704  715  716  718  719  722  723  740  745  748  756  758  763  765
  782  786  790  795  804  813  823  826  837  840  841  850  854  856
  859  865  869  873  874  875  877  879  883  887  888  898  910  916
  925  926  940  942  943  945  946  955  961  970  975  976  989  994
 1004 1012 1017 1018 1020 1023 1024 1025 1027 1034 1048 1052 1053 1069
 1072 1075 1079 1082 1089 1093 1104 1107 1115 1127 1129 1131 1133 1138
 1144 1150 1151 1153 1171 1173 1175 1176 1177 1181 1189 1193 1195 1205
 1210 1227 1231 1232 1240 1241 1263 1264 1274 1276 1280 1281 1293 1297
 1301 1307 1311 1317 1318 1322 1323 1325 1328 1330 1332 1334 1337 1338
 1340 1348 1360 1361 1363 1365 1366 1367 1369 1380 1385 1388 1389 1392
 1394 1409 1413 1415 1431 1433 1438 1447 1460 1462 1469 1471 1481 1483
 1485 1486 1491 1493 1499 1508 1511 1515 1520 1527 1533 1534 1536 1540
 1542 1543 1545 1547 1548 1554 1563 1569 1572 1579 1582 1590 1591 1592
 1596 1603 1604 1607 1614 1616 1618 1631 1634 1635 1639 1644 1671 1677
 1679 1680 1683 1685 1688 1694 1703 1704 1706 1711 1715 1719 1730 1736
 1744 1751 1753 1756 1763 1764 1772 1776 1787 1802 1807 1818 1821 1824
 1826 1829 1832 1840 1851 1857 1858 1861 1869 1880 1887 1888 1891 1892
 1893 1895 1900 1917 1926 1928 1937 1938 1939 1945 1954 1956 1960 1962
 1965 1970 1978 1980 1981 1985 1987 1993 2001 2006 2008 2011 2016 2018
 2019 2020 2023 2027 2029 2034 2037 2040 2042 2043 2051 2060 2068 2079
 2086 2089 2095 2103 2104 2105 2121 2127 2131 2132 2134 2137 2140 2148
 2149 2151 2157 2162 2171 2183 2204 2213 2221 2222 2226 2229 2236 2242
 2259 2260 2262 2269 2279 2280 2288 2292 2302 2304 2313 2320 2323 2325
 2327 2328 2332 2333 2339 2341 2343 2352 2353 2361 2376 2387 2388 2392
 2396 2398 2402 2403 2407 2411 2416 2434 2437 2438 2440 2442 2450 2460
 2461 2462 2463 2466 2469 2474 2478 2492 2493 2498]


config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 50, 'learning_rate': 0.0001, 'seed': 1800, 'k_fold': 5, 'model': 'GCN', 'dataset': 'NCI-H23'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'name': 'NCI-H23', 'num_class': 2, 'label_dict': {'0': 0, '1': 1}, 'have_node_labels': True, 'have_node_attributions': False, 'node_dict': {'0': 0, '1': 1, '13': 2, '16': 3, '18': 4, '2': 5, '20': 6, '21': 7, '25': 8, '26': 9, '3': 10, '4': 11, '44': 12, '49': 13, '5': 14, '6': 15, '8': 16, '9': 17, 'UNKNOWN': 18}, 'feat_dim': 19, 'edge_feat_dim': 0, 'max_num_nodes': 93, 'avg_num_nodes': 27, 'graph_sizes_list': [34, 30, 34, 19, 20, 20, 13, 45, 22, 20, 26, 20, 15, 25, 56, 34, 19, 56, 24, 22, 25, 23, 24, 27, 25, 29, 27, 24, 22, 38, 22, 26, 23, 9, 7, 25, 35, 22, 18, 15, 24, 24, 27, 20, 18, 31, 25, 28, 46, 13, 29, 31, 21, 29, 20, 25, 48, 42, 28, 23, 34, 17, 28, 28, 15, 41, 35, 30, 20, 27, 25, 19, 31, 17, 29, 30, 75, 34, 22, 24, 33, 14, 22, 13, 24, 16, 17, 22, 13, 31, 26, 19, 15, 34, 26, 28, 24, 29, 18, 21, 12, 23, 23, 25, 22, 18, 16, 12, 17, 20, 27, 22, 19, 48, 23, 25, 33, 24, 17, 19, 24, 24, 10, 23, 14, 30, 22, 18, 26, 32, 31, 27, 30, 15, 29, 20, 19, 20, 34, 14, 15, 31, 18, 18, 24, 41, 44, 20, 38, 30, 28, 16, 29, 23, 31, 15, 11, 35, 29, 34, 19, 40, 18, 29, 27, 32, 17, 20, 20, 21, 15, 21, 31, 20, 57, 23, 13, 49, 32, 34, 31, 9, 20, 12, 16, 21, 28, 27, 17, 37, 32, 27, 16, 40, 21, 7, 30, 24, 16, 31, 18, 30, 20, 13, 20, 22, 23, 31, 20, 26, 32, 43, 14, 20, 14, 16, 20, 62, 19, 8, 31, 23, 35, 37, 20, 42, 26, 17, 46, 23, 19, 15, 23, 24, 23, 29, 33, 21, 20, 12, 58, 25, 15, 14, 26, 37, 15, 28, 48, 24, 21, 23, 22, 22, 51, 14, 17, 24, 23, 11, 18, 26, 35, 10, 19, 18, 14, 28, 23, 28, 18, 38, 42, 29, 24, 21, 28, 18, 44, 27, 27, 19, 32, 32, 20, 41, 24, 34, 32, 25, 18, 16, 28, 10, 22, 29, 21, 29, 17, 21, 24, 12, 41, 20, 24, 21, 26, 29, 16, 31, 30, 41, 35, 29, 27, 25, 31, 32, 29, 53, 13, 42, 27, 17, 23, 19, 40, 21, 21, 23, 18, 48, 33, 25, 29, 21, 18, 24, 22, 34, 27, 25, 15, 24, 23, 19, 13, 24, 19, 30, 27, 31, 41, 24, 60, 36, 40, 13, 16, 23, 28, 15, 28, 13, 13, 35, 21, 23, 24, 14, 25, 52, 25, 19, 16, 24, 26, 16, 13, 33, 12, 24, 26, 21, 11, 20, 23, 15, 21, 18, 47, 24, 23, 24, 24, 24, 22, 18, 27, 19, 18, 32, 24, 23, 30, 19, 21, 24, 21, 20, 50, 32, 26, 37, 17, 22, 31, 22, 19, 31, 14, 14, 36, 17, 22, 36, 17, 22, 21, 34, 24, 27, 16, 15, 16, 16, 29, 15, 21, 20, 32, 42, 35, 17, 27, 17, 28, 25, 28, 13, 20, 29, 20, 23, 16, 10, 16, 24, 24, 15, 26, 14, 27, 30, 8, 21, 27, 45, 34, 26, 17, 35, 31, 26, 31, 24, 22, 18, 29, 18, 21, 16, 21, 28, 18, 51, 41, 32, 19, 26, 24, 16, 20, 14, 41, 29, 43, 17, 29, 27, 32, 22, 20, 16, 15, 30, 26, 25, 25, 20, 41, 17, 14, 24, 21, 15, 24, 29, 24, 39, 22, 33, 15, 33, 28, 17, 19, 22, 9, 30, 25, 28, 17, 12, 38, 35, 15, 31, 24, 64, 18, 26, 22, 34, 11, 26, 19, 15, 36, 33, 38, 19, 20, 68, 18, 18, 19, 26, 12, 24, 30, 17, 26, 22, 22, 20, 59, 9, 32, 30, 16, 33, 33, 18, 40, 22, 30, 36, 20, 39, 40, 16, 19, 19, 30, 23, 33, 43, 22, 35, 26, 34, 30, 36, 50, 22, 17, 19, 24, 30, 26, 14, 26, 22, 22, 22, 34, 32, 24, 18, 43, 15, 35, 26, 14, 20, 24, 33, 25, 19, 16, 12, 25, 30, 15, 22, 20, 52, 16, 31, 21, 14, 18, 17, 18, 38, 37, 15, 27, 16, 19, 32, 19, 36, 21, 24, 23, 22, 55, 25, 15, 21, 25, 28, 19, 16, 29, 15, 30, 27, 55, 23, 15, 30, 21, 21, 35, 24, 27, 20, 14, 31, 31, 27, 20, 19, 33, 18, 12, 24, 44, 19, 23, 26, 22, 16, 12, 28, 32, 26, 14, 25, 18, 22, 20, 22, 30, 34, 16, 30, 13, 14, 37, 17, 54, 29, 22, 23, 29, 20, 15, 76, 27, 42, 20, 35, 23, 20, 21, 30, 39, 27, 18, 27, 28, 31, 30, 24, 11, 31, 35, 45, 20, 17, 31, 28, 19, 17, 19, 19, 29, 28, 18, 25, 21, 12, 37, 21, 19, 32, 22, 29, 19, 24, 22, 22, 21, 26, 29, 31, 25, 10, 19, 35, 18, 17, 22, 29, 30, 17, 37, 17, 29, 18, 20, 37, 31, 28, 9, 18, 19, 18, 27, 26, 27, 16, 15, 20, 10, 20, 21, 32, 41, 14, 19, 24, 36, 33, 29, 25, 38, 27, 23, 7, 25, 20, 34, 38, 24, 22, 19, 27, 23, 20, 15, 23, 21, 16, 23, 41, 17, 26, 24, 14, 18, 28, 27, 34, 29, 58, 28, 27, 29, 58, 17, 35, 23, 24, 17, 27, 28, 29, 17, 19, 17, 30, 17, 41, 16, 45, 18, 22, 24, 21, 30, 20, 17, 32, 16, 25, 19, 36, 23, 39, 23, 32, 24, 25, 36, 26, 20, 70, 16, 24, 17, 23, 17, 24, 25, 31, 19, 25, 32, 35, 16, 25, 18, 22, 20, 22, 36, 24, 20, 22, 34, 38, 22, 15, 13, 43, 24, 17, 48, 38, 18, 36, 27, 25, 23, 14, 20, 25, 38, 21, 80, 15, 20, 34, 22, 22, 20, 38, 26, 31, 23, 22, 52, 40, 22, 15, 32, 25, 24, 21, 36, 40, 38, 24, 47, 19, 25, 23, 10, 28, 35, 24, 37, 13, 37, 19, 46, 28, 18, 18, 10, 21, 40, 40, 14, 25, 39, 32, 20, 32, 18, 22, 22, 23, 14, 37, 19, 20, 30, 34, 20, 30, 15, 27, 22, 23, 45, 34, 25, 23, 25, 25, 23, 16, 25, 22, 18, 32, 46, 39, 20, 20, 37, 16, 26, 30, 27, 38, 27, 30, 22, 21, 22, 20, 26, 19, 13, 28, 15, 37, 26, 18, 28, 62, 23, 33, 16, 22, 13, 21, 27, 60, 58, 27, 30, 35, 18, 27, 27, 24, 23, 20, 23, 24, 28, 15, 62, 30, 24, 22, 46, 25, 20, 27, 24, 36, 22, 23, 12, 21, 33, 27, 11, 23, 23, 12, 20, 31, 17, 20, 23, 8, 20, 22, 21, 27, 15, 32, 17, 35, 30, 42, 22, 17, 28, 21, 17, 44, 12, 15, 29, 24, 14, 16, 18, 63, 33, 30, 30, 37, 21, 20, 59, 13, 15, 18, 30, 28, 31, 22, 19, 42, 32, 35, 24, 26, 26, 23, 23, 27, 21, 19, 23, 17, 24, 32, 21, 16, 32, 13, 19, 20, 41, 17, 27, 26, 7, 18, 22, 21, 15, 23, 42, 30, 32, 19, 17, 54, 20, 22, 24, 31, 31, 14, 18, 19, 28, 15, 20, 10, 26, 21, 16, 19, 14, 29, 27, 22, 14, 23, 34, 37, 34, 27, 22, 20, 17, 24, 31, 57, 20, 45, 36, 23, 40, 22, 16, 24, 26, 23, 3, 19, 17, 32, 17, 38, 32, 20, 23, 25, 45, 19, 39, 21, 35, 29, 35, 24, 39, 26, 21, 15, 29, 38, 16, 43, 26, 31, 24, 27, 30, 25, 21, 28, 16, 24, 22, 16, 25, 14, 28, 32, 35, 23, 18, 18, 20, 12, 23, 22, 16, 22, 18, 30, 31, 24, 29, 26, 42, 7, 14, 44, 15, 23, 21, 22, 25, 35, 21, 15, 14, 21, 29, 23, 29, 23, 19, 28, 37, 30, 40, 30, 8, 20, 20, 28, 20, 15, 14, 33, 22, 26, 46, 30, 28, 26, 15, 26, 9, 20, 22, 26, 17, 15, 15, 31, 21, 21, 27, 22, 19, 29, 31, 22, 17, 12, 23, 26, 17, 24, 20, 31, 35, 21, 16, 17, 48, 40, 10, 27, 12, 19, 20, 19, 51, 30, 19, 15, 29, 22, 38, 34, 23, 63, 20, 22, 15, 27, 36, 18, 51, 17, 2, 26, 28, 30, 22, 20, 50, 15, 27, 21, 26, 21, 22, 18, 33, 20, 21, 39, 40, 18, 16, 16, 21, 17, 30, 17, 26, 26, 31, 44, 27, 31, 44, 19, 29, 34, 24, 16, 20, 18, 26, 29, 39, 35, 58, 20, 10, 27, 41, 21, 19, 19, 13, 28, 47, 20, 28, 14, 20, 17, 15, 28, 25, 24, 15, 27, 21, 27, 17, 47, 37, 5, 33, 31, 17, 45, 34, 35, 18, 22, 29, 25, 22, 21, 29, 24, 13, 26, 28, 23, 28, 28, 13, 22, 11, 29, 51, 24, 14, 25, 17, 41, 20, 16, 24, 25, 10, 31, 26, 25, 19, 18, 11, 14, 27, 18, 11, 17, 26, 23, 32, 28, 27, 27, 18, 15, 16, 31, 33, 33, 18, 22, 21, 22, 18, 31, 65, 31, 22, 25, 18, 22, 32, 27, 15, 22, 32, 41, 26, 79, 21, 21, 25, 10, 29, 35, 26, 13, 28, 29, 39, 31, 12, 39, 14, 27, 14, 23, 9, 29, 38, 21, 24, 31, 19, 33, 12, 32, 46, 17, 20, 21, 17, 23, 25, 43, 27, 24, 25, 17, 19, 20, 27, 14, 21, 14, 24, 29, 22, 29, 28, 20, 27, 25, 31, 26, 22, 25, 22, 29, 29, 26, 19, 23, 27, 22, 20, 17, 31, 17, 18, 22, 22, 21, 28, 22, 28, 45, 19, 34, 23, 28, 26, 19, 28, 29, 22, 24, 19, 19, 25, 14, 25, 19, 31, 29, 36, 22, 28, 14, 29, 20, 32, 31, 18, 34, 14, 27, 25, 13, 32, 39, 17, 35, 16, 22, 22, 26, 60, 30, 29, 27, 29, 30, 53, 20, 18, 26, 22, 33, 15, 19, 21, 19, 31, 33, 19, 58, 21, 12, 24, 31, 16, 28, 40, 30, 18, 62, 40, 33, 21, 11, 34, 21, 32, 22, 20, 26, 31, 61, 25, 17, 21, 34, 28, 23, 28, 26, 43, 31, 69, 17, 14, 18, 14, 33, 38, 27, 27, 27, 42, 20, 33, 20, 17, 44, 27, 33, 27, 27, 30, 22, 24, 25, 31, 20, 14, 14, 44, 32, 36, 40, 25, 40, 25, 26, 25, 20, 47, 26, 17, 16, 18, 20, 12, 19, 8, 20, 26, 29, 9, 25, 15, 21, 25, 40, 25, 25, 21, 26, 22, 26, 15, 12, 25, 32, 19, 27, 28, 54, 21, 22, 14, 36, 27, 29, 22, 23, 22, 32, 34, 18, 30, 33, 17, 35, 27, 23, 30, 31, 26, 32, 25, 34, 32, 29, 23, 30, 21, 34, 27, 25, 26, 38, 25, 18, 23, 24, 48, 25, 33, 23, 43, 26, 32, 19, 22, 37, 27, 28, 31, 16, 22, 30, 19, 29, 32, 26, 24, 39, 22, 17, 35, 38, 45, 19, 24, 23, 18, 21, 22, 23, 42, 12, 21, 25, 15, 28, 18, 24, 21, 34, 32, 18, 43, 41, 22, 46, 14, 27, 11, 22, 15, 28, 34, 22, 22, 31, 28, 30, 17, 80, 15, 26, 23, 16, 62, 18, 23, 37, 23, 23, 12, 27, 26, 21, 19, 18, 29, 19, 61, 25, 27, 43, 37, 20, 29, 19, 26, 21, 24, 22, 39, 37, 10, 41, 12, 31, 20, 17, 16, 21, 29, 20, 27, 12, 16, 21, 21, 27, 16, 38, 28, 20, 25, 35, 15, 14, 38, 26, 30, 33, 27, 9, 21, 26, 26, 18, 28, 31, 12, 36, 41, 23, 14, 24, 32, 13, 23, 57, 32, 32, 16, 39, 18, 29, 24, 35, 29, 9, 34, 20, 19, 36, 30, 34, 40, 35, 17, 18, 7, 31, 40, 21, 35, 13, 29, 18, 35, 19, 33, 32, 28, 22, 18, 22, 18, 32, 18, 32, 20, 27, 16, 21, 14, 20, 20, 18, 20, 24, 38, 17, 16, 26, 12, 20, 34, 20, 18, 27, 39, 59, 12, 28, 27, 24, 27, 15, 17, 17, 36, 15, 26, 18, 19, 24, 14, 21, 20, 30, 21, 36, 31, 32, 36, 29, 27, 25, 21, 47, 32, 26, 33, 26, 63, 57, 31, 39, 13, 40, 21, 34, 33, 62, 23, 38, 23, 23, 18, 34, 63, 43, 34, 17, 34, 70, 28, 53, 29, 26, 29, 17, 42, 18, 17, 62, 22, 24, 27, 16, 45, 41, 36, 42, 45, 28, 20, 32, 70, 67, 17, 46, 47, 40, 24, 22, 30, 37, 37, 29, 14, 29, 47, 53, 20, 23, 41, 44, 19, 61, 42, 28, 22, 34, 34, 51, 29, 11, 39, 33, 36, 82, 61, 32, 44, 69, 23, 27, 26, 39, 37, 58, 27, 23, 28, 21, 35, 39, 29, 60, 28, 30, 31, 21, 37, 19, 21, 27, 11, 44, 60, 19, 35, 57, 35, 78, 27, 62, 31, 19, 32, 39, 25, 29, 65, 53, 47, 22, 35, 62, 66, 93, 65, 35, 46, 27, 35, 35, 41, 27, 40, 24, 32, 33, 26, 28, 32, 20, 25, 25, 17, 41, 57, 35, 27, 42, 25, 16, 19, 53, 38, 28, 34, 40, 22, 27, 65, 30, 22, 27, 12, 26, 44, 24, 20, 12, 21, 39, 37, 33, 28, 30, 33, 45, 19, 68, 25, 28, 47, 52, 21, 28, 39, 36, 25, 22, 39, 52, 61, 16, 45, 38, 3, 45, 40, 25, 52, 20, 19, 19, 26, 19, 27, 23, 29, 26, 29, 27, 35, 20, 23, 26, 46, 34, 35, 20, 25, 35, 54, 36, 31, 42, 33, 23, 28, 27, 19, 33, 25, 41, 21, 16, 41, 19, 24, 37, 39, 26, 31, 26, 22, 20, 18, 39, 26, 30, 20, 31, 26, 22, 24, 41, 30, 28, 33, 22, 40, 42, 30, 22, 28, 20, 30, 36, 42, 52, 24, 32, 38, 26, 34, 24, 40, 19, 26, 28, 27, 27, 16, 21, 31, 20, 58, 58, 23, 24, 38, 39, 34, 29, 34, 26, 25, 17, 30, 31, 25, 21, 28, 30, 13, 42, 26, 40, 23, 40, 38, 55, 25, 19, 39, 20, 32, 32, 66, 35, 61, 28, 53, 46, 39, 37, 28, 90, 35, 45, 38, 22, 21, 20, 62, 40, 46, 41, 24, 28, 32, 38, 56, 29, 30, 21, 47, 33, 67, 27, 20, 13, 47, 12, 25, 22, 45, 38, 25, 36, 25, 26, 33, 48, 34, 19, 25, 21, 32, 35, 33, 36, 67, 38, 35, 27, 47, 28, 26, 49, 29, 38, 32, 28, 26, 21, 30, 30, 22, 25, 35, 45, 25, 32, 34, 57, 23, 66, 27, 25, 46, 31, 35, 29, 26, 33, 39, 28, 23, 38, 14, 55, 32, 30, 29, 24, 79, 46, 11, 30, 32, 44, 30, 39, 37, 25, 76, 30, 84, 31, 24, 39, 28, 37, 28, 59, 21, 34, 23, 42, 45, 18, 30, 25, 34, 34, 20, 24, 46, 31, 28, 46, 30, 37, 35, 32, 37, 19, 18, 52, 18, 31, 34, 32, 18, 22, 37, 26, 50, 40, 28, 20, 30, 19, 15, 16, 22, 30, 38, 19, 35, 17, 22, 20, 29, 59, 59, 43, 27, 28, 25, 26, 62, 28, 26, 30, 34, 35], 'attr_dim': 0, 'dataset_info': '==== Dataset Information ====\n== General Information == \nNumber of graphs: 2500\nNumber of classes: 2\nClass distribution: \n0:2000 1:500 \n\n== Node information== \nAverage number of nodes: 27\nAverage number of edges (undirected): 29\nMax number of nodes: 93\nNumber of distinct node labels: 18\nAverage number of distinct node labels: 3\nNode labels distribution: \n0:9454 1:6143 13:139 16:2 18:1 2:49955 20:15 21:29 25:5 26:3 3:921 4:570 44:1 49:1 5:70 6:485 8:14 9:4 \n'}}


Training a new model: GCN
Training model with dataset, testing using fold 0
[92maverage training of epoch 0: loss 0.03454 acc 0.99500 roc_auc 0.99383 prc_auc 0.99147[0m
[93maverage test of epoch 0: loss 6.11796 acc 0.20000 roc_auc 0.45886 prc_auc 0.17416[0m
[92maverage training of epoch 1: loss 0.12172 acc 0.97250 roc_auc 0.98295 prc_auc 0.93507[0m
[93maverage test of epoch 1: loss 5.44605 acc 0.20000 roc_auc 0.50346 prc_auc 0.18724[0m
[92maverage training of epoch 2: loss 0.13161 acc 0.96900 roc_auc 0.98078 prc_auc 0.94709[0m
[93maverage test of epoch 2: loss 5.38712 acc 0.20000 roc_auc 0.46173 prc_auc 0.17634[0m
[92maverage training of epoch 3: loss 0.13791 acc 0.96600 roc_auc 0.97713 prc_auc 0.94061[0m
[93maverage test of epoch 3: loss 5.40442 acc 0.20000 roc_auc 0.49300 prc_auc 0.18644[0m
[92maverage training of epoch 4: loss 0.14703 acc 0.96250 roc_auc 0.97708 prc_auc 0.93876[0m
[93maverage test of epoch 4: loss 5.34006 acc 0.20000 roc_auc 0.47776 prc_auc 0.18364[0m
[92maverage training of epoch 5: loss 0.15317 acc 0.95950 roc_auc 0.97594 prc_auc 0.94622[0m
[93maverage test of epoch 5: loss 5.74285 acc 0.20000 roc_auc 0.50470 prc_auc 0.19054[0m
[92maverage training of epoch 6: loss 0.16189 acc 0.95800 roc_auc 0.97149 prc_auc 0.91884[0m
[93maverage test of epoch 6: loss 5.33683 acc 0.20000 roc_auc 0.54215 prc_auc 0.20515[0m
[92maverage training of epoch 7: loss 0.17519 acc 0.95050 roc_auc 0.96694 prc_auc 0.91718[0m
[93maverage test of epoch 7: loss 5.25284 acc 0.20000 roc_auc 0.59208 prc_auc 0.25119[0m
[92maverage training of epoch 8: loss 0.17626 acc 0.95250 roc_auc 0.96925 prc_auc 0.92734[0m
[93maverage test of epoch 8: loss 5.37472 acc 0.20000 roc_auc 0.59251 prc_auc 0.25671[0m
[92maverage training of epoch 9: loss 0.16356 acc 0.95900 roc_auc 0.96895 prc_auc 0.92718[0m
[93maverage test of epoch 9: loss 5.25919 acc 0.20000 roc_auc 0.56648 prc_auc 0.24002[0m
[92maverage training of epoch 10: loss 0.15230 acc 0.96500 roc_auc 0.97342 prc_auc 0.94245[0m
[93maverage test of epoch 10: loss 4.95565 acc 0.20000 roc_auc 0.52438 prc_auc 0.19740[0m
[92maverage training of epoch 11: loss 0.14517 acc 0.96450 roc_auc 0.97662 prc_auc 0.94187[0m
[93maverage test of epoch 11: loss 5.32146 acc 0.20000 roc_auc 0.58586 prc_auc 0.23935[0m
[92maverage training of epoch 12: loss 0.15229 acc 0.96400 roc_auc 0.97335 prc_auc 0.93683[0m
[93maverage test of epoch 12: loss 5.42571 acc 0.20000 roc_auc 0.58987 prc_auc 0.24339[0m
[92maverage training of epoch 13: loss 0.15033 acc 0.96350 roc_auc 0.97495 prc_auc 0.93734[0m
[93maverage test of epoch 13: loss 5.12254 acc 0.20000 roc_auc 0.59519 prc_auc 0.24904[0m
[92maverage training of epoch 14: loss 0.15522 acc 0.96250 roc_auc 0.97454 prc_auc 0.93840[0m
[93maverage test of epoch 14: loss 5.40296 acc 0.20000 roc_auc 0.59916 prc_auc 0.26216[0m
[92maverage training of epoch 15: loss 0.15503 acc 0.95850 roc_auc 0.97168 prc_auc 0.93077[0m
[93maverage test of epoch 15: loss 5.38783 acc 0.20000 roc_auc 0.59721 prc_auc 0.26355[0m
[92maverage training of epoch 16: loss 0.14953 acc 0.96200 roc_auc 0.97560 prc_auc 0.93525[0m
[93maverage test of epoch 16: loss 5.23672 acc 0.20000 roc_auc 0.60695 prc_auc 0.26492[0m
[92maverage training of epoch 17: loss 0.15973 acc 0.96300 roc_auc 0.97081 prc_auc 0.93124[0m
[93maverage test of epoch 17: loss 5.26149 acc 0.20000 roc_auc 0.61065 prc_auc 0.26961[0m
[92maverage training of epoch 18: loss 0.14189 acc 0.96850 roc_auc 0.97176 prc_auc 0.94347[0m
[93maverage test of epoch 18: loss 5.62437 acc 0.20000 roc_auc 0.59395 prc_auc 0.26535[0m
[92maverage training of epoch 19: loss 0.15945 acc 0.96300 roc_auc 0.96941 prc_auc 0.93005[0m
[93maverage test of epoch 19: loss 5.31167 acc 0.20000 roc_auc 0.60026 prc_auc 0.25626[0m
[92maverage training of epoch 20: loss 0.17086 acc 0.95900 roc_auc 0.96336 prc_auc 0.91663[0m
[93maverage test of epoch 20: loss 5.20743 acc 0.20000 roc_auc 0.60371 prc_auc 0.25868[0m
[92maverage training of epoch 21: loss 0.17408 acc 0.96100 roc_auc 0.96340 prc_auc 0.92696[0m
[93maverage test of epoch 21: loss 5.63257 acc 0.20000 roc_auc 0.58968 prc_auc 0.22783[0m
[92maverage training of epoch 22: loss 0.15597 acc 0.96650 roc_auc 0.96696 prc_auc 0.93120[0m
[93maverage test of epoch 22: loss 5.13616 acc 0.20000 roc_auc 0.57185 prc_auc 0.22840[0m
[92maverage training of epoch 23: loss 0.14292 acc 0.96800 roc_auc 0.96792 prc_auc 0.94010[0m
[93maverage test of epoch 23: loss 4.84539 acc 0.20000 roc_auc 0.51262 prc_auc 0.19703[0m
[92maverage training of epoch 24: loss 0.13549 acc 0.97200 roc_auc 0.96827 prc_auc 0.94856[0m
[93maverage test of epoch 24: loss 4.83607 acc 0.20000 roc_auc 0.52666 prc_auc 0.19698[0m
[92maverage training of epoch 25: loss 0.13734 acc 0.96900 roc_auc 0.96613 prc_auc 0.94405[0m
[93maverage test of epoch 25: loss 5.55275 acc 0.20000 roc_auc 0.51239 prc_auc 0.19364[0m
[92maverage training of epoch 26: loss 0.15566 acc 0.96650 roc_auc 0.96526 prc_auc 0.92315[0m
[93maverage test of epoch 26: loss 4.90846 acc 0.20000 roc_auc 0.54324 prc_auc 0.21354[0m
[92maverage training of epoch 27: loss 0.14617 acc 0.96750 roc_auc 0.96986 prc_auc 0.94308[0m
[93maverage test of epoch 27: loss 5.04992 acc 0.20000 roc_auc 0.51323 prc_auc 0.19210[0m
[92maverage training of epoch 28: loss 0.14585 acc 0.96600 roc_auc 0.96729 prc_auc 0.93849[0m
[93maverage test of epoch 28: loss 5.65721 acc 0.20000 roc_auc 0.60518 prc_auc 0.25025[0m
[92maverage training of epoch 29: loss 0.14347 acc 0.96600 roc_auc 0.97064 prc_auc 0.93783[0m
[93maverage test of epoch 29: loss 5.72650 acc 0.20000 roc_auc 0.61533 prc_auc 0.26546[0m
[92maverage training of epoch 30: loss 0.13908 acc 0.96800 roc_auc 0.97236 prc_auc 0.92609[0m
[93maverage test of epoch 30: loss 4.71529 acc 0.20000 roc_auc 0.61854 prc_auc 0.26240[0m
[92maverage training of epoch 31: loss 0.13243 acc 0.96750 roc_auc 0.97390 prc_auc 0.94891[0m
[93maverage test of epoch 31: loss 5.18827 acc 0.20000 roc_auc 0.50121 prc_auc 0.19543[0m
[92maverage training of epoch 32: loss 0.15509 acc 0.96350 roc_auc 0.96818 prc_auc 0.93517[0m
[93maverage test of epoch 32: loss 5.40904 acc 0.20000 roc_auc 0.59545 prc_auc 0.24623[0m
[92maverage training of epoch 33: loss 0.16190 acc 0.96500 roc_auc 0.96518 prc_auc 0.93082[0m
[93maverage test of epoch 33: loss 5.34071 acc 0.20000 roc_auc 0.50551 prc_auc 0.20087[0m
[92maverage training of epoch 34: loss 0.15183 acc 0.96850 roc_auc 0.96836 prc_auc 0.93275[0m
[93maverage test of epoch 34: loss 4.90450 acc 0.20000 roc_auc 0.52545 prc_auc 0.19855[0m
[92maverage training of epoch 35: loss 0.13899 acc 0.96600 roc_auc 0.96754 prc_auc 0.94270[0m
[93maverage test of epoch 35: loss 5.15129 acc 0.20000 roc_auc 0.60726 prc_auc 0.26137[0m
[92maverage training of epoch 36: loss 0.14421 acc 0.96600 roc_auc 0.96864 prc_auc 0.93344[0m
[93maverage test of epoch 36: loss 4.88144 acc 0.20000 roc_auc 0.60953 prc_auc 0.26822[0m
[92maverage training of epoch 37: loss 0.13071 acc 0.97200 roc_auc 0.97430 prc_auc 0.96023[0m
[93maverage test of epoch 37: loss 6.02932 acc 0.20000 roc_auc 0.46769 prc_auc 0.19561[0m
[92maverage training of epoch 38: loss 0.12422 acc 0.97550 roc_auc 0.97295 prc_auc 0.95306[0m
[93maverage test of epoch 38: loss 5.01742 acc 0.20000 roc_auc 0.47423 prc_auc 0.19566[0m
[92maverage training of epoch 39: loss 0.12836 acc 0.96700 roc_auc 0.97257 prc_auc 0.94608[0m
[93maverage test of epoch 39: loss 4.95369 acc 0.20000 roc_auc 0.61035 prc_auc 0.26323[0m
[92maverage training of epoch 40: loss 0.13159 acc 0.96950 roc_auc 0.97407 prc_auc 0.95446[0m
[93maverage test of epoch 40: loss 5.44725 acc 0.20000 roc_auc 0.48723 prc_auc 0.19753[0m
[92maverage training of epoch 41: loss 0.13792 acc 0.97100 roc_auc 0.96829 prc_auc 0.94440[0m
[93maverage test of epoch 41: loss 5.07029 acc 0.20000 roc_auc 0.55697 prc_auc 0.21700[0m
[92maverage training of epoch 42: loss 0.12925 acc 0.96900 roc_auc 0.97178 prc_auc 0.93158[0m
[93maverage test of epoch 42: loss 4.64193 acc 0.20000 roc_auc 0.56821 prc_auc 0.22878[0m
[92maverage training of epoch 43: loss 0.14647 acc 0.96300 roc_auc 0.96922 prc_auc 0.94370[0m
[93maverage test of epoch 43: loss 5.49439 acc 0.20000 roc_auc 0.61000 prc_auc 0.26623[0m
[92maverage training of epoch 44: loss 0.13176 acc 0.96950 roc_auc 0.97109 prc_auc 0.92626[0m
[93maverage test of epoch 44: loss 4.79256 acc 0.20000 roc_auc 0.61124 prc_auc 0.26269[0m
[92maverage training of epoch 45: loss 0.15611 acc 0.96700 roc_auc 0.96775 prc_auc 0.94505[0m
[93maverage test of epoch 45: loss 5.04000 acc 0.20000 roc_auc 0.61345 prc_auc 0.27121[0m
[92maverage training of epoch 46: loss 0.13977 acc 0.96700 roc_auc 0.96633 prc_auc 0.93968[0m
[93maverage test of epoch 46: loss 4.96209 acc 0.20000 roc_auc 0.61598 prc_auc 0.27271[0m
[92maverage training of epoch 47: loss 0.14915 acc 0.96700 roc_auc 0.96648 prc_auc 0.93325[0m
[93maverage test of epoch 47: loss 4.56896 acc 0.20000 roc_auc 0.56827 prc_auc 0.23291[0m
[92maverage training of epoch 48: loss 0.13997 acc 0.96850 roc_auc 0.96937 prc_auc 0.94236[0m
[93maverage test of epoch 48: loss 4.85181 acc 0.20000 roc_auc 0.56927 prc_auc 0.23589[0m
[92maverage training of epoch 49: loss 0.13298 acc 0.97400 roc_auc 0.97058 prc_auc 0.94416[0m
[93maverage test of epoch 49: loss 4.60295 acc 0.20000 roc_auc 0.57575 prc_auc 0.23542[0m
Training model with dataset, testing using fold 1
[92maverage training of epoch 0: loss 0.04126 acc 0.99150 roc_auc 0.99210 prc_auc 0.98987[0m
[93maverage test of epoch 0: loss 5.70857 acc 0.20000 roc_auc 0.52958 prc_auc 0.19879[0m
[92maverage training of epoch 1: loss 0.12477 acc 0.97050 roc_auc 0.98258 prc_auc 0.93546[0m
[93maverage test of epoch 1: loss 5.53675 acc 0.20000 roc_auc 0.54466 prc_auc 0.21069[0m
[92maverage training of epoch 2: loss 0.12903 acc 0.96950 roc_auc 0.98078 prc_auc 0.94760[0m
[93maverage test of epoch 2: loss 5.44553 acc 0.20000 roc_auc 0.55126 prc_auc 0.21539[0m
[92maverage training of epoch 3: loss 0.13402 acc 0.96550 roc_auc 0.97913 prc_auc 0.94741[0m
[93maverage test of epoch 3: loss 5.46949 acc 0.20000 roc_auc 0.53368 prc_auc 0.20089[0m
[92maverage training of epoch 4: loss 0.14386 acc 0.96350 roc_auc 0.97737 prc_auc 0.94265[0m
[93maverage test of epoch 4: loss 5.48863 acc 0.20000 roc_auc 0.53703 prc_auc 0.20739[0m
[92maverage training of epoch 5: loss 0.15633 acc 0.95900 roc_auc 0.97359 prc_auc 0.93254[0m
[93maverage test of epoch 5: loss 5.46354 acc 0.20000 roc_auc 0.52303 prc_auc 0.20259[0m
[92maverage training of epoch 6: loss 0.16898 acc 0.95450 roc_auc 0.97160 prc_auc 0.93182[0m
[93maverage test of epoch 6: loss 5.74174 acc 0.20000 roc_auc 0.56460 prc_auc 0.21931[0m
[92maverage training of epoch 7: loss 0.17284 acc 0.95350 roc_auc 0.96860 prc_auc 0.91433[0m
[93maverage test of epoch 7: loss 5.37628 acc 0.20000 roc_auc 0.54900 prc_auc 0.25303[0m
[92maverage training of epoch 8: loss 0.16303 acc 0.95550 roc_auc 0.97037 prc_auc 0.93298[0m
[93maverage test of epoch 8: loss 5.79373 acc 0.20000 roc_auc 0.56318 prc_auc 0.26930[0m
[92maverage training of epoch 9: loss 0.16103 acc 0.96100 roc_auc 0.97111 prc_auc 0.92273[0m
[93maverage test of epoch 9: loss 5.28179 acc 0.20000 roc_auc 0.58155 prc_auc 0.27933[0m
[92maverage training of epoch 10: loss 0.15105 acc 0.96200 roc_auc 0.97286 prc_auc 0.92960[0m
[93maverage test of epoch 10: loss 5.30405 acc 0.20000 roc_auc 0.57913 prc_auc 0.26651[0m
[92maverage training of epoch 11: loss 0.15628 acc 0.95900 roc_auc 0.97133 prc_auc 0.92592[0m
[93maverage test of epoch 11: loss 5.15690 acc 0.20000 roc_auc 0.57685 prc_auc 0.27831[0m
[92maverage training of epoch 12: loss 0.15148 acc 0.96300 roc_auc 0.97638 prc_auc 0.93764[0m
[93maverage test of epoch 12: loss 5.33546 acc 0.20000 roc_auc 0.58284 prc_auc 0.28464[0m
[92maverage training of epoch 13: loss 0.13906 acc 0.96650 roc_auc 0.97862 prc_auc 0.94374[0m
[93maverage test of epoch 13: loss 5.21711 acc 0.20000 roc_auc 0.58745 prc_auc 0.28455[0m
[92maverage training of epoch 14: loss 0.13548 acc 0.96700 roc_auc 0.97836 prc_auc 0.95229[0m
[93maverage test of epoch 14: loss 5.85968 acc 0.20000 roc_auc 0.58551 prc_auc 0.24945[0m
[92maverage training of epoch 15: loss 0.14841 acc 0.96500 roc_auc 0.97571 prc_auc 0.92876[0m
[93maverage test of epoch 15: loss 5.91655 acc 0.20000 roc_auc 0.58015 prc_auc 0.27252[0m
[92maverage training of epoch 16: loss 0.16157 acc 0.96500 roc_auc 0.97095 prc_auc 0.90185[0m
[93maverage test of epoch 16: loss 4.83953 acc 0.20000 roc_auc 0.57994 prc_auc 0.26948[0m
[92maverage training of epoch 17: loss 0.14843 acc 0.96400 roc_auc 0.97168 prc_auc 0.93811[0m
[93maverage test of epoch 17: loss 5.00268 acc 0.20000 roc_auc 0.57459 prc_auc 0.25723[0m
[92maverage training of epoch 18: loss 0.14170 acc 0.96550 roc_auc 0.97445 prc_auc 0.94980[0m
[93maverage test of epoch 18: loss 5.64493 acc 0.20000 roc_auc 0.57669 prc_auc 0.26869[0m
[92maverage training of epoch 19: loss 0.15348 acc 0.96750 roc_auc 0.96968 prc_auc 0.93680[0m
[93maverage test of epoch 19: loss 5.25915 acc 0.20000 roc_auc 0.58567 prc_auc 0.27229[0m
[92maverage training of epoch 20: loss 0.15288 acc 0.96650 roc_auc 0.96455 prc_auc 0.93038[0m
[93maverage test of epoch 20: loss 5.10482 acc 0.20000 roc_auc 0.57810 prc_auc 0.26657[0m
[92maverage training of epoch 21: loss 0.14909 acc 0.96750 roc_auc 0.96821 prc_auc 0.94182[0m
[93maverage test of epoch 21: loss 5.70434 acc 0.20000 roc_auc 0.58122 prc_auc 0.27395[0m
[92maverage training of epoch 22: loss 0.16009 acc 0.96450 roc_auc 0.96607 prc_auc 0.92686[0m
[93maverage test of epoch 22: loss 5.46502 acc 0.20000 roc_auc 0.57082 prc_auc 0.26143[0m
[92maverage training of epoch 23: loss 0.14123 acc 0.96900 roc_auc 0.97113 prc_auc 0.93991[0m
[93maverage test of epoch 23: loss 5.14879 acc 0.20000 roc_auc 0.54327 prc_auc 0.20598[0m
[92maverage training of epoch 24: loss 0.12939 acc 0.97200 roc_auc 0.97285 prc_auc 0.95168[0m
[93maverage test of epoch 24: loss 5.40131 acc 0.20000 roc_auc 0.51196 prc_auc 0.18735[0m
[92maverage training of epoch 25: loss 0.12443 acc 0.97400 roc_auc 0.97484 prc_auc 0.95322[0m
[93maverage test of epoch 25: loss 5.13853 acc 0.20000 roc_auc 0.50933 prc_auc 0.18638[0m
[92maverage training of epoch 26: loss 0.15060 acc 0.96450 roc_auc 0.96707 prc_auc 0.93945[0m
[93maverage test of epoch 26: loss 5.89218 acc 0.20000 roc_auc 0.56312 prc_auc 0.25038[0m
[92maverage training of epoch 27: loss 0.14992 acc 0.96550 roc_auc 0.96763 prc_auc 0.92910[0m
[93maverage test of epoch 27: loss 5.59268 acc 0.20000 roc_auc 0.55893 prc_auc 0.22395[0m
[92maverage training of epoch 28: loss 0.16199 acc 0.96000 roc_auc 0.96241 prc_auc 0.92911[0m
[93maverage test of epoch 28: loss 5.95623 acc 0.20000 roc_auc 0.58074 prc_auc 0.27691[0m
[92maverage training of epoch 29: loss 0.17197 acc 0.95850 roc_auc 0.96036 prc_auc 0.92466[0m
[93maverage test of epoch 29: loss 5.96703 acc 0.20000 roc_auc 0.56584 prc_auc 0.24623[0m
[92maverage training of epoch 30: loss 0.14577 acc 0.96650 roc_auc 0.96915 prc_auc 0.93359[0m
[93maverage test of epoch 30: loss 5.45408 acc 0.20000 roc_auc 0.51390 prc_auc 0.18878[0m
[92maverage training of epoch 31: loss 0.14251 acc 0.96200 roc_auc 0.96778 prc_auc 0.93233[0m
[93maverage test of epoch 31: loss 5.12665 acc 0.20000 roc_auc 0.56191 prc_auc 0.24304[0m
[92maverage training of epoch 32: loss 0.14072 acc 0.96850 roc_auc 0.96990 prc_auc 0.94357[0m
[93maverage test of epoch 32: loss 4.93018 acc 0.20000 roc_auc 0.50883 prc_auc 0.19312[0m
[92maverage training of epoch 33: loss 0.14849 acc 0.96150 roc_auc 0.96563 prc_auc 0.92761[0m
[93maverage test of epoch 33: loss 5.33027 acc 0.20000 roc_auc 0.57155 prc_auc 0.25744[0m
[92maverage training of epoch 34: loss 0.14982 acc 0.96550 roc_auc 0.96917 prc_auc 0.94195[0m
[93maverage test of epoch 34: loss 4.79407 acc 0.20000 roc_auc 0.55739 prc_auc 0.21362[0m
[92maverage training of epoch 35: loss 0.14373 acc 0.96550 roc_auc 0.96717 prc_auc 0.94092[0m
[93maverage test of epoch 35: loss 5.27033 acc 0.20000 roc_auc 0.57117 prc_auc 0.25307[0m
[92maverage training of epoch 36: loss 0.13717 acc 0.96800 roc_auc 0.96831 prc_auc 0.93283[0m
[93maverage test of epoch 36: loss 4.88060 acc 0.20000 roc_auc 0.56445 prc_auc 0.22189[0m
[92maverage training of epoch 37: loss 0.13463 acc 0.97250 roc_auc 0.97242 prc_auc 0.95182[0m
[93maverage test of epoch 37: loss 4.86218 acc 0.20000 roc_auc 0.58019 prc_auc 0.22461[0m
[92maverage training of epoch 38: loss 0.13220 acc 0.97100 roc_auc 0.97044 prc_auc 0.94868[0m
[93maverage test of epoch 38: loss 5.34274 acc 0.20000 roc_auc 0.56200 prc_auc 0.21800[0m
[92maverage training of epoch 39: loss 0.15055 acc 0.96500 roc_auc 0.96649 prc_auc 0.94122[0m
[93maverage test of epoch 39: loss 5.67829 acc 0.20000 roc_auc 0.57751 prc_auc 0.26059[0m
[92maverage training of epoch 40: loss 0.14573 acc 0.96900 roc_auc 0.96680 prc_auc 0.94153[0m
[93maverage test of epoch 40: loss 5.50996 acc 0.20000 roc_auc 0.55408 prc_auc 0.22326[0m
[92maverage training of epoch 41: loss 0.14006 acc 0.97000 roc_auc 0.96904 prc_auc 0.92773[0m
[93maverage test of epoch 41: loss 4.52884 acc 0.20000 roc_auc 0.55829 prc_auc 0.21419[0m
[92maverage training of epoch 42: loss 0.13710 acc 0.96700 roc_auc 0.96726 prc_auc 0.94776[0m
[93maverage test of epoch 42: loss 6.22764 acc 0.20000 roc_auc 0.58971 prc_auc 0.27948[0m
[92maverage training of epoch 43: loss 0.14571 acc 0.97050 roc_auc 0.97122 prc_auc 0.93167[0m
[93maverage test of epoch 43: loss 5.26064 acc 0.20000 roc_auc 0.56380 prc_auc 0.22928[0m
[92maverage training of epoch 44: loss 0.12746 acc 0.97300 roc_auc 0.96957 prc_auc 0.94521[0m
[93maverage test of epoch 44: loss 5.67768 acc 0.20000 roc_auc 0.55771 prc_auc 0.20903[0m
[92maverage training of epoch 45: loss 0.16163 acc 0.96650 roc_auc 0.96354 prc_auc 0.93314[0m
[93maverage test of epoch 45: loss 4.93665 acc 0.20000 roc_auc 0.57325 prc_auc 0.23999[0m
[92maverage training of epoch 46: loss 0.15861 acc 0.96600 roc_auc 0.96425 prc_auc 0.93249[0m
[93maverage test of epoch 46: loss 4.98912 acc 0.20000 roc_auc 0.55912 prc_auc 0.22714[0m
[92maverage training of epoch 47: loss 0.13394 acc 0.97000 roc_auc 0.97110 prc_auc 0.94156[0m
[93maverage test of epoch 47: loss 4.73174 acc 0.20000 roc_auc 0.57466 prc_auc 0.21878[0m
[92maverage training of epoch 48: loss 0.12607 acc 0.97000 roc_auc 0.97338 prc_auc 0.94887[0m
[93maverage test of epoch 48: loss 5.24841 acc 0.20000 roc_auc 0.57148 prc_auc 0.24082[0m
[92maverage training of epoch 49: loss 0.12900 acc 0.96900 roc_auc 0.97331 prc_auc 0.93827[0m
[93maverage test of epoch 49: loss 4.68476 acc 0.20000 roc_auc 0.57171 prc_auc 0.22804[0m
Training model with dataset, testing using fold 2
[92maverage training of epoch 0: loss 0.03946 acc 0.99300 roc_auc 0.99230 prc_auc 0.99005[0m
[93maverage test of epoch 0: loss 6.26269 acc 0.20000 roc_auc 0.52304 prc_auc 0.19318[0m
[92maverage training of epoch 1: loss 0.12185 acc 0.97250 roc_auc 0.98103 prc_auc 0.92754[0m
[93maverage test of epoch 1: loss 5.56923 acc 0.20000 roc_auc 0.57827 prc_auc 0.22129[0m
[92maverage training of epoch 2: loss 0.13189 acc 0.96800 roc_auc 0.97933 prc_auc 0.94197[0m
[93maverage test of epoch 2: loss 5.37085 acc 0.20000 roc_auc 0.57544 prc_auc 0.21820[0m
[92maverage training of epoch 3: loss 0.13547 acc 0.96550 roc_auc 0.97904 prc_auc 0.94268[0m
[93maverage test of epoch 3: loss 5.22224 acc 0.20000 roc_auc 0.53479 prc_auc 0.19786[0m
[92maverage training of epoch 4: loss 0.14920 acc 0.96400 roc_auc 0.97650 prc_auc 0.94519[0m
[93maverage test of epoch 4: loss 5.48012 acc 0.20000 roc_auc 0.57247 prc_auc 0.21945[0m
[92maverage training of epoch 5: loss 0.15241 acc 0.96100 roc_auc 0.97422 prc_auc 0.93666[0m
[93maverage test of epoch 5: loss 5.62727 acc 0.20000 roc_auc 0.55054 prc_auc 0.20808[0m
[92maverage training of epoch 6: loss 0.17148 acc 0.95450 roc_auc 0.97027 prc_auc 0.92316[0m
[93maverage test of epoch 6: loss 5.64930 acc 0.20000 roc_auc 0.56974 prc_auc 0.22698[0m
[92maverage training of epoch 7: loss 0.17315 acc 0.95600 roc_auc 0.96961 prc_auc 0.92022[0m
[93maverage test of epoch 7: loss 5.33327 acc 0.20000 roc_auc 0.61001 prc_auc 0.25642[0m
[92maverage training of epoch 8: loss 0.16343 acc 0.95950 roc_auc 0.96951 prc_auc 0.93133[0m
[93maverage test of epoch 8: loss 5.26257 acc 0.20000 roc_auc 0.58809 prc_auc 0.24044[0m
[92maverage training of epoch 9: loss 0.15519 acc 0.96250 roc_auc 0.97038 prc_auc 0.93329[0m
[93maverage test of epoch 9: loss 5.43740 acc 0.20000 roc_auc 0.62526 prc_auc 0.26761[0m
[92maverage training of epoch 10: loss 0.15622 acc 0.96350 roc_auc 0.97463 prc_auc 0.93942[0m
[93maverage test of epoch 10: loss 5.78583 acc 0.20000 roc_auc 0.61616 prc_auc 0.24720[0m
[92maverage training of epoch 11: loss 0.15621 acc 0.96150 roc_auc 0.97441 prc_auc 0.92448[0m
[93maverage test of epoch 11: loss 5.56688 acc 0.20000 roc_auc 0.64981 prc_auc 0.31561[0m
[92maverage training of epoch 12: loss 0.14559 acc 0.96450 roc_auc 0.97325 prc_auc 0.93856[0m
[93maverage test of epoch 12: loss 5.56016 acc 0.20000 roc_auc 0.64930 prc_auc 0.31269[0m
[92maverage training of epoch 13: loss 0.13976 acc 0.96500 roc_auc 0.97761 prc_auc 0.93958[0m
[93maverage test of epoch 13: loss 4.99727 acc 0.20000 roc_auc 0.65811 prc_auc 0.31679[0m
[92maverage training of epoch 14: loss 0.13885 acc 0.96900 roc_auc 0.97713 prc_auc 0.95142[0m
[93maverage test of epoch 14: loss 5.99719 acc 0.20000 roc_auc 0.62528 prc_auc 0.25230[0m
[92maverage training of epoch 15: loss 0.14753 acc 0.96650 roc_auc 0.97411 prc_auc 0.93126[0m
[93maverage test of epoch 15: loss 5.23597 acc 0.20000 roc_auc 0.63919 prc_auc 0.29677[0m
[92maverage training of epoch 16: loss 0.15670 acc 0.96050 roc_auc 0.97303 prc_auc 0.93936[0m
[93maverage test of epoch 16: loss 5.69767 acc 0.20000 roc_auc 0.64091 prc_auc 0.29987[0m
[92maverage training of epoch 17: loss 0.15747 acc 0.96050 roc_auc 0.97067 prc_auc 0.91474[0m
[93maverage test of epoch 17: loss 5.04916 acc 0.20000 roc_auc 0.65077 prc_auc 0.31286[0m
[92maverage training of epoch 18: loss 0.14155 acc 0.96550 roc_auc 0.97570 prc_auc 0.94677[0m
[93maverage test of epoch 18: loss 5.28984 acc 0.20000 roc_auc 0.62948 prc_auc 0.26388[0m
[92maverage training of epoch 19: loss 0.14826 acc 0.96650 roc_auc 0.97131 prc_auc 0.93767[0m
[93maverage test of epoch 19: loss 5.78358 acc 0.20000 roc_auc 0.65445 prc_auc 0.31577[0m
[92maverage training of epoch 20: loss 0.14485 acc 0.96850 roc_auc 0.97123 prc_auc 0.93470[0m
[93maverage test of epoch 20: loss 5.40096 acc 0.20000 roc_auc 0.65677 prc_auc 0.31436[0m
[92maverage training of epoch 21: loss 0.15640 acc 0.96200 roc_auc 0.97132 prc_auc 0.93395[0m
[93maverage test of epoch 21: loss 5.41837 acc 0.20000 roc_auc 0.65315 prc_auc 0.30642[0m
[92maverage training of epoch 22: loss 0.15550 acc 0.96000 roc_auc 0.96450 prc_auc 0.91220[0m
[93maverage test of epoch 22: loss 4.69349 acc 0.20000 roc_auc 0.65868 prc_auc 0.31535[0m
[92maverage training of epoch 23: loss 0.17437 acc 0.95400 roc_auc 0.96259 prc_auc 0.92374[0m
[93maverage test of epoch 23: loss 5.12962 acc 0.20000 roc_auc 0.65459 prc_auc 0.31376[0m
[92maverage training of epoch 24: loss 0.16742 acc 0.96150 roc_auc 0.96647 prc_auc 0.93625[0m
[93maverage test of epoch 24: loss 5.45628 acc 0.20000 roc_auc 0.66114 prc_auc 0.29426[0m
[92maverage training of epoch 25: loss 0.14288 acc 0.96700 roc_auc 0.97180 prc_auc 0.93960[0m
[93maverage test of epoch 25: loss 5.31954 acc 0.20000 roc_auc 0.65492 prc_auc 0.28984[0m
[92maverage training of epoch 26: loss 0.14362 acc 0.96950 roc_auc 0.97040 prc_auc 0.94658[0m
[93maverage test of epoch 26: loss 5.32841 acc 0.20000 roc_auc 0.61508 prc_auc 0.24324[0m
[92maverage training of epoch 27: loss 0.12917 acc 0.96950 roc_auc 0.97005 prc_auc 0.94219[0m
[93maverage test of epoch 27: loss 5.31869 acc 0.20000 roc_auc 0.65016 prc_auc 0.29680[0m
[92maverage training of epoch 28: loss 0.13445 acc 0.97250 roc_auc 0.97460 prc_auc 0.95011[0m
[93maverage test of epoch 28: loss 5.18525 acc 0.20000 roc_auc 0.49310 prc_auc 0.19997[0m
[92maverage training of epoch 29: loss 0.12927 acc 0.97200 roc_auc 0.97012 prc_auc 0.94098[0m
[93maverage test of epoch 29: loss 5.07946 acc 0.20000 roc_auc 0.55691 prc_auc 0.21121[0m
[92maverage training of epoch 30: loss 0.13526 acc 0.97200 roc_auc 0.97193 prc_auc 0.94184[0m
[93maverage test of epoch 30: loss 4.93672 acc 0.20000 roc_auc 0.52836 prc_auc 0.20575[0m
[92maverage training of epoch 31: loss 0.14720 acc 0.96550 roc_auc 0.96565 prc_auc 0.94094[0m
[93maverage test of epoch 31: loss 5.69723 acc 0.20000 roc_auc 0.67045 prc_auc 0.33137[0m
[92maverage training of epoch 32: loss 0.14863 acc 0.96750 roc_auc 0.96806 prc_auc 0.93472[0m
[93maverage test of epoch 32: loss 4.86929 acc 0.20000 roc_auc 0.67820 prc_auc 0.32979[0m
[92maverage training of epoch 33: loss 0.12957 acc 0.97250 roc_auc 0.97189 prc_auc 0.95586[0m
[93maverage test of epoch 33: loss 5.30080 acc 0.20000 roc_auc 0.56742 prc_auc 0.22109[0m
[92maverage training of epoch 34: loss 0.14213 acc 0.96750 roc_auc 0.96846 prc_auc 0.93225[0m
[93maverage test of epoch 34: loss 5.31438 acc 0.20000 roc_auc 0.68080 prc_auc 0.34429[0m
[92maverage training of epoch 35: loss 0.12394 acc 0.97350 roc_auc 0.97472 prc_auc 0.94885[0m
[93maverage test of epoch 35: loss 4.54557 acc 0.20000 roc_auc 0.69017 prc_auc 0.34345[0m
[92maverage training of epoch 36: loss 0.13736 acc 0.96950 roc_auc 0.96632 prc_auc 0.94329[0m
[93maverage test of epoch 36: loss 4.77708 acc 0.20000 roc_auc 0.68536 prc_auc 0.34798[0m
[92maverage training of epoch 37: loss 0.13819 acc 0.96900 roc_auc 0.96994 prc_auc 0.94778[0m
[93maverage test of epoch 37: loss 4.97916 acc 0.20000 roc_auc 0.69164 prc_auc 0.35309[0m
[92maverage training of epoch 38: loss 0.13753 acc 0.96800 roc_auc 0.97022 prc_auc 0.94646[0m
[93maverage test of epoch 38: loss 5.37227 acc 0.20000 roc_auc 0.69649 prc_auc 0.36025[0m
[92maverage training of epoch 39: loss 0.12221 acc 0.97350 roc_auc 0.97270 prc_auc 0.95013[0m
[93maverage test of epoch 39: loss 5.29921 acc 0.20000 roc_auc 0.62748 prc_auc 0.24927[0m
[92maverage training of epoch 40: loss 0.16040 acc 0.96400 roc_auc 0.96521 prc_auc 0.93487[0m
[93maverage test of epoch 40: loss 5.40030 acc 0.20000 roc_auc 0.70821 prc_auc 0.37061[0m
[92maverage training of epoch 41: loss 0.15879 acc 0.96850 roc_auc 0.96311 prc_auc 0.93493[0m
[93maverage test of epoch 41: loss 4.78644 acc 0.20000 roc_auc 0.71816 prc_auc 0.37835[0m
[92maverage training of epoch 42: loss 0.12389 acc 0.96900 roc_auc 0.97234 prc_auc 0.94059[0m
[93maverage test of epoch 42: loss 4.24817 acc 0.20000 roc_auc 0.70980 prc_auc 0.36917[0m
[92maverage training of epoch 43: loss 0.15464 acc 0.96600 roc_auc 0.96764 prc_auc 0.94780[0m
[93maverage test of epoch 43: loss 4.96099 acc 0.20000 roc_auc 0.71205 prc_auc 0.36806[0m
[92maverage training of epoch 44: loss 0.13142 acc 0.96800 roc_auc 0.96634 prc_auc 0.93791[0m
[93maverage test of epoch 44: loss 4.51733 acc 0.20000 roc_auc 0.70293 prc_auc 0.36474[0m
[92maverage training of epoch 45: loss 0.17291 acc 0.96100 roc_auc 0.95682 prc_auc 0.92781[0m
[93maverage test of epoch 45: loss 4.84904 acc 0.20000 roc_auc 0.70157 prc_auc 0.35253[0m
[92maverage training of epoch 46: loss 0.15219 acc 0.96950 roc_auc 0.96242 prc_auc 0.94171[0m
[93maverage test of epoch 46: loss 4.56208 acc 0.20000 roc_auc 0.70950 prc_auc 0.36988[0m
[92maverage training of epoch 47: loss 0.12417 acc 0.96900 roc_auc 0.97230 prc_auc 0.94464[0m
[93maverage test of epoch 47: loss 4.88418 acc 0.20000 roc_auc 0.70127 prc_auc 0.36411[0m
[92maverage training of epoch 48: loss 0.16845 acc 0.96000 roc_auc 0.96215 prc_auc 0.92182[0m
[93maverage test of epoch 48: loss 4.74372 acc 0.20000 roc_auc 0.68532 prc_auc 0.34506[0m
[92maverage training of epoch 49: loss 0.17027 acc 0.96250 roc_auc 0.96348 prc_auc 0.93217[0m
[93maverage test of epoch 49: loss 4.81466 acc 0.20000 roc_auc 0.68573 prc_auc 0.34691[0m
Training model with dataset, testing using fold 3
[92maverage training of epoch 0: loss 0.04045 acc 0.99200 roc_auc 0.99250 prc_auc 0.98979[0m
[93maverage test of epoch 0: loss 6.11897 acc 0.20000 roc_auc 0.52416 prc_auc 0.19799[0m
[92maverage training of epoch 1: loss 0.13234 acc 0.96900 roc_auc 0.98070 prc_auc 0.92833[0m
[93maverage test of epoch 1: loss 5.23444 acc 0.20000 roc_auc 0.54846 prc_auc 0.21554[0m
[92maverage training of epoch 2: loss 0.13309 acc 0.96750 roc_auc 0.98015 prc_auc 0.95097[0m
[93maverage test of epoch 2: loss 5.32986 acc 0.20000 roc_auc 0.55132 prc_auc 0.21685[0m
[92maverage training of epoch 3: loss 0.13858 acc 0.96650 roc_auc 0.97773 prc_auc 0.93686[0m
[93maverage test of epoch 3: loss 5.42456 acc 0.20000 roc_auc 0.55011 prc_auc 0.21561[0m
[92maverage training of epoch 4: loss 0.14007 acc 0.96550 roc_auc 0.97757 prc_auc 0.92291[0m
[93maverage test of epoch 4: loss 5.04285 acc 0.20000 roc_auc 0.56533 prc_auc 0.22573[0m
[92maverage training of epoch 5: loss 0.14304 acc 0.96350 roc_auc 0.97731 prc_auc 0.94526[0m
[93maverage test of epoch 5: loss 5.38530 acc 0.20000 roc_auc 0.56476 prc_auc 0.22302[0m
[92maverage training of epoch 6: loss 0.14884 acc 0.96350 roc_auc 0.97441 prc_auc 0.93782[0m
[93maverage test of epoch 6: loss 5.37743 acc 0.20000 roc_auc 0.56026 prc_auc 0.22879[0m
[92maverage training of epoch 7: loss 0.15396 acc 0.96050 roc_auc 0.97109 prc_auc 0.93713[0m
[93maverage test of epoch 7: loss 5.51027 acc 0.20000 roc_auc 0.58599 prc_auc 0.26694[0m
[92maverage training of epoch 8: loss 0.15580 acc 0.96050 roc_auc 0.97191 prc_auc 0.93775[0m
[93maverage test of epoch 8: loss 5.79660 acc 0.20000 roc_auc 0.60208 prc_auc 0.27148[0m
[92maverage training of epoch 9: loss 0.14710 acc 0.96450 roc_auc 0.97145 prc_auc 0.93612[0m
[93maverage test of epoch 9: loss 5.37590 acc 0.20000 roc_auc 0.58631 prc_auc 0.24853[0m
[92maverage training of epoch 10: loss 0.14648 acc 0.96450 roc_auc 0.97408 prc_auc 0.94743[0m
[93maverage test of epoch 10: loss 5.55619 acc 0.20000 roc_auc 0.57340 prc_auc 0.23695[0m
[92maverage training of epoch 11: loss 0.14326 acc 0.96800 roc_auc 0.97595 prc_auc 0.94689[0m
[93maverage test of epoch 11: loss 5.51679 acc 0.20000 roc_auc 0.57445 prc_auc 0.24131[0m
[92maverage training of epoch 12: loss 0.14099 acc 0.96900 roc_auc 0.97585 prc_auc 0.94367[0m
[93maverage test of epoch 12: loss 5.48126 acc 0.20000 roc_auc 0.58555 prc_auc 0.25229[0m
[92maverage training of epoch 13: loss 0.14966 acc 0.96300 roc_auc 0.97316 prc_auc 0.94413[0m
[93maverage test of epoch 13: loss 5.85676 acc 0.20000 roc_auc 0.60603 prc_auc 0.27551[0m
[92maverage training of epoch 14: loss 0.14754 acc 0.96750 roc_auc 0.97260 prc_auc 0.94194[0m
[93maverage test of epoch 14: loss 5.62031 acc 0.20000 roc_auc 0.60891 prc_auc 0.27719[0m
[92maverage training of epoch 15: loss 0.12793 acc 0.97050 roc_auc 0.97495 prc_auc 0.95144[0m
[93maverage test of epoch 15: loss 5.51419 acc 0.20000 roc_auc 0.59016 prc_auc 0.24964[0m
[92maverage training of epoch 16: loss 0.13481 acc 0.96700 roc_auc 0.97676 prc_auc 0.95131[0m
[93maverage test of epoch 16: loss 6.14524 acc 0.20000 roc_auc 0.60636 prc_auc 0.27330[0m
[92maverage training of epoch 17: loss 0.13930 acc 0.96850 roc_auc 0.97372 prc_auc 0.94625[0m
[93maverage test of epoch 17: loss 5.69089 acc 0.20000 roc_auc 0.61144 prc_auc 0.27678[0m
[92maverage training of epoch 18: loss 0.13691 acc 0.96750 roc_auc 0.97426 prc_auc 0.94010[0m
[93maverage test of epoch 18: loss 5.17421 acc 0.20000 roc_auc 0.61254 prc_auc 0.27719[0m
[92maverage training of epoch 19: loss 0.13960 acc 0.96800 roc_auc 0.97518 prc_auc 0.95137[0m
[93maverage test of epoch 19: loss 5.64256 acc 0.20000 roc_auc 0.61586 prc_auc 0.27948[0m
[92maverage training of epoch 20: loss 0.15026 acc 0.96300 roc_auc 0.97229 prc_auc 0.94182[0m
[93maverage test of epoch 20: loss 5.77919 acc 0.20200 roc_auc 0.61390 prc_auc 0.27835[0m
[92maverage training of epoch 21: loss 0.16038 acc 0.96700 roc_auc 0.96403 prc_auc 0.93909[0m
[93maverage test of epoch 21: loss 5.35222 acc 0.20000 roc_auc 0.61305 prc_auc 0.27719[0m
[92maverage training of epoch 22: loss 0.13493 acc 0.96850 roc_auc 0.97060 prc_auc 0.94099[0m
[93maverage test of epoch 22: loss 5.42167 acc 0.20000 roc_auc 0.61480 prc_auc 0.27816[0m
[92maverage training of epoch 23: loss 0.14527 acc 0.96700 roc_auc 0.96830 prc_auc 0.93794[0m
[93maverage test of epoch 23: loss 5.07296 acc 0.20200 roc_auc 0.61450 prc_auc 0.27886[0m
[92maverage training of epoch 24: loss 0.12882 acc 0.97050 roc_auc 0.97532 prc_auc 0.95678[0m
[93maverage test of epoch 24: loss 5.55810 acc 0.20000 roc_auc 0.61779 prc_auc 0.27798[0m
[92maverage training of epoch 25: loss 0.14327 acc 0.96750 roc_auc 0.97237 prc_auc 0.94302[0m
[93maverage test of epoch 25: loss 5.62880 acc 0.20000 roc_auc 0.61607 prc_auc 0.27954[0m
[92maverage training of epoch 26: loss 0.14338 acc 0.96750 roc_auc 0.96787 prc_auc 0.94156[0m
[93maverage test of epoch 26: loss 5.33740 acc 0.20000 roc_auc 0.61489 prc_auc 0.27862[0m
[92maverage training of epoch 27: loss 0.14092 acc 0.96900 roc_auc 0.96784 prc_auc 0.94793[0m
[93maverage test of epoch 27: loss 5.65983 acc 0.20200 roc_auc 0.61713 prc_auc 0.27961[0m
[92maverage training of epoch 28: loss 0.12669 acc 0.96900 roc_auc 0.97399 prc_auc 0.94549[0m
[93maverage test of epoch 28: loss 4.96665 acc 0.20200 roc_auc 0.61973 prc_auc 0.28250[0m
[92maverage training of epoch 29: loss 0.13790 acc 0.96850 roc_auc 0.97183 prc_auc 0.94805[0m
[93maverage test of epoch 29: loss 5.38080 acc 0.20000 roc_auc 0.62120 prc_auc 0.28214[0m
[92maverage training of epoch 30: loss 0.12898 acc 0.97150 roc_auc 0.97553 prc_auc 0.95245[0m
[93maverage test of epoch 30: loss 5.04699 acc 0.20000 roc_auc 0.62662 prc_auc 0.29085[0m
[92maverage training of epoch 31: loss 0.13118 acc 0.96750 roc_auc 0.97308 prc_auc 0.94610[0m
[93maverage test of epoch 31: loss 5.17078 acc 0.20000 roc_auc 0.62391 prc_auc 0.28475[0m
[92maverage training of epoch 32: loss 0.13191 acc 0.97050 roc_auc 0.97373 prc_auc 0.95500[0m
[93maverage test of epoch 32: loss 5.35765 acc 0.20000 roc_auc 0.63769 prc_auc 0.30719[0m
[92maverage training of epoch 33: loss 0.13470 acc 0.97050 roc_auc 0.97298 prc_auc 0.94738[0m
[93maverage test of epoch 33: loss 5.37845 acc 0.20000 roc_auc 0.63604 prc_auc 0.28735[0m
[92maverage training of epoch 34: loss 0.13443 acc 0.97200 roc_auc 0.97272 prc_auc 0.95274[0m
[93maverage test of epoch 34: loss 5.78745 acc 0.20000 roc_auc 0.62819 prc_auc 0.28582[0m
[92maverage training of epoch 35: loss 0.12458 acc 0.97150 roc_auc 0.97183 prc_auc 0.95132[0m
[93maverage test of epoch 35: loss 4.98285 acc 0.20000 roc_auc 0.63233 prc_auc 0.29613[0m
[92maverage training of epoch 36: loss 0.13398 acc 0.97200 roc_auc 0.97344 prc_auc 0.95467[0m
[93maverage test of epoch 36: loss 5.23034 acc 0.20000 roc_auc 0.63847 prc_auc 0.30218[0m
[92maverage training of epoch 37: loss 0.11524 acc 0.97250 roc_auc 0.97337 prc_auc 0.95542[0m
[93maverage test of epoch 37: loss 4.79422 acc 0.20000 roc_auc 0.63680 prc_auc 0.30964[0m
[92maverage training of epoch 38: loss 0.12562 acc 0.97100 roc_auc 0.97157 prc_auc 0.94825[0m
[93maverage test of epoch 38: loss 5.17109 acc 0.20000 roc_auc 0.63892 prc_auc 0.30797[0m
[92maverage training of epoch 39: loss 0.13217 acc 0.96800 roc_auc 0.97314 prc_auc 0.93999[0m
[93maverage test of epoch 39: loss 4.86599 acc 0.20000 roc_auc 0.63545 prc_auc 0.30419[0m
[92maverage training of epoch 40: loss 0.13926 acc 0.96350 roc_auc 0.97272 prc_auc 0.94200[0m
[93maverage test of epoch 40: loss 5.02998 acc 0.20000 roc_auc 0.63450 prc_auc 0.28798[0m
[92maverage training of epoch 41: loss 0.14635 acc 0.96600 roc_auc 0.97178 prc_auc 0.94931[0m
[93maverage test of epoch 41: loss 5.60123 acc 0.20000 roc_auc 0.64151 prc_auc 0.30396[0m
[92maverage training of epoch 42: loss 0.12790 acc 0.96750 roc_auc 0.97365 prc_auc 0.94933[0m
[93maverage test of epoch 42: loss 5.49792 acc 0.20000 roc_auc 0.65221 prc_auc 0.33851[0m
[92maverage training of epoch 43: loss 0.15279 acc 0.96700 roc_auc 0.96853 prc_auc 0.93694[0m
[93maverage test of epoch 43: loss 5.47647 acc 0.20000 roc_auc 0.63653 prc_auc 0.29689[0m
[92maverage training of epoch 44: loss 0.13110 acc 0.96850 roc_auc 0.97165 prc_auc 0.94264[0m
[93maverage test of epoch 44: loss 5.03397 acc 0.20000 roc_auc 0.63795 prc_auc 0.30625[0m
[92maverage training of epoch 45: loss 0.14570 acc 0.96850 roc_auc 0.97068 prc_auc 0.94944[0m
[93maverage test of epoch 45: loss 5.53250 acc 0.20000 roc_auc 0.63711 prc_auc 0.31376[0m
[92maverage training of epoch 46: loss 0.12846 acc 0.97250 roc_auc 0.97565 prc_auc 0.95582[0m
[93maverage test of epoch 46: loss 5.50083 acc 0.20000 roc_auc 0.63921 prc_auc 0.32052[0m
[92maverage training of epoch 47: loss 0.11906 acc 0.97100 roc_auc 0.97633 prc_auc 0.95123[0m
[93maverage test of epoch 47: loss 5.18637 acc 0.20000 roc_auc 0.64081 prc_auc 0.31665[0m
[92maverage training of epoch 48: loss 0.11990 acc 0.97400 roc_auc 0.97758 prc_auc 0.96001[0m
[93maverage test of epoch 48: loss 4.99790 acc 0.20000 roc_auc 0.64489 prc_auc 0.32101[0m
[92maverage training of epoch 49: loss 0.12717 acc 0.97100 roc_auc 0.97106 prc_auc 0.95006[0m
[93maverage test of epoch 49: loss 5.62783 acc 0.20000 roc_auc 0.64106 prc_auc 0.32446[0m
Training model with dataset, testing using fold 4
[92maverage training of epoch 0: loss 0.03973 acc 0.99400 roc_auc 0.99197 prc_auc 0.99022[0m
[93maverage test of epoch 0: loss 6.73939 acc 0.20000 roc_auc 0.49259 prc_auc 0.18485[0m
[92maverage training of epoch 1: loss 0.12483 acc 0.97550 roc_auc 0.98252 prc_auc 0.92505[0m
[93maverage test of epoch 1: loss 5.34556 acc 0.20000 roc_auc 0.49710 prc_auc 0.18626[0m
[92maverage training of epoch 2: loss 0.12907 acc 0.97000 roc_auc 0.97761 prc_auc 0.94575[0m
[93maverage test of epoch 2: loss 5.21749 acc 0.20000 roc_auc 0.52232 prc_auc 0.19595[0m
[92maverage training of epoch 3: loss 0.13609 acc 0.96550 roc_auc 0.97746 prc_auc 0.94556[0m
[93maverage test of epoch 3: loss 5.33673 acc 0.20000 roc_auc 0.52708 prc_auc 0.20024[0m
[92maverage training of epoch 4: loss 0.14571 acc 0.96150 roc_auc 0.97569 prc_auc 0.93854[0m
[93maverage test of epoch 4: loss 5.51168 acc 0.20000 roc_auc 0.55375 prc_auc 0.21299[0m
[92maverage training of epoch 5: loss 0.16048 acc 0.95700 roc_auc 0.97393 prc_auc 0.92599[0m
[93maverage test of epoch 5: loss 5.50749 acc 0.20000 roc_auc 0.54975 prc_auc 0.21116[0m
[92maverage training of epoch 6: loss 0.16552 acc 0.95400 roc_auc 0.97173 prc_auc 0.93180[0m
[93maverage test of epoch 6: loss 5.64328 acc 0.20000 roc_auc 0.55973 prc_auc 0.21602[0m
[92maverage training of epoch 7: loss 0.16495 acc 0.95800 roc_auc 0.97195 prc_auc 0.93568[0m
[93maverage test of epoch 7: loss 5.75488 acc 0.20000 roc_auc 0.53623 prc_auc 0.20223[0m
[92maverage training of epoch 8: loss 0.16363 acc 0.95850 roc_auc 0.97281 prc_auc 0.90654[0m
[93maverage test of epoch 8: loss 5.25048 acc 0.20000 roc_auc 0.60706 prc_auc 0.25443[0m
[92maverage training of epoch 9: loss 0.16210 acc 0.95900 roc_auc 0.96999 prc_auc 0.92408[0m
[93maverage test of epoch 9: loss 5.16115 acc 0.20000 roc_auc 0.60591 prc_auc 0.25142[0m
[92maverage training of epoch 10: loss 0.15370 acc 0.95900 roc_auc 0.96984 prc_auc 0.92385[0m
[93maverage test of epoch 10: loss 5.22885 acc 0.20000 roc_auc 0.61319 prc_auc 0.26285[0m
[92maverage training of epoch 11: loss 0.15178 acc 0.96050 roc_auc 0.97047 prc_auc 0.93626[0m
[93maverage test of epoch 11: loss 5.23007 acc 0.20000 roc_auc 0.60123 prc_auc 0.24908[0m
[92maverage training of epoch 12: loss 0.14872 acc 0.96450 roc_auc 0.97235 prc_auc 0.92783[0m
[93maverage test of epoch 12: loss 5.72915 acc 0.20000 roc_auc 0.60590 prc_auc 0.25414[0m
[92maverage training of epoch 13: loss 0.14344 acc 0.96400 roc_auc 0.97244 prc_auc 0.91651[0m
[93maverage test of epoch 13: loss 5.22983 acc 0.20000 roc_auc 0.60198 prc_auc 0.24884[0m
[92maverage training of epoch 14: loss 0.14627 acc 0.96300 roc_auc 0.97534 prc_auc 0.94643[0m
[93maverage test of epoch 14: loss 5.95392 acc 0.20000 roc_auc 0.59647 prc_auc 0.24104[0m
[92maverage training of epoch 15: loss 0.15079 acc 0.96350 roc_auc 0.97310 prc_auc 0.90196[0m
[93maverage test of epoch 15: loss 4.88290 acc 0.20000 roc_auc 0.60867 prc_auc 0.25461[0m
[92maverage training of epoch 16: loss 0.14862 acc 0.96150 roc_auc 0.97435 prc_auc 0.94112[0m
[93maverage test of epoch 16: loss 5.34921 acc 0.20000 roc_auc 0.61816 prc_auc 0.26835[0m
[92maverage training of epoch 17: loss 0.14696 acc 0.96300 roc_auc 0.97226 prc_auc 0.92586[0m
[93maverage test of epoch 17: loss 5.05912 acc 0.20000 roc_auc 0.61399 prc_auc 0.26165[0m
[92maverage training of epoch 18: loss 0.14401 acc 0.96200 roc_auc 0.97158 prc_auc 0.91325[0m
[93maverage test of epoch 18: loss 4.79311 acc 0.20000 roc_auc 0.62071 prc_auc 0.27282[0m
[92maverage training of epoch 19: loss 0.14288 acc 0.96100 roc_auc 0.97260 prc_auc 0.93305[0m
[93maverage test of epoch 19: loss 5.10957 acc 0.20000 roc_auc 0.62341 prc_auc 0.27316[0m
[92maverage training of epoch 20: loss 0.15205 acc 0.95950 roc_auc 0.96999 prc_auc 0.92466[0m
[93maverage test of epoch 20: loss 5.32834 acc 0.20000 roc_auc 0.61306 prc_auc 0.25870[0m
[92maverage training of epoch 21: loss 0.14331 acc 0.96250 roc_auc 0.97051 prc_auc 0.90633[0m
[93maverage test of epoch 21: loss 4.65397 acc 0.20000 roc_auc 0.62461 prc_auc 0.27681[0m
[92maverage training of epoch 22: loss 0.14249 acc 0.96450 roc_auc 0.97282 prc_auc 0.94507[0m
[93maverage test of epoch 22: loss 5.78567 acc 0.20000 roc_auc 0.63265 prc_auc 0.27964[0m
[92maverage training of epoch 23: loss 0.15237 acc 0.96250 roc_auc 0.96738 prc_auc 0.89643[0m
[93maverage test of epoch 23: loss 4.88451 acc 0.20000 roc_auc 0.63645 prc_auc 0.28451[0m
[92maverage training of epoch 24: loss 0.15629 acc 0.96350 roc_auc 0.96579 prc_auc 0.93821[0m
[93maverage test of epoch 24: loss 5.47007 acc 0.20000 roc_auc 0.61512 prc_auc 0.25925[0m
[92maverage training of epoch 25: loss 0.15254 acc 0.96450 roc_auc 0.96368 prc_auc 0.90061[0m
[93maverage test of epoch 25: loss 4.56039 acc 0.20000 roc_auc 0.62677 prc_auc 0.27306[0m
[92maverage training of epoch 26: loss 0.13896 acc 0.96600 roc_auc 0.96932 prc_auc 0.94741[0m
[93maverage test of epoch 26: loss 5.37759 acc 0.20000 roc_auc 0.62459 prc_auc 0.26923[0m
[92maverage training of epoch 27: loss 0.14459 acc 0.96950 roc_auc 0.97209 prc_auc 0.93101[0m
[93maverage test of epoch 27: loss 4.91218 acc 0.20000 roc_auc 0.61352 prc_auc 0.25867[0m
[92maverage training of epoch 28: loss 0.13266 acc 0.96950 roc_auc 0.97079 prc_auc 0.94096[0m
[93maverage test of epoch 28: loss 4.94947 acc 0.20000 roc_auc 0.58214 prc_auc 0.23708[0m
[92maverage training of epoch 29: loss 0.13740 acc 0.96700 roc_auc 0.96978 prc_auc 0.93878[0m
[93maverage test of epoch 29: loss 5.34912 acc 0.20000 roc_auc 0.56666 prc_auc 0.22278[0m
[92maverage training of epoch 30: loss 0.13850 acc 0.96500 roc_auc 0.97165 prc_auc 0.92939[0m
[93maverage test of epoch 30: loss 5.28498 acc 0.20000 roc_auc 0.59036 prc_auc 0.24414[0m
[92maverage training of epoch 31: loss 0.14027 acc 0.96800 roc_auc 0.97295 prc_auc 0.93779[0m
[93maverage test of epoch 31: loss 5.02066 acc 0.20000 roc_auc 0.58942 prc_auc 0.24374[0m
[92maverage training of epoch 32: loss 0.13898 acc 0.96850 roc_auc 0.97103 prc_auc 0.94203[0m
[93maverage test of epoch 32: loss 4.82988 acc 0.20000 roc_auc 0.60890 prc_auc 0.26129[0m
[92maverage training of epoch 33: loss 0.13279 acc 0.96950 roc_auc 0.96952 prc_auc 0.93272[0m
[93maverage test of epoch 33: loss 4.76083 acc 0.20000 roc_auc 0.61924 prc_auc 0.27496[0m
[92maverage training of epoch 34: loss 0.14818 acc 0.96450 roc_auc 0.96659 prc_auc 0.94027[0m
[93maverage test of epoch 34: loss 5.66820 acc 0.20000 roc_auc 0.61914 prc_auc 0.27382[0m
[92maverage training of epoch 35: loss 0.15874 acc 0.96150 roc_auc 0.96424 prc_auc 0.90938[0m
[93maverage test of epoch 35: loss 5.18024 acc 0.20000 roc_auc 0.63518 prc_auc 0.29391[0m
[92maverage training of epoch 36: loss 0.16791 acc 0.95750 roc_auc 0.96184 prc_auc 0.91889[0m
[93maverage test of epoch 36: loss 5.20070 acc 0.20000 roc_auc 0.62149 prc_auc 0.27857[0m
[92maverage training of epoch 37: loss 0.16133 acc 0.96250 roc_auc 0.96531 prc_auc 0.93119[0m
[93maverage test of epoch 37: loss 4.82367 acc 0.20000 roc_auc 0.51514 prc_auc 0.19239[0m
[92maverage training of epoch 38: loss 0.17045 acc 0.95700 roc_auc 0.95829 prc_auc 0.92567[0m
[93maverage test of epoch 38: loss 5.50855 acc 0.20000 roc_auc 0.64339 prc_auc 0.31164[0m
[92maverage training of epoch 39: loss 0.15156 acc 0.97050 roc_auc 0.96503 prc_auc 0.93637[0m
[93maverage test of epoch 39: loss 4.84478 acc 0.20000 roc_auc 0.58010 prc_auc 0.24416[0m
[92maverage training of epoch 40: loss 0.14575 acc 0.96600 roc_auc 0.96346 prc_auc 0.91238[0m
[93maverage test of epoch 40: loss 4.41714 acc 0.20000 roc_auc 0.62852 prc_auc 0.28908[0m
[92maverage training of epoch 41: loss 0.12103 acc 0.97550 roc_auc 0.97152 prc_auc 0.96104[0m
[93maverage test of epoch 41: loss 5.10036 acc 0.20000 roc_auc 0.51040 prc_auc 0.19446[0m
[92maverage training of epoch 42: loss 0.11243 acc 0.97300 roc_auc 0.97171 prc_auc 0.93888[0m
[93maverage test of epoch 42: loss 4.34848 acc 0.20000 roc_auc 0.57294 prc_auc 0.23768[0m
[92maverage training of epoch 43: loss 0.13050 acc 0.97050 roc_auc 0.97053 prc_auc 0.94490[0m
[93maverage test of epoch 43: loss 4.88520 acc 0.20000 roc_auc 0.51500 prc_auc 0.19463[0m
[92maverage training of epoch 44: loss 0.14803 acc 0.96600 roc_auc 0.96220 prc_auc 0.93142[0m
[93maverage test of epoch 44: loss 5.19301 acc 0.20000 roc_auc 0.63397 prc_auc 0.29587[0m
[92maverage training of epoch 45: loss 0.14092 acc 0.97100 roc_auc 0.96786 prc_auc 0.94244[0m
[93maverage test of epoch 45: loss 4.87459 acc 0.20000 roc_auc 0.66494 prc_auc 0.32563[0m
[92maverage training of epoch 46: loss 0.13818 acc 0.97000 roc_auc 0.96730 prc_auc 0.93246[0m
[93maverage test of epoch 46: loss 4.49956 acc 0.20000 roc_auc 0.63297 prc_auc 0.28881[0m
[92maverage training of epoch 47: loss 0.13359 acc 0.97000 roc_auc 0.96753 prc_auc 0.93857[0m
[93maverage test of epoch 47: loss 4.85029 acc 0.20000 roc_auc 0.63341 prc_auc 0.28772[0m
[92maverage training of epoch 48: loss 0.13150 acc 0.96800 roc_auc 0.96947 prc_auc 0.93226[0m
[93maverage test of epoch 48: loss 4.55409 acc 0.20000 roc_auc 0.65485 prc_auc 0.32792[0m
[92maverage training of epoch 49: loss 0.15640 acc 0.96200 roc_auc 0.96296 prc_auc 0.93186[0m
[93maverage test of epoch 49: loss 4.98239 acc 0.20000 roc_auc 0.64950 prc_auc 0.32661[0m
Run statistics: 
==== Configuration Settings ====
== Run Settings ==
Model: GCN, Dataset: NCI-H23
num_epochs: 50
learning_rate: 0.0001
seed: 1800
k_fold: 5
model: GCN
dataset: NCI-H23

== Model Settings and results ==
convolution_layers_size: 128-256-512
dropout: 0.5

Accuracy (avg): 0.2 ROC_AUC (avg): 0.62475 PRC_AUC (avg): 0.29229 

Average forward propagation time taken(ms): 2.3444202647600334
Average backward propagation time taken(ms): 1.5386096301125358

