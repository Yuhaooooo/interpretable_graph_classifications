# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======


torch.cuda.is_available():  True 


args.base_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/
args.graph_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/graphgen/graphs/
args.min_dfscode_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/graphgen/min_dfscodes/
args.min_dfscode_tensor_path: /home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/graphgen/min_dfscode_tensors/
*** 1 train_index:  [  0   1   2   3   4   6   8   9  10  11  12  13  14  15  16  17  19  20
  21  22  23  24  25  26  27  28  29  30  31  32  33  36  37  38  40  42
  43  44  45  46  48  50  53  54  56  57  58  60  61  62  63  64  65  66
  68  71  72  73  74  75  77  78  79  80  81  82  83  84  85  86  88  89
  90  92  94  95  97  98 100 101 102 103 104 106 108 109 111 112 113 114
 115 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132 134
 135 136 137 138 139 140 141 143 145 146 147 148 149 150 151 152 154 155
 156 157 158 159 160 161 162 163 164 165 166 168 169 171 173 174 176 180
 181 182 184 185 187 188 189 193 194 196 197 198 199 200 201 202 203 204
 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223
 226 227 228 230 231 232 233 234 235 236 238 239 242 243 244 245 246 248
 250 251 252 253 254 255 256 257 258 260 261 262 263 264 265 266 268 270
 272 273 276 277 278 279 280 281 282 283 284 285 286 287 289 290 291 293
 294 295 296 297 298 299 300 301 303 304 306 307 308 309 311 312 313 316
 317 318 320 321 323 324 325 326 327 328 329 330 331 332 333 334 335 336
 338 339 340 341 342 343 346 348 349 350]
*** 2 test_index:  [  5   7  18  34  35  39  41  47  49  51  52  55  59  67  69  70  76  87
  91  93  96  99 105 107 110 122 133 142 144 153 167 170 172 175 177 178
 179 183 186 190 191 192 195 205 224 225 229 237 240 241 247 249 259 267
 269 271 274 275 288 292 302 305 310 314 315 319 322 337 344 345 347]
*** 1 train_index:  [  0   1   2   3   5   6   7   9  11  12  13  15  16  17  18  19  20  21
  22  23  25  26  27  28  29  32  33  34  35  37  38  39  40  41  42  46
  47  48  49  50  51  52  53  54  55  56  58  59  60  61  62  63  64  65
  67  69  70  72  75  76  77  80  81  82  83  84  85  86  87  88  90  91
  93  94  95  96  98  99 100 102 103 104 105 107 108 109 110 111 112 113
 114 115 116 117 118 120 121 122 123 124 125 126 127 128 130 131 132 133
 134 135 137 138 139 140 141 142 144 145 146 147 149 151 153 154 155 156
 157 158 159 160 161 162 163 165 166 167 168 169 170 171 172 173 175 176
 177 178 179 180 182 183 185 186 187 188 189 190 191 192 193 194 195 197
 198 199 200 202 203 204 205 207 208 210 211 212 213 214 215 216 217 218
 220 222 224 225 226 227 228 229 230 231 232 233 235 236 237 239 240 241
 242 243 244 247 248 249 250 252 255 257 258 259 260 261 262 263 264 266
 267 268 269 270 271 272 273 274 275 277 278 280 282 284 285 286 287 288
 289 290 291 292 293 295 296 297 302 303 304 305 306 308 309 310 311 312
 314 315 316 318 319 320 321 322 323 324 325 327 328 330 331 332 335 336
 337 338 339 341 342 343 344 345 347 349 350]
*** 2 test_index:  [  4   8  10  14  24  30  31  36  43  44  45  57  66  68  71  73  74  78
  79  89  92  97 101 106 119 129 136 143 148 150 152 164 174 181 184 196
 201 206 209 219 221 223 234 238 245 246 251 253 254 256 265 276 279 281
 283 294 298 299 300 301 307 313 317 326 329 333 334 340 346 348]
*** 1 train_index:  [  1   2   4   5   6   7   8   9  10  12  13  14  15  16  17  18  21  22
  23  24  26  27  28  30  31  32  34  35  36  37  39  40  41  43  44  45
  46  47  48  49  51  52  53  55  56  57  58  59  60  61  63  65  66  67
  68  69  70  71  72  73  74  75  76  78  79  80  81  83  87  89  90  91
  92  93  96  97  98  99 100 101 102 104 105 106 107 109 110 111 113 116
 119 120 121 122 123 125 126 127 129 130 131 133 134 136 137 138 139 140
 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158
 159 161 162 164 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 181 182 183 184 186 189 190 191 192 195 196 197 198 199 200 201 202 203
 204 205 206 208 209 211 212 217 218 219 221 222 223 224 225 226 227 228
 229 231 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248
 249 250 251 252 253 254 256 257 258 259 261 262 263 264 265 266 267 269
 271 273 274 275 276 277 278 279 281 282 283 284 288 290 291 292 294 295
 296 297 298 299 300 301 302 303 304 305 307 309 310 311 312 313 314 315
 316 317 318 319 321 322 324 325 326 327 328 329 330 332 333 334 335 337
 338 339 340 343 344 345 346 347 348 349 350]
*** 2 test_index:  [  0   3  11  19  20  25  29  33  38  42  50  54  62  64  77  82  84  85
  86  88  94  95 103 108 112 114 115 117 118 124 128 132 135 160 163 165
 180 185 187 188 193 194 207 210 213 214 215 216 220 230 232 255 260 268
 270 272 280 285 286 287 289 293 306 308 320 323 331 336 341 342]
*** 1 train_index:  [  0   1   2   3   4   5   6   7   8  10  11  12  14  15  16  17  18  19
  20  21  24  25  28  29  30  31  33  34  35  36  37  38  39  40  41  42
  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60
  62  63  64  65  66  67  68  69  70  71  73  74  76  77  78  79  82  83
  84  85  86  87  88  89  91  92  93  94  95  96  97  99 101 103 104 105
 106 107 108 110 112 114 115 116 117 118 119 122 123 124 128 129 132 133
 134 135 136 137 138 141 142 143 144 146 147 148 149 150 152 153 155 156
 157 158 160 163 164 165 167 168 169 170 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 200 201 204 205 206 207 209 210 211 212 213 214 215 216 218 219 220
 221 222 223 224 225 226 227 229 230 232 234 235 236 237 238 240 241 243
 245 246 247 248 249 250 251 252 253 254 255 256 258 259 260 261 262 264
 265 266 267 268 269 270 271 272 274 275 276 277 279 280 281 282 283 285
 286 287 288 289 290 292 293 294 295 296 298 299 300 301 302 303 305 306
 307 308 310 312 313 314 315 317 319 320 322 323 326 329 331 332 333 334
 336 337 340 341 342 343 344 345 346 347 348]
*** 2 test_index:  [  9  13  22  23  26  27  32  61  72  75  80  81  90  98 100 102 109 111
 113 120 121 125 126 127 130 131 139 140 145 151 154 159 161 162 166 171
 199 202 203 208 217 228 231 233 239 242 244 257 263 273 278 284 291 297
 304 309 311 316 318 321 324 325 327 328 330 335 338 339 349 350]
*** 1 train_index:  [  0   3   4   5   7   8   9  10  11  13  14  18  19  20  22  23  24  25
  26  27  29  30  31  32  33  34  35  36  38  39  41  42  43  44  45  47
  49  50  51  52  54  55  57  59  61  62  64  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  84  85  86  87  88  89  90  91
  92  93  94  95  96  97  98  99 100 101 102 103 105 106 107 108 109 110
 111 112 113 114 115 117 118 119 120 121 122 124 125 126 127 128 129 130
 131 132 133 135 136 139 140 142 143 144 145 148 150 151 152 153 154 159
 160 161 162 163 164 165 166 167 170 171 172 174 175 177 178 179 180 181
 183 184 185 186 187 188 190 191 192 193 194 195 196 199 201 202 203 205
 206 207 208 209 210 213 214 215 216 217 219 220 221 223 224 225 228 229
 230 231 232 233 234 237 238 239 240 241 242 244 245 246 247 249 251 253
 254 255 256 257 259 260 263 265 267 268 269 270 271 272 273 274 275 276
 278 279 280 281 283 284 285 286 287 288 289 291 292 293 294 297 298 299
 300 301 302 304 305 306 307 308 309 310 311 313 314 315 316 317 318 319
 320 321 322 323 324 325 326 327 328 329 330 331 333 334 335 336 337 338
 339 340 341 342 344 345 346 347 348 349 350]
*** 2 test_index:  [  1   2   6  12  15  16  17  21  28  37  40  46  48  53  56  58  60  63
  65  83 104 116 123 134 137 138 141 146 147 149 155 156 157 158 168 169
 173 176 182 189 197 198 200 204 211 212 218 222 226 227 235 236 243 248
 250 252 258 261 262 264 266 277 282 290 295 296 303 312 332 343]
{'base_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/',
 'batch_size': 1,
 'clean_temp': False,
 'clean_tensorboard': False,
 'current_dataset_path': None,
 'current_min_dfscode_path': None,
 'current_model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/graphgen/model_save/DFScodeRNN_cls_PTC_FR_2021-01-11-20-53-12/',
 'current_processed_dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/graphgen/min_dfscode_tensors/',
 'current_temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/graphgen/tmp/DFScodeRNN_cls_PTC_FR_2021-01-11-20-53-12/',
 'dataset_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/graphgen/datasets/',
 'device': device(type='cuda', index=0),
 'dfscode_rnn_dropout': 0,
 'embedding_size_dfscode_rnn': 8,
 'epochs': 50,
 'epochs_end': 10000,
 'epochs_save': 20,
 'epochs_validate': 1,
 'fname': 'DFScodeRNN_cls_PTC_FR',
 'gamma': 0.3,
 'gradient_clipping': True,
 'graph_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/graphgen/graphs/',
 'graph_type': 'PTC_FR',
 'graphgen_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/graphgen/',
 'hidden_size_dfscode_rnn': 4,
 'load_device': device(type='cuda', index=0),
 'load_model': False,
 'load_model_path': '',
 'log_tensorboard': False,
 'lr': 0.0001,
 'max_prev_node': None,
 'milestones': [100, 200, 400, 800],
 'min_dfscode_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/graphgen/min_dfscodes/',
 'min_dfscode_tensor_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/graphgen/min_dfscode_tensors/',
 'model_save_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/graphgen/model_save/',
 'note': 'DFScodeRNN_cls',
 'num_graphs': None,
 'num_layers': 1,
 'num_workers': 8,
 'rnn_type': 'LSTM',
 'save_model': False,
 'temp_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/graphgen/tmp/',
 'tensorboard_path': '/home/FYP/heyu0012/projects/interpretable_graph_classifications/data/PTC_FR/graphgen/tensorboard/',
 'time': '2021-01-11-20-53-12',
 'weights': False}


graphgen args.__dict__: None




dataset_features: {'node_forward': {'1': 0, '0': 1, '3': 2, '2': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '12': 9, '13': 10, '14': 11, '9': 12, '15': 13, '16': 14, '17': 15, '18': 16, '10': 17, '11': 18}, 'node_backward': {0: '1', 1: '0', 2: '3', 3: '2', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '12', 10: '13', 11: '14', 12: '9', 13: '15', 14: '16', 15: '17', 16: '18', 17: '10', 18: '11'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 64, 'min_nodes': 2, 'max_edges': 71, 'min_edges': 1, 'max_degree': 4, 'label_size': 2}




config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 50, 'learning_rate': 0.0001, 'seed': 1800, 'k_fold': 5, 'model': 'DFScodeRNN_cls', 'dataset': 'PTC_FR'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'node_forward': {'1': 0, '0': 1, '3': 2, '2': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '12': 9, '13': 10, '14': 11, '9': 12, '15': 13, '16': 14, '17': 15, '18': 16, '10': 17, '11': 18}, 'node_backward': {0: '1', 1: '0', 2: '3', 3: '2', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '12', 10: '13', 11: '14', 12: '9', 13: '15', 14: '16', 15: '17', 16: '18', 17: '10', 18: '11'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 64, 'min_nodes': 2, 'max_edges': 71, 'min_edges': 1, 'max_degree': 4, 'label_size': 2}}


Training a new model: DFScodeRNN_cls
Training model with dataset, testing using fold 0
DfscodeRnn_cls args.lr:  0.0001
[92maverage training of epoch 0: loss -0.14153 acc 0.34286 roc_auc 0.40750 prc_auc 0.30282[0m
[93maverage test of epoch 0: loss -0.18452 acc 0.35211 roc_auc 0.45826 prc_auc 0.33677[0m
[92maverage training of epoch 1: loss -0.18780 acc 0.34286 roc_auc 0.44826 prc_auc 0.32160[0m
[93maverage test of epoch 1: loss -0.19828 acc 0.35211 roc_auc 0.47522 prc_auc 0.34521[0m
[92maverage training of epoch 2: loss -0.19615 acc 0.34286 roc_auc 0.46179 prc_auc 0.32717[0m
[93maverage test of epoch 2: loss -0.20446 acc 0.35211 roc_auc 0.47522 prc_auc 0.34521[0m
[92maverage training of epoch 3: loss -0.20000 acc 0.34286 roc_auc 0.46241 prc_auc 0.32560[0m
[93maverage test of epoch 3: loss -0.20652 acc 0.35211 roc_auc 0.54435 prc_auc 0.40137[0m
[92maverage training of epoch 4: loss -0.20206 acc 0.34286 roc_auc 0.46165 prc_auc 0.32452[0m
[93maverage test of epoch 4: loss -0.20859 acc 0.35211 roc_auc 0.50435 prc_auc 0.36403[0m
[92maverage training of epoch 5: loss -0.20413 acc 0.34286 roc_auc 0.46215 prc_auc 0.32547[0m
[93maverage test of epoch 5: loss -0.21065 acc 0.35211 roc_auc 0.49000 prc_auc 0.35445[0m
[92maverage training of epoch 6: loss -0.20620 acc 0.34286 roc_auc 0.46258 prc_auc 0.32541[0m
[93maverage test of epoch 6: loss -0.21271 acc 0.35211 roc_auc 0.49391 prc_auc 0.35465[0m
[92maverage training of epoch 7: loss -0.20827 acc 0.34286 roc_auc 0.46278 prc_auc 0.32553[0m
[93maverage test of epoch 7: loss -0.21477 acc 0.35211 roc_auc 0.48609 prc_auc 0.35124[0m
[92maverage training of epoch 8: loss -0.21033 acc 0.34286 roc_auc 0.46357 prc_auc 0.32579[0m
[93maverage test of epoch 8: loss -0.21683 acc 0.35211 roc_auc 0.56696 prc_auc 0.40833[0m
[92maverage training of epoch 9: loss -0.21240 acc 0.34286 roc_auc 0.46312 prc_auc 0.32535[0m
[93maverage test of epoch 9: loss -0.21889 acc 0.35211 roc_auc 0.49087 prc_auc 0.35445[0m
[92maverage training of epoch 10: loss -0.21446 acc 0.34286 roc_auc 0.46377 prc_auc 0.32626[0m
[93maverage test of epoch 10: loss -0.22095 acc 0.35211 roc_auc 0.45957 prc_auc 0.34113[0m
[92maverage training of epoch 11: loss -0.21652 acc 0.34286 roc_auc 0.46411 prc_auc 0.32680[0m
[93maverage test of epoch 11: loss -0.22300 acc 0.35211 roc_auc 0.51739 prc_auc 0.37040[0m
[92maverage training of epoch 12: loss -0.21858 acc 0.34286 roc_auc 0.46411 prc_auc 0.32716[0m
[93maverage test of epoch 12: loss -0.22506 acc 0.35211 roc_auc 0.49304 prc_auc 0.35465[0m
[92maverage training of epoch 13: loss -0.22064 acc 0.34286 roc_auc 0.46419 prc_auc 0.32734[0m
[93maverage test of epoch 13: loss -0.22711 acc 0.35211 roc_auc 0.50261 prc_auc 0.36674[0m
[92maverage training of epoch 14: loss -0.22270 acc 0.34286 roc_auc 0.46473 prc_auc 0.32782[0m
[93maverage test of epoch 14: loss -0.22916 acc 0.35211 roc_auc 0.50870 prc_auc 0.36557[0m
[92maverage training of epoch 15: loss -0.22475 acc 0.34286 roc_auc 0.46510 prc_auc 0.32847[0m
[93maverage test of epoch 15: loss -0.23121 acc 0.35211 roc_auc 0.51087 prc_auc 0.36772[0m
[92maverage training of epoch 16: loss -0.22680 acc 0.34286 roc_auc 0.46510 prc_auc 0.32863[0m
[93maverage test of epoch 16: loss -0.23325 acc 0.35211 roc_auc 0.47957 prc_auc 0.34941[0m
[92maverage training of epoch 17: loss -0.22885 acc 0.34286 roc_auc 0.46595 prc_auc 0.33064[0m
[93maverage test of epoch 17: loss -0.23530 acc 0.35211 roc_auc 0.50000 prc_auc 0.35929[0m
[92maverage training of epoch 18: loss -0.23090 acc 0.34286 roc_auc 0.46603 prc_auc 0.32968[0m
[93maverage test of epoch 18: loss -0.23734 acc 0.35211 roc_auc 0.50087 prc_auc 0.35941[0m
[92maverage training of epoch 19: loss -0.23295 acc 0.34286 roc_auc 0.46640 prc_auc 0.33063[0m
[93maverage test of epoch 19: loss -0.23938 acc 0.35211 roc_auc 0.47522 prc_auc 0.34541[0m
[92maverage training of epoch 20: loss -0.23500 acc 0.34286 roc_auc 0.46702 prc_auc 0.33123[0m
[93maverage test of epoch 20: loss -0.24142 acc 0.35211 roc_auc 0.47652 prc_auc 0.34804[0m
[92maverage training of epoch 21: loss -0.23704 acc 0.34286 roc_auc 0.46733 prc_auc 0.33240[0m
[93maverage test of epoch 21: loss -0.24346 acc 0.35211 roc_auc 0.46783 prc_auc 0.34132[0m
[92maverage training of epoch 22: loss -0.23909 acc 0.34286 roc_auc 0.46736 prc_auc 0.33238[0m
[93maverage test of epoch 22: loss -0.24550 acc 0.35211 roc_auc 0.47609 prc_auc 0.34523[0m
[92maverage training of epoch 23: loss -0.24113 acc 0.34286 roc_auc 0.46790 prc_auc 0.33350[0m
[93maverage test of epoch 23: loss -0.24754 acc 0.35211 roc_auc 0.52435 prc_auc 0.38285[0m
[92maverage training of epoch 24: loss -0.24317 acc 0.34286 roc_auc 0.46779 prc_auc 0.33447[0m
[93maverage test of epoch 24: loss -0.24957 acc 0.35211 roc_auc 0.50565 prc_auc 0.36741[0m
[92maverage training of epoch 25: loss -0.24520 acc 0.34286 roc_auc 0.46748 prc_auc 0.33190[0m
[93maverage test of epoch 25: loss -0.25160 acc 0.35211 roc_auc 0.51565 prc_auc 0.37055[0m
[92maverage training of epoch 26: loss -0.24724 acc 0.34286 roc_auc 0.46767 prc_auc 0.33250[0m
[93maverage test of epoch 26: loss -0.25363 acc 0.35211 roc_auc 0.48217 prc_auc 0.35125[0m
[92maverage training of epoch 27: loss -0.24928 acc 0.34286 roc_auc 0.46793 prc_auc 0.33245[0m
[93maverage test of epoch 27: loss -0.25566 acc 0.35211 roc_auc 0.51391 prc_auc 0.36930[0m
[92maverage training of epoch 28: loss -0.25131 acc 0.34286 roc_auc 0.46838 prc_auc 0.33320[0m
[93maverage test of epoch 28: loss -0.25769 acc 0.35211 roc_auc 0.48478 prc_auc 0.34986[0m
[92maverage training of epoch 29: loss -0.25334 acc 0.34286 roc_auc 0.46830 prc_auc 0.33345[0m
[93maverage test of epoch 29: loss -0.25971 acc 0.35211 roc_auc 0.48870 prc_auc 0.35377[0m
[92maverage training of epoch 30: loss -0.25537 acc 0.34286 roc_auc 0.46844 prc_auc 0.33381[0m
[93maverage test of epoch 30: loss -0.26174 acc 0.35211 roc_auc 0.46522 prc_auc 0.34050[0m
[92maverage training of epoch 31: loss -0.25740 acc 0.34286 roc_auc 0.46830 prc_auc 0.33353[0m
[93maverage test of epoch 31: loss -0.26376 acc 0.35211 roc_auc 0.48652 prc_auc 0.35060[0m
[92maverage training of epoch 32: loss -0.25942 acc 0.34286 roc_auc 0.46838 prc_auc 0.33360[0m
[93maverage test of epoch 32: loss -0.26578 acc 0.35211 roc_auc 0.50174 prc_auc 0.36000[0m
[92maverage training of epoch 33: loss -0.26145 acc 0.34286 roc_auc 0.46830 prc_auc 0.33420[0m
[93maverage test of epoch 33: loss -0.26780 acc 0.35211 roc_auc 0.50217 prc_auc 0.36417[0m
[92maverage training of epoch 34: loss -0.26347 acc 0.34286 roc_auc 0.46920 prc_auc 0.33424[0m
[93maverage test of epoch 34: loss -0.26981 acc 0.35211 roc_auc 0.48000 prc_auc 0.34874[0m
[92maverage training of epoch 35: loss -0.26549 acc 0.34286 roc_auc 0.46932 prc_auc 0.33393[0m
[93maverage test of epoch 35: loss -0.27183 acc 0.35211 roc_auc 0.49304 prc_auc 0.35425[0m
[92maverage training of epoch 36: loss -0.26751 acc 0.34286 roc_auc 0.46937 prc_auc 0.33531[0m
[93maverage test of epoch 36: loss -0.27384 acc 0.35211 roc_auc 0.48304 prc_auc 0.35024[0m
[92maverage training of epoch 37: loss -0.26953 acc 0.34286 roc_auc 0.47062 prc_auc 0.33494[0m
[93maverage test of epoch 37: loss -0.27585 acc 0.35211 roc_auc 0.48783 prc_auc 0.35143[0m
[92maverage training of epoch 38: loss -0.27155 acc 0.34286 roc_auc 0.47124 prc_auc 0.33627[0m
[93maverage test of epoch 38: loss -0.27786 acc 0.35211 roc_auc 0.50130 prc_auc 0.35902[0m
[92maverage training of epoch 39: loss -0.27356 acc 0.34286 roc_auc 0.47192 prc_auc 0.33643[0m
[93maverage test of epoch 39: loss -0.27987 acc 0.35211 roc_auc 0.48783 prc_auc 0.35154[0m
[92maverage training of epoch 40: loss -0.27557 acc 0.34286 roc_auc 0.47201 prc_auc 0.33688[0m
[93maverage test of epoch 40: loss -0.28188 acc 0.35211 roc_auc 0.48913 prc_auc 0.35425[0m
[92maverage training of epoch 41: loss -0.27758 acc 0.34286 roc_auc 0.47257 prc_auc 0.33730[0m
[93maverage test of epoch 41: loss -0.28388 acc 0.35211 roc_auc 0.50870 prc_auc 0.36376[0m
[92maverage training of epoch 42: loss -0.27959 acc 0.34286 roc_auc 0.47339 prc_auc 0.33842[0m
[93maverage test of epoch 42: loss -0.28589 acc 0.35211 roc_auc 0.49087 prc_auc 0.35466[0m
[92maverage training of epoch 43: loss -0.28160 acc 0.34286 roc_auc 0.47345 prc_auc 0.33843[0m
[93maverage test of epoch 43: loss -0.28789 acc 0.35211 roc_auc 0.50087 prc_auc 0.35922[0m
[92maverage training of epoch 44: loss -0.28361 acc 0.34286 roc_auc 0.47328 prc_auc 0.33794[0m
[93maverage test of epoch 44: loss -0.28989 acc 0.35211 roc_auc 0.49478 prc_auc 0.35535[0m
[92maverage training of epoch 45: loss -0.28561 acc 0.34286 roc_auc 0.47416 prc_auc 0.33981[0m
[93maverage test of epoch 45: loss -0.29189 acc 0.35211 roc_auc 0.49783 prc_auc 0.35862[0m
[92maverage training of epoch 46: loss -0.28761 acc 0.34286 roc_auc 0.47427 prc_auc 0.33924[0m
[93maverage test of epoch 46: loss -0.29388 acc 0.35211 roc_auc 0.54000 prc_auc 0.38451[0m
[92maverage training of epoch 47: loss -0.28961 acc 0.34286 roc_auc 0.47498 prc_auc 0.33963[0m
[93maverage test of epoch 47: loss -0.29588 acc 0.35211 roc_auc 0.42217 prc_auc 0.33486[0m
[92maverage training of epoch 48: loss -0.29161 acc 0.34286 roc_auc 0.47501 prc_auc 0.33960[0m
[93maverage test of epoch 48: loss -0.29787 acc 0.35211 roc_auc 0.50043 prc_auc 0.36253[0m
[92maverage training of epoch 49: loss -0.29361 acc 0.34286 roc_auc 0.47503 prc_auc 0.33965[0m
[93maverage test of epoch 49: loss -0.29986 acc 0.35211 roc_auc 0.50174 prc_auc 0.35902[0m
Training model with dataset, testing using fold 1
DfscodeRnn_cls args.lr:  0.0001
[92maverage training of epoch 0: loss 0.09276 acc 0.65480 roc_auc 0.46565 prc_auc 0.35358[0m
[93maverage test of epoch 0: loss 0.06145 acc 0.65714 roc_auc 0.48324 prc_auc 0.36789[0m
[92maverage training of epoch 1: loss 0.05576 acc 0.65480 roc_auc 0.46523 prc_auc 0.35328[0m
[93maverage test of epoch 1: loss 0.05073 acc 0.65714 roc_auc 0.56703 prc_auc 0.41023[0m
[92maverage training of epoch 2: loss 0.04928 acc 0.65480 roc_auc 0.46512 prc_auc 0.35325[0m
[93maverage test of epoch 2: loss 0.04608 acc 0.65714 roc_auc 0.50679 prc_auc 0.37022[0m
[92maverage training of epoch 3: loss 0.04640 acc 0.65480 roc_auc 0.46493 prc_auc 0.35321[0m
[93maverage test of epoch 3: loss 0.04450 acc 0.65714 roc_auc 0.49592 prc_auc 0.37335[0m
[92maverage training of epoch 4: loss 0.04482 acc 0.65480 roc_auc 0.46495 prc_auc 0.35322[0m
[93maverage test of epoch 4: loss 0.04292 acc 0.65714 roc_auc 0.45199 prc_auc 0.35729[0m
[92maverage training of epoch 5: loss 0.04323 acc 0.65480 roc_auc 0.46501 prc_auc 0.35319[0m
[93maverage test of epoch 5: loss 0.04134 acc 0.65714 roc_auc 0.47826 prc_auc 0.36105[0m
[92maverage training of epoch 6: loss 0.04165 acc 0.65480 roc_auc 0.46479 prc_auc 0.35310[0m
[93maverage test of epoch 6: loss 0.03976 acc 0.65714 roc_auc 0.48732 prc_auc 0.36617[0m
[92maverage training of epoch 7: loss 0.04007 acc 0.65480 roc_auc 0.46470 prc_auc 0.35307[0m
[93maverage test of epoch 7: loss 0.03819 acc 0.65714 roc_auc 0.53080 prc_auc 0.38080[0m
[92maverage training of epoch 8: loss 0.03849 acc 0.65480 roc_auc 0.46479 prc_auc 0.35310[0m
[93maverage test of epoch 8: loss 0.03661 acc 0.65714 roc_auc 0.51993 prc_auc 0.37835[0m
[92maverage training of epoch 9: loss 0.03691 acc 0.65480 roc_auc 0.46459 prc_auc 0.35303[0m
[93maverage test of epoch 9: loss 0.03504 acc 0.65714 roc_auc 0.48596 prc_auc 0.36435[0m
[92maverage training of epoch 10: loss 0.03534 acc 0.65480 roc_auc 0.46465 prc_auc 0.35302[0m
[93maverage test of epoch 10: loss 0.03347 acc 0.65714 roc_auc 0.46920 prc_auc 0.36467[0m
[92maverage training of epoch 11: loss 0.03376 acc 0.65480 roc_auc 0.46459 prc_auc 0.35304[0m
[93maverage test of epoch 11: loss 0.03190 acc 0.65714 roc_auc 0.48596 prc_auc 0.36490[0m
[92maverage training of epoch 12: loss 0.03219 acc 0.65480 roc_auc 0.46453 prc_auc 0.35291[0m
[93maverage test of epoch 12: loss 0.03034 acc 0.65714 roc_auc 0.51993 prc_auc 0.38119[0m
[92maverage training of epoch 13: loss 0.03063 acc 0.65480 roc_auc 0.46442 prc_auc 0.35287[0m
[93maverage test of epoch 13: loss 0.02877 acc 0.65714 roc_auc 0.49139 prc_auc 0.36441[0m
[92maverage training of epoch 14: loss 0.02906 acc 0.65480 roc_auc 0.46434 prc_auc 0.35286[0m
[93maverage test of epoch 14: loss 0.02721 acc 0.65714 roc_auc 0.45335 prc_auc 0.35743[0m
[92maverage training of epoch 15: loss 0.02750 acc 0.65480 roc_auc 0.46431 prc_auc 0.35281[0m
[93maverage test of epoch 15: loss 0.02566 acc 0.65714 roc_auc 0.47554 prc_auc 0.36225[0m
[92maverage training of epoch 16: loss 0.02594 acc 0.65480 roc_auc 0.46409 prc_auc 0.35273[0m
[93maverage test of epoch 16: loss 0.02410 acc 0.65714 roc_auc 0.48505 prc_auc 0.36691[0m
[92maverage training of epoch 17: loss 0.02439 acc 0.65125 roc_auc 0.46403 prc_auc 0.35271[0m
[93maverage test of epoch 17: loss 0.02255 acc 0.65714 roc_auc 0.47871 prc_auc 0.35879[0m
[92maverage training of epoch 18: loss 0.02284 acc 0.65125 roc_auc 0.46397 prc_auc 0.35267[0m
[93maverage test of epoch 18: loss 0.02100 acc 0.65714 roc_auc 0.45516 prc_auc 0.35261[0m
[92maverage training of epoch 19: loss 0.02128 acc 0.65125 roc_auc 0.46403 prc_auc 0.35269[0m
[93maverage test of epoch 19: loss 0.01946 acc 0.65714 roc_auc 0.48324 prc_auc 0.36366[0m
[92maverage training of epoch 20: loss 0.01974 acc 0.65125 roc_auc 0.46389 prc_auc 0.35264[0m
[93maverage test of epoch 20: loss 0.01791 acc 0.65714 roc_auc 0.51585 prc_auc 0.37732[0m
[92maverage training of epoch 21: loss 0.01819 acc 0.65125 roc_auc 0.46383 prc_auc 0.35264[0m
[93maverage test of epoch 21: loss 0.01637 acc 0.65714 roc_auc 0.46920 prc_auc 0.35756[0m
[92maverage training of epoch 22: loss 0.01665 acc 0.65125 roc_auc 0.46361 prc_auc 0.35254[0m
[93maverage test of epoch 22: loss 0.01483 acc 0.65714 roc_auc 0.52853 prc_auc 0.38611[0m
[92maverage training of epoch 23: loss 0.01511 acc 0.65125 roc_auc 0.46369 prc_auc 0.35259[0m
[93maverage test of epoch 23: loss 0.01330 acc 0.65714 roc_auc 0.46332 prc_auc 0.36289[0m
[92maverage training of epoch 24: loss 0.01357 acc 0.65125 roc_auc 0.46372 prc_auc 0.35258[0m
[93maverage test of epoch 24: loss 0.01177 acc 0.65714 roc_auc 0.44611 prc_auc 0.35608[0m
[92maverage training of epoch 25: loss 0.01204 acc 0.65125 roc_auc 0.46358 prc_auc 0.35251[0m
[93maverage test of epoch 25: loss 0.01024 acc 0.65714 roc_auc 0.47237 prc_auc 0.36042[0m
[92maverage training of epoch 26: loss 0.01051 acc 0.65125 roc_auc 0.46350 prc_auc 0.35247[0m
[93maverage test of epoch 26: loss 0.00871 acc 0.65714 roc_auc 0.49774 prc_auc 0.36733[0m
[92maverage training of epoch 27: loss 0.00898 acc 0.65125 roc_auc 0.46369 prc_auc 0.35257[0m
[93maverage test of epoch 27: loss 0.00718 acc 0.65714 roc_auc 0.50770 prc_auc 0.37733[0m
[92maverage training of epoch 28: loss 0.00745 acc 0.65125 roc_auc 0.46350 prc_auc 0.35243[0m
[93maverage test of epoch 28: loss 0.00566 acc 0.65714 roc_auc 0.50000 prc_auc 0.37630[0m
[92maverage training of epoch 29: loss 0.00593 acc 0.65125 roc_auc 0.46344 prc_auc 0.35244[0m
[93maverage test of epoch 29: loss 0.00414 acc 0.65714 roc_auc 0.49275 prc_auc 0.36200[0m
[92maverage training of epoch 30: loss 0.00441 acc 0.65125 roc_auc 0.46350 prc_auc 0.35243[0m
[93maverage test of epoch 30: loss 0.00262 acc 0.65714 roc_auc 0.48234 prc_auc 0.36360[0m
[92maverage training of epoch 31: loss 0.00289 acc 0.65125 roc_auc 0.46322 prc_auc 0.35235[0m
[93maverage test of epoch 31: loss 0.00111 acc 0.65714 roc_auc 0.49139 prc_auc 0.36870[0m
[92maverage training of epoch 32: loss 0.00137 acc 0.65125 roc_auc 0.46333 prc_auc 0.35234[0m
[93maverage test of epoch 32: loss -0.00040 acc 0.65714 roc_auc 0.44611 prc_auc 0.34998[0m
[92maverage training of epoch 33: loss -0.00014 acc 0.65125 roc_auc 0.46311 prc_auc 0.35218[0m
[93maverage test of epoch 33: loss -0.00191 acc 0.65714 roc_auc 0.46105 prc_auc 0.36335[0m
[92maverage training of epoch 34: loss -0.00166 acc 0.65125 roc_auc 0.46308 prc_auc 0.35225[0m
[93maverage test of epoch 34: loss -0.00342 acc 0.65714 roc_auc 0.48732 prc_auc 0.36677[0m
[92maverage training of epoch 35: loss -0.00316 acc 0.65125 roc_auc 0.46291 prc_auc 0.35219[0m
[93maverage test of epoch 35: loss -0.00493 acc 0.65714 roc_auc 0.47328 prc_auc 0.36159[0m
[92maverage training of epoch 36: loss -0.00467 acc 0.65125 roc_auc 0.46311 prc_auc 0.35227[0m
[93maverage test of epoch 36: loss -0.00643 acc 0.65714 roc_auc 0.50815 prc_auc 0.38064[0m
[92maverage training of epoch 37: loss -0.00617 acc 0.65125 roc_auc 0.46311 prc_auc 0.35216[0m
[93maverage test of epoch 37: loss -0.00793 acc 0.65714 roc_auc 0.48505 prc_auc 0.36994[0m
[92maverage training of epoch 38: loss -0.00767 acc 0.65125 roc_auc 0.46313 prc_auc 0.35223[0m
[93maverage test of epoch 38: loss -0.00942 acc 0.65714 roc_auc 0.45426 prc_auc 0.35869[0m
[92maverage training of epoch 39: loss -0.00917 acc 0.65125 roc_auc 0.46305 prc_auc 0.35220[0m
[93maverage test of epoch 39: loss -0.01092 acc 0.65714 roc_auc 0.45562 prc_auc 0.35788[0m
[92maverage training of epoch 40: loss -0.01067 acc 0.65125 roc_auc 0.46308 prc_auc 0.35226[0m
[93maverage test of epoch 40: loss -0.01241 acc 0.65714 roc_auc 0.49185 prc_auc 0.36764[0m
[92maverage training of epoch 41: loss -0.01216 acc 0.65125 roc_auc 0.46316 prc_auc 0.35225[0m
[93maverage test of epoch 41: loss -0.01390 acc 0.65714 roc_auc 0.46603 prc_auc 0.36439[0m
[92maverage training of epoch 42: loss -0.01365 acc 0.65125 roc_auc 0.46302 prc_auc 0.35227[0m
[93maverage test of epoch 42: loss -0.01539 acc 0.65714 roc_auc 0.49094 prc_auc 0.36654[0m
[92maverage training of epoch 43: loss -0.01514 acc 0.65125 roc_auc 0.46297 prc_auc 0.35220[0m
[93maverage test of epoch 43: loss -0.01687 acc 0.65714 roc_auc 0.49774 prc_auc 0.36988[0m
[92maverage training of epoch 44: loss -0.01663 acc 0.65125 roc_auc 0.46305 prc_auc 0.35220[0m
[93maverage test of epoch 44: loss -0.01836 acc 0.65714 roc_auc 0.44429 prc_auc 0.35543[0m
[92maverage training of epoch 45: loss -0.01811 acc 0.65125 roc_auc 0.46285 prc_auc 0.35216[0m
[93maverage test of epoch 45: loss -0.01984 acc 0.65714 roc_auc 0.49728 prc_auc 0.38057[0m
[92maverage training of epoch 46: loss -0.01960 acc 0.65125 roc_auc 0.46285 prc_auc 0.35211[0m
[93maverage test of epoch 46: loss -0.02131 acc 0.65714 roc_auc 0.41938 prc_auc 0.34420[0m
[92maverage training of epoch 47: loss -0.02108 acc 0.65125 roc_auc 0.46266 prc_auc 0.35200[0m
[93maverage test of epoch 47: loss -0.02279 acc 0.65714 roc_auc 0.44293 prc_auc 0.35299[0m
[92maverage training of epoch 48: loss -0.02255 acc 0.65125 roc_auc 0.46263 prc_auc 0.35191[0m
[93maverage test of epoch 48: loss -0.02426 acc 0.65714 roc_auc 0.54076 prc_auc 0.40091[0m
[92maverage training of epoch 49: loss -0.02403 acc 0.65125 roc_auc 0.46282 prc_auc 0.35213[0m
[93maverage test of epoch 49: loss -0.02574 acc 0.65714 roc_auc 0.50226 prc_auc 0.37119[0m
Training model with dataset, testing using fold 2
DfscodeRnn_cls args.lr:  0.0001
[92maverage training of epoch 0: loss 0.08256 acc 0.34520 roc_auc 0.50571 prc_auc 0.36975[0m
[93maverage test of epoch 0: loss 0.06212 acc 0.34286 roc_auc 0.48505 prc_auc 0.33889[0m
[92maverage training of epoch 1: loss 0.05817 acc 0.34520 roc_auc 0.50941 prc_auc 0.38141[0m
[93maverage test of epoch 1: loss 0.05477 acc 0.34286 roc_auc 0.49728 prc_auc 0.34153[0m
[92maverage training of epoch 2: loss 0.05371 acc 0.34520 roc_auc 0.50995 prc_auc 0.38135[0m
[93maverage test of epoch 2: loss 0.05136 acc 0.34286 roc_auc 0.40308 prc_auc 0.32589[0m
[92maverage training of epoch 3: loss 0.05160 acc 0.34520 roc_auc 0.50658 prc_auc 0.37551[0m
[93maverage test of epoch 3: loss 0.05024 acc 0.34286 roc_auc 0.46603 prc_auc 0.33661[0m
[92maverage training of epoch 4: loss 0.05048 acc 0.34520 roc_auc 0.50653 prc_auc 0.37556[0m
[93maverage test of epoch 4: loss 0.04911 acc 0.34286 roc_auc 0.48777 prc_auc 0.34246[0m
[92maverage training of epoch 5: loss 0.04936 acc 0.34520 roc_auc 0.50644 prc_auc 0.37551[0m
[93maverage test of epoch 5: loss 0.04799 acc 0.34286 roc_auc 0.48777 prc_auc 0.34246[0m
[92maverage training of epoch 6: loss 0.04824 acc 0.34520 roc_auc 0.50642 prc_auc 0.37534[0m
[93maverage test of epoch 6: loss 0.04686 acc 0.34286 roc_auc 0.49819 prc_auc 0.34246[0m
[92maverage training of epoch 7: loss 0.04712 acc 0.34520 roc_auc 0.50644 prc_auc 0.37534[0m
[93maverage test of epoch 7: loss 0.04574 acc 0.34286 roc_auc 0.47554 prc_auc 0.33512[0m
[92maverage training of epoch 8: loss 0.04600 acc 0.34520 roc_auc 0.50644 prc_auc 0.37555[0m
[93maverage test of epoch 8: loss 0.04461 acc 0.34286 roc_auc 0.48732 prc_auc 0.33899[0m
[92maverage training of epoch 9: loss 0.04488 acc 0.34520 roc_auc 0.50644 prc_auc 0.37537[0m
[93maverage test of epoch 9: loss 0.04349 acc 0.34286 roc_auc 0.46513 prc_auc 0.33396[0m
[92maverage training of epoch 10: loss 0.04376 acc 0.34520 roc_auc 0.50642 prc_auc 0.37536[0m
[93maverage test of epoch 10: loss 0.04237 acc 0.34286 roc_auc 0.47645 prc_auc 0.33859[0m
[92maverage training of epoch 11: loss 0.04265 acc 0.34520 roc_auc 0.50644 prc_auc 0.37554[0m
[93maverage test of epoch 11: loss 0.04124 acc 0.34286 roc_auc 0.48777 prc_auc 0.34246[0m
[92maverage training of epoch 12: loss 0.04153 acc 0.34520 roc_auc 0.50647 prc_auc 0.37565[0m
[93maverage test of epoch 12: loss 0.04012 acc 0.34286 roc_auc 0.48777 prc_auc 0.34246[0m
[92maverage training of epoch 13: loss 0.04041 acc 0.34520 roc_auc 0.50653 prc_auc 0.37539[0m
[93maverage test of epoch 13: loss 0.03900 acc 0.34286 roc_auc 0.48234 prc_auc 0.33740[0m
[92maverage training of epoch 14: loss 0.03929 acc 0.34520 roc_auc 0.50644 prc_auc 0.37554[0m
[93maverage test of epoch 14: loss 0.03787 acc 0.34286 roc_auc 0.48732 prc_auc 0.33899[0m
[92maverage training of epoch 15: loss 0.03818 acc 0.34520 roc_auc 0.50647 prc_auc 0.37569[0m
[93maverage test of epoch 15: loss 0.03675 acc 0.34286 roc_auc 0.47736 prc_auc 0.34246[0m
[92maverage training of epoch 16: loss 0.03706 acc 0.34520 roc_auc 0.50639 prc_auc 0.37527[0m
[93maverage test of epoch 16: loss 0.03563 acc 0.34286 roc_auc 0.46286 prc_auc 0.33224[0m
[92maverage training of epoch 17: loss 0.03595 acc 0.34520 roc_auc 0.50644 prc_auc 0.37538[0m
[93maverage test of epoch 17: loss 0.03451 acc 0.34286 roc_auc 0.49819 prc_auc 0.34246[0m
[92maverage training of epoch 18: loss 0.03483 acc 0.34520 roc_auc 0.50633 prc_auc 0.37516[0m
[93maverage test of epoch 18: loss 0.03339 acc 0.34286 roc_auc 0.49819 prc_auc 0.34246[0m
[92maverage training of epoch 19: loss 0.03371 acc 0.34520 roc_auc 0.50639 prc_auc 0.37532[0m
[93maverage test of epoch 19: loss 0.03227 acc 0.34286 roc_auc 0.48732 prc_auc 0.33899[0m
[92maverage training of epoch 20: loss 0.03260 acc 0.34520 roc_auc 0.50636 prc_auc 0.37536[0m
[93maverage test of epoch 20: loss 0.03114 acc 0.34286 roc_auc 0.49728 prc_auc 0.34206[0m
[92maverage training of epoch 21: loss 0.03149 acc 0.34520 roc_auc 0.50642 prc_auc 0.37538[0m
[93maverage test of epoch 21: loss 0.03002 acc 0.34286 roc_auc 0.47600 prc_auc 0.33859[0m
[92maverage training of epoch 22: loss 0.03037 acc 0.34520 roc_auc 0.50630 prc_auc 0.37510[0m
[93maverage test of epoch 22: loss 0.02891 acc 0.34286 roc_auc 0.49502 prc_auc 0.33819[0m
[92maverage training of epoch 23: loss 0.02926 acc 0.34520 roc_auc 0.50636 prc_auc 0.37517[0m
[93maverage test of epoch 23: loss 0.02779 acc 0.34286 roc_auc 0.47600 prc_auc 0.33859[0m
[92maverage training of epoch 24: loss 0.02814 acc 0.34520 roc_auc 0.50639 prc_auc 0.37516[0m
[93maverage test of epoch 24: loss 0.02667 acc 0.34286 roc_auc 0.49819 prc_auc 0.34246[0m
[92maverage training of epoch 25: loss 0.02703 acc 0.34520 roc_auc 0.50639 prc_auc 0.37517[0m
[93maverage test of epoch 25: loss 0.02555 acc 0.34286 roc_auc 0.48732 prc_auc 0.33899[0m
[92maverage training of epoch 26: loss 0.02592 acc 0.34520 roc_auc 0.50639 prc_auc 0.37516[0m
[93maverage test of epoch 26: loss 0.02443 acc 0.34286 roc_auc 0.50181 prc_auc 0.34402[0m
[92maverage training of epoch 27: loss 0.02481 acc 0.34520 roc_auc 0.50636 prc_auc 0.37516[0m
[93maverage test of epoch 27: loss 0.02331 acc 0.34286 roc_auc 0.48732 prc_auc 0.33899[0m
[92maverage training of epoch 28: loss 0.02369 acc 0.34520 roc_auc 0.50636 prc_auc 0.37509[0m
[93maverage test of epoch 28: loss 0.02220 acc 0.34286 roc_auc 0.47101 prc_auc 0.33220[0m
[92maverage training of epoch 29: loss 0.02258 acc 0.34520 roc_auc 0.50633 prc_auc 0.37506[0m
[93maverage test of epoch 29: loss 0.02108 acc 0.34286 roc_auc 0.47509 prc_auc 0.33512[0m
[92maverage training of epoch 30: loss 0.02147 acc 0.34520 roc_auc 0.50633 prc_auc 0.37520[0m
[93maverage test of epoch 30: loss 0.01996 acc 0.34286 roc_auc 0.48777 prc_auc 0.34246[0m
[92maverage training of epoch 31: loss 0.02036 acc 0.34520 roc_auc 0.50636 prc_auc 0.37516[0m
[93maverage test of epoch 31: loss 0.01885 acc 0.34286 roc_auc 0.46694 prc_auc 0.34246[0m
[92maverage training of epoch 32: loss 0.01925 acc 0.34520 roc_auc 0.50636 prc_auc 0.37533[0m
[93maverage test of epoch 32: loss 0.01773 acc 0.34286 roc_auc 0.45380 prc_auc 0.33210[0m
[92maverage training of epoch 33: loss 0.01814 acc 0.34520 roc_auc 0.50630 prc_auc 0.37506[0m
[93maverage test of epoch 33: loss 0.01662 acc 0.34286 roc_auc 0.51902 prc_auc 0.35212[0m
[92maverage training of epoch 34: loss 0.01703 acc 0.34520 roc_auc 0.50630 prc_auc 0.37496[0m
[93maverage test of epoch 34: loss 0.01550 acc 0.34286 roc_auc 0.48777 prc_auc 0.34246[0m
[92maverage training of epoch 35: loss 0.01592 acc 0.34520 roc_auc 0.50616 prc_auc 0.37501[0m
[93maverage test of epoch 35: loss 0.01439 acc 0.34286 roc_auc 0.48777 prc_auc 0.34246[0m
[92maverage training of epoch 36: loss 0.01482 acc 0.34520 roc_auc 0.50628 prc_auc 0.37504[0m
[93maverage test of epoch 36: loss 0.01328 acc 0.34286 roc_auc 0.47736 prc_auc 0.34246[0m
[92maverage training of epoch 37: loss 0.01371 acc 0.34520 roc_auc 0.50630 prc_auc 0.37523[0m
[93maverage test of epoch 37: loss 0.01216 acc 0.34286 roc_auc 0.49909 prc_auc 0.34415[0m
[92maverage training of epoch 38: loss 0.01260 acc 0.34520 roc_auc 0.50625 prc_auc 0.37515[0m
[93maverage test of epoch 38: loss 0.01105 acc 0.34286 roc_auc 0.50951 prc_auc 0.34906[0m
[92maverage training of epoch 39: loss 0.01150 acc 0.34520 roc_auc 0.50619 prc_auc 0.37500[0m
[93maverage test of epoch 39: loss 0.00994 acc 0.34286 roc_auc 0.48188 prc_auc 0.33393[0m
[92maverage training of epoch 40: loss 0.01039 acc 0.34520 roc_auc 0.50619 prc_auc 0.37503[0m
[93maverage test of epoch 40: loss 0.00883 acc 0.34286 roc_auc 0.51993 prc_auc 0.35253[0m
[92maverage training of epoch 41: loss 0.00928 acc 0.34520 roc_auc 0.50625 prc_auc 0.37503[0m
[93maverage test of epoch 41: loss 0.00772 acc 0.34286 roc_auc 0.47554 prc_auc 0.33512[0m
[92maverage training of epoch 42: loss 0.00818 acc 0.34520 roc_auc 0.50625 prc_auc 0.37501[0m
[93maverage test of epoch 42: loss 0.00661 acc 0.34286 roc_auc 0.47600 prc_auc 0.33859[0m
[92maverage training of epoch 43: loss 0.00707 acc 0.34520 roc_auc 0.50619 prc_auc 0.37494[0m
[93maverage test of epoch 43: loss 0.00550 acc 0.34286 roc_auc 0.48551 prc_auc 0.33810[0m
[92maverage training of epoch 44: loss 0.00597 acc 0.34520 roc_auc 0.50619 prc_auc 0.37501[0m
[93maverage test of epoch 44: loss 0.00439 acc 0.34286 roc_auc 0.46603 prc_auc 0.33743[0m
[92maverage training of epoch 45: loss 0.00487 acc 0.34520 roc_auc 0.50619 prc_auc 0.37501[0m
[93maverage test of epoch 45: loss 0.00328 acc 0.34286 roc_auc 0.48777 prc_auc 0.34246[0m
[92maverage training of epoch 46: loss 0.00376 acc 0.52669 roc_auc 0.50616 prc_auc 0.37493[0m
[93maverage test of epoch 46: loss 0.00217 acc 0.65714 roc_auc 0.46558 prc_auc 0.33743[0m
[92maverage training of epoch 47: loss 0.00266 acc 0.65836 roc_auc 0.50608 prc_auc 0.37500[0m
[93maverage test of epoch 47: loss 0.00106 acc 0.65714 roc_auc 0.48687 prc_auc 0.34008[0m
[92maverage training of epoch 48: loss 0.00156 acc 0.65836 roc_auc 0.50608 prc_auc 0.37499[0m
[93maverage test of epoch 48: loss -0.00005 acc 0.65714 roc_auc 0.48007 prc_auc 0.33861[0m
[92maverage training of epoch 49: loss 0.00046 acc 0.65836 roc_auc 0.50611 prc_auc 0.37502[0m
[93maverage test of epoch 49: loss -0.00115 acc 0.65714 roc_auc 0.48777 prc_auc 0.34246[0m
Training model with dataset, testing using fold 3
DfscodeRnn_cls args.lr:  0.0001
[92maverage training of epoch 0: loss 0.64263 acc 0.34520 roc_auc 0.48636 prc_auc 0.34827[0m
[93maverage test of epoch 0: loss 0.61254 acc 0.34286 roc_auc 0.49004 prc_auc 0.33843[0m
[92maverage training of epoch 1: loss 0.60168 acc 0.34520 roc_auc 0.48005 prc_auc 0.34107[0m
[93maverage test of epoch 1: loss 0.60034 acc 0.34286 roc_auc 0.49004 prc_auc 0.33843[0m
[92maverage training of epoch 2: loss 0.59428 acc 0.34520 roc_auc 0.47963 prc_auc 0.34007[0m
[93maverage test of epoch 2: loss 0.59484 acc 0.34286 roc_auc 0.57065 prc_auc 0.37919[0m
[92maverage training of epoch 3: loss 0.59087 acc 0.34520 roc_auc 0.48017 prc_auc 0.34083[0m
[93maverage test of epoch 3: loss 0.59300 acc 0.34286 roc_auc 0.51178 prc_auc 0.34825[0m
[92maverage training of epoch 4: loss 0.58903 acc 0.34520 roc_auc 0.47991 prc_auc 0.34049[0m
[93maverage test of epoch 4: loss 0.59115 acc 0.34286 roc_auc 0.51087 prc_auc 0.34783[0m
[92maverage training of epoch 5: loss 0.58719 acc 0.34520 roc_auc 0.48017 prc_auc 0.34025[0m
[93maverage test of epoch 5: loss 0.58931 acc 0.34286 roc_auc 0.49049 prc_auc 0.33863[0m
[92maverage training of epoch 6: loss 0.58535 acc 0.34520 roc_auc 0.48005 prc_auc 0.34064[0m
[93maverage test of epoch 6: loss 0.58748 acc 0.34286 roc_auc 0.51087 prc_auc 0.34783[0m
[92maverage training of epoch 7: loss 0.58351 acc 0.34520 roc_auc 0.48031 prc_auc 0.34065[0m
[93maverage test of epoch 7: loss 0.58564 acc 0.34286 roc_auc 0.51087 prc_auc 0.34783[0m
[92maverage training of epoch 8: loss 0.58167 acc 0.34520 roc_auc 0.48017 prc_auc 0.34009[0m
[93maverage test of epoch 8: loss 0.58380 acc 0.34286 roc_auc 0.49004 prc_auc 0.33843[0m
[92maverage training of epoch 9: loss 0.57984 acc 0.34520 roc_auc 0.48011 prc_auc 0.34016[0m
[93maverage test of epoch 9: loss 0.58197 acc 0.34286 roc_auc 0.51178 prc_auc 0.34825[0m
[92maverage training of epoch 10: loss 0.57800 acc 0.34520 roc_auc 0.48005 prc_auc 0.34035[0m
[93maverage test of epoch 10: loss 0.58013 acc 0.34286 roc_auc 0.50000 prc_auc 0.34286[0m
[92maverage training of epoch 11: loss 0.57617 acc 0.34520 roc_auc 0.47997 prc_auc 0.34006[0m
[93maverage test of epoch 11: loss 0.57830 acc 0.34286 roc_auc 0.51087 prc_auc 0.34783[0m
[92maverage training of epoch 12: loss 0.57434 acc 0.34520 roc_auc 0.47991 prc_auc 0.34011[0m
[93maverage test of epoch 12: loss 0.57647 acc 0.34286 roc_auc 0.52355 prc_auc 0.35384[0m
[92maverage training of epoch 13: loss 0.57250 acc 0.34520 roc_auc 0.48019 prc_auc 0.34033[0m
[93maverage test of epoch 13: loss 0.57464 acc 0.34286 roc_auc 0.48777 prc_auc 0.34167[0m
[92maverage training of epoch 14: loss 0.57067 acc 0.34520 roc_auc 0.47991 prc_auc 0.34054[0m
[93maverage test of epoch 14: loss 0.57281 acc 0.34286 roc_auc 0.51087 prc_auc 0.34783[0m
[92maverage training of epoch 15: loss 0.56885 acc 0.34520 roc_auc 0.48011 prc_auc 0.34073[0m
[93maverage test of epoch 15: loss 0.57098 acc 0.34286 roc_auc 0.53533 prc_auc 0.35959[0m
[92maverage training of epoch 16: loss 0.56702 acc 0.34520 roc_auc 0.48003 prc_auc 0.34073[0m
[93maverage test of epoch 16: loss 0.56916 acc 0.34286 roc_auc 0.49592 prc_auc 0.34106[0m
[92maverage training of epoch 17: loss 0.56520 acc 0.34520 roc_auc 0.48022 prc_auc 0.34052[0m
[93maverage test of epoch 17: loss 0.56733 acc 0.34286 roc_auc 0.50000 prc_auc 0.34286[0m
[92maverage training of epoch 18: loss 0.56337 acc 0.34520 roc_auc 0.48022 prc_auc 0.34072[0m
[93maverage test of epoch 18: loss 0.56551 acc 0.34286 roc_auc 0.51087 prc_auc 0.34783[0m
[92maverage training of epoch 19: loss 0.56155 acc 0.34520 roc_auc 0.48025 prc_auc 0.34032[0m
[93maverage test of epoch 19: loss 0.56369 acc 0.34286 roc_auc 0.49004 prc_auc 0.33843[0m
[92maverage training of epoch 20: loss 0.55973 acc 0.34520 roc_auc 0.48008 prc_auc 0.34049[0m
[93maverage test of epoch 20: loss 0.56187 acc 0.34286 roc_auc 0.51178 prc_auc 0.34825[0m
[92maverage training of epoch 21: loss 0.55791 acc 0.34520 roc_auc 0.48028 prc_auc 0.34075[0m
[93maverage test of epoch 21: loss 0.56005 acc 0.34286 roc_auc 0.49004 prc_auc 0.33843[0m
[92maverage training of epoch 22: loss 0.55610 acc 0.34520 roc_auc 0.48025 prc_auc 0.34080[0m
[93maverage test of epoch 22: loss 0.55824 acc 0.34286 roc_auc 0.54484 prc_auc 0.36436[0m
[92maverage training of epoch 23: loss 0.55428 acc 0.34520 roc_auc 0.48028 prc_auc 0.34061[0m
[93maverage test of epoch 23: loss 0.55642 acc 0.34286 roc_auc 0.51087 prc_auc 0.34783[0m
[92maverage training of epoch 24: loss 0.55247 acc 0.34520 roc_auc 0.48033 prc_auc 0.34015[0m
[93maverage test of epoch 24: loss 0.55461 acc 0.34286 roc_auc 0.53261 prc_auc 0.35889[0m
[92maverage training of epoch 25: loss 0.55065 acc 0.34520 roc_auc 0.48039 prc_auc 0.34024[0m
[93maverage test of epoch 25: loss 0.55280 acc 0.34286 roc_auc 0.51178 prc_auc 0.34825[0m
[92maverage training of epoch 26: loss 0.54884 acc 0.34520 roc_auc 0.48036 prc_auc 0.34038[0m
[93maverage test of epoch 26: loss 0.55099 acc 0.34286 roc_auc 0.52853 prc_auc 0.35646[0m
[92maverage training of epoch 27: loss 0.54703 acc 0.34520 roc_auc 0.48042 prc_auc 0.34039[0m
[93maverage test of epoch 27: loss 0.54918 acc 0.34286 roc_auc 0.51087 prc_auc 0.34825[0m
[92maverage training of epoch 28: loss 0.54523 acc 0.34520 roc_auc 0.48061 prc_auc 0.34065[0m
[93maverage test of epoch 28: loss 0.54737 acc 0.34286 roc_auc 0.51087 prc_auc 0.34783[0m
[92maverage training of epoch 29: loss 0.54342 acc 0.34520 roc_auc 0.48042 prc_auc 0.34048[0m
[93maverage test of epoch 29: loss 0.54557 acc 0.34286 roc_auc 0.50091 prc_auc 0.34327[0m
[92maverage training of epoch 30: loss 0.54161 acc 0.34520 roc_auc 0.48047 prc_auc 0.34021[0m
[93maverage test of epoch 30: loss 0.54376 acc 0.34286 roc_auc 0.51087 prc_auc 0.34783[0m
[92maverage training of epoch 31: loss 0.53981 acc 0.34520 roc_auc 0.48036 prc_auc 0.34046[0m
[93maverage test of epoch 31: loss 0.54196 acc 0.34286 roc_auc 0.51087 prc_auc 0.34783[0m
[92maverage training of epoch 32: loss 0.53801 acc 0.34520 roc_auc 0.48042 prc_auc 0.34029[0m
[93maverage test of epoch 32: loss 0.54016 acc 0.34286 roc_auc 0.50000 prc_auc 0.34286[0m
[92maverage training of epoch 33: loss 0.53621 acc 0.34520 roc_auc 0.48067 prc_auc 0.34056[0m
[93maverage test of epoch 33: loss 0.53836 acc 0.34286 roc_auc 0.51087 prc_auc 0.34783[0m
[92maverage training of epoch 34: loss 0.53441 acc 0.34520 roc_auc 0.48045 prc_auc 0.34043[0m
[93maverage test of epoch 34: loss 0.53656 acc 0.34286 roc_auc 0.51087 prc_auc 0.34783[0m
[92maverage training of epoch 35: loss 0.53262 acc 0.34520 roc_auc 0.48061 prc_auc 0.34043[0m
[93maverage test of epoch 35: loss 0.53477 acc 0.34286 roc_auc 0.49049 prc_auc 0.33863[0m
[92maverage training of epoch 36: loss 0.53082 acc 0.34520 roc_auc 0.48075 prc_auc 0.34063[0m
[93maverage test of epoch 36: loss 0.53297 acc 0.34286 roc_auc 0.49049 prc_auc 0.33863[0m
[92maverage training of epoch 37: loss 0.52903 acc 0.34520 roc_auc 0.48087 prc_auc 0.34069[0m
[93maverage test of epoch 37: loss 0.53118 acc 0.34286 roc_auc 0.49004 prc_auc 0.33843[0m
[92maverage training of epoch 38: loss 0.52724 acc 0.34520 roc_auc 0.48070 prc_auc 0.34068[0m
[93maverage test of epoch 38: loss 0.52939 acc 0.34286 roc_auc 0.53351 prc_auc 0.35869[0m
[92maverage training of epoch 39: loss 0.52545 acc 0.34520 roc_auc 0.48101 prc_auc 0.34090[0m
[93maverage test of epoch 39: loss 0.52760 acc 0.34286 roc_auc 0.50091 prc_auc 0.34327[0m
[92maverage training of epoch 40: loss 0.52366 acc 0.34520 roc_auc 0.48089 prc_auc 0.34098[0m
[93maverage test of epoch 40: loss 0.52581 acc 0.34286 roc_auc 0.49049 prc_auc 0.33863[0m
[92maverage training of epoch 41: loss 0.52187 acc 0.34520 roc_auc 0.48089 prc_auc 0.34116[0m
[93maverage test of epoch 41: loss 0.52402 acc 0.34286 roc_auc 0.51223 prc_auc 0.34846[0m
[92maverage training of epoch 42: loss 0.52009 acc 0.34520 roc_auc 0.48106 prc_auc 0.34142[0m
[93maverage test of epoch 42: loss 0.52224 acc 0.34286 roc_auc 0.53397 prc_auc 0.35889[0m
[92maverage training of epoch 43: loss 0.51830 acc 0.34520 roc_auc 0.48087 prc_auc 0.34057[0m
[93maverage test of epoch 43: loss 0.52046 acc 0.34286 roc_auc 0.46784 prc_auc 0.33024[0m
[92maverage training of epoch 44: loss 0.51652 acc 0.34520 roc_auc 0.48126 prc_auc 0.34097[0m
[93maverage test of epoch 44: loss 0.51868 acc 0.34286 roc_auc 0.49004 prc_auc 0.33843[0m
[92maverage training of epoch 45: loss 0.51474 acc 0.34520 roc_auc 0.48129 prc_auc 0.34097[0m
[93maverage test of epoch 45: loss 0.51690 acc 0.34286 roc_auc 0.49004 prc_auc 0.33843[0mUsing backend: pytorch

[92maverage training of epoch 46: loss 0.51296 acc 0.34520 roc_auc 0.48112 prc_auc 0.34074[0m
[93maverage test of epoch 46: loss 0.51512 acc 0.34286 roc_auc 0.49004 prc_auc 0.33843[0m
[92maverage training of epoch 47: loss 0.51119 acc 0.34520 roc_auc 0.48115 prc_auc 0.34089[0m
[93maverage test of epoch 47: loss 0.51334 acc 0.34286 roc_auc 0.46603 prc_auc 0.32865[0m
[92maverage training of epoch 48: loss 0.50941 acc 0.34520 roc_auc 0.48148 prc_auc 0.34104[0m
[93maverage test of epoch 48: loss 0.51157 acc 0.34286 roc_auc 0.50000 prc_auc 0.34286[0m
[92maverage training of epoch 49: loss 0.50764 acc 0.34520 roc_auc 0.48123 prc_auc 0.34090[0m
[93maverage test of epoch 49: loss 0.50979 acc 0.34286 roc_auc 0.51087 prc_auc 0.34783[0m
Training model with dataset, testing using fold 4
DfscodeRnn_cls args.lr:  0.0001
[92maverage training of epoch 0: loss -0.44837 acc 0.34520 roc_auc 0.51031 prc_auc 0.37458[0m
[93maverage test of epoch 0: loss -0.45939 acc 0.34286 roc_auc 0.46739 prc_auc 0.34286[0m
[92maverage training of epoch 1: loss -0.46623 acc 0.34520 roc_auc 0.51546 prc_auc 0.39075[0m
[93maverage test of epoch 1: loss -0.46467 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 2: loss -0.46953 acc 0.34520 roc_auc 0.51437 prc_auc 0.39014[0m
[93maverage test of epoch 2: loss -0.46718 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 3: loss -0.47112 acc 0.34520 roc_auc 0.51418 prc_auc 0.39098[0m
[93maverage test of epoch 3: loss -0.46801 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 4: loss -0.47195 acc 0.34520 roc_auc 0.51401 prc_auc 0.38921[0m
[93maverage test of epoch 4: loss -0.46884 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 5: loss -0.47278 acc 0.34520 roc_auc 0.51390 prc_auc 0.38901[0m
[93maverage test of epoch 5: loss -0.46967 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 6: loss -0.47361 acc 0.34520 roc_auc 0.51401 prc_auc 0.38896[0m
[93maverage test of epoch 6: loss -0.47050 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 7: loss -0.47444 acc 0.34520 roc_auc 0.51404 prc_auc 0.38950[0m
[93maverage test of epoch 7: loss -0.47132 acc 0.34286 roc_auc 0.47826 prc_auc 0.34783[0m
[92maverage training of epoch 8: loss -0.47527 acc 0.34520 roc_auc 0.51398 prc_auc 0.38830[0m
[93maverage test of epoch 8: loss -0.47215 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 9: loss -0.47611 acc 0.34520 roc_auc 0.51384 prc_auc 0.38880[0m
[93maverage test of epoch 9: loss -0.47298 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 10: loss -0.47694 acc 0.34520 roc_auc 0.51392 prc_auc 0.38877[0m
[93maverage test of epoch 10: loss -0.47381 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 11: loss -0.47777 acc 0.34520 roc_auc 0.51384 prc_auc 0.38834[0m
[93maverage test of epoch 11: loss -0.47464 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 12: loss -0.47860 acc 0.34520 roc_auc 0.51370 prc_auc 0.38858[0m
[93maverage test of epoch 12: loss -0.47547 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 13: loss -0.47943 acc 0.34520 roc_auc 0.51381 prc_auc 0.38827[0m
[93maverage test of epoch 13: loss -0.47629 acc 0.34286 roc_auc 0.51268 prc_auc 0.35621[0m
[92maverage training of epoch 14: loss -0.48026 acc 0.34520 roc_auc 0.51390 prc_auc 0.38902[0m
[93maverage test of epoch 14: loss -0.47712 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 15: loss -0.48109 acc 0.34520 roc_auc 0.51381 prc_auc 0.38793[0m
[93maverage test of epoch 15: loss -0.47795 acc 0.34286 roc_auc 0.44248 prc_auc 0.32576[0m
[92maverage training of epoch 16: loss -0.48192 acc 0.34520 roc_auc 0.51406 prc_auc 0.38949[0m
[93maverage test of epoch 16: loss -0.47878 acc 0.34286 roc_auc 0.46739 prc_auc 0.34286[0m
[92maverage training of epoch 17: loss -0.48275 acc 0.34520 roc_auc 0.51378 prc_auc 0.38875[0m
[93maverage test of epoch 17: loss -0.47960 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 18: loss -0.48358 acc 0.34520 roc_auc 0.51376 prc_auc 0.38822[0m
[93maverage test of epoch 18: loss -0.48043 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 19: loss -0.48441 acc 0.34520 roc_auc 0.51398 prc_auc 0.38906[0m
[93maverage test of epoch 19: loss -0.48126 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 20: loss -0.48524 acc 0.34520 roc_auc 0.51384 prc_auc 0.38893[0m
[93maverage test of epoch 20: loss -0.48209 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 21: loss -0.48607 acc 0.34520 roc_auc 0.51390 prc_auc 0.38816[0m
[93maverage test of epoch 21: loss -0.48291 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 22: loss -0.48690 acc 0.34520 roc_auc 0.51390 prc_auc 0.38819[0m
[93maverage test of epoch 22: loss -0.48374 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 23: loss -0.48773 acc 0.34520 roc_auc 0.51395 prc_auc 0.38909[0m
[93maverage test of epoch 23: loss -0.48457 acc 0.34286 roc_auc 0.50181 prc_auc 0.34835[0m
[92maverage training of epoch 24: loss -0.48856 acc 0.34520 roc_auc 0.51395 prc_auc 0.38881[0m
[93maverage test of epoch 24: loss -0.48539 acc 0.34286 roc_auc 0.44022 prc_auc 0.32679[0m
[92maverage training of epoch 25: loss -0.48939 acc 0.34520 roc_auc 0.51381 prc_auc 0.38846[0m
[93maverage test of epoch 25: loss -0.48622 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 26: loss -0.49022 acc 0.34520 roc_auc 0.51390 prc_auc 0.38869[0m
[93maverage test of epoch 26: loss -0.48705 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 27: loss -0.49105 acc 0.34520 roc_auc 0.51392 prc_auc 0.38885[0m
[93maverage test of epoch 27: loss -0.48787 acc 0.34286 roc_auc 0.46739 prc_auc 0.34286[0m
[92maverage training of epoch 28: loss -0.49188 acc 0.34520 roc_auc 0.51395 prc_auc 0.38944[0m
[93maverage test of epoch 28: loss -0.48870 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 29: loss -0.49270 acc 0.34520 roc_auc 0.51395 prc_auc 0.38886[0m
[93maverage test of epoch 29: loss -0.48953 acc 0.34286 roc_auc 0.47826 prc_auc 0.34783[0m
[92maverage training of epoch 30: loss -0.49353 acc 0.34520 roc_auc 0.51381 prc_auc 0.38851[0m
[93maverage test of epoch 30: loss -0.49035 acc 0.34286 roc_auc 0.46739 prc_auc 0.34286[0m
[92maverage training of epoch 31: loss -0.49436 acc 0.34520 roc_auc 0.51392 prc_auc 0.38873[0m
[93maverage test of epoch 31: loss -0.49118 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 32: loss -0.49519 acc 0.34520 roc_auc 0.51387 prc_auc 0.38913[0m
[93maverage test of epoch 32: loss -0.49201 acc 0.34286 roc_auc 0.48732 prc_auc 0.33899[0m
[92maverage training of epoch 33: loss -0.49602 acc 0.34520 roc_auc 0.51387 prc_auc 0.38950[0m
[93maverage test of epoch 33: loss -0.49283 acc 0.34286 roc_auc 0.46739 prc_auc 0.34286[0m
[92maverage training of epoch 34: loss -0.49685 acc 0.34520 roc_auc 0.51398 prc_auc 0.38913[0m
[93maverage test of epoch 34: loss -0.49366 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 35: loss -0.49768 acc 0.34520 roc_auc 0.51387 prc_auc 0.38877[0m
[93maverage test of epoch 35: loss -0.49448 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 36: loss -0.49850 acc 0.34520 roc_auc 0.51381 prc_auc 0.38864[0m
[93maverage test of epoch 36: loss -0.49531 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 37: loss -0.49933 acc 0.34520 roc_auc 0.51378 prc_auc 0.38871[0m
[93maverage test of epoch 37: loss -0.49613 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 38: loss -0.50016 acc 0.34520 roc_auc 0.51387 prc_auc 0.38817[0m
[93maverage test of epoch 38: loss -0.49696 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 39: loss -0.50099 acc 0.34520 roc_auc 0.51381 prc_auc 0.38939[0m
[93maverage test of epoch 39: loss -0.49778 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 40: loss -0.50182 acc 0.34520 roc_auc 0.51381 prc_auc 0.38924[0m
[93maverage test of epoch 40: loss -0.49861 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 41: loss -0.50264 acc 0.34520 roc_auc 0.51398 prc_auc 0.38868[0m
[93maverage test of epoch 41: loss -0.49943 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 42: loss -0.50347 acc 0.34520 roc_auc 0.51401 prc_auc 0.38927[0m
[93maverage test of epoch 42: loss -0.50026 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 43: loss -0.50430 acc 0.34520 roc_auc 0.51404 prc_auc 0.38919[0m
[93maverage test of epoch 43: loss -0.50108 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 44: loss -0.50512 acc 0.34520 roc_auc 0.51395 prc_auc 0.38875[0m
[93maverage test of epoch 44: loss -0.50191 acc 0.34286 roc_auc 0.46739 prc_auc 0.34286[0m
[92maverage training of epoch 45: loss -0.50595 acc 0.34520 roc_auc 0.51387 prc_auc 0.38916[0m
[93maverage test of epoch 45: loss -0.50273 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 46: loss -0.50678 acc 0.34520 roc_auc 0.51376 prc_auc 0.38860[0m
[93maverage test of epoch 46: loss -0.50356 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 47: loss -0.50760 acc 0.34520 roc_auc 0.51384 prc_auc 0.38878[0m
[93maverage test of epoch 47: loss -0.50438 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
[92maverage training of epoch 48: loss -0.50843 acc 0.34520 roc_auc 0.51398 prc_auc 0.38938[0m
[93maverage test of epoch 48: loss -0.50521 acc 0.34286 roc_auc 0.47826 prc_auc 0.34286[0m
[92maverage training of epoch 49: loss -0.50926 acc 0.34520 roc_auc 0.51395 prc_auc 0.38817[0m
[93maverage test of epoch 49: loss -0.50603 acc 0.34286 roc_auc 0.48913 prc_auc 0.34783[0m
Run statistics: 
==== Configuration Settings ====
== Run Settings ==
Model: DFScodeRNN_cls, Dataset: PTC_FR
num_epochs: 50
learning_rate: 0.0001
seed: 1800
k_fold: 5
model: DFScodeRNN_cls
dataset: PTC_FR

== Model Settings and results ==
dummy: 0

Accuracy (avg): 0.47042 ROC_AUC (avg): 0.49836 PRC_AUC (avg): 0.35366 

Average forward propagation time taken(ms): 3.214854350733302
Average backward propagation time taken(ms): 1.2165276328546824

