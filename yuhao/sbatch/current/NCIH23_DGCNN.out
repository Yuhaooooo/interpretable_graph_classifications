# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
GCNN_GAP_graphgen     *  /home/FYP/heyu0012/.conda/envs/GCNN_GAP_graphgen
graphgen                 /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

====== begin of gnn configuration ======
| msg_average = 0
======   end of gnn configuration ======


torch.cuda.is_available():  True 


load_data.py load_model_data(): Unserialising pickled dataset into Graph objects
==== Dataset Information ====
== General Information == 
Number of graphs: 2500
Number of classes: 2
Class distribution: 
0:2000 1:500 

== Node information== 
Average number of nodes: 27
Average number of edges (undirected): 29
Max number of nodes: 93
Number of distinct node labels: 18
Average number of distinct node labels: 3
Node labels distribution: 
0:9454 1:6143 13:139 16:2 18:1 2:49955 20:15 21:29 25:5 26:3 3:921 4:570 44:1 49:1 5:70 6:485 8:14 9:4 

*** 3 dataset_features:  {'name': 'NCI-H23', 'num_class': 2, 'label_dict': {'0': 0, '1': 1}, 'have_node_labels': True, 'have_node_attributions': False, 'node_dict': {'0': 0, '1': 1, '13': 2, '16': 3, '18': 4, '2': 5, '20': 6, '21': 7, '25': 8, '26': 9, '3': 10, '4': 11, '44': 12, '49': 13, '5': 14, '6': 15, '8': 16, '9': 17, 'UNKNOWN': 18}, 'feat_dim': 19, 'edge_feat_dim': 0, 'max_num_nodes': 93, 'avg_num_nodes': 27, 'graph_sizes_list': [34, 30, 34, 19, 20, 20, 13, 45, 22, 20, 26, 20, 15, 25, 56, 34, 19, 56, 24, 22, 25, 23, 24, 27, 25, 29, 27, 24, 22, 38, 22, 26, 23, 9, 7, 25, 35, 22, 18, 15, 24, 24, 27, 20, 18, 31, 25, 28, 46, 13, 29, 31, 21, 29, 20, 25, 48, 42, 28, 23, 34, 17, 28, 28, 15, 41, 35, 30, 20, 27, 25, 19, 31, 17, 29, 30, 75, 34, 22, 24, 33, 14, 22, 13, 24, 16, 17, 22, 13, 31, 26, 19, 15, 34, 26, 28, 24, 29, 18, 21, 12, 23, 23, 25, 22, 18, 16, 12, 17, 20, 27, 22, 19, 48, 23, 25, 33, 24, 17, 19, 24, 24, 10, 23, 14, 30, 22, 18, 26, 32, 31, 27, 30, 15, 29, 20, 19, 20, 34, 14, 15, 31, 18, 18, 24, 41, 44, 20, 38, 30, 28, 16, 29, 23, 31, 15, 11, 35, 29, 34, 19, 40, 18, 29, 27, 32, 17, 20, 20, 21, 15, 21, 31, 20, 57, 23, 13, 49, 32, 34, 31, 9, 20, 12, 16, 21, 28, 27, 17, 37, 32, 27, 16, 40, 21, 7, 30, 24, 16, 31, 18, 30, 20, 13, 20, 22, 23, 31, 20, 26, 32, 43, 14, 20, 14, 16, 20, 62, 19, 8, 31, 23, 35, 37, 20, 42, 26, 17, 46, 23, 19, 15, 23, 24, 23, 29, 33, 21, 20, 12, 58, 25, 15, 14, 26, 37, 15, 28, 48, 24, 21, 23, 22, 22, 51, 14, 17, 24, 23, 11, 18, 26, 35, 10, 19, 18, 14, 28, 23, 28, 18, 38, 42, 29, 24, 21, 28, 18, 44, 27, 27, 19, 32, 32, 20, 41, 24, 34, 32, 25, 18, 16, 28, 10, 22, 29, 21, 29, 17, 21, 24, 12, 41, 20, 24, 21, 26, 29, 16, 31, 30, 41, 35, 29, 27, 25, 31, 32, 29, 53, 13, 42, 27, 17, 23, 19, 40, 21, 21, 23, 18, 48, 33, 25, 29, 21, 18, 24, 22, 34, 27, 25, 15, 24, 23, 19, 13, 24, 19, 30, 27, 31, 41, 24, 60, 36, 40, 13, 16, 23, 28, 15, 28, 13, 13, 35, 21, 23, 24, 14, 25, 52, 25, 19, 16, 24, 26, 16, 13, 33, 12, 24, 26, 21, 11, 20, 23, 15, 21, 18, 47, 24, 23, 24, 24, 24, 22, 18, 27, 19, 18, 32, 24, 23, 30, 19, 21, 24, 21, 20, 50, 32, 26, 37, 17, 22, 31, 22, 19, 31, 14, 14, 36, 17, 22, 36, 17, 22, 21, 34, 24, 27, 16, 15, 16, 16, 29, 15, 21, 20, 32, 42, 35, 17, 27, 17, 28, 25, 28, 13, 20, 29, 20, 23, 16, 10, 16, 24, 24, 15, 26, 14, 27, 30, 8, 21, 27, 45, 34, 26, 17, 35, 31, 26, 31, 24, 22, 18, 29, 18, 21, 16, 21, 28, 18, 51, 41, 32, 19, 26, 24, 16, 20, 14, 41, 29, 43, 17, 29, 27, 32, 22, 20, 16, 15, 30, 26, 25, 25, 20, 41, 17, 14, 24, 21, 15, 24, 29, 24, 39, 22, 33, 15, 33, 28, 17, 19, 22, 9, 30, 25, 28, 17, 12, 38, 35, 15, 31, 24, 64, 18, 26, 22, 34, 11, 26, 19, 15, 36, 33, 38, 19, 20, 68, 18, 18, 19, 26, 12, 24, 30, 17, 26, 22, 22, 20, 59, 9, 32, 30, 16, 33, 33, 18, 40, 22, 30, 36, 20, 39, 40, 16, 19, 19, 30, 23, 33, 43, 22, 35, 26, 34, 30, 36, 50, 22, 17, 19, 24, 30, 26, 14, 26, 22, 22, 22, 34, 32, 24, 18, 43, 15, 35, 26, 14, 20, 24, 33, 25, 19, 16, 12, 25, 30, 15, 22, 20, 52, 16, 31, 21, 14, 18, 17, 18, 38, 37, 15, 27, 16, 19, 32, 19, 36, 21, 24, 23, 22, 55, 25, 15, 21, 25, 28, 19, 16, 29, 15, 30, 27, 55, 23, 15, 30, 21, 21, 35, 24, 27, 20, 14, 31, 31, 27, 20, 19, 33, 18, 12, 24, 44, 19, 23, 26, 22, 16, 12, 28, 32, 26, 14, 25, 18, 22, 20, 22, 30, 34, 16, 30, 13, 14, 37, 17, 54, 29, 22, 23, 29, 20, 15, 76, 27, 42, 20, 35, 23, 20, 21, 30, 39, 27, 18, 27, 28, 31, 30, 24, 11, 31, 35, 45, 20, 17, 31, 28, 19, 17, 19, 19, 29, 28, 18, 25, 21, 12, 37, 21, 19, 32, 22, 29, 19, 24, 22, 22, 21, 26, 29, 31, 25, 10, 19, 35, 18, 17, 22, 29, 30, 17, 37, 17, 29, 18, 20, 37, 31, 28, 9, 18, 19, 18, 27, 26, 27, 16, 15, 20, 10, 20, 21, 32, 41, 14, 19, 24, 36, 33, 29, 25, 38, 27, 23, 7, 25, 20, 34, 38, 24, 22, 19, 27, 23, 20, 15, 23, 21, 16, 23, 41, 17, 26, 24, 14, 18, 28, 27, 34, 29, 58, 28, 27, 29, 58, 17, 35, 23, 24, 17, 27, 28, 29, 17, 19, 17, 30, 17, 41, 16, 45, 18, 22, 24, 21, 30, 20, 17, 32, 16, 25, 19, 36, 23, 39, 23, 32, 24, 25, 36, 26, 20, 70, 16, 24, 17, 23, 17, 24, 25, 31, 19, 25, 32, 35, 16, 25, 18, 22, 20, 22, 36, 24, 20, 22, 34, 38, 22, 15, 13, 43, 24, 17, 48, 38, 18, 36, 27, 25, 23, 14, 20, 25, 38, 21, 80, 15, 20, 34, 22, 22, 20, 38, 26, 31, 23, 22, 52, 40, 22, 15, 32, 25, 24, 21, 36, 40, 38, 24, 47, 19, 25, 23, 10, 28, 35, 24, 37, 13, 37, 19, 46, 28, 18, 18, 10, 21, 40, 40, 14, 25, 39, 32, 20, 32, 18, 22, 22, 23, 14, 37, 19, 20, 30, 34, 20, 30, 15, 27, 22, 23, 45, 34, 25, 23, 25, 25, 23, 16, 25, 22, 18, 32, 46, 39, 20, 20, 37, 16, 26, 30, 27, 38, 27, 30, 22, 21, 22, 20, 26, 19, 13, 28, 15, 37, 26, 18, 28, 62, 23, 33, 16, 22, 13, 21, 27, 60, 58, 27, 30, 35, 18, 27, 27, 24, 23, 20, 23, 24, 28, 15, 62, 30, 24, 22, 46, 25, 20, 27, 24, 36, 22, 23, 12, 21, 33, 27, 11, 23, 23, 12, 20, 31, 17, 20, 23, 8, 20, 22, 21, 27, 15, 32, 17, 35, 30, 42, 22, 17, 28, 21, 17, 44, 12, 15, 29, 24, 14, 16, 18, 63, 33, 30, 30, 37, 21, 20, 59, 13, 15, 18, 30, 28, 31, 22, 19, 42, 32, 35, 24, 26, 26, 23, 23, 27, 21, 19, 23, 17, 24, 32, 21, 16, 32, 13, 19, 20, 41, 17, 27, 26, 7, 18, 22, 21, 15, 23, 42, 30, 32, 19, 17, 54, 20, 22, 24, 31, 31, 14, 18, 19, 28, 15, 20, 10, 26, 21, 16, 19, 14, 29, 27, 22, 14, 23, 34, 37, 34, 27, 22, 20, 17, 24, 31, 57, 20, 45, 36, 23, 40, 22, 16, 24, 26, 23, 3, 19, 17, 32, 17, 38, 32, 20, 23, 25, 45, 19, 39, 21, 35, 29, 35, 24, 39, 26, 21, 15, 29, 38, 16, 43, 26, 31, 24, 27, 30, 25, 21, 28, 16, 24, 22, 16, 25, 14, 28, 32, 35, 23, 18, 18, 20, 12, 23, 22, 16, 22, 18, 30, 31, 24, 29, 26, 42, 7, 14, 44, 15, 23, 21, 22, 25, 35, 21, 15, 14, 21, 29, 23, 29, 23, 19, 28, 37, 30, 40, 30, 8, 20, 20, 28, 20, 15, 14, 33, 22, 26, 46, 30, 28, 26, 15, 26, 9, 20, 22, 26, 17, 15, 15, 31, 21, 21, 27, 22, 19, 29, 31, 22, 17, 12, 23, 26, 17, 24, 20, 31, 35, 21, 16, 17, 48, 40, 10, 27, 12, 19, 20, 19, 51, 30, 19, 15, 29, 22, 38, 34, 23, 63, 20, 22, 15, 27, 36, 18, 51, 17, 2, 26, 28, 30, 22, 20, 50, 15, 27, 21, 26, 21, 22, 18, 33, 20, 21, 39, 40, 18, 16, 16, 21, 17, 30, 17, 26, 26, 31, 44, 27, 31, 44, 19, 29, 34, 24, 16, 20, 18, 26, 29, 39, 35, 58, 20, 10, 27, 41, 21, 19, 19, 13, 28, 47, 20, 28, 14, 20, 17, 15, 28, 25, 24, 15, 27, 21, 27, 17, 47, 37, 5, 33, 31, 17, 45, 34, 35, 18, 22, 29, 25, 22, 21, 29, 24, 13, 26, 28, 23, 28, 28, 13, 22, 11, 29, 51, 24, 14, 25, 17, 41, 20, 16, 24, 25, 10, 31, 26, 25, 19, 18, 11, 14, 27, 18, 11, 17, 26, 23, 32, 28, 27, 27, 18, 15, 16, 31, 33, 33, 18, 22, 21, 22, 18, 31, 65, 31, 22, 25, 18, 22, 32, 27, 15, 22, 32, 41, 26, 79, 21, 21, 25, 10, 29, 35, 26, 13, 28, 29, 39, 31, 12, 39, 14, 27, 14, 23, 9, 29, 38, 21, 24, 31, 19, 33, 12, 32, 46, 17, 20, 21, 17, 23, 25, 43, 27, 24, 25, 17, 19, 20, 27, 14, 21, 14, 24, 29, 22, 29, 28, 20, 27, 25, 31, 26, 22, 25, 22, 29, 29, 26, 19, 23, 27, 22, 20, 17, 31, 17, 18, 22, 22, 21, 28, 22, 28, 45, 19, 34, 23, 28, 26, 19, 28, 29, 22, 24, 19, 19, 25, 14, 25, 19, 31, 29, 36, 22, 28, 14, 29, 20, 32, 31, 18, 34, 14, 27, 25, 13, 32, 39, 17, 35, 16, 22, 22, 26, 60, 30, 29, 27, 29, 30, 53, 20, 18, 26, 22, 33, 15, 19, 21, 19, 31, 33, 19, 58, 21, 12, 24, 31, 16, 28, 40, 30, 18, 62, 40, 33, 21, 11, 34, 21, 32, 22, 20, 26, 31, 61, 25, 17, 21, 34, 28, 23, 28, 26, 43, 31, 69, 17, 14, 18, 14, 33, 38, 27, 27, 27, 42, 20, 33, 20, 17, 44, 27, 33, 27, 27, 30, 22, 24, 25, 31, 20, 14, 14, 44, 32, 36, 40, 25, 40, 25, 26, 25, 20, 47, 26, 17, 16, 18, 20, 12, 19, 8, 20, 26, 29, 9, 25, 15, 21, 25, 40, 25, 25, 21, 26, 22, 26, 15, 12, 25, 32, 19, 27, 28, 54, 21, 22, 14, 36, 27, 29, 22, 23, 22, 32, 34, 18, 30, 33, 17, 35, 27, 23, 30, 31, 26, 32, 25, 34, 32, 29, 23, 30, 21, 34, 27, 25, 26, 38, 25, 18, 23, 24, 48, 25, 33, 23, 43, 26, 32, 19, 22, 37, 27, 28, 31, 16, 22, 30, 19, 29, 32, 26, 24, 39, 22, 17, 35, 38, 45, 19, 24, 23, 18, 21, 22, 23, 42, 12, 21, 25, 15, 28, 18, 24, 21, 34, 32, 18, 43, 41, 22, 46, 14, 27, 11, 22, 15, 28, 34, 22, 22, 31, 28, 30, 17, 80, 15, 26, 23, 16, 62, 18, 23, 37, 23, 23, 12, 27, 26, 21, 19, 18, 29, 19, 61, 25, 27, 43, 37, 20, 29, 19, 26, 21, 24, 22, 39, 37, 10, 41, 12, 31, 20, 17, 16, 21, 29, 20, 27, 12, 16, 21, 21, 27, 16, 38, 28, 20, 25, 35, 15, 14, 38, 26, 30, 33, 27, 9, 21, 26, 26, 18, 28, 31, 12, 36, 41, 23, 14, 24, 32, 13, 23, 57, 32, 32, 16, 39, 18, 29, 24, 35, 29, 9, 34, 20, 19, 36, 30, 34, 40, 35, 17, 18, 7, 31, 40, 21, 35, 13, 29, 18, 35, 19, 33, 32, 28, 22, 18, 22, 18, 32, 18, 32, 20, 27, 16, 21, 14, 20, 20, 18, 20, 24, 38, 17, 16, 26, 12, 20, 34, 20, 18, 27, 39, 59, 12, 28, 27, 24, 27, 15, 17, 17, 36, 15, 26, 18, 19, 24, 14, 21, 20, 30, 21, 36, 31, 32, 36, 29, 27, 25, 21, 47, 32, 26, 33, 26, 63, 57, 31, 39, 13, 40, 21, 34, 33, 62, 23, 38, 23, 23, 18, 34, 63, 43, 34, 17, 34, 70, 28, 53, 29, 26, 29, 17, 42, 18, 17, 62, 22, 24, 27, 16, 45, 41, 36, 42, 45, 28, 20, 32, 70, 67, 17, 46, 47, 40, 24, 22, 30, 37, 37, 29, 14, 29, 47, 53, 20, 23, 41, 44, 19, 61, 42, 28, 22, 34, 34, 51, 29, 11, 39, 33, 36, 82, 61, 32, 44, 69, 23, 27, 26, 39, 37, 58, 27, 23, 28, 21, 35, 39, 29, 60, 28, 30, 31, 21, 37, 19, 21, 27, 11, 44, 60, 19, 35, 57, 35, 78, 27, 62, 31, 19, 32, 39, 25, 29, 65, 53, 47, 22, 35, 62, 66, 93, 65, 35, 46, 27, 35, 35, 41, 27, 40, 24, 32, 33, 26, 28, 32, 20, 25, 25, 17, 41, 57, 35, 27, 42, 25, 16, 19, 53, 38, 28, 34, 40, 22, 27, 65, 30, 22, 27, 12, 26, 44, 24, 20, 12, 21, 39, 37, 33, 28, 30, 33, 45, 19, 68, 25, 28, 47, 52, 21, 28, 39, 36, 25, 22, 39, 52, 61, 16, 45, 38, 3, 45, 40, 25, 52, 20, 19, 19, 26, 19, 27, 23, 29, 26, 29, 27, 35, 20, 23, 26, 46, 34, 35, 20, 25, 35, 54, 36, 31, 42, 33, 23, 28, 27, 19, 33, 25, 41, 21, 16, 41, 19, 24, 37, 39, 26, 31, 26, 22, 20, 18, 39, 26, 30, 20, 31, 26, 22, 24, 41, 30, 28, 33, 22, 40, 42, 30, 22, 28, 20, 30, 36, 42, 52, 24, 32, 38, 26, 34, 24, 40, 19, 26, 28, 27, 27, 16, 21, 31, 20, 58, 58, 23, 24, 38, 39, 34, 29, 34, 26, 25, 17, 30, 31, 25, 21, 28, 30, 13, 42, 26, 40, 23, 40, 38, 55, 25, 19, 39, 20, 32, 32, 66, 35, 61, 28, 53, 46, 39, 37, 28, 90, 35, 45, 38, 22, 21, 20, 62, 40, 46, 41, 24, 28, 32, 38, 56, 29, 30, 21, 47, 33, 67, 27, 20, 13, 47, 12, 25, 22, 45, 38, 25, 36, 25, 26, 33, 48, 34, 19, 25, 21, 32, 35, 33, 36, 67, 38, 35, 27, 47, 28, 26, 49, 29, 38, 32, 28, 26, 21, 30, 30, 22, 25, 35, 45, 25, 32, 34, 57, 23, 66, 27, 25, 46, 31, 35, 29, 26, 33, 39, 28, 23, 38, 14, 55, 32, 30, 29, 24, 79, 46, 11, 30, 32, 44, 30, 39, 37, 25, 76, 30, 84, 31, 24, 39, 28, 37, 28, 59, 21, 34, 23, 42, 45, 18, 30, 25, 34, 34, 20, 24, 46, 31, 28, 46, 30, 37, 35, 32, 37, 19, 18, 52, 18, 31, 34, 32, 18, 22, 37, 26, 50, 40, 28, 20, 30, 19, 15, 16, 22, 30, 38, 19, 35, 17, 22, 20, 29, 59, 59, 43, 27, 28, 25, 26, 62, 28, 26, 30, 34, 35], 'attr_dim': 0, 'dataset_info': '==== Dataset Information ====\n== General Information == \nNumber of graphs: 2500\nNumber of classes: 2\nClass distribution: \n0:2000 1:500 \n\n== Node information== \nAverage number of nodes: 27\nAverage number of edges (undirected): 29\nMax number of nodes: 93\nNumber of distinct node labels: 18\nAverage number of distinct node labels: 3\nNode labels distribution: \n0:9454 1:6143 13:139 16:2 18:1 2:49955 20:15 21:29 25:5 26:3 3:921 4:570 44:1 49:1 5:70 6:485 8:14 9:4 \n'}
*** 1 train_index:  [   1    2    3 ... 2497 2498 2499]
*** 2 test_index:  [   0   20   26   28   33   37   38   43   44   50   52   63   66   67
   70   71   75   85   86   87   97  102  103  106  107  111  114  117
  118  124  132  146  150  157  163  165  169  175  181  182  184  194
  198  200  203  207  208  215  216  217  219  221  229  230  231  239
  249  253  265  271  278  285  287  297  302  309  312  313  317  322
  323  329  333  334  335  336  337  342  348  353  354  364  365  366
  367  368  369  377  378  383  384  390  391  400  406  408  415  416
  423  426  435  437  438  443  448  454  463  479  483  485  486  491
  499  510  511  515  521  535  537  539  541  545  548  552  559  561
  568  575  582  591  592  600  606  619  637  644  645  647  650  673
  675  681  686  690  695  701  712  713  714  721  726  727  732  736
  739  741  743  750  752  754  757  761  764  775  778  788  789  797
  802  806  810  811  812  827  829  836  847  866  867  868  871  872
  882  884  885  890  891  892  894  903  914  915  918  922  928  929
  931  936  947  950  952  953  968  972  980  983  986  988 1000 1010
 1011 1014 1016 1022 1030 1031 1032 1039 1040 1050 1056 1057 1059 1064
 1066 1068 1073 1091 1092 1113 1116 1125 1126 1134 1135 1137 1145 1158
 1162 1167 1169 1178 1180 1194 1199 1209 1211 1212 1215 1219 1223 1225
 1233 1234 1238 1243 1252 1256 1257 1259 1260 1267 1273 1278 1279 1285
 1289 1290 1292 1294 1315 1316 1333 1336 1346 1347 1353 1356 1378 1386
 1387 1391 1396 1403 1406 1407 1410 1420 1427 1428 1429 1437 1441 1446
 1453 1455 1456 1458 1463 1465 1475 1476 1477 1482 1488 1496 1503 1505
 1506 1507 1509 1516 1522 1523 1524 1526 1528 1532 1535 1538 1541 1544
 1549 1553 1571 1574 1583 1595 1598 1611 1624 1629 1637 1642 1647 1653
 1655 1663 1667 1668 1673 1674 1676 1678 1689 1696 1698 1702 1707 1721
 1722 1740 1747 1762 1766 1769 1773 1775 1780 1783 1788 1797 1798 1801
 1806 1816 1831 1833 1835 1844 1845 1846 1847 1859 1865 1867 1868 1875
 1879 1883 1885 1898 1906 1907 1914 1915 1929 1932 1946 1947 1948 1955
 1958 1967 1975 1984 1988 1989 1995 1996 2007 2009 2012 2022 2025 2026
 2030 2048 2049 2065 2071 2073 2077 2080 2084 2090 2094 2100 2101 2109
 2111 2126 2142 2143 2150 2152 2153 2159 2169 2172 2175 2176 2178 2179
 2184 2185 2196 2201 2202 2206 2208 2210 2214 2216 2219 2224 2227 2231
 2233 2240 2244 2250 2251 2254 2266 2267 2274 2277 2278 2281 2284 2287
 2293 2297 2298 2303 2305 2315 2319 2331 2334 2336 2337 2338 2349 2357
 2358 2362 2373 2377 2384 2397 2413 2418 2425 2429 2431 2436 2439 2441
 2452 2453 2458 2459 2468 2473 2476 2481 2484 2491]
*** 1 train_index:  [   0    1    2 ... 2497 2498 2499]
*** 2 test_index:  [   3    4    6   11   12   21   22   23   27   29   34   42   47   48
   54   58   62   64   65   76   77   81   82   91   96   98   99  105
  112  113  120  126  128  131  141  142  149  151  153  160  171  176
  178  183  188  190  197  206  212  213  214  223  224  234  238  242
  244  250  251  254  256  261  270  273  276  277  279  281  283  288
  293  300  308  311  328  331  344  349  356  361  362  374  375  380
  382  385  387  389  399  404  405  411  417  418  427  430  445  449
  452  455  456  460  462  465  469  473  475  481  504  505  513  518
  528  532  538  543  544  547  553  562  567  569  581  583  584  599
  601  609  612  616  627  631  632  638  639  641  648  652  656  671
  679  685  687  688  693  696  709  724  734  744  749  753  762  768
  769  770  771  772  785  787  791  792  799  803  807  815  818  821
  828  833  838  842  855  858  880  886  897  900  906  907  908  912
  913  923  924  930  934  938  939  948  949  951  956  958  960  963
  964  966  974  977  981  982  984  991  992  997 1001 1002 1006 1007
 1008 1019 1028 1044 1047 1049 1058 1061 1067 1071 1076 1081 1086 1087
 1088 1090 1097 1108 1110 1112 1119 1140 1148 1152 1155 1156 1157 1163
 1172 1185 1187 1191 1197 1198 1207 1213 1216 1221 1222 1224 1226 1228
 1229 1235 1239 1242 1245 1246 1261 1265 1268 1269 1272 1298 1299 1300
 1309 1310 1312 1313 1319 1326 1327 1331 1339 1341 1345 1352 1354 1362
 1370 1372 1375 1390 1393 1397 1400 1401 1404 1411 1417 1418 1424 1425
 1432 1435 1436 1439 1442 1454 1461 1479 1490 1498 1500 1510 1518 1519
 1529 1530 1531 1537 1550 1551 1559 1565 1577 1578 1586 1587 1597 1599
 1602 1606 1610 1615 1619 1620 1627 1628 1630 1641 1646 1648 1650 1651
 1658 1661 1670 1672 1682 1684 1690 1691 1708 1716 1717 1725 1727 1728
 1732 1733 1739 1743 1745 1754 1770 1771 1779 1781 1785 1790 1791 1793
 1795 1805 1813 1815 1817 1827 1834 1838 1841 1843 1850 1853 1856 1872
 1873 1882 1886 1894 1899 1904 1913 1916 1918 1935 1941 1950 1951 1952
 1957 1969 1972 1973 1979 1983 1994 1999 2005 2010 2014 2028 2031 2038
 2041 2045 2046 2050 2056 2059 2061 2062 2063 2064 2072 2074 2078 2082
 2083 2085 2091 2093 2098 2106 2114 2115 2116 2119 2125 2136 2138 2139
 2141 2144 2160 2174 2182 2190 2195 2197 2205 2211 2212 2218 2225 2230
 2234 2243 2246 2255 2258 2270 2275 2276 2289 2290 2294 2299 2307 2309
 2310 2314 2316 2318 2322 2324 2335 2344 2363 2364 2369 2372 2379 2380
 2381 2385 2386 2393 2395 2400 2405 2406 2409 2410 2412 2415 2420 2422
 2424 2426 2427 2428 2444 2457 2470 2471 2475 2494]
*** 1 train_index:  [   0    1    2 ... 2494 2495 2498]
*** 2 test_index:  [  15   17   25   32   39   40   53   56   59   61   69   73   79   80
   84   88   92   95  100  104  108  116  119  121  122  123  134  135
  137  155  156  159  167  168  173  179  186  187  191  195  196  210
  220  233  243  252  257  264  267  268  269  280  282  286  290  291
  298  304  310  315  324  338  339  345  347  350  370  386  388  392
  394  396  397  403  412  421  422  431  442  457  459  461  466  468
  474  476  488  496  498  503  508  514  529  536  546  550  555  563
  571  573  574  576  579  589  595  598  602  605  610  613  614  615
  624  626  629  630  634  635  655  657  660  664  669  672  680  691
  694  699  700  702  705  706  725  728  733  735  737  738  747  751
  759  773  774  776  777  781  783  784  793  794  796  798  800  805
  809  816  825  830  831  832  834  839  844  846  848  849  851  852
  853  857  860  863  889  899  902  909  917  920  927  933  937  941
  944  957  962  965  969  973  990  999 1003 1005 1013 1026 1033 1035
 1036 1038 1041 1043 1045 1051 1055 1060 1063 1065 1070 1077 1078 1085
 1094 1098 1100 1102 1105 1111 1114 1118 1120 1123 1128 1132 1136 1139
 1142 1149 1154 1165 1170 1174 1183 1186 1188 1192 1196 1201 1202 1203
 1206 1214 1218 1220 1244 1249 1254 1255 1270 1283 1284 1288 1291 1296
 1306 1308 1320 1324 1335 1343 1349 1350 1351 1355 1357 1364 1368 1373
 1374 1376 1377 1379 1382 1395 1402 1412 1426 1430 1440 1443 1444 1445
 1450 1451 1452 1464 1466 1467 1468 1470 1472 1473 1474 1480 1495 1504
 1512 1539 1552 1555 1557 1558 1560 1562 1566 1567 1568 1570 1573 1575
 1581 1585 1589 1593 1601 1605 1612 1621 1625 1638 1640 1645 1649 1652
 1654 1656 1657 1659 1662 1665 1681 1686 1693 1695 1697 1700 1718 1720
 1724 1729 1731 1734 1737 1742 1746 1748 1750 1752 1755 1757 1760 1761
 1765 1777 1778 1782 1784 1792 1799 1803 1808 1809 1810 1812 1814 1819
 1825 1828 1830 1836 1842 1848 1854 1855 1862 1870 1876 1877 1881 1889
 1897 1901 1908 1911 1912 1919 1922 1924 1927 1936 1940 1942 1943 1944
 1953 1968 1974 1976 1977 1982 1990 1998 2000 2002 2004 2015 2024 2036
 2044 2052 2053 2057 2058 2070 2075 2076 2087 2097 2102 2108 2110 2113
 2117 2118 2120 2128 2129 2154 2155 2156 2158 2163 2164 2165 2167 2168
 2170 2180 2181 2186 2188 2189 2192 2193 2199 2207 2209 2217 2220 2228
 2235 2241 2245 2247 2248 2253 2256 2268 2282 2283 2285 2296 2300 2311
 2317 2321 2346 2348 2351 2354 2359 2365 2366 2367 2371 2374 2375 2382
 2389 2390 2391 2408 2417 2421 2423 2435 2443 2446 2449 2454 2456 2465
 2467 2482 2483 2486 2487 2488 2490 2496 2497 2499]
*** 1 train_index:  [   0    2    3 ... 2497 2498 2499]
*** 2 test_index:  [   1    5   10   14   19   24   30   35   36   45   49   51   55   57
   74   78   83  109  125  129  133  140  143  144  145  148  152  154
  158  161  164  166  170  174  180  189  192  199  202  205  211  222
  225  228  232  241  245  246  248  259  272  274  275  284  289  294
  301  306  318  320  325  326  332  340  341  343  351  352  358  363
  379  393  398  401  407  410  414  420  424  428  434  440  441  446
  450  451  453  464  478  480  482  484  487  492  506  509  516  517
  524  525  527  530  534  540  549  551  554  556  560  564  565  566
  577  580  585  586  590  593  611  617  618  620  622  625  628  633
  640  642  651  653  658  661  662  665  668  674  676  678  684  692
  697  698  707  708  710  711  717  720  729  730  731  742  746  755
  760  766  767  779  780  801  808  814  817  819  820  822  824  835
  843  845  861  862  864  870  876  878  881  893  895  896  901  904
  905  911  919  921  932  935  954  959  967  971  978  979  985  987
  993  995  996  998 1009 1015 1021 1029 1037 1042 1046 1054 1062 1074
 1080 1083 1084 1095 1096 1099 1101 1103 1106 1109 1117 1121 1122 1124
 1130 1141 1143 1146 1147 1159 1160 1161 1164 1166 1168 1179 1182 1184
 1190 1200 1204 1208 1217 1230 1236 1237 1247 1248 1250 1251 1253 1258
 1262 1266 1271 1275 1277 1282 1286 1287 1295 1302 1303 1304 1305 1314
 1321 1329 1342 1344 1358 1359 1371 1381 1383 1384 1398 1399 1405 1408
 1414 1416 1419 1421 1422 1423 1434 1448 1449 1457 1459 1478 1484 1487
 1489 1492 1494 1497 1501 1502 1513 1514 1517 1521 1525 1546 1556 1561
 1564 1576 1580 1584 1588 1594 1600 1608 1609 1613 1617 1622 1623 1626
 1632 1633 1636 1643 1660 1664 1666 1669 1675 1687 1692 1699 1701 1705
 1709 1710 1712 1713 1714 1723 1726 1735 1738 1741 1749 1758 1759 1767
 1768 1774 1786 1789 1794 1796 1800 1804 1811 1820 1822 1823 1837 1839
 1849 1852 1860 1863 1864 1866 1871 1874 1878 1884 1890 1896 1902 1903
 1905 1909 1910 1920 1921 1923 1925 1930 1931 1933 1934 1949 1959 1961
 1963 1964 1966 1971 1986 1991 1992 1997 2003 2013 2017 2021 2032 2033
 2035 2039 2047 2054 2055 2066 2067 2069 2081 2088 2092 2096 2099 2107
 2112 2122 2123 2124 2130 2133 2135 2145 2146 2147 2161 2166 2173 2177
 2187 2191 2194 2198 2200 2203 2215 2223 2232 2237 2238 2239 2249 2252
 2257 2261 2263 2264 2265 2271 2272 2273 2286 2291 2295 2301 2306 2308
 2312 2326 2329 2330 2340 2342 2345 2347 2350 2355 2356 2360 2368 2370
 2378 2383 2394 2399 2401 2404 2414 2419 2430 2432 2433 2445 2447 2448
 2451 2455 2464 2472 2477 2479 2480 2485 2489 2495]
*** 1 train_index:  [   0    1    3 ... 2496 2497 2499]
*** 2 test_index:  [   2    7    8    9   13   16   18   31   41   46   60   68   72   89
   90   93   94  101  110  115  127  130  136  138  139  147  162  172
  177  185  193  201  204  209  218  226  227  235  236  237  240  247
  255  258  260  262  263  266  292  295  296  299  303  305  307  314
  316  319  321  327  330  346  355  357  359  360  371  372  373  376
  381  395  402  409  413  419  425  429  432  433  436  439  444  447
  458  467  470  471  472  477  489  490  493  494  495  497  500  501
  502  507  512  519  520  522  523  526  531  533  542  557  558  570
  572  578  587  588  594  596  597  603  604  607  608  621  623  636
  643  646  649  654  659  663  666  667  670  677  682  683  689  703
  704  715  716  718  719  722  723  740  745  748  756  758  763  765
  782  786  790  795  804  813  823  826  837  840  841  850  854  856
  859  865  869  873  874  875  877  879  883  887  888  898  910  916
  925  926  940  942  943  945  946  955  961  970  975  976  989  994
 1004 1012 1017 1018 1020 1023 1024 1025 1027 1034 1048 1052 1053 1069
 1072 1075 1079 1082 1089 1093 1104 1107 1115 1127 1129 1131 1133 1138
 1144 1150 1151 1153 1171 1173 1175 1176 1177 1181 1189 1193 1195 1205
 1210 1227 1231 1232 1240 1241 1263 1264 1274 1276 1280 1281 1293 1297
 1301 1307 1311 1317 1318 1322 1323 1325 1328 1330 1332 1334 1337 1338
 1340 1348 1360 1361 1363 1365 1366 1367 1369 1380 1385 1388 1389 1392
 1394 1409 1413 1415 1431 1433 1438 1447 1460 1462 1469 1471 1481 1483
 1485 1486 1491 1493 1499 1508 1511 1515 1520 1527 1533 1534 1536 1540
 1542 1543 1545 1547 1548 1554 1563 1569 1572 1579 1582 1590 1591 1592
 1596 1603 1604 1607 1614 1616 1618 1631 1634 1635 1639 1644 1671 1677
 1679 1680 1683 1685 1688 1694 1703 1704 1706 1711 1715 1719 1730 1736
 1744 1751 1753 1756 1763 1764 1772 1776 1787 1802 1807 1818 1821 1824
 1826 1829 1832 1840 1851 1857 1858 1861 1869 1880 1887 1888 1891 1892
 1893 1895 1900 1917 1926 1928 1937 1938 1939 1945 1954 1956 1960 1962
 1965 1970 1978 1980 1981 1985 1987 1993 2001 2006 2008 2011 2016 2018
 2019 2020 2023 2027 2029 2034 2037 2040 2042 2043 2051 2060 2068 2079
 2086 2089 2095 2103 2104 2105 2121 2127 2131 2132 2134 2137 2140 2148
 2149 2151 2157 2162 2171 2183 2204 2213 2221 2222 2226 2229 2236 2242
 2259 2260 2262 2269 2279 2280 2288 2292 2302 2304 2313 2320 2323 2325
 2327 2328 2332 2333 2339 2341 2343 2352 2353 2361 2376 2387 2388 2392
 2396 2398 2402 2403 2407 2411 2416 2434 2437 2438 2440 2442 2450 2460
 2461 2462 2463 2466 2469 2474 2478 2492 2493 2498]


config: {'general': {'data_autobalance': False, 'print_dataset_features': True, 'batch_size': 1, 'extract_features': False}, 'run': {'num_epochs': 50, 'learning_rate': 0.0001, 'seed': 1800, 'k_fold': 5, 'model': 'DGCNN', 'dataset': 'NCI-H23'}, 'GNN_models': {'DGCNN': {'convolution_layers_size': '32-32-32-1', 'sortpooling_k': 0.6, 'n_hidden': 128, 'convolution_dropout': 0.5, 'pred_dropout': 0.5, 'FP_len': 0}, 'GCN': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'GCND': {'convolution_layers_size': '128-256-512', 'dropout': 0.5}, 'DiffPool': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DiffPoolD': {'convolution_layers_size': '64-64-64', 'pred_hidden_layers': '50-50-50', 'assign_ratio': 0.25, 'number_of_pooling': 1, 'concat_tensors': False}, 'DFScodeRNN_cls': {'dummy': 0}}, 'dataset_features': {'name': 'NCI-H23', 'num_class': 2, 'label_dict': {'0': 0, '1': 1}, 'have_node_labels': True, 'have_node_attributions': False, 'node_dict': {'0': 0, '1': 1, '13': 2, '16': 3, '18': 4, '2': 5, '20': 6, '21': 7, '25': 8, '26': 9, '3': 10, '4': 11, '44': 12, '49': 13, '5': 14, '6': 15, '8': 16, '9': 17, 'UNKNOWN': 18}, 'feat_dim': 19, 'edge_feat_dim': 0, 'max_num_nodes': 93, 'avg_num_nodes': 27, 'graph_sizes_list': [34, 30, 34, 19, 20, 20, 13, 45, 22, 20, 26, 20, 15, 25, 56, 34, 19, 56, 24, 22, 25, 23, 24, 27, 25, 29, 27, 24, 22, 38, 22, 26, 23, 9, 7, 25, 35, 22, 18, 15, 24, 24, 27, 20, 18, 31, 25, 28, 46, 13, 29, 31, 21, 29, 20, 25, 48, 42, 28, 23, 34, 17, 28, 28, 15, 41, 35, 30, 20, 27, 25, 19, 31, 17, 29, 30, 75, 34, 22, 24, 33, 14, 22, 13, 24, 16, 17, 22, 13, 31, 26, 19, 15, 34, 26, 28, 24, 29, 18, 21, 12, 23, 23, 25, 22, 18, 16, 12, 17, 20, 27, 22, 19, 48, 23, 25, 33, 24, 17, 19, 24, 24, 10, 23, 14, 30, 22, 18, 26, 32, 31, 27, 30, 15, 29, 20, 19, 20, 34, 14, 15, 31, 18, 18, 24, 41, 44, 20, 38, 30, 28, 16, 29, 23, 31, 15, 11, 35, 29, 34, 19, 40, 18, 29, 27, 32, 17, 20, 20, 21, 15, 21, 31, 20, 57, 23, 13, 49, 32, 34, 31, 9, 20, 12, 16, 21, 28, 27, 17, 37, 32, 27, 16, 40, 21, 7, 30, 24, 16, 31, 18, 30, 20, 13, 20, 22, 23, 31, 20, 26, 32, 43, 14, 20, 14, 16, 20, 62, 19, 8, 31, 23, 35, 37, 20, 42, 26, 17, 46, 23, 19, 15, 23, 24, 23, 29, 33, 21, 20, 12, 58, 25, 15, 14, 26, 37, 15, 28, 48, 24, 21, 23, 22, 22, 51, 14, 17, 24, 23, 11, 18, 26, 35, 10, 19, 18, 14, 28, 23, 28, 18, 38, 42, 29, 24, 21, 28, 18, 44, 27, 27, 19, 32, 32, 20, 41, 24, 34, 32, 25, 18, 16, 28, 10, 22, 29, 21, 29, 17, 21, 24, 12, 41, 20, 24, 21, 26, 29, 16, 31, 30, 41, 35, 29, 27, 25, 31, 32, 29, 53, 13, 42, 27, 17, 23, 19, 40, 21, 21, 23, 18, 48, 33, 25, 29, 21, 18, 24, 22, 34, 27, 25, 15, 24, 23, 19, 13, 24, 19, 30, 27, 31, 41, 24, 60, 36, 40, 13, 16, 23, 28, 15, 28, 13, 13, 35, 21, 23, 24, 14, 25, 52, 25, 19, 16, 24, 26, 16, 13, 33, 12, 24, 26, 21, 11, 20, 23, 15, 21, 18, 47, 24, 23, 24, 24, 24, 22, 18, 27, 19, 18, 32, 24, 23, 30, 19, 21, 24, 21, 20, 50, 32, 26, 37, 17, 22, 31, 22, 19, 31, 14, 14, 36, 17, 22, 36, 17, 22, 21, 34, 24, 27, 16, 15, 16, 16, 29, 15, 21, 20, 32, 42, 35, 17, 27, 17, 28, 25, 28, 13, 20, 29, 20, 23, 16, 10, 16, 24, 24, 15, 26, 14, 27, 30, 8, 21, 27, 45, 34, 26, 17, 35, 31, 26, 31, 24, 22, 18, 29, 18, 21, 16, 21, 28, 18, 51, 41, 32, 19, 26, 24, 16, 20, 14, 41, 29, 43, 17, 29, 27, 32, 22, 20, 16, 15, 30, 26, 25, 25, 20, 41, 17, 14, 24, 21, 15, 24, 29, 24, 39, 22, 33, 15, 33, 28, 17, 19, 22, 9, 30, 25, 28, 17, 12, 38, 35, 15, 31, 24, 64, 18, 26, 22, 34, 11, 26, 19, 15, 36, 33, 38, 19, 20, 68, 18, 18, 19, 26, 12, 24, 30, 17, 26, 22, 22, 20, 59, 9, 32, 30, 16, 33, 33, 18, 40, 22, 30, 36, 20, 39, 40, 16, 19, 19, 30, 23, 33, 43, 22, 35, 26, 34, 30, 36, 50, 22, 17, 19, 24, 30, 26, 14, 26, 22, 22, 22, 34, 32, 24, 18, 43, 15, 35, 26, 14, 20, 24, 33, 25, 19, 16, 12, 25, 30, 15, 22, 20, 52, 16, 31, 21, 14, 18, 17, 18, 38, 37, 15, 27, 16, 19, 32, 19, 36, 21, 24, 23, 22, 55, 25, 15, 21, 25, 28, 19, 16, 29, 15, 30, 27, 55, 23, 15, 30, 21, 21, 35, 24, 27, 20, 14, 31, 31, 27, 20, 19, 33, 18, 12, 24, 44, 19, 23, 26, 22, 16, 12, 28, 32, 26, 14, 25, 18, 22, 20, 22, 30, 34, 16, 30, 13, 14, 37, 17, 54, 29, 22, 23, 29, 20, 15, 76, 27, 42, 20, 35, 23, 20, 21, 30, 39, 27, 18, 27, 28, 31, 30, 24, 11, 31, 35, 45, 20, 17, 31, 28, 19, 17, 19, 19, 29, 28, 18, 25, 21, 12, 37, 21, 19, 32, 22, 29, 19, 24, 22, 22, 21, 26, 29, 31, 25, 10, 19, 35, 18, 17, 22, 29, 30, 17, 37, 17, 29, 18, 20, 37, 31, 28, 9, 18, 19, 18, 27, 26, 27, 16, 15, 20, 10, 20, 21, 32, 41, 14, 19, 24, 36, 33, 29, 25, 38, 27, 23, 7, 25, 20, 34, 38, 24, 22, 19, 27, 23, 20, 15, 23, 21, 16, 23, 41, 17, 26, 24, 14, 18, 28, 27, 34, 29, 58, 28, 27, 29, 58, 17, 35, 23, 24, 17, 27, 28, 29, 17, 19, 17, 30, 17, 41, 16, 45, 18, 22, 24, 21, 30, 20, 17, 32, 16, 25, 19, 36, 23, 39, 23, 32, 24, 25, 36, 26, 20, 70, 16, 24, 17, 23, 17, 24, 25, 31, 19, 25, 32, 35, 16, 25, 18, 22, 20, 22, 36, 24, 20, 22, 34, 38, 22, 15, 13, 43, 24, 17, 48, 38, 18, 36, 27, 25, 23, 14, 20, 25, 38, 21, 80, 15, 20, 34, 22, 22, 20, 38, 26, 31, 23, 22, 52, 40, 22, 15, 32, 25, 24, 21, 36, 40, 38, 24, 47, 19, 25, 23, 10, 28, 35, 24, 37, 13, 37, 19, 46, 28, 18, 18, 10, 21, 40, 40, 14, 25, 39, 32, 20, 32, 18, 22, 22, 23, 14, 37, 19, 20, 30, 34, 20, 30, 15, 27, 22, 23, 45, 34, 25, 23, 25, 25, 23, 16, 25, 22, 18, 32, 46, 39, 20, 20, 37, 16, 26, 30, 27, 38, 27, 30, 22, 21, 22, 20, 26, 19, 13, 28, 15, 37, 26, 18, 28, 62, 23, 33, 16, 22, 13, 21, 27, 60, 58, 27, 30, 35, 18, 27, 27, 24, 23, 20, 23, 24, 28, 15, 62, 30, 24, 22, 46, 25, 20, 27, 24, 36, 22, 23, 12, 21, 33, 27, 11, 23, 23, 12, 20, 31, 17, 20, 23, 8, 20, 22, 21, 27, 15, 32, 17, 35, 30, 42, 22, 17, 28, 21, 17, 44, 12, 15, 29, 24, 14, 16, 18, 63, 33, 30, 30, 37, 21, 20, 59, 13, 15, 18, 30, 28, 31, 22, 19, 42, 32, 35, 24, 26, 26, 23, 23, 27, 21, 19, 23, 17, 24, 32, 21, 16, 32, 13, 19, 20, 41, 17, 27, 26, 7, 18, 22, 21, 15, 23, 42, 30, 32, 19, 17, 54, 20, 22, 24, 31, 31, 14, 18, 19, 28, 15, 20, 10, 26, 21, 16, 19, 14, 29, 27, 22, 14, 23, 34, 37, 34, 27, 22, 20, 17, 24, 31, 57, 20, 45, 36, 23, 40, 22, 16, 24, 26, 23, 3, 19, 17, 32, 17, 38, 32, 20, 23, 25, 45, 19, 39, 21, 35, 29, 35, 24, 39, 26, 21, 15, 29, 38, 16, 43, 26, 31, 24, 27, 30, 25, 21, 28, 16, 24, 22, 16, 25, 14, 28, 32, 35, 23, 18, 18, 20, 12, 23, 22, 16, 22, 18, 30, 31, 24, 29, 26, 42, 7, 14, 44, 15, 23, 21, 22, 25, 35, 21, 15, 14, 21, 29, 23, 29, 23, 19, 28, 37, 30, 40, 30, 8, 20, 20, 28, 20, 15, 14, 33, 22, 26, 46, 30, 28, 26, 15, 26, 9, 20, 22, 26, 17, 15, 15, 31, 21, 21, 27, 22, 19, 29, 31, 22, 17, 12, 23, 26, 17, 24, 20, 31, 35, 21, 16, 17, 48, 40, 10, 27, 12, 19, 20, 19, 51, 30, 19, 15, 29, 22, 38, 34, 23, 63, 20, 22, 15, 27, 36, 18, 51, 17, 2, 26, 28, 30, 22, 20, 50, 15, 27, 21, 26, 21, 22, 18, 33, 20, 21, 39, 40, 18, 16, 16, 21, 17, 30, 17, 26, 26, 31, 44, 27, 31, 44, 19, 29, 34, 24, 16, 20, 18, 26, 29, 39, 35, 58, 20, 10, 27, 41, 21, 19, 19, 13, 28, 47, 20, 28, 14, 20, 17, 15, 28, 25, 24, 15, 27, 21, 27, 17, 47, 37, 5, 33, 31, 17, 45, 34, 35, 18, 22, 29, 25, 22, 21, 29, 24, 13, 26, 28, 23, 28, 28, 13, 22, 11, 29, 51, 24, 14, 25, 17, 41, 20, 16, 24, 25, 10, 31, 26, 25, 19, 18, 11, 14, 27, 18, 11, 17, 26, 23, 32, 28, 27, 27, 18, 15, 16, 31, 33, 33, 18, 22, 21, 22, 18, 31, 65, 31, 22, 25, 18, 22, 32, 27, 15, 22, 32, 41, 26, 79, 21, 21, 25, 10, 29, 35, 26, 13, 28, 29, 39, 31, 12, 39, 14, 27, 14, 23, 9, 29, 38, 21, 24, 31, 19, 33, 12, 32, 46, 17, 20, 21, 17, 23, 25, 43, 27, 24, 25, 17, 19, 20, 27, 14, 21, 14, 24, 29, 22, 29, 28, 20, 27, 25, 31, 26, 22, 25, 22, 29, 29, 26, 19, 23, 27, 22, 20, 17, 31, 17, 18, 22, 22, 21, 28, 22, 28, 45, 19, 34, 23, 28, 26, 19, 28, 29, 22, 24, 19, 19, 25, 14, 25, 19, 31, 29, 36, 22, 28, 14, 29, 20, 32, 31, 18, 34, 14, 27, 25, 13, 32, 39, 17, 35, 16, 22, 22, 26, 60, 30, 29, 27, 29, 30, 53, 20, 18, 26, 22, 33, 15, 19, 21, 19, 31, 33, 19, 58, 21, 12, 24, 31, 16, 28, 40, 30, 18, 62, 40, 33, 21, 11, 34, 21, 32, 22, 20, 26, 31, 61, 25, 17, 21, 34, 28, 23, 28, 26, 43, 31, 69, 17, 14, 18, 14, 33, 38, 27, 27, 27, 42, 20, 33, 20, 17, 44, 27, 33, 27, 27, 30, 22, 24, 25, 31, 20, 14, 14, 44, 32, 36, 40, 25, 40, 25, 26, 25, 20, 47, 26, 17, 16, 18, 20, 12, 19, 8, 20, 26, 29, 9, 25, 15, 21, 25, 40, 25, 25, 21, 26, 22, 26, 15, 12, 25, 32, 19, 27, 28, 54, 21, 22, 14, 36, 27, 29, 22, 23, 22, 32, 34, 18, 30, 33, 17, 35, 27, 23, 30, 31, 26, 32, 25, 34, 32, 29, 23, 30, 21, 34, 27, 25, 26, 38, 25, 18, 23, 24, 48, 25, 33, 23, 43, 26, 32, 19, 22, 37, 27, 28, 31, 16, 22, 30, 19, 29, 32, 26, 24, 39, 22, 17, 35, 38, 45, 19, 24, 23, 18, 21, 22, 23, 42, 12, 21, 25, 15, 28, 18, 24, 21, 34, 32, 18, 43, 41, 22, 46, 14, 27, 11, 22, 15, 28, 34, 22, 22, 31, 28, 30, 17, 80, 15, 26, 23, 16, 62, 18, 23, 37, 23, 23, 12, 27, 26, 21, 19, 18, 29, 19, 61, 25, 27, 43, 37, 20, 29, 19, 26, 21, 24, 22, 39, 37, 10, 41, 12, 31, 20, 17, 16, 21, 29, 20, 27, 12, 16, 21, 21, 27, 16, 38, 28, 20, 25, 35, 15, 14, 38, 26, 30, 33, 27, 9, 21, 26, 26, 18, 28, 31, 12, 36, 41, 23, 14, 24, 32, 13, 23, 57, 32, 32, 16, 39, 18, 29, 24, 35, 29, 9, 34, 20, 19, 36, 30, 34, 40, 35, 17, 18, 7, 31, 40, 21, 35, 13, 29, 18, 35, 19, 33, 32, 28, 22, 18, 22, 18, 32, 18, 32, 20, 27, 16, 21, 14, 20, 20, 18, 20, 24, 38, 17, 16, 26, 12, 20, 34, 20, 18, 27, 39, 59, 12, 28, 27, 24, 27, 15, 17, 17, 36, 15, 26, 18, 19, 24, 14, 21, 20, 30, 21, 36, 31, 32, 36, 29, 27, 25, 21, 47, 32, 26, 33, 26, 63, 57, 31, 39, 13, 40, 21, 34, 33, 62, 23, 38, 23, 23, 18, 34, 63, 43, 34, 17, 34, 70, 28, 53, 29, 26, 29, 17, 42, 18, 17, 62, 22, 24, 27, 16, 45, 41, 36, 42, 45, 28, 20, 32, 70, 67, 17, 46, 47, 40, 24, 22, 30, 37, 37, 29, 14, 29, 47, 53, 20, 23, 41, 44, 19, 61, 42, 28, 22, 34, 34, 51, 29, 11, 39, 33, 36, 82, 61, 32, 44, 69, 23, 27, 26, 39, 37, 58, 27, 23, 28, 21, 35, 39, 29, 60, 28, 30, 31, 21, 37, 19, 21, 27, 11, 44, 60, 19, 35, 57, 35, 78, 27, 62, 31, 19, 32, 39, 25, 29, 65, 53, 47, 22, 35, 62, 66, 93, 65, 35, 46, 27, 35, 35, 41, 27, 40, 24, 32, 33, 26, 28, 32, 20, 25, 25, 17, 41, 57, 35, 27, 42, 25, 16, 19, 53, 38, 28, 34, 40, 22, 27, 65, 30, 22, 27, 12, 26, 44, 24, 20, 12, 21, 39, 37, 33, 28, 30, 33, 45, 19, 68, 25, 28, 47, 52, 21, 28, 39, 36, 25, 22, 39, 52, 61, 16, 45, 38, 3, 45, 40, 25, 52, 20, 19, 19, 26, 19, 27, 23, 29, 26, 29, 27, 35, 20, 23, 26, 46, 34, 35, 20, 25, 35, 54, 36, 31, 42, 33, 23, 28, 27, 19, 33, 25, 41, 21, 16, 41, 19, 24, 37, 39, 26, 31, 26, 22, 20, 18, 39, 26, 30, 20, 31, 26, 22, 24, 41, 30, 28, 33, 22, 40, 42, 30, 22, 28, 20, 30, 36, 42, 52, 24, 32, 38, 26, 34, 24, 40, 19, 26, 28, 27, 27, 16, 21, 31, 20, 58, 58, 23, 24, 38, 39, 34, 29, 34, 26, 25, 17, 30, 31, 25, 21, 28, 30, 13, 42, 26, 40, 23, 40, 38, 55, 25, 19, 39, 20, 32, 32, 66, 35, 61, 28, 53, 46, 39, 37, 28, 90, 35, 45, 38, 22, 21, 20, 62, 40, 46, 41, 24, 28, 32, 38, 56, 29, 30, 21, 47, 33, 67, 27, 20, 13, 47, 12, 25, 22, 45, 38, 25, 36, 25, 26, 33, 48, 34, 19, 25, 21, 32, 35, 33, 36, 67, 38, 35, 27, 47, 28, 26, 49, 29, 38, 32, 28, 26, 21, 30, 30, 22, 25, 35, 45, 25, 32, 34, 57, 23, 66, 27, 25, 46, 31, 35, 29, 26, 33, 39, 28, 23, 38, 14, 55, 32, 30, 29, 24, 79, 46, 11, 30, 32, 44, 30, 39, 37, 25, 76, 30, 84, 31, 24, 39, 28, 37, 28, 59, 21, 34, 23, 42, 45, 18, 30, 25, 34, 34, 20, 24, 46, 31, 28, 46, 30, 37, 35, 32, 37, 19, 18, 52, 18, 31, 34, 32, 18, 22, 37, 26, 50, 40, 28, 20, 30, 19, 15, 16, 22, 30, 38, 19, 35, 17, 22, 20, 29, 59, 59, 43, 27, 28, 25, 26, 62, 28, 26, 30, 34, 35], 'attr_dim': 0, 'dataset_info': '==== Dataset Information ====\n== General Information == \nNumber of graphs: 2500\nNumber of classes: 2\nClass distribution: \n0:2000 1:500 \n\n== Node information== \nAverage number of nodes: 27\nAverage number of edges (undirected): 29\nMax number of nodes: 93\nNumber of distinct node labels: 18\nAverage number of distinct node labels: 3\nNode labels distribution: \n0:9454 1:6143 13:139 16:2 18:1 2:49955 20:15 21:29 25:5 26:3 3:921 4:570 44:1 49:1 5:70 6:485 8:14 9:4 \n'}}


Training a new model: DGCNN
Training model with dataset, testing using fold 0
k used in SortPooling is: 27
[92maverage training of epoch 0: loss 0.13451 acc 0.97400 roc_auc 0.97956 prc_auc 0.96537[0m
[93maverage test of epoch 0: loss 4.14296 acc 0.20000 roc_auc 0.53520 prc_auc 0.20882[0m
[92maverage training of epoch 1: loss 0.18760 acc 0.94050 roc_auc 0.96710 prc_auc 0.90014[0m
[93maverage test of epoch 1: loss 4.03881 acc 0.20000 roc_auc 0.54965 prc_auc 0.20991[0m
[92maverage training of epoch 2: loss 0.17886 acc 0.95200 roc_auc 0.96558 prc_auc 0.89427[0m
[93maverage test of epoch 2: loss 3.71750 acc 0.20000 roc_auc 0.65344 prc_auc 0.27249[0m
[92maverage training of epoch 3: loss 0.15788 acc 0.95250 roc_auc 0.96735 prc_auc 0.91983[0m
[93maverage test of epoch 3: loss 3.81303 acc 0.20000 roc_auc 0.68661 prc_auc 0.35949[0m
[92maverage training of epoch 4: loss 0.16733 acc 0.95100 roc_auc 0.96588 prc_auc 0.90710[0m
[93maverage test of epoch 4: loss 3.66285 acc 0.20000 roc_auc 0.70129 prc_auc 0.39322[0m
[92maverage training of epoch 5: loss 0.17677 acc 0.95000 roc_auc 0.96297 prc_auc 0.91125[0m
[93maverage test of epoch 5: loss 3.68684 acc 0.20000 roc_auc 0.70126 prc_auc 0.39487[0m
[92maverage training of epoch 6: loss 0.16266 acc 0.94950 roc_auc 0.96594 prc_auc 0.91722[0m
[93maverage test of epoch 6: loss 3.57945 acc 0.20000 roc_auc 0.69586 prc_auc 0.39539[0m
[92maverage training of epoch 7: loss 0.16309 acc 0.95050 roc_auc 0.96675 prc_auc 0.90945[0m
[93maverage test of epoch 7: loss 3.55281 acc 0.20000 roc_auc 0.69556 prc_auc 0.39819[0m
[92maverage training of epoch 8: loss 0.15819 acc 0.95550 roc_auc 0.96770 prc_auc 0.92064[0m
[93maverage test of epoch 8: loss 3.46007 acc 0.20000 roc_auc 0.69155 prc_auc 0.40563[0m
[92maverage training of epoch 9: loss 0.15645 acc 0.95150 roc_auc 0.96895 prc_auc 0.91848[0m
[93maverage test of epoch 9: loss 3.48719 acc 0.20000 roc_auc 0.70070 prc_auc 0.40798[0m
[92maverage training of epoch 10: loss 0.15701 acc 0.95250 roc_auc 0.96963 prc_auc 0.92016[0m
[93maverage test of epoch 10: loss 3.99743 acc 0.20000 roc_auc 0.70590 prc_auc 0.40556[0m
[92maverage training of epoch 11: loss 0.15348 acc 0.95650 roc_auc 0.97009 prc_auc 0.91328[0m
[93maverage test of epoch 11: loss 3.57863 acc 0.20000 roc_auc 0.70260 prc_auc 0.39384[0m
[92maverage training of epoch 12: loss 0.15274 acc 0.95750 roc_auc 0.96839 prc_auc 0.92424[0m
[93maverage test of epoch 12: loss 3.55191 acc 0.20000 roc_auc 0.70667 prc_auc 0.40248[0m
[92maverage training of epoch 13: loss 0.14803 acc 0.95550 roc_auc 0.97278 prc_auc 0.92328[0m
[93maverage test of epoch 13: loss 3.48171 acc 0.20000 roc_auc 0.70329 prc_auc 0.39529[0m
[92maverage training of epoch 14: loss 0.16105 acc 0.95450 roc_auc 0.96946 prc_auc 0.92111[0m
[93maverage test of epoch 14: loss 3.39368 acc 0.20000 roc_auc 0.70738 prc_auc 0.39392[0m
[92maverage training of epoch 15: loss 0.15999 acc 0.95950 roc_auc 0.97029 prc_auc 0.91600[0m
[93maverage test of epoch 15: loss 3.31778 acc 0.20000 roc_auc 0.70850 prc_auc 0.39846[0m
[92maverage training of epoch 16: loss 0.14679 acc 0.95750 roc_auc 0.97354 prc_auc 0.91656[0m
[93maverage test of epoch 16: loss 3.21142 acc 0.20000 roc_auc 0.70980 prc_auc 0.40048[0m
[92maverage training of epoch 17: loss 0.15019 acc 0.95200 roc_auc 0.97234 prc_auc 0.92792[0m
[93maverage test of epoch 17: loss 3.35201 acc 0.20000 roc_auc 0.70612 prc_auc 0.39821[0m
[92maverage training of epoch 18: loss 0.15623 acc 0.95200 roc_auc 0.97080 prc_auc 0.92053[0m
[93maverage test of epoch 18: loss 3.35707 acc 0.20000 roc_auc 0.70625 prc_auc 0.40164[0m
[92maverage training of epoch 19: loss 0.16731 acc 0.95350 roc_auc 0.96562 prc_auc 0.90825[0m
[93maverage test of epoch 19: loss 3.33527 acc 0.20000 roc_auc 0.71032 prc_auc 0.39742[0m
[92maverage training of epoch 20: loss 0.15005 acc 0.95700 roc_auc 0.97114 prc_auc 0.92523[0m
[93maverage test of epoch 20: loss 3.37464 acc 0.20000 roc_auc 0.71556 prc_auc 0.41056[0m
[92maverage training of epoch 21: loss 0.16006 acc 0.95400 roc_auc 0.96841 prc_auc 0.90623[0m
[93maverage test of epoch 21: loss 3.29309 acc 0.20000 roc_auc 0.71715 prc_auc 0.41113[0m
[92maverage training of epoch 22: loss 0.16642 acc 0.95400 roc_auc 0.96516 prc_auc 0.91447[0m
[93maverage test of epoch 22: loss 3.28634 acc 0.20000 roc_auc 0.72439 prc_auc 0.41888[0m
[92maverage training of epoch 23: loss 0.14956 acc 0.96050 roc_auc 0.97304 prc_auc 0.91729[0m
[93maverage test of epoch 23: loss 3.35029 acc 0.20000 roc_auc 0.72190 prc_auc 0.42092[0m
[92maverage training of epoch 24: loss 0.15170 acc 0.95350 roc_auc 0.97080 prc_auc 0.92029[0m
[93maverage test of epoch 24: loss 3.34760 acc 0.20000 roc_auc 0.72633 prc_auc 0.41663[0m
[92maverage training of epoch 25: loss 0.16044 acc 0.95500 roc_auc 0.96922 prc_auc 0.91759[0m
[93maverage test of epoch 25: loss 3.22719 acc 0.20000 roc_auc 0.72532 prc_auc 0.40814[0m
[92maverage training of epoch 26: loss 0.15703 acc 0.95450 roc_auc 0.97048 prc_auc 0.91441[0m
[93maverage test of epoch 26: loss 3.09293 acc 0.20000 roc_auc 0.72240 prc_auc 0.41128[0m
[92maverage training of epoch 27: loss 0.16285 acc 0.95250 roc_auc 0.96885 prc_auc 0.91078[0m
[93maverage test of epoch 27: loss 3.18617 acc 0.20000 roc_auc 0.72315 prc_auc 0.41124[0m
[92maverage training of epoch 28: loss 0.15864 acc 0.94900 roc_auc 0.96903 prc_auc 0.91526[0m
[93maverage test of epoch 28: loss 3.37711 acc 0.20000 roc_auc 0.73115 prc_auc 0.42020[0m
[92maverage training of epoch 29: loss 0.15623 acc 0.95300 roc_auc 0.96986 prc_auc 0.91913[0m
[93maverage test of epoch 29: loss 3.24025 acc 0.20000 roc_auc 0.72952 prc_auc 0.39679[0m
[92maverage training of epoch 30: loss 0.16236 acc 0.95400 roc_auc 0.97017 prc_auc 0.91503[0m
[93maverage test of epoch 30: loss 3.20650 acc 0.20000 roc_auc 0.72872 prc_auc 0.39492[0m
[92maverage training of epoch 31: loss 0.15099 acc 0.95400 roc_auc 0.97257 prc_auc 0.92232[0m
[93maverage test of epoch 31: loss 3.29377 acc 0.20000 roc_auc 0.72215 prc_auc 0.39858[0m
[92maverage training of epoch 32: loss 0.15139 acc 0.95700 roc_auc 0.97200 prc_auc 0.91949[0m
[93maverage test of epoch 32: loss 3.37481 acc 0.20000 roc_auc 0.71978 prc_auc 0.40213[0m
[92maverage training of epoch 33: loss 0.15349 acc 0.95400 roc_auc 0.97135 prc_auc 0.91770[0m
[93maverage test of epoch 33: loss 3.37338 acc 0.20000 roc_auc 0.72117 prc_auc 0.41976[0m
[92maverage training of epoch 34: loss 0.15360 acc 0.95550 roc_auc 0.97058 prc_auc 0.91961[0m
[93maverage test of epoch 34: loss 3.33836 acc 0.20000 roc_auc 0.71327 prc_auc 0.38700[0m
[92maverage training of epoch 35: loss 0.15401 acc 0.95300 roc_auc 0.97058 prc_auc 0.91930[0m
[93maverage test of epoch 35: loss 3.31877 acc 0.20000 roc_auc 0.70597 prc_auc 0.38044[0m
[92maverage training of epoch 36: loss 0.15761 acc 0.95700 roc_auc 0.97049 prc_auc 0.92643[0m
[93maverage test of epoch 36: loss 3.43831 acc 0.20000 roc_auc 0.70420 prc_auc 0.38551[0m
[92maverage training of epoch 37: loss 0.15633 acc 0.95350 roc_auc 0.97065 prc_auc 0.90514[0m
[93maverage test of epoch 37: loss 3.24776 acc 0.20000 roc_auc 0.70805 prc_auc 0.38135[0m
[92maverage training of epoch 38: loss 0.15328 acc 0.95200 roc_auc 0.96994 prc_auc 0.92535[0m
[93maverage test of epoch 38: loss 3.40541 acc 0.20000 roc_auc 0.70780 prc_auc 0.37913[0m
[92maverage training of epoch 39: loss 0.16193 acc 0.95700 roc_auc 0.97006 prc_auc 0.91447[0m
[93maverage test of epoch 39: loss 3.35290 acc 0.20000 roc_auc 0.69943 prc_auc 0.38192[0m
[92maverage training of epoch 40: loss 0.16119 acc 0.95500 roc_auc 0.96785 prc_auc 0.91116[0m
[93maverage test of epoch 40: loss 3.14580 acc 0.20000 roc_auc 0.69703 prc_auc 0.37130[0m
[92maverage training of epoch 41: loss 0.16602 acc 0.95000 roc_auc 0.96528 prc_auc 0.90895[0m
[93maverage test of epoch 41: loss 3.18382 acc 0.20000 roc_auc 0.69190 prc_auc 0.36827[0m
[92maverage training of epoch 42: loss 0.16240 acc 0.95350 roc_auc 0.96698 prc_auc 0.91063[0m
[93maverage test of epoch 42: loss 3.11772 acc 0.20000 roc_auc 0.68525 prc_auc 0.35994[0m
[92maverage training of epoch 43: loss 0.15698 acc 0.95250 roc_auc 0.96932 prc_auc 0.91317[0m
[93maverage test of epoch 43: loss 3.11157 acc 0.20000 roc_auc 0.69715 prc_auc 0.37812[0m
[92maverage training of epoch 44: loss 0.15956 acc 0.94950 roc_auc 0.96912 prc_auc 0.92070[0m
[93maverage test of epoch 44: loss 3.38803 acc 0.20000 roc_auc 0.70575 prc_auc 0.39103[0m
[92maverage training of epoch 45: loss 0.15086 acc 0.95650 roc_auc 0.97088 prc_auc 0.91946[0m
[93maverage test of epoch 45: loss 3.44550 acc 0.20000 roc_auc 0.71527 prc_auc 0.40453[0m
[92maverage training of epoch 46: loss 0.15279 acc 0.95700 roc_auc 0.96989 prc_auc 0.92039[0m
[93maverage test of epoch 46: loss 3.38025 acc 0.20000 roc_auc 0.71700 prc_auc 0.39064[0m
[92maverage training of epoch 47: loss 0.14880 acc 0.95650 roc_auc 0.97189 prc_auc 0.92186[0m
[93maverage test of epoch 47: loss 3.19311 acc 0.20000 roc_auc 0.71237 prc_auc 0.39799[0m
[92maverage training of epoch 48: loss 0.14306 acc 0.95700 roc_auc 0.97340 prc_auc 0.93277[0m
[93maverage test of epoch 48: loss 3.78880 acc 0.20000 roc_auc 0.72033 prc_auc 0.40283[0m
[92maverage training of epoch 49: loss 0.14298 acc 0.96150 roc_auc 0.97487 prc_auc 0.92498[0m
[93maverage test of epoch 49: loss 3.27797 acc 0.20000 roc_auc 0.71760 prc_auc 0.39896[0m
Training model with dataset, testing using fold 1
k used in SortPooling is: 27
[92maverage training of epoch 0: loss 0.13660 acc 0.97150 roc_auc 0.97730 prc_auc 0.96244[0m
[93maverage test of epoch 0: loss 4.25685 acc 0.20000 roc_auc 0.66669 prc_auc 0.35512[0m
[92maverage training of epoch 1: loss 0.20824 acc 0.94050 roc_auc 0.96251 prc_auc 0.87794[0m
[93maverage test of epoch 1: loss 3.69595 acc 0.20000 roc_auc 0.65780 prc_auc 0.35276[0m
[92maverage training of epoch 2: loss 0.17633 acc 0.94750 roc_auc 0.96355 prc_auc 0.90987[0m
[93maverage test of epoch 2: loss 3.57675 acc 0.20000 roc_auc 0.64640 prc_auc 0.36916[0m
[92maverage training of epoch 3: loss 0.17428 acc 0.94850 roc_auc 0.96472 prc_auc 0.90117[0m
[93maverage test of epoch 3: loss 3.40515 acc 0.20000 roc_auc 0.64040 prc_auc 0.31635[0m
[92maverage training of epoch 4: loss 0.16440 acc 0.94900 roc_auc 0.96772 prc_auc 0.91295[0m
[93maverage test of epoch 4: loss 3.60701 acc 0.20000 roc_auc 0.64478 prc_auc 0.31829[0m
[92maverage training of epoch 5: loss 0.16391 acc 0.95000 roc_auc 0.96889 prc_auc 0.90878[0m
[93maverage test of epoch 5: loss 3.67080 acc 0.20000 roc_auc 0.65330 prc_auc 0.33130[0m
[92maverage training of epoch 6: loss 0.17158 acc 0.95200 roc_auc 0.96716 prc_auc 0.91618[0m
[93maverage test of epoch 6: loss 4.03959 acc 0.20000 roc_auc 0.63509 prc_auc 0.26791[0m
[92maverage training of epoch 7: loss 0.15746 acc 0.95800 roc_auc 0.97183 prc_auc 0.91857[0m
[93maverage test of epoch 7: loss 4.26722 acc 0.20000 roc_auc 0.61021 prc_auc 0.25297[0m
[92maverage training of epoch 8: loss 0.16301 acc 0.95850 roc_auc 0.96778 prc_auc 0.90847[0m
[93maverage test of epoch 8: loss 3.46647 acc 0.20000 roc_auc 0.57885 prc_auc 0.22611[0m
[92maverage training of epoch 9: loss 0.16555 acc 0.96050 roc_auc 0.97258 prc_auc 0.92696[0m
[93maverage test of epoch 9: loss 3.69551 acc 0.20000 roc_auc 0.58680 prc_auc 0.23247[0m
[92maverage training of epoch 10: loss 0.15407 acc 0.95750 roc_auc 0.97111 prc_auc 0.90767[0m
[93maverage test of epoch 10: loss 3.46792 acc 0.20000 roc_auc 0.56948 prc_auc 0.22460[0m
[92maverage training of epoch 11: loss 0.15090 acc 0.95650 roc_auc 0.97335 prc_auc 0.92148[0m
[93maverage test of epoch 11: loss 3.50739 acc 0.20000 roc_auc 0.56470 prc_auc 0.22180[0m
[92maverage training of epoch 12: loss 0.14601 acc 0.95950 roc_auc 0.97564 prc_auc 0.92299[0m
[93maverage test of epoch 12: loss 3.45744 acc 0.20000 roc_auc 0.56120 prc_auc 0.21975[0m
[92maverage training of epoch 13: loss 0.14853 acc 0.95600 roc_auc 0.97541 prc_auc 0.92079[0m
[93maverage test of epoch 13: loss 3.46563 acc 0.20000 roc_auc 0.55472 prc_auc 0.21933[0m
[92maverage training of epoch 14: loss 0.15269 acc 0.95950 roc_auc 0.97235 prc_auc 0.91736[0m
[93maverage test of epoch 14: loss 3.38776 acc 0.20000 roc_auc 0.54247 prc_auc 0.21440[0m
[92maverage training of epoch 15: loss 0.14860 acc 0.95800 roc_auc 0.97314 prc_auc 0.92267[0m
[93maverage test of epoch 15: loss 3.43329 acc 0.20000 roc_auc 0.54040 prc_auc 0.21468[0m
[92maverage training of epoch 16: loss 0.15577 acc 0.95900 roc_auc 0.96912 prc_auc 0.92080[0m
[93maverage test of epoch 16: loss 3.40174 acc 0.20000 roc_auc 0.52272 prc_auc 0.20417[0m
[92maverage training of epoch 17: loss 0.15312 acc 0.95600 roc_auc 0.97293 prc_auc 0.90863[0m
[93maverage test of epoch 17: loss 3.34166 acc 0.20000 roc_auc 0.50462 prc_auc 0.19886[0m
[92maverage training of epoch 18: loss 0.15432 acc 0.95600 roc_auc 0.97058 prc_auc 0.91912[0m
[93maverage test of epoch 18: loss 3.44218 acc 0.20000 roc_auc 0.51794 prc_auc 0.20384[0m
[92maverage training of epoch 19: loss 0.15667 acc 0.95600 roc_auc 0.97007 prc_auc 0.90442[0m
[93maverage test of epoch 19: loss 3.26171 acc 0.20000 roc_auc 0.49105 prc_auc 0.19442[0m
[92maverage training of epoch 20: loss 0.15645 acc 0.95550 roc_auc 0.97000 prc_auc 0.90924[0m
[93maverage test of epoch 20: loss 3.27616 acc 0.20000 roc_auc 0.49635 prc_auc 0.19621[0m
[92maverage training of epoch 21: loss 0.16271 acc 0.95700 roc_auc 0.96843 prc_auc 0.90244[0m
[93maverage test of epoch 21: loss 3.23242 acc 0.20000 roc_auc 0.45504 prc_auc 0.18195[0m
[92maverage training of epoch 22: loss 0.17432 acc 0.95450 roc_auc 0.96023 prc_auc 0.89887[0m
[93maverage test of epoch 22: loss 3.34192 acc 0.20000 roc_auc 0.41780 prc_auc 0.17311[0m
[92maverage training of epoch 23: loss 0.17342 acc 0.95150 roc_auc 0.96272 prc_auc 0.88770[0m
[93maverage test of epoch 23: loss 3.18812 acc 0.20000 roc_auc 0.41547 prc_auc 0.17358[0m
[92maverage training of epoch 24: loss 0.16180 acc 0.95600 roc_auc 0.96665 prc_auc 0.91070[0m
[93maverage test of epoch 24: loss 3.24595 acc 0.20000 roc_auc 0.44482 prc_auc 0.18134[0m
[92maverage training of epoch 25: loss 0.16104 acc 0.95700 roc_auc 0.96563 prc_auc 0.91004[0m
[93maverage test of epoch 25: loss 3.21953 acc 0.20000 roc_auc 0.45350 prc_auc 0.18156[0m
[92maverage training of epoch 26: loss 0.15291 acc 0.95950 roc_auc 0.97365 prc_auc 0.92718[0m
[93maverage test of epoch 26: loss 3.12203 acc 0.20000 roc_auc 0.40865 prc_auc 0.16855[0m
[92maverage training of epoch 27: loss 0.14526 acc 0.95900 roc_auc 0.97546 prc_auc 0.93221[0m
[93maverage test of epoch 27: loss 3.16095 acc 0.20000 roc_auc 0.42635 prc_auc 0.17380[0m
[92maverage training of epoch 28: loss 0.14768 acc 0.95700 roc_auc 0.97454 prc_auc 0.92275[0m
[93maverage test of epoch 28: loss 3.12391 acc 0.20000 roc_auc 0.42795 prc_auc 0.17597[0m
[92maverage training of epoch 29: loss 0.15097 acc 0.95750 roc_auc 0.97413 prc_auc 0.92668[0m
[93maverage test of epoch 29: loss 3.22011 acc 0.20000 roc_auc 0.44098 prc_auc 0.17911[0m
[92maverage training of epoch 30: loss 0.15427 acc 0.95900 roc_auc 0.97247 prc_auc 0.92663[0m
[93maverage test of epoch 30: loss 3.27394 acc 0.20000 roc_auc 0.44333 prc_auc 0.18119[0m
[92maverage training of epoch 31: loss 0.14347 acc 0.95800 roc_auc 0.97500 prc_auc 0.93220[0m
[93maverage test of epoch 31: loss 3.24604 acc 0.20000 roc_auc 0.44343 prc_auc 0.18012[0m
[92maverage training of epoch 32: loss 0.15349 acc 0.95750 roc_auc 0.97253 prc_auc 0.90990[0m
[93maverage test of epoch 32: loss 3.18060 acc 0.20000 roc_auc 0.42900 prc_auc 0.17485[0m
[92maverage training of epoch 33: loss 0.15581 acc 0.95700 roc_auc 0.97168 prc_auc 0.92136[0m
[93maverage test of epoch 33: loss 3.53707 acc 0.20000 roc_auc 0.43498 prc_auc 0.17694[0m
[92maverage training of epoch 34: loss 0.15213 acc 0.95800 roc_auc 0.97084 prc_auc 0.90920[0m
[93maverage test of epoch 34: loss 3.31139 acc 0.20000 roc_auc 0.44216 prc_auc 0.18055[0m
[92maverage training of epoch 35: loss 0.14803 acc 0.95900 roc_auc 0.97168 prc_auc 0.92610[0m
[93maverage test of epoch 35: loss 3.35247 acc 0.20000 roc_auc 0.44000 prc_auc 0.18126[0m
[92maverage training of epoch 36: loss 0.14410 acc 0.96200 roc_auc 0.97433 prc_auc 0.92250[0m
[93maverage test of epoch 36: loss 3.38980 acc 0.20000 roc_auc 0.42605 prc_auc 0.17972[0m
[92maverage training of epoch 37: loss 0.14860 acc 0.95600 roc_auc 0.97385 prc_auc 0.91511[0m
[93maverage test of epoch 37: loss 3.27077 acc 0.20000 roc_auc 0.43192 prc_auc 0.17573[0m
[92maverage training of epoch 38: loss 0.14556 acc 0.95800 roc_auc 0.97522 prc_auc 0.91841[0m
[93maverage test of epoch 38: loss 3.35071 acc 0.20000 roc_auc 0.43664 prc_auc 0.17944[0m
[92maverage training of epoch 39: loss 0.15080 acc 0.95650 roc_auc 0.97316 prc_auc 0.91570[0m
[93maverage test of epoch 39: loss 3.23674 acc 0.20000 roc_auc 0.43882 prc_auc 0.18086[0m
[92maverage training of epoch 40: loss 0.14666 acc 0.95700 roc_auc 0.97484 prc_auc 0.92666[0m
[93maverage test of epoch 40: loss 3.22864 acc 0.20000 roc_auc 0.43685 prc_auc 0.18475[0m
[92maverage training of epoch 41: loss 0.14570 acc 0.96150 roc_auc 0.97557 prc_auc 0.91509[0m
[93maverage test of epoch 41: loss 3.22661 acc 0.20000 roc_auc 0.43100 prc_auc 0.17888[0m
[92maverage training of epoch 42: loss 0.14687 acc 0.96000 roc_auc 0.97403 prc_auc 0.90769[0m
[93maverage test of epoch 42: loss 3.17813 acc 0.20000 roc_auc 0.44090 prc_auc 0.18270[0m
[92maverage training of epoch 43: loss 0.16700 acc 0.95550 roc_auc 0.96750 prc_auc 0.89768[0m
[93maverage test of epoch 43: loss 3.10123 acc 0.20000 roc_auc 0.46210 prc_auc 0.18350[0m
[92maverage training of epoch 44: loss 0.15640 acc 0.95450 roc_auc 0.97255 prc_auc 0.91076[0m
[93maverage test of epoch 44: loss 3.13621 acc 0.20000 roc_auc 0.48680 prc_auc 0.20048[0m
[92maverage training of epoch 45: loss 0.15919 acc 0.95250 roc_auc 0.97051 prc_auc 0.90853[0m
[93maverage test of epoch 45: loss 3.06632 acc 0.20000 roc_auc 0.47147 prc_auc 0.19099[0m
[92maverage training of epoch 46: loss 0.14844 acc 0.95800 roc_auc 0.97433 prc_auc 0.92696[0m
[93maverage test of epoch 46: loss 3.13735 acc 0.20000 roc_auc 0.49536 prc_auc 0.20483[0m
[92maverage training of epoch 47: loss 0.14999 acc 0.95750 roc_auc 0.97300 prc_auc 0.92258[0m
[93maverage test of epoch 47: loss 3.26242 acc 0.20000 roc_auc 0.51529 prc_auc 0.22521[0m
[92maverage training of epoch 48: loss 0.15252 acc 0.95750 roc_auc 0.97146 prc_auc 0.90344[0m
[93maverage test of epoch 48: loss 3.21116 acc 0.20000 roc_auc 0.52282 prc_auc 0.23100[0m
[92maverage training of epoch 49: loss 0.15301 acc 0.95400 roc_auc 0.97186 prc_auc 0.92486[0m
[93maverage test of epoch 49: loss 3.25900 acc 0.20000 roc_auc 0.52265 prc_auc 0.23503[0m
Training model with dataset, testing using fold 2
k used in SortPooling is: 27
[92maverage training of epoch 0: loss 0.15413 acc 0.95850 roc_auc 0.97426 prc_auc 0.95289[0m
[93maverage test of epoch 0: loss 4.39087 acc 0.20000 roc_auc 0.66407 prc_auc 0.27282[0m
[92maverage training of epoch 1: loss 0.23846 acc 0.91750 roc_auc 0.95269 prc_auc 0.85367[0m
[93maverage test of epoch 1: loss 4.30115 acc 0.20000 roc_auc 0.65076 prc_auc 0.25753[0m
[92maverage training of epoch 2: loss 0.17797 acc 0.94450 roc_auc 0.96643 prc_auc 0.89646[0m
[93maverage test of epoch 2: loss 3.67075 acc 0.20000 roc_auc 0.74240 prc_auc 0.49840[0m
[92maverage training of epoch 3: loss 0.15670 acc 0.94900 roc_auc 0.97101 prc_auc 0.92691[0m
[93maverage test of epoch 3: loss 3.81183 acc 0.20000 roc_auc 0.74121 prc_auc 0.50946[0m
[92maverage training of epoch 4: loss 0.16283 acc 0.95150 roc_auc 0.96794 prc_auc 0.91472[0m
[93maverage test of epoch 4: loss 3.70942 acc 0.20000 roc_auc 0.74391 prc_auc 0.49328[0m
[92maverage training of epoch 5: loss 0.16147 acc 0.95350 roc_auc 0.96841 prc_auc 0.92065[0m
[93maverage test of epoch 5: loss 3.76916 acc 0.20000 roc_auc 0.74822 prc_auc 0.50445[0m
[92maverage training of epoch 6: loss 0.15008 acc 0.95650 roc_auc 0.97072 prc_auc 0.92728[0m
[93maverage test of epoch 6: loss 3.52690 acc 0.20000 roc_auc 0.74482 prc_auc 0.46662[0m
[92maverage training of epoch 7: loss 0.14998 acc 0.95600 roc_auc 0.97006 prc_auc 0.92869[0m
[93maverage test of epoch 7: loss 3.69192 acc 0.20000 roc_auc 0.74528 prc_auc 0.46133[0m
[92maverage training of epoch 8: loss 0.15397 acc 0.96050 roc_auc 0.96880 prc_auc 0.92005[0m
[93maverage test of epoch 8: loss 3.54178 acc 0.20000 roc_auc 0.74695 prc_auc 0.45268[0m
[92maverage training of epoch 9: loss 0.15009 acc 0.95600 roc_auc 0.97001 prc_auc 0.92267[0m
[93maverage test of epoch 9: loss 3.58002 acc 0.20000 roc_auc 0.74445 prc_auc 0.43484[0m
[92maverage training of epoch 10: loss 0.15243 acc 0.95800 roc_auc 0.96950 prc_auc 0.91773[0m
[93maverage test of epoch 10: loss 3.47322 acc 0.20000 roc_auc 0.74257 prc_auc 0.43896[0m
[92maverage training of epoch 11: loss 0.15776 acc 0.95450 roc_auc 0.97123 prc_auc 0.93274[0m
[93maverage test of epoch 11: loss 3.75466 acc 0.20000 roc_auc 0.74050 prc_auc 0.42213[0m
[92maverage training of epoch 12: loss 0.15148 acc 0.95850 roc_auc 0.97199 prc_auc 0.92161[0m
[93maverage test of epoch 12: loss 3.37541 acc 0.20000 roc_auc 0.74243 prc_auc 0.42914[0m
[92maverage training of epoch 13: loss 0.14426 acc 0.95800 roc_auc 0.97393 prc_auc 0.93063[0m
[93maverage test of epoch 13: loss 3.27878 acc 0.20000 roc_auc 0.74767 prc_auc 0.43214[0m
[92maverage training of epoch 14: loss 0.15157 acc 0.95800 roc_auc 0.97155 prc_auc 0.91624[0m
[93maverage test of epoch 14: loss 3.31746 acc 0.20000 roc_auc 0.74215 prc_auc 0.44484[0m
[92maverage training of epoch 15: loss 0.16282 acc 0.95900 roc_auc 0.96956 prc_auc 0.93718[0m
[93maverage test of epoch 15: loss 4.07212 acc 0.20000 roc_auc 0.74552 prc_auc 0.45223[0m
[92maverage training of epoch 16: loss 0.16725 acc 0.96500 roc_auc 0.97012 prc_auc 0.92747[0m
[93maverage test of epoch 16: loss 3.16686 acc 0.20000 roc_auc 0.73755 prc_auc 0.42382[0m
[92maverage training of epoch 17: loss 0.13864 acc 0.95900 roc_auc 0.97442 prc_auc 0.93552[0m
[93maverage test of epoch 17: loss 3.26345 acc 0.20000 roc_auc 0.73802 prc_auc 0.43010[0m
[92maverage training of epoch 18: loss 0.15010 acc 0.95300 roc_auc 0.97350 prc_auc 0.92452[0m
[93maverage test of epoch 18: loss 3.24087 acc 0.20000 roc_auc 0.74170 prc_auc 0.44167[0m
[92maverage training of epoch 19: loss 0.15482 acc 0.95450 roc_auc 0.97210 prc_auc 0.91963[0m
[93maverage test of epoch 19: loss 3.29036 acc 0.20000 roc_auc 0.73718 prc_auc 0.43602[0m
[92maverage training of epoch 20: loss 0.15814 acc 0.95350 roc_auc 0.97125 prc_auc 0.91035[0m
[93maverage test of epoch 20: loss 3.20533 acc 0.20000 roc_auc 0.74577 prc_auc 0.44246[0m
[92maverage training of epoch 21: loss 0.15563 acc 0.95200 roc_auc 0.97025 prc_auc 0.92406[0m
[93maverage test of epoch 21: loss 3.37427 acc 0.20000 roc_auc 0.74510 prc_auc 0.44842[0m
[92maverage training of epoch 22: loss 0.16120 acc 0.95600 roc_auc 0.96846 prc_auc 0.91894[0m
[93maverage test of epoch 22: loss 3.35506 acc 0.20000 roc_auc 0.75210 prc_auc 0.46815[0m
[92maverage training of epoch 23: loss 0.15691 acc 0.95150 roc_auc 0.97077 prc_auc 0.91806[0m
[93maverage test of epoch 23: loss 3.35842 acc 0.20000 roc_auc 0.75230 prc_auc 0.48114[0m
[92maverage training of epoch 24: loss 0.15339 acc 0.95600 roc_auc 0.97068 prc_auc 0.92856[0m
[93maverage test of epoch 24: loss 3.39366 acc 0.20000 roc_auc 0.74925 prc_auc 0.47678[0m
[92maverage training of epoch 25: loss 0.15726 acc 0.95200 roc_auc 0.97006 prc_auc 0.92358[0m
[93maverage test of epoch 25: loss 3.39138 acc 0.20000 roc_auc 0.74825 prc_auc 0.48266[0m
[92maverage training of epoch 26: loss 0.15529 acc 0.95550 roc_auc 0.96868 prc_auc 0.92868[0m
[93maverage test of epoch 26: loss 3.17309 acc 0.20000 roc_auc 0.75155 prc_auc 0.46850[0m
[92maverage training of epoch 27: loss 0.15085 acc 0.95350 roc_auc 0.97067 prc_auc 0.93016[0m
[93maverage test of epoch 27: loss 3.25336 acc 0.20000 roc_auc 0.75203 prc_auc 0.46185[0m
[92maverage training of epoch 28: loss 0.15220 acc 0.95250 roc_auc 0.97317 prc_auc 0.92157[0m
[93maverage test of epoch 28: loss 3.16973 acc 0.20000 roc_auc 0.75343 prc_auc 0.46574[0m
[92maverage training of epoch 29: loss 0.15221 acc 0.95400 roc_auc 0.97195 prc_auc 0.93115[0m
[93maverage test of epoch 29: loss 3.34697 acc 0.20000 roc_auc 0.75631 prc_auc 0.46583[0m
[92maverage training of epoch 30: loss 0.14319 acc 0.95800 roc_auc 0.97516 prc_auc 0.93553[0m
[93maverage test of epoch 30: loss 3.38648 acc 0.20000 roc_auc 0.75558 prc_auc 0.45473[0m
[92maverage training of epoch 31: loss 0.15327 acc 0.95450 roc_auc 0.97344 prc_auc 0.91328[0m
[93maverage test of epoch 31: loss 3.13662 acc 0.20000 roc_auc 0.75255 prc_auc 0.45282[0m
[92maverage training of epoch 32: loss 0.15452 acc 0.95250 roc_auc 0.97201 prc_auc 0.92848[0m
[93maverage test of epoch 32: loss 3.35012 acc 0.20000 roc_auc 0.75622 prc_auc 0.45919[0m
[92maverage training of epoch 33: loss 0.15231 acc 0.95650 roc_auc 0.97153 prc_auc 0.92433[0m
[93maverage test of epoch 33: loss 3.33424 acc 0.20000 roc_auc 0.75787 prc_auc 0.46053[0m
[92maverage training of epoch 34: loss 0.14775 acc 0.95250 roc_auc 0.97449 prc_auc 0.91324[0m
[93maverage test of epoch 34: loss 3.26560 acc 0.20000 roc_auc 0.75008 prc_auc 0.46136[0m
[92maverage training of epoch 35: loss 0.15145 acc 0.95550 roc_auc 0.96976 prc_auc 0.92916[0m
[93maverage test of epoch 35: loss 3.35050 acc 0.20000 roc_auc 0.74510 prc_auc 0.44272[0m
[92maverage training of epoch 36: loss 0.15085 acc 0.95350 roc_auc 0.97479 prc_auc 0.92449[0m
[93maverage test of epoch 36: loss 3.21059 acc 0.20000 roc_auc 0.75075 prc_auc 0.45216[0m
[92maverage training of epoch 37: loss 0.15598 acc 0.95500 roc_auc 0.96911 prc_auc 0.92784[0m
[93maverage test of epoch 37: loss 3.38510 acc 0.20000 roc_auc 0.73580 prc_auc 0.42333[0m
[92maverage training of epoch 38: loss 0.14623 acc 0.95650 roc_auc 0.97542 prc_auc 0.92580[0m
[93maverage test of epoch 38: loss 3.42786 acc 0.20000 roc_auc 0.73750 prc_auc 0.42397[0m
[92maverage training of epoch 39: loss 0.14716 acc 0.95900 roc_auc 0.97194 prc_auc 0.93260[0m
[93maverage test of epoch 39: loss 3.42256 acc 0.20000 roc_auc 0.74155 prc_auc 0.42077[0m
[92maverage training of epoch 40: loss 0.14960 acc 0.95600 roc_auc 0.97027 prc_auc 0.92913[0m
[93maverage test of epoch 40: loss 3.50503 acc 0.20000 roc_auc 0.73745 prc_auc 0.41977[0m
[92maverage training of epoch 41: loss 0.14631 acc 0.95650 roc_auc 0.97358 prc_auc 0.93312[0m
[93maverage test of epoch 41: loss 3.74553 acc 0.20000 roc_auc 0.74580 prc_auc 0.43668[0m
[92maverage training of epoch 42: loss 0.15362 acc 0.95450 roc_auc 0.97113 prc_auc 0.92049[0m
[93maverage test of epoch 42: loss 3.43226 acc 0.20000 roc_auc 0.75615 prc_auc 0.45920[0m
[92maverage training of epoch 43: loss 0.14925 acc 0.95600 roc_auc 0.97120 prc_auc 0.92751[0m
[93maverage test of epoch 43: loss 3.54178 acc 0.20000 roc_auc 0.74930 prc_auc 0.44237[0m
[92maverage training of epoch 44: loss 0.14611 acc 0.95700 roc_auc 0.97515 prc_auc 0.92757[0m
[93maverage test of epoch 44: loss 3.51897 acc 0.20000 roc_auc 0.74180 prc_auc 0.41941[0m
[92maverage training of epoch 45: loss 0.14636 acc 0.95300 roc_auc 0.97349 prc_auc 0.92754[0m
[93maverage test of epoch 45: loss 3.57544 acc 0.20000 roc_auc 0.74652 prc_auc 0.41887[0m
[92maverage training of epoch 46: loss 0.14523 acc 0.95700 roc_auc 0.97339 prc_auc 0.92713[0m
[93maverage test of epoch 46: loss 3.53336 acc 0.20000 roc_auc 0.74310 prc_auc 0.42995[0m
[92maverage training of epoch 47: loss 0.14278 acc 0.95600 roc_auc 0.97536 prc_auc 0.93062[0m
[93maverage test of epoch 47: loss 3.48591 acc 0.20000 roc_auc 0.75880 prc_auc 0.44324[0m
[92maverage training of epoch 48: loss 0.14437 acc 0.95850 roc_auc 0.97170 prc_auc 0.93406[0m
[93maverage test of epoch 48: loss 3.61354 acc 0.20000 roc_auc 0.75885 prc_auc 0.42910[0m
[92maverage training of epoch 49: loss 0.14435 acc 0.95850 roc_auc 0.97423 prc_auc 0.93089[0m
[93maverage test of epoch 49: loss 3.44155 acc 0.20000 roc_auc 0.75643 prc_auc 0.42248[0m
Training model with dataset, testing using fold 3
k used in SortPooling is: 27
[92maverage training of epoch 0: loss 0.12275 acc 0.97800 roc_auc 0.97878 prc_auc 0.96540[0m
[93maverage test of epoch 0: loss 5.54533 acc 0.20000 roc_auc 0.53399 prc_auc 0.20510[0m
[92maverage training of epoch 1: loss 0.17989 acc 0.95650 roc_auc 0.96513 prc_auc 0.88246[0m
[93maverage test of epoch 1: loss 3.99777 acc 0.19800 roc_auc 0.62647 prc_auc 0.25770[0m
[92maverage training of epoch 2: loss 0.16317 acc 0.95600 roc_auc 0.96661 prc_auc 0.92268[0m
[93maverage test of epoch 2: loss 4.12055 acc 0.20000 roc_auc 0.63185 prc_auc 0.26436[0m
[92maverage training of epoch 3: loss 0.15787 acc 0.95250 roc_auc 0.96932 prc_auc 0.91528[0m
[93maverage test of epoch 3: loss 3.97957 acc 0.20000 roc_auc 0.63643 prc_auc 0.27137[0m
[92maverage training of epoch 4: loss 0.15588 acc 0.95450 roc_auc 0.96792 prc_auc 0.92295[0m
[93maverage test of epoch 4: loss 3.93357 acc 0.20000 roc_auc 0.64069 prc_auc 0.27935[0m
[92maverage training of epoch 5: loss 0.15583 acc 0.95750 roc_auc 0.96665 prc_auc 0.92715[0m
[93maverage test of epoch 5: loss 4.03155 acc 0.20400 roc_auc 0.64853 prc_auc 0.30047[0m
[92maverage training of epoch 6: loss 0.14396 acc 0.95950 roc_auc 0.96962 prc_auc 0.92872[0m
[93maverage test of epoch 6: loss 4.01170 acc 0.20400 roc_auc 0.65525 prc_auc 0.33665[0m
[92maverage training of epoch 7: loss 0.14975 acc 0.95700 roc_auc 0.97044 prc_auc 0.93142[0m
[93maverage test of epoch 7: loss 4.03398 acc 0.20400 roc_auc 0.65839 prc_auc 0.34537[0m
[92maverage training of epoch 8: loss 0.14570 acc 0.95900 roc_auc 0.97033 prc_auc 0.92808[0m
[93maverage test of epoch 8: loss 3.95966 acc 0.20400 roc_auc 0.65887 prc_auc 0.33976[0m
[92maverage training of epoch 9: loss 0.15784 acc 0.95850 roc_auc 0.96956 prc_auc 0.93094[0m
[93maverage test of epoch 9: loss 4.01943 acc 0.20800 roc_auc 0.65604 prc_auc 0.33676[0m
[92maverage training of epoch 10: loss 0.15041 acc 0.95750 roc_auc 0.96960 prc_auc 0.92747[0m
[93maverage test of epoch 10: loss 3.89260 acc 0.20600 roc_auc 0.65089 prc_auc 0.33631[0m
[92maverage training of epoch 11: loss 0.15114 acc 0.95700 roc_auc 0.96807 prc_auc 0.92448[0m
[93maverage test of epoch 11: loss 3.78372 acc 0.20600 roc_auc 0.65563 prc_auc 0.35171[0m
[92maverage training of epoch 12: loss 0.15369 acc 0.95700 roc_auc 0.96798 prc_auc 0.92324[0m
[93maverage test of epoch 12: loss 3.99066 acc 0.20600 roc_auc 0.65655 prc_auc 0.35449[0m
[92maverage training of epoch 13: loss 0.14874 acc 0.95600 roc_auc 0.97186 prc_auc 0.93086[0m
[93maverage test of epoch 13: loss 3.91091 acc 0.20400 roc_auc 0.65182 prc_auc 0.34401[0m
[92maverage training of epoch 14: loss 0.15238 acc 0.95750 roc_auc 0.97005 prc_auc 0.93439[0m
[93maverage test of epoch 14: loss 4.01141 acc 0.20600 roc_auc 0.65581 prc_auc 0.35291[0m
[92maverage training of epoch 15: loss 0.16332 acc 0.95750 roc_auc 0.96521 prc_auc 0.93029[0m
[93maverage test of epoch 15: loss 3.76707 acc 0.20600 roc_auc 0.65545 prc_auc 0.36375[0m
[92maverage training of epoch 16: loss 0.13935 acc 0.95900 roc_auc 0.97538 prc_auc 0.93432[0m
[93maverage test of epoch 16: loss 3.72600 acc 0.20400 roc_auc 0.65600 prc_auc 0.36420[0m
[92maverage training of epoch 17: loss 0.14840 acc 0.95600 roc_auc 0.97119 prc_auc 0.93002[0m
[93maverage test of epoch 17: loss 3.73762 acc 0.20400 roc_auc 0.65597 prc_auc 0.36910[0m
[92maverage training of epoch 18: loss 0.14481 acc 0.95850 roc_auc 0.97146 prc_auc 0.93224[0m
[93maverage test of epoch 18: loss 3.65789 acc 0.20400 roc_auc 0.66130 prc_auc 0.37565[0m
[92maverage training of epoch 19: loss 0.14527 acc 0.95500 roc_auc 0.97327 prc_auc 0.93426[0m
[93maverage test of epoch 19: loss 3.64177 acc 0.20600 roc_auc 0.66033 prc_auc 0.37458[0m
[92maverage training of epoch 20: loss 0.14489 acc 0.95650 roc_auc 0.97223 prc_auc 0.93631[0m
[93maverage test of epoch 20: loss 3.66846 acc 0.20600 roc_auc 0.66398 prc_auc 0.38248[0m
[92maverage training of epoch 21: loss 0.14925 acc 0.95900 roc_auc 0.96874 prc_auc 0.93502[0m
[93maverage test of epoch 21: loss 3.87013 acc 0.20600 roc_auc 0.66305 prc_auc 0.37939[0m
[92maverage training of epoch 22: loss 0.14818 acc 0.95700 roc_auc 0.97068 prc_auc 0.93043[0m
[93maverage test of epoch 22: loss 3.77843 acc 0.20600 roc_auc 0.65980 prc_auc 0.36840[0m
[92maverage training of epoch 23: loss 0.14606 acc 0.95650 roc_auc 0.97192 prc_auc 0.93339[0m
[93maverage test of epoch 23: loss 3.58469 acc 0.20600 roc_auc 0.66232 prc_auc 0.37716[0m
[92maverage training of epoch 24: loss 0.14454 acc 0.95750 roc_auc 0.97088 prc_auc 0.93825[0m
[93maverage test of epoch 24: loss 3.90982 acc 0.20600 roc_auc 0.66078 prc_auc 0.36749[0m
[92maverage training of epoch 25: loss 0.15198 acc 0.95750 roc_auc 0.96857 prc_auc 0.92437[0m
[93maverage test of epoch 25: loss 3.66787 acc 0.20600 roc_auc 0.65560 prc_auc 0.35807[0m
[92maverage training of epoch 26: loss 0.15976 acc 0.95650 roc_auc 0.96480 prc_auc 0.92681[0m
[93maverage test of epoch 26: loss 3.68306 acc 0.20800 roc_auc 0.65712 prc_auc 0.35928[0m
[92maverage training of epoch 27: loss 0.14520 acc 0.96100 roc_auc 0.96742 prc_auc 0.93268[0m
[93maverage test of epoch 27: loss 3.64302 acc 0.20800 roc_auc 0.65915 prc_auc 0.36849[0m
[92maverage training of epoch 28: loss 0.14655 acc 0.95850 roc_auc 0.96795 prc_auc 0.93043[0m
[93maverage test of epoch 28: loss 3.75136 acc 0.20600 roc_auc 0.65476 prc_auc 0.35795[0m
[92maverage training of epoch 29: loss 0.14388 acc 0.95700 roc_auc 0.97199 prc_auc 0.92785[0m
[93maverage test of epoch 29: loss 3.46785 acc 0.20600 roc_auc 0.66307 prc_auc 0.38144[0m
[92maverage training of epoch 30: loss 0.15058 acc 0.95550 roc_auc 0.96802 prc_auc 0.92817[0m
[93maverage test of epoch 30: loss 3.62744 acc 0.20600 roc_auc 0.66237 prc_auc 0.37866[0m
[92maverage training of epoch 31: loss 0.14938 acc 0.95750 roc_auc 0.96973 prc_auc 0.92784[0m
[93maverage test of epoch 31: loss 3.62462 acc 0.20600 roc_auc 0.66009 prc_auc 0.36826[0m
[92maverage training of epoch 32: loss 0.14935 acc 0.95750 roc_auc 0.96802 prc_auc 0.92781[0m
[93maverage test of epoch 32: loss 3.70907 acc 0.20600 roc_auc 0.66155 prc_auc 0.36835[0m
[92maverage training of epoch 33: loss 0.14644 acc 0.95800 roc_auc 0.97193 prc_auc 0.92823[0m
[93maverage test of epoch 33: loss 3.58828 acc 0.20400 roc_auc 0.66135 prc_auc 0.36826[0m
[92maverage training of epoch 34: loss 0.14954 acc 0.95350 roc_auc 0.96983 prc_auc 0.92979[0m
[93maverage test of epoch 34: loss 3.63900 acc 0.20200 roc_auc 0.66448 prc_auc 0.37523[0m
[92maverage training of epoch 35: loss 0.14272 acc 0.95750 roc_auc 0.97320 prc_auc 0.93462[0m
[93maverage test of epoch 35: loss 3.69898 acc 0.20200 roc_auc 0.66195 prc_auc 0.36937[0m
[92maverage training of epoch 36: loss 0.14974 acc 0.95800 roc_auc 0.96875 prc_auc 0.92290[0m
[93maverage test of epoch 36: loss 3.71087 acc 0.20600 roc_auc 0.66221 prc_auc 0.36868[0m
[92maverage training of epoch 37: loss 0.14977 acc 0.95550 roc_auc 0.97077 prc_auc 0.93078[0m
[93maverage test of epoch 37: loss 3.65469 acc 0.20200 roc_auc 0.66375 prc_auc 0.37075[0m
[92maverage training of epoch 38: loss 0.16015 acc 0.95550 roc_auc 0.96499 prc_auc 0.92243[0m
[93maverage test of epoch 38: loss 3.67278 acc 0.20600 roc_auc 0.66298 prc_auc 0.36048[0m
[92maverage training of epoch 39: loss 0.15752 acc 0.95350 roc_auc 0.96702 prc_auc 0.92458[0m
[93maverage test of epoch 39: loss 3.69522 acc 0.20400 roc_auc 0.66658 prc_auc 0.37164[0m
[92maverage training of epoch 40: loss 0.14940 acc 0.95900 roc_auc 0.97079 prc_auc 0.92911[0m
[93maverage test of epoch 40: loss 3.89926 acc 0.20200 roc_auc 0.66154 prc_auc 0.35964[0m
[92maverage training of epoch 41: loss 0.14794 acc 0.95700 roc_auc 0.97222 prc_auc 0.93347[0m
[93maverage test of epoch 41: loss 3.57669 acc 0.20200 roc_auc 0.66222 prc_auc 0.36502[0m
[92maverage training of epoch 42: loss 0.14318 acc 0.95750 roc_auc 0.97429 prc_auc 0.94017[0m
[93maverage test of epoch 42: loss 3.55185 acc 0.20000 roc_auc 0.66978 prc_auc 0.37894[0m
[92maverage training of epoch 43: loss 0.14172 acc 0.95350 roc_auc 0.97644 prc_auc 0.93968[0m
[93maverage test of epoch 43: loss 3.84380 acc 0.20000 roc_auc 0.66620 prc_auc 0.37663[0m
[92maverage training of epoch 44: loss 0.14471 acc 0.95750 roc_auc 0.97488 prc_auc 0.94092[0m
[93maverage test of epoch 44: loss 3.57524 acc 0.20000 roc_auc 0.66685 prc_auc 0.37069[0m
[92maverage training of epoch 45: loss 0.14907 acc 0.95450 roc_auc 0.97431 prc_auc 0.93377[0m
[93maverage test of epoch 45: loss 3.73236 acc 0.20000 roc_auc 0.66405 prc_auc 0.35943[0m
[92maverage training of epoch 46: loss 0.15232 acc 0.95400 roc_auc 0.97188 prc_auc 0.93055[0m
[93maverage test of epoch 46: loss 3.85794 acc 0.20000 roc_auc 0.66703 prc_auc 0.37267[0m
[92maverage training of epoch 47: loss 0.15258 acc 0.95400 roc_auc 0.97179 prc_auc 0.92678[0m
[93maverage test of epoch 47: loss 3.66492 acc 0.20000 roc_auc 0.66603 prc_auc 0.37045[0m
[92maverage training of epoch 48: loss 0.15289 acc 0.95500 roc_auc 0.97073 prc_auc 0.92561[0m
[93maverage test of epoch 48: loss 3.69540 acc 0.20000 roc_auc 0.66889 prc_auc 0.35921[0m
[92maverage training of epoch 49: loss 0.15359 acc 0.95600 roc_auc 0.97087 prc_auc 0.92944[0m
[93maverage test of epoch 49: loss 3.51845 acc 0.20000 roc_auc 0.66696 prc_auc 0.36738[0m
Training model with dataset, testing using fold 4
k used in SortPooling is: 27
[92maverage training of epoch 0: loss 0.14583 acc 0.96500 roc_auc 0.97516 prc_auc 0.95633[0m
[93maverage test of epoch 0: loss 4.73093 acc 0.20000 roc_auc 0.47895 prc_auc 0.18228[0m
[92maverage training of epoch 1: loss 0.19082 acc 0.94500 roc_auc 0.96753 prc_auc 0.89668[0m
[93maverage test of epoch 1: loss 4.70892 acc 0.20000 roc_auc 0.48855 prc_auc 0.18588[0m
[92maverage training of epoch 2: loss 0.18169 acc 0.95100 roc_auc 0.96538 prc_auc 0.89856[0m
[93maverage test of epoch 2: loss 4.33200 acc 0.20000 roc_auc 0.51834 prc_auc 0.19434[0m
[92maverage training of epoch 3: loss 0.16844 acc 0.95100 roc_auc 0.96679 prc_auc 0.89445[0m
[93maverage test of epoch 3: loss 3.56520 acc 0.20000 roc_auc 0.52131 prc_auc 0.19676[0m
[92maverage training of epoch 4: loss 0.17297 acc 0.94600 roc_auc 0.96553 prc_auc 0.90570[0m
[93maverage test of epoch 4: loss 3.59661 acc 0.20000 roc_auc 0.53065 prc_auc 0.20196[0m
[92maverage training of epoch 5: loss 0.17158 acc 0.94750 roc_auc 0.96488 prc_auc 0.89957[0m
[93maverage test of epoch 5: loss 3.54493 acc 0.20000 roc_auc 0.57774 prc_auc 0.22867[0m
[92maverage training of epoch 6: loss 0.16361 acc 0.94950 roc_auc 0.96780 prc_auc 0.90763[0m
[93maverage test of epoch 6: loss 3.57768 acc 0.20000 roc_auc 0.62343 prc_auc 0.26319[0m
[92maverage training of epoch 7: loss 0.16365 acc 0.95200 roc_auc 0.96566 prc_auc 0.91234[0m
[93maverage test of epoch 7: loss 3.53162 acc 0.20000 roc_auc 0.62793 prc_auc 0.26281[0m
[92maverage training of epoch 8: loss 0.16529 acc 0.95300 roc_auc 0.96389 prc_auc 0.91195[0m
[93maverage test of epoch 8: loss 3.71278 acc 0.20000 roc_auc 0.66011 prc_auc 0.32904[0m
[92maverage training of epoch 9: loss 0.15148 acc 0.95600 roc_auc 0.96966 prc_auc 0.91970[0m
[93maverage test of epoch 9: loss 3.69235 acc 0.20000 roc_auc 0.68020 prc_auc 0.42888[0m
[92maverage training of epoch 10: loss 0.15303 acc 0.95500 roc_auc 0.97043 prc_auc 0.90325[0m
[93maverage test of epoch 10: loss 3.47182 acc 0.20000 roc_auc 0.67800 prc_auc 0.42033[0m
[92maverage training of epoch 11: loss 0.16566 acc 0.95550 roc_auc 0.96451 prc_auc 0.91089[0m
[93maverage test of epoch 11: loss 3.55995 acc 0.20000 roc_auc 0.66707 prc_auc 0.39715[0m
[92maverage training of epoch 12: loss 0.15456 acc 0.95550 roc_auc 0.96777 prc_auc 0.92341[0m
[93maverage test of epoch 12: loss 3.78880 acc 0.20000 roc_auc 0.67497 prc_auc 0.41130[0m
[92maverage training of epoch 13: loss 0.15682 acc 0.95550 roc_auc 0.96803 prc_auc 0.90070[0m
[93maverage test of epoch 13: loss 3.37158 acc 0.20000 roc_auc 0.66370 prc_auc 0.39687[0m
[92maverage training of epoch 14: loss 0.15834 acc 0.95550 roc_auc 0.96990 prc_auc 0.91093[0m
[93maverage test of epoch 14: loss 3.47591 acc 0.20000 roc_auc 0.67960 prc_auc 0.39362[0m
[92maverage training of epoch 15: loss 0.14782 acc 0.95400 roc_auc 0.97267 prc_auc 0.92300[0m
[93maverage test of epoch 15: loss 3.45142 acc 0.20000 roc_auc 0.67960 prc_auc 0.41177[0m
[92maverage training of epoch 16: loss 0.15439 acc 0.95750 roc_auc 0.96892 prc_auc 0.91187[0m
[93maverage test of epoch 16: loss 3.49125 acc 0.20000 roc_auc 0.66736 prc_auc 0.39293[0m
[92maverage training of epoch 17: loss 0.14647 acc 0.95800 roc_auc 0.97328 prc_auc 0.91620[0m
[93maverage test of epoch 17: loss 3.45139 acc 0.20000 roc_auc 0.67175 prc_auc 0.40836[0m
[92maverage training of epoch 18: loss 0.15007 acc 0.95750 roc_auc 0.97068 prc_auc 0.92531[0m
[93maverage test of epoch 18: loss 3.43062 acc 0.20000 roc_auc 0.67727 prc_auc 0.38086[0m
[92maverage training of epoch 19: loss 0.15011 acc 0.95550 roc_auc 0.97054 prc_auc 0.91785[0m
[93maverage test of epoch 19: loss 3.33905 acc 0.20000 roc_auc 0.67015 prc_auc 0.40498[0m
[92maverage training of epoch 20: loss 0.14890 acc 0.95650 roc_auc 0.97154 prc_auc 0.92620[0m
[93maverage test of epoch 20: loss 3.88409 acc 0.20000 roc_auc 0.66206 prc_auc 0.38234[0m
[92maverage training of epoch 21: loss 0.15153 acc 0.95750 roc_auc 0.96975 prc_auc 0.90754[0m
[93maverage test of epoch 21: loss 3.36899 acc 0.20000 roc_auc 0.66465 prc_auc 0.37954[0m
[92maverage training of epoch 22: loss 0.14860 acc 0.95750 roc_auc 0.97205 prc_auc 0.91485[0m
[93maverage test of epoch 22: loss 3.29556 acc 0.20000 roc_auc 0.66915 prc_auc 0.37884[0m
[92maverage training of epoch 23: loss 0.15708 acc 0.95750 roc_auc 0.96745 prc_auc 0.91763[0m
[93maverage test of epoch 23: loss 3.24024 acc 0.20000 roc_auc 0.67258 prc_auc 0.35863[0m
[92maverage training of epoch 24: loss 0.14904 acc 0.95600 roc_auc 0.97224 prc_auc 0.92141[0m
[93maverage test of epoch 24: loss 3.25067 acc 0.20000 roc_auc 0.67015 prc_auc 0.35470[0m
[92maverage training of epoch 25: loss 0.15392 acc 0.95550 roc_auc 0.97118 prc_auc 0.91261[0m
[93maverage test of epoch 25: loss 3.26254 acc 0.20000 roc_auc 0.67770 prc_auc 0.38391[0m
[92maverage training of epoch 26: loss 0.15371 acc 0.95600 roc_auc 0.96952 prc_auc 0.91929[0m
[93maverage test of epoch 26: loss 3.37661 acc 0.20000 roc_auc 0.67450 prc_auc 0.35604[0m
[92maverage training of epoch 27: loss 0.15719 acc 0.95650 roc_auc 0.96782 prc_auc 0.91993[0m
[93maverage test of epoch 27: loss 3.36919 acc 0.20000 roc_auc 0.67952 prc_auc 0.37632[0m
[92maverage training of epoch 28: loss 0.15360 acc 0.95550 roc_auc 0.96815 prc_auc 0.91254[0m
[93maverage test of epoch 28: loss 3.33266 acc 0.20000 roc_auc 0.66508 prc_auc 0.35109[0m
[92maverage training of epoch 29: loss 0.15045 acc 0.95650 roc_auc 0.97150 prc_auc 0.91209[0m
[93maverage test of epoch 29: loss 3.35679 acc 0.20000 roc_auc 0.66320 prc_auc 0.36065[0m
[92maverage training of epoch 30: loss 0.15381 acc 0.95650 roc_auc 0.96800 prc_auc 0.91772[0m
[93maverage test of epoch 30: loss 3.36803 acc 0.20000 roc_auc 0.67061 prc_auc 0.38092[0m
[92maverage training of epoch 31: loss 0.15119 acc 0.95600 roc_auc 0.96937 prc_auc 0.91515[0m
[93maverage test of epoch 31: loss 3.30256 acc 0.20000 roc_auc 0.67247 prc_auc 0.36549[0m
[92maverage training of epoch 32: loss 0.15474 acc 0.95600 roc_auc 0.96814 prc_auc 0.91355[0m
[93maverage test of epoch 32: loss 3.26587 acc 0.20000 roc_auc 0.67527 prc_auc 0.38191[0m
[92maverage training of epoch 33: loss 0.16574 acc 0.95650 roc_auc 0.96633 prc_auc 0.91719[0m
[93maverage test of epoch 33: loss 3.15685 acc 0.20000 roc_auc 0.64296 prc_auc 0.29823[0m
[92maverage training of epoch 34: loss 0.15339 acc 0.95350 roc_auc 0.97100 prc_auc 0.91195[0m
[93maverage test of epoch 34: loss 3.33522 acc 0.20000 roc_auc 0.56373 prc_auc 0.22208[0m
[92maverage training of epoch 35: loss 0.16092 acc 0.95200 roc_auc 0.96802 prc_auc 0.91292[0m
[93maverage test of epoch 35: loss 3.27044 acc 0.20000 roc_auc 0.65352 prc_auc 0.37976[0m
[92maverage training of epoch 36: loss 0.15642 acc 0.95300 roc_auc 0.97263 prc_auc 0.90181[0m
[93maverage test of epoch 36: loss 3.16788 acc 0.20000 roc_auc 0.63435 prc_auc 0.29123[0m
[92maverage training of epoch 37: loss 0.15120 acc 0.95650 roc_auc 0.97360 prc_auc 0.92064[0m
[93maverage test of epoch 37: loss 3.02271 acc 0.20000 roc_auc 0.52448 prc_auc 0.20178[0m
[92maverage training of epoch 38: loss 0.15171 acc 0.95250 roc_auc 0.97310 prc_auc 0.92386[0m
[93maverage test of epoch 38: loss 3.14433 acc 0.20000 roc_auc 0.60475 prc_auc 0.26357[0m
[92maverage training of epoch 39: loss 0.15115 acc 0.95600 roc_auc 0.97101 prc_auc 0.91605[0m
[93maverage test of epoch 39: loss 3.10554 acc 0.20000 roc_auc 0.55199 prc_auc 0.21807[0m
[92maverage training of epoch 40: loss 0.14799 acc 0.95750 roc_auc 0.97276 prc_auc 0.91977[0m
[93maverage test of epoch 40: loss 3.15361 acc 0.20000 roc_auc 0.63224 prc_auc 0.29984[0m
[92maverage training of epoch 41: loss 0.15122 acc 0.96000 roc_auc 0.96957 prc_auc 0.90925[0m
[93maverage test of epoch 41: loss 3.18652 acc 0.20000 roc_auc 0.62341 prc_auc 0.29142[0m
[92maverage training of epoch 42: loss 0.14555 acc 0.95950 roc_auc 0.97334 prc_auc 0.91834[0m
[93maverage test of epoch 42: loss 3.23488 acc 0.20000 roc_auc 0.63407 prc_auc 0.31683[0m
[92maverage training of epoch 43: loss 0.14340 acc 0.95750 roc_auc 0.97410 prc_auc 0.91974[0m
[93maverage test of epoch 43: loss 3.30021 acc 0.20000 roc_auc 0.62145 prc_auc 0.28939[0m
[92maverage training of epoch 44: loss 0.15352 acc 0.95550 roc_auc 0.97083 prc_auc 0.90715[0m
[93maverage test of epoch 44: loss 3.22273 acc 0.20000 roc_auc 0.55990 prc_auc 0.21717[0m
[92maverage training of epoch 45: loss 0.14783 acc 0.95750 roc_auc 0.97396 prc_auc 0.91202[0m
[93maverage test of epoch 45: loss 3.23230 acc 0.20000 roc_auc 0.55390 prc_auc 0.21233[0m
[92maverage training of epoch 46: loss 0.14986 acc 0.95550 roc_auc 0.97111 prc_auc 0.92073[0m
[93maverage test of epoch 46: loss 3.31276 acc 0.20200 roc_auc 0.58090 prc_auc 0.24928[0m
[92maverage training of epoch 47: loss 0.14826 acc 0.95850 roc_auc 0.97056 prc_auc 0.91904[0m
[93maverage test of epoch 47: loss 3.35308 acc 0.20000 roc_auc 0.57635 prc_auc 0.23613[0m
[92maverage training of epoch 48: loss 0.14791 acc 0.95650 roc_auc 0.97235 prc_auc 0.91841[0m
[93maverage test of epoch 48: loss 3.40721 acc 0.20000 roc_auc 0.56272 prc_auc 0.21496[0m
[92maverage training of epoch 49: loss 0.15070 acc 0.95750 roc_auc 0.97164 prc_auc 0.91993[0m
[93maverage test of epoch 49: loss 3.36007 acc 0.20200 roc_auc 0.54630 prc_auc 0.20922[0m
Run statistics: 
==== Configuration Settings ====
== Run Settings ==
Model: DGCNN, Dataset: NCI-H23
num_epochs: 50
learning_rate: 0.0001
seed: 1800
k_fold: 5
model: DGCNN
dataset: NCI-H23

== Model Settings and results ==
convolution_layers_size: 32-32-32-1
sortpooling_k: 0.6
n_hidden: 128
convolution_dropout: 0.5
pred_dropout: 0.5
FP_len: 0

Accuracy (avg): 0.2004 ROC_AUC (avg): 0.64199 PRC_AUC (avg): 0.32661 

Average forward propagation time taken(ms): 2.9748405263507713
Average backward propagation time taken(ms): 2.5445171644957743

